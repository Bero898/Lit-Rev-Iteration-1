key,title,authors,year,journal,abstract,doi
10.1007/s00165-019-00491-2,Estimating costs of multi-component enterprise applications,"Brogi, Antonio; Corradini, Andrea; Soldani, Jacopo",2019,Form. Asp. Comput.,"Estimating the cost of a multi-component application (e.g., its resource or energy consumption) is fundamental in nowadays enterprise IT, especially if we consider that current pricing models are mainly pay per-use. While this is still manageable on small applications, it is really hard to manually estimate the cost of large-scale enterprise applications involving hundreds of interdependent application components. In this article, we formalise the problem of estimating costs of multi-component applications, by representing the structure of an application as a typed directed graph, and by allowing to associate different types of costs with different application components. We show that costs can be fully customised, and that associating different costs with the same application leads to different cost estimation problems defined on that application.We then present an approach for solving cost estimation problems on multi-component applications, which is based on terminating and confluent graph transformations. We also present a prototype implemenation of our approach, which we use to run a case study based on a third-party application.",10.1007/s00165-019-00491-2
10.1109/TCBB.2019.2916810,End-to-End Security for Local and Remote Human Genetic Data Applications at the EGA,"Senf, Alexander",2019,IEEE/ACM Trans. Comput. Biol. Bioinformatics,"Sensitive genomic data should remain secure - whether on disk for storage, or analysis, or in transport. However, secure storage, delivery, and usage of genomic data is complicated by the size of files and diversity of workflows. This paper presents solutions developed by GA4GH and EGA to use custom-ized encryption, encrypted file formats, toolchain integration, and intelligent APIs to help solve this problem.",10.1109/TCBB.2019.2916810
10.1109/TNET.2020.2973800,Vehicular-OBUs-As-On-Demand-Fogs: Resource and Context Aware Deployment of Containerized Micro-Services,"Sami, Hani; Mourad, Azzam; El-Hajj, Wassim",2020,IEEE/ACM Trans. Netw.,"Observing the headway in vehicular industry, new applications are developed demanding more resources. For instance, real-time vehicular applications require fast processing of the vast amount of generated data by vehicles in order to maintain service availability and reachability while driving. Fog devices are capable of bringing cloud intelligence near the edge, making them a suitable candidate to process vehicular requests. However, their location, processing power, and technology used to host and update services affect their availability and performance while considering the mobility patterns of vehicles. In this paper, we overcome the aforementioned limitations by taking advantage of the evolvement of On-Board Units, Kubeadm Clustering, Docker Containerization, and micro-services technologies. In this context, we propose an efficient resource and context aware approach for deploying containerized micro-services on on-demand fogs called Vehicular-OBUs-As-On-Demand-Fogs. Our proposed scheme embeds (1) a Kubeadm based approach for clustering OBUs and enabling on-demand micro-services deployment with the least costs and time using Docker containerization technology, (2) a hybrid multi-layered networking architecture to maintain reachability between the requesting user and available vehicular fog cluster, and (3) a vehicular multi-objective container placement model for producing efficient vehicles selection and services distribution. An Evolutionary Memetic Algorithm is elaborated to solve our vehicular container placement problem. Experiments and simulations demonstrate the relevance and efficiency of our approach compared to other recent techniques in the literature.",10.1109/TNET.2020.2973800
10.1109/TNET.2020.2979361,Edge Federation: Towards an Integrated Service Provisioning Model,"Cao, Xiaofeng; Tang, Guoming; Guo, Deke; Li, Yan; Zhang, Weiming",2020,IEEE/ACM Trans. Netw.,"Edge computing is a promising computing paradigm by pushing the cloud service to the network edge. To this end, edge infrastructure providers (EIPs) need to bring computation and storage resources to the network edge and allow edge service providers (ESPs) to provision latency-critical services for end users. Currently, EIPs prefer to establish a series of private edge-computing environments to serve specific requirements of users. This kind of resource provisioning mechanism severely limits the development and spread of edge computing in serving diverse user requirements. In this paper, we propose an integrated resource provisioning model, named edge federation, to seamlessly realize the resource cooperation and service provisioning across standalone edge computing providers and clouds. To efficiently schedule and utilize the resources across multiple EIPs, we systematically characterize the provisioning process as a large-scale linear programming (LP) problem and transform it into an easily solved form. Accordingly, we design a dynamic algorithm to tackle the varying service demands from users. We conduct extensive experiments over the base station networks in Toronto. Compared with the fixed contract model and multihoming model, edge federation can reduce the overall costs of EIPs by 23.3\% to 24.5\%, and 15.5\% to 16.3\%, respectively.",10.1109/TNET.2020.2979361
10.1109/TNET.2020.3019098,A Coverage-Aware Resource Provisioning Method for Network Slicing,"Luu, Quang-Trung; Kerboeuf, Sylvaine; Mouradian, Alexandre; Kieffer, Michel",2020,IEEE/ACM Trans. Netw.,"With network slicing in 5G networks, Mobile Network Operators can create various slices for Service Providers (SPs) to accommodate customized services. Usually, the various Service Function Chains (SFCs) belonging to a slice are deployed on a best-effort basis. Nothing ensures that the Infrastructure Provider (InP) will be able to allocate enough resources to cope with the increasing demands of some SP. Moreover, in many situations, slices have to be deployed over some geographical area: coverage as well as minimum per-user rate constraints have then to be taken into account. This paper takes the InP perspective and proposes a slice resource &lt;italic&gt;provisioning&lt;/italic&gt; approach to cope with multiple slice demands in terms of computing, storage, coverage, and rate constraints. The resource requirements of the various SFCs within a slice are aggregated within a graph of Slice Resource Demands (SRD). Infrastructure nodes and links have then to be provisioned so as to satisfy all SRDs. This problem leads to a Mixed Integer Linear Programming formulation. A two-step approach is considered, with several variants, depending on whether the constraints of each slice to be provisioned are taken into account sequentially or jointly. Once provisioning has been performed, any slice deployment strategy may be considered on the reduced-size infrastructure graph on which resources have been provisioned. Simulation results demonstrate the effectiveness of the proposed approach compared to a more classical direct slice embedding approach.",10.1109/TNET.2020.3019098
10.1109/TNET.2021.3114448,Fast Configuration Change Impact Analysis for Network Overlay Data Center Networks,"You, Lizhao; Zhang, Jiahua; Jin, Yili; Tang, Hao; Li, Xiao",2021,IEEE/ACM Trans. Netw.,"This paper presents the first network configuration verifier that provides fast all-pair reachability analysis of incremental configuration changes for network overlay data center networks (DCNs). Network overlay DCNs leverage distributed routing protocol on edge leaf switches to disseminate overlay routes and establish overlay tunnels. In addition, network overlay DCNs use access control lists, microsegmentation policy, policy-based routing and firewall policy to control east-west and north-south traffic. Although some incremental verification approaches have been proposed, they either do not support certain forwarding features of the network, or are not efficient. Our configuration verifier addresses these issues through the following components: 1) a port predicate based forwarding model that is general to support all features; 2) fine-grained association technique to index possibly affected reachable pairs by changed interfaces in the original network; and 3) required waypoint path computation that finds all reachable pairs related to changed interfaces in the new network. Based on these components, our verifier presents two incremental verification algorithms that are specially designed for different service update cases. Experiment results show that our incremental verification algorithms are accurate and fast. For all-pair reachability, our verifier performs change-impact analysis within 15s for networks with 200 leafs (4000 subnets and 16 million pairs), outperforming existing approaches by up to 10x.",10.1109/TNET.2021.3114448
10.1109/TNET.2022.3190730,Southbound Message Delivery With Virtual Network Topology Awareness in Clouds,"Zhao, Gongming; Luo, Luyao; Xu, Hongli; Chung, Chun-Jen; Xie, Liguang",2022,IEEE/ACM Trans. Netw.,"Southbound message delivery from the control plane to the data plane is one of the essential issues in multi-tenant clouds. A natural method of southbound message delivery is that the control plane directly communicates with compute nodes in the data plane. However, due to the large number of compute nodes, this method may result in massive control overhead. The Message Queue (MQ) model can solve this challenge by aggregating and distributing messages to queues. Existing MQ-based solutions often perform message aggregation based on the physical network topology, which do not align with the fundamental requirements of southbound message delivery, leading to high message redundancy on compute nodes. To address this issue, we design and implement VITA, the first-of-its-kind work on virtual network topology-aware southbound message delivery. However, it is intractable to optimally deliver southbound messages according to the virtual attributes of messages. Thus, we design two algorithms, submodular-based approximation algorithm and simulated annealing-based algorithm, to solve different scenarios of the problem. Both experiment and simulation results show that VITA can reduce the total traffic amount of redundant messages by 45%-75% and reduce the control overhead by 33%-80% compared with state-of-the-art solutions.",10.1109/TNET.2022.3190730
10.1109/TNET.2022.3206781,Secure Inter-Container Communications Using XDP/eBPF,"Nam, Jaehyun; Lee, Seungsoo; Porras, Phillip; Yegneswaran, Vinod; Shin, Seungwon",2022,IEEE/ACM Trans. Netw.,"While the use of containerization technologies for virtual application deployment has grown at an astonishing rate, the question of the robustness of container networking has not been well scrutinized from a security perspective, even though inter-container networking is indispensable for microservices. Thus, this paper first analyzes container networks from a security perspective, discussing the implications based on their architectural limitations. Then, it presents Bastion+, a secure inter-container communication bridge. Bastion+ introduces (&lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$i$ &lt;/tex-math&gt;&lt;/inline-formula&gt;) a network security enforcement stack that provides fine-grained control per container application and securely isolates inter- container traffic in a point-to-point manner. Bastion+ also supports (&lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$ii$ &lt;/tex-math&gt;&lt;/inline-formula&gt;) selective security function chaining, enabling various security functions to be chained between containers for further security inspections (e.g., deep packet inspection) according to the container’s network context. Bastion+ incorporates (&lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$iii$ &lt;/tex-math&gt;&lt;/inline-formula&gt;) a security policy assistant that helps an administrator discover inter-container networking dependencies correctly. Our evaluation demonstrates how Bastion+ can effectively mitigate several adversarial attacks in container networks while improving the overall performance up to 25.4% within single-host containers and 17.7% for cross-host container communications.",10.1109/TNET.2022.3206781
10.1109/TNET.2022.3217280,Efficient Revenue-Based MEC Server Deployment and Management in Mobile Edge-Cloud Computing,"Zhang, Yongmin; Wang, Wei; Ren, Ju; Huang, Jinge; He, Shibo; Zhang, Yaoxue",2022,IEEE/ACM Trans. Netw.,"With the explosive growth of mobile applications, the development of mobile edge computing (MEC) has been greatly promoted since it can ably improve the quality of service for mobile applications by providing low latency and high-quality computation services. Most existing works focus on improving the efficiency of MEC with an assumption that the MEC servers have already been deployed. However, without appropriate deployment of MEC servers, the profitability of the MEC system can be significantly restrained, which hinders the rapid promotion of the MEC. To address this issue, we formulate an MEC server deployment problem for the MEC operator as a revenue maximization problem. Firstly, we model and analyze the various factors that affect the revenue. Secondly, we formulate a revenue maximization problem, which is NP-hard, but it is proved to be convex with respect to the total available computation units. Based on this feature, we propose a three-layer optimization algorithm, named EDM, in which the location, the deployed computation units, and the wholesaled computation resources are determined gradually, to maximize the total revenue. Experimental results demonstrate that the proposed EDM algorithm has significant advantages on revenue improvement compared to competitive benchmarks.",10.1109/TNET.2022.3217280
10.1109/TNET.2022.3220427,Scalable and Secure Virtualization of HSM With ScaleTrust,"Han, Juhyeng; Yun, Insu; Kim, Seongmin; Kim, Taesoo; Son, Sooel; Han, Dongsu",2022,IEEE/ACM Trans. Netw.,"Hardware security modules (HSMs) have been utilized as a trustworthy foundation for cloud services. Unfortunately, existing systems using HSMs fail to meet multi-tenant scalability arising from the emerging trends such as microservices, which utilize frequent cryptographic operations. As an alternative, cloud vendors provide HSMs as a service. However, such cloud-managed HSM usage models raise security concerns due to their untrusted and shared operating environment. We propose ScaleTrust, a scalable and secure system for key management. ScaleTrust allows us to scale the number of virtual HSM partitions, each of which is isolated with respect to each other and is robust against cloud insider attacks, while preserving physical isolation of the root of trust. To enable this, ScaleTrust uses Intel SGX and multiple HSM features, such as restricting key usage by controlling key attributes of in-HSM keys and establishing a secure channel using only HSM commands. Finally, we apply ScaleTrust to four real-world systems: Keyless SSL for TLS private key offloading, JSON Web Token authentication for microservices, key provisioning, and encryption in database systems. Our evaluation shows that ScaleTrust achieves multi-tenancy in a scalable way by providing multiple virtual HSMs with legacy HSM devices that are designed to support a single tenant. ScaleTrust provides security against insider threats while incurring 11.9% and 39.0% of end-to-end throughput and latency overhead for Keyless SSL compared to stand-alone HSMs.",10.1109/TNET.2022.3220427
10.1109/TNET.2022.3224369,Octopus: Exploiting the Edge Intelligence for Accessible 5G Mobile Performance Enhancement,"An, Congkai; Zhou, Anfu; Pei, Jialiang; Liu, Xi; Xu, Dongzhu; Liu, Liang; Ma, Huadong",2022,IEEE/ACM Trans. Netw.,"While 5G has rolled out since 2019 and exhibited versatile advantages, its performance under high/extreme mobility scenes (e.g., driving, high-speed railway or HSR) remains mysterious. In this work, we carry out a large-scale field-trial campaign, taking &gt;13,000 Km round-trips on HSR moving at 250–350 Km/h, with operational 5G cellular coverage along the railway. Our empirical study reveals that coupling interaction among high mobility, 5G handover characteristics, and applications’ sluggish reaction to handover, results in catastrophic damage to user experience: low TCP bandwidth utilization of 26.6% and glitchy 4K VoD streaming. To solve the problem, we propose an edge-assisted mobility management framework called Octopus. Different from previous works, Octopus aims at a standard-compatible and easy-to-deploy solution, thus we take a new design paradigm of exploiting the edge intelligence on multi-access edge computing (MEC). We realize Octopus as a universal MEC service ready for benefiting any third-party mobile applications. We prototype, deploy, and evaluate Octopus in operational 5G, which demonstrates the significant performance gain across the full-range mobile scenarios, e.g., HSR, driving, and walking.",10.1109/TNET.2022.3224369
10.1109/TNET.2022.3224610,Buffer-Based High-Coverage and Low-Overhead Request Event Monitoring in the Cloud,"Gao, Kaihui; Sun, Chen; Wang, Shuai; Li, Dan; Zhou, Yu; Liu, Hongqiang Harry; Zhu, Lingjun; Zhang, Ming; Deng, Xiang; Zhou, Cheng; Lu, Lu",2023,IEEE/ACM Trans. Netw.,"Request latency directly affects the performance of modern cloud applications. Due to various causes in hosts and networks, requests can suffer from request latency anomalies (RLAs), which may violate the Service-Level Agreement. However, existing performance monitoring tools have incomplete coverage and inconsistent semantics for monitoring requests and cannot accurately diagnose RLAs. This paper presents &lt;italic&gt;BufScope&lt;/italic&gt;, a high-coverage and low-overhead request event monitoring system, which monitors &lt;italic&gt;buffers&lt;/italic&gt; to capture most RLA-related abnormal events with consistent request-level semantics in the end-to-end datapath of request. First, &lt;italic&gt;BufScope&lt;/italic&gt; models the datapath of request as a buffer chain and defines events based on three properties of buffers, so as to &lt;italic&gt;end-to-end monitor&lt;/italic&gt; the root causes of RLA. Then, to achieve &lt;italic&gt;consistent semantics&lt;/italic&gt; for captured events, &lt;italic&gt;BufScope&lt;/italic&gt; designs a request-level semantics injection mechanism to make events captured in networks have the victim requests’ ID. Finally, &lt;italic&gt;BufScope&lt;/italic&gt; offloads the semantics operations and event collection in software to SmartNICs for &lt;italic&gt;low CPU overhead&lt;/italic&gt;. We have implemented &lt;italic&gt;BufScope&lt;/italic&gt; on commodity SmartNICs and programmable switches. Evaluation results show that &lt;italic&gt;BufScope&lt;/italic&gt; can diagnose 98% RLAs with &lt; 0.08% network bandwidth overhead and 0.6% application throughput decline.",10.1109/TNET.2022.3224610
10.1109/TNET.2023.3319434,Achieving Cost Optimization for Tenant Task Placement in Geo-Distributed Clouds,"Luo, Luyao; Zhao, Gongming; Xu, Hongli; Yu, Zhuolong; Xie, Liguang",2023,IEEE/ACM Trans. Netw.,"Cloud infrastructure has gradually displayed a tendency of geographical distribution in order to provide anywhere, anytime connectivity to tenants all over the world. The tenant task placement in geo-distributed clouds comes with three critical and coupled factors: regional diversity in electricity prices, access delay for tenants, and traffic demand among tasks. However, existing works disregard either the regional difference in electricity prices or the tenant requirements in geo-distributed clouds, resulting in increased operating costs or low user QoS. To bridge the gap, we design a cost optimization framework for tenant task placement in geo-distributed clouds, called TanGo. However, it is non-trivial to achieve an optimization framework while meeting all the tenant requirements. To this end, we first formulate the electricity cost minimization for task placement problem as a constrained mixed-integer non-linear programming problem. We then propose a near-optimal algorithm with a tight approximation ratio &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$(1-1/e)$ &lt;/tex-math&gt;&lt;/inline-formula&gt; using an effective submodular-based method. Results of in-depth simulations based on real-world datasets show the effectiveness of our algorithm as well as the overall 10%-30% reduction in electricity expenses compared to commonly-adopted alternatives.",10.1109/TNET.2023.3319434
10.1109/TNET.2023.3330255,Multi-User Layer-Aware Online Container Migration in Edge-Assisted Vehicular Networks,"Tang, Zhiqing; Mou, Fangyi; Lou, Jiong; Jia, Weijia; Wu, Yuan; Zhao, Wei",2023,IEEE/ACM Trans. Netw.,"In edge-assisted vehicular networks, containers are very suitable for deploying applications and providing services due to their lightweight and rapid deployment. To provide high-quality services, many existing studies show that the containers need to be migrated to follow the vehicles’ trajectory. However, it has been conspicuously neglected by existing work that making full use of the complex layer-sharing information of containers among multiple users can significantly reduce migration latency. In this paper, we propose a novel online container migration algorithm to reduce the overall task latency. Specifically: 1) we model the multi-user layer-aware online container migration problem in edge-assisted vehicular networks, comprehensively considering the initialization latency, computation latency, and migration latency. 2) A feature extraction method based on attention and long short-term memory is proposed to fully extract the multi-user layer-sharing information. Then, a policy gradient-based reinforcement learning algorithm is proposed to make the online migration decisions. 3) The experiments are conducted with real-world data traces. Compared with the baselines, our algorithms effectively reduce the total latency by 8% to 30% on average.",10.1109/TNET.2023.3330255
10.1109/TNET.2024.3366561,"SPRIGHT: High-Performance eBPF-Based Event-Driven, Shared-Memory Processing for Serverless Computing","Qi, Shixiong; Monis, Leslie; Zeng, Ziteng; Wang, Ian-Chin; Ramakrishnan, K. K.",2024,IEEE/ACM Trans. Netw.,"Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. ‘Cold-start’ latency is another deterrent. We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF’s socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$10times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; less CPU usage under realistic workloads. Additionally, eBPF benefits SPRIGHT, by replacing heavyweight serverless components, allowing us to keep functions ‘warm’ with negligible penalty. Our preliminary experimental results show that SPRIGHT achieves an order of magnitude improvement in throughput and latency compared to Knative, while substantially reducing CPU usage, and obviates the need for ‘cold-start’.",10.1109/TNET.2024.3366561
10.1109/TNET.2024.3393427,Graph Neural Network-Based SLO-Aware Proactive Resource Autoscaling Framework for Microservices,"Park, Jinwoo; Choi, Byungkwon; Lee, Chunghan; Han, Dongsu",2024,IEEE/ACM Trans. Netw.,"Microservice is an architectural style widely adopted in various latency-sensitive cloud applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing the resource utilization of microservices. However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. In this paper, we present GRAF, a graph neural network-based SLO-aware proactive resource autoscaling framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to (a) observe/estimate the impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. GRAF also handles a traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler. Moreover, we verify the scalability of GRAF on large-scale deployments, where GRAF saves 21.6% and 25.4% for CPU resources and memory resources, respectively.",10.1109/TNET.2024.3393427
10.1109/TNET.2024.3394514,Zero+: Monitoring Large-Scale Cloud-Native Infrastructure Using One-Sided RDMA,"Song, Zhuo; Wu, Jiejian; Ma, Teng; Wang, Zhe; Kong, Linghe; Wen, Zhenzao; Li, Jingxuan; Lu, Yang; Yang, Yong; Ma, Tao; Liu, Zheng; Chen, Guihai",2024,IEEE/ACM Trans. Netw.,"Cloud services have shifted from monolithic designs to microservices running on cloud-native infrastructure with monitoring systems to ensure service level agreements (SLAs). However, traditional monitoring systems no longer meet the demands of cloud-native monitoring. In Alibaba’s “double eleven” shopping festival, it is observed that the monitor occupies resources of the monitored infrastructure and even disrupts services. In this paper, we propose a novel monitoring system named Zero+ for cloud-native monitoring. Zero+ achieves zero overhead in collecting raw metrics using one-sided remote direct memory access (RDMA) and remedies network congestion by adopting a receiver-driven flow control scheme. Zero+ also features a priority queue mechanism to meet different quality of service requirements and an efficient batch processing design to relieve CPU occupation. Zero+ has been deployed and evaluated in four different clusters with heterogeneous RDMA NIC devices and architectures in Alibaba Cloud. Results show that Zero+ achieves no CPU occupation at the monitored host and supports &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1sim 10k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$0.1sim 1s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; sampling interval using a single thread for network I/O. Zero+ significantly relieves the incast issue and maintains &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$80sim 95\%$ &lt;/tex-math&gt;&lt;/inline-formula&gt; of bandwidth utilization in several clusters when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts. Zero+ also ensures services with high priority accomplish collecting metrics earlier than low priority ones by at least &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$400 mu s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts.",10.1109/TNET.2024.3394514
10.1109/TNET.2024.3400953,DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Wang, Ziliang; Zhu, Shiyi; Li, Jianguo; Jiang, Wei; Ramakrishnan, K. K.; Yan, Meng; Zhang, Xiaohong; Liu, Alex X.",2024,IEEE/ACM Trans. Netw.,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",10.1109/TNET.2024.3400953
10.1145/2735632,A UI-Centric Approach for the End-User Development of Multidevice Mashups,"Cappiello, Cinzia; Matera, Maristella; Picozzi, Matteo",2015,ACM Trans. Web,"In recent years, models, composition paradigms, and tools for mashup development have been proposed to support the integration of information sources, services and APIs available on the Web. The challenge is to provide a gate to a “programmable Web,” where end users are allowed to construct easily composite applications that merge content and functions so as to satisfy the long tail of their specific needs. The approaches proposed so far do not fully accommodate this vision. This article, therefore, proposes a mashup development framework that is oriented toward the End-User Development. Given the fundamental role of user interfaces (UIs) as a medium easily understandable by the end users, the proposed approach is characterized by UI-centric models able to support a WYSIWYG (What You See Is What You Get) specification of data integration and service orchestration. It, therefore, contributes to the definition of adequate abstractions that, by hiding the technology and implementation complexity, can be adopted by the end users in a kind of “democratic” paradigm for mashup development. This article also shows how model-to-code generative techniques translate models into application schemas, which in turn guide the dynamic instantiation of the composite applications at runtime. This is achieved through lightweight execution environments that can be deployed on the Web and on mobile devices to support the pervasive use of the created applications.",10.1145/2735632
10.1145/2850416,A Scalable Framework for Provisioning Large-Scale IoT Deployments,"V\""{o}gler, Michael; Schleicher, Johannes M.; Inzinger, Christian; Dustdar, Schahram",2016,ACM Trans. Internet Technol.,"Internet of Things (IoT) devices are usually considered external application dependencies that only provide data or process and execute simple instructions. The recent emergence of IoT devices with embedded execution environments allows practitioners to deploy and execute custom application logic directly on the device. This approach fundamentally changes the overall process of designing, developing, deploying, and managing IoT systems. However, these devices exhibit significant differences in available execution environments, processing, and storage capabilities. To accommodate this diversity, a structured approach is needed to uniformly and transparently deploy application components onto a large number of heterogeneous devices. This is especially important in the context of large-scale IoT systems, such as in the smart city domain. In this article, we present LEONORE, an infrastructure toolset that provides elastic provisioning of application components on resource-constrained and heterogeneous edge devices in large-scale IoT deployments. LEONORE supports push-based as well as pull-based deployments. To improve scalability and reduce generated network traffic between cloud and edge infrastructure, we present a distributed provisioning approach that deploys LEONORE local nodes within the deployment infrastructure close to the actual edge devices. We show that our solution is able to elastically provision large numbers of devices using a testbed based on a real-world industry scenario.",10.1145/2850416
10.1145/2998575,An Introduction to Dynamic Data Quality Challenges,"Labouseur, Alan G.; Matheus, Carolyn C.",2017,J. Data and Information Quality,,10.1145/2998575
10.1145/3012429,Scientific Workflows: Moving Across Paradigms,"Liew, Chee Sun; Atkinson, Malcolm P.; Galea, Michelle; Ang, Tan Fong; Martin, Paul; Hemert, Jano I. Van",2016,ACM Comput. Surv.,"Modern scientific collaborations have opened up the opportunity to solve complex problems that require both multidisciplinary expertise and large-scale computational experiments. These experiments typically consist of a sequence of processing steps that need to be executed on selected computing platforms. Execution poses a challenge, however, due to (1) the complexity and diversity of applications, (2) the diversity of analysis goals, (3) the heterogeneity of computing platforms, and (4) the volume and distribution of data.A common strategy to make these in silico experiments more manageable is to model them as workflows and to use a workflow management system to organize their execution. This article looks at the overall challenge posed by a new order of scientific experiments and the systems they need to be run on, and examines how this challenge can be addressed by workflows and workflow management systems. It proposes a taxonomy of workflow management system (WMS) characteristics, including aspects previously overlooked. This frames a review of prevalent WMSs used by the scientific community, elucidates their evolution to handle the challenges arising with the emergence of the “fourth paradigm,” and identifies research needed to maintain progress in this area.",10.1145/3012429
10.1145/3064527,ARIADNE: A Research Infrastructure for Archaeology,"Meghini, Carlo; Scopigno, Roberto; Richards, Julian; Wright, Holly; Geser, Guntram; Cuy, Sebastian; Fihn, Johan; Fanini, Bruno; Hollander, Hella; Niccolucci, Franco; Felicetti, Achille; Ronzino, Paola; Nurra, Federico; Papatheodorou, Christos; Gavrilis, Dimitris; Theodoridou, Maria; Doerr, Martin; Tudhope, Douglas; Binding, Ceri; Vlachidis, Andreas",2017,J. Comput. Cult. Herit.,"Research e-infrastructures, digital archives, and data services have become important pillars of scientific enterprise that in recent decades have become ever more collaborative, distributed, and data intensive. The archaeological research community has been an early adopter of digital tools for data acquisition, organization, analysis, and presentation of research results of individual projects. However, the provision of e-infrastructure and services for data sharing, discovery, access, and (re)use have lagged behind. This situation is being addressed by ARIADNE, the Advanced Research Infrastructure for Archaeological Dataset Networking in Europe. This EU-funded network has developed an e-infrastructure that enables data providers to register and provide access to their resources (datasets, collections) through the ARIADNE data portal, facilitating discovery, access, and other services across the integrated resources. This article describes the current landscape of data repositories and services for archaeologists in Europe, and the issues that make interoperability between them difficult to realize. The results of the ARIADNE surveys on users’ expectations and requirements are also presented. The main section of the article describes the architecture of the e-infrastructure, core services (data registration, discovery, and access), and various other extant or experimental services. The ongoing evaluation of the data integration and services is also discussed. Finally, the article summarizes lessons learned and outlines the prospects for the wider engagement of the archaeological research community in the sharing of data through ARIADNE.",10.1145/3064527
10.1145/3104028,Architectural Principles for Cloud Software,"Pahl, Claus; Jamshidi, Pooyan; Zimmermann, Olaf",2018,ACM Trans. Internet Technol.,"A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.",10.1145/3104028
10.1145/3110280,Whip: higher-order contracts for modern services,"Waye, Lucas; Chong, Stephen; Dimoulas, Christos",2017,Proc. ACM Program. Lang.,"Modern service-oriented applications forgo semantically rich protocols and middleware when composing services. Instead, they embrace the loosely-coupled development and deployment of services that communicate via simple network protocols. Even though these applications do expose interfaces that are higher-order in spirit, the simplicity of the network protocols forces them to rely on brittle low-level encodings. To bridge the apparent semantic gap, programmers introduce ad-hoc and error-prone defensive code. Inspired by Design by Contract, we choose a different route to bridge this gap. We introduce Whip, a contract system for modern services. Whip (i) provides programmers with a higher-order contract language tailored to the needs of modern services; and (ii) monitors services at run time to detect services that do not live up to their advertised interfaces. Contract monitoring is local to a service. Services are treated as black boxes, allowing heterogeneous implementation languages without modification to services' code. Thus, Whip does not disturb the loosely coupled nature of modern services.",10.1145/3110280
10.1145/3110282,Gradual session types,"Igarashi, Atsushi; Thiemann, Peter; Vasconcelos, Vasco T.; Wadler, Philip",2017,Proc. ACM Program. Lang.,"Session types are a rich type discipline, based on linear types, that lift the sort of safety claims that come with type systems to communications. However, web-based applications and micro services are often written in a mix of languages, with type disciplines in a spectrum between static and dynamic typing. Gradual session types address this mixed setting by providing a framework which grants seamless transition between statically typed handling of sessions and any required degree of dynamic typing. We propose GradualGV as an extension of the functional session type system GV with dynamic types and casts. We demonstrate type and communication safety as well as blame safety, thus extending previous results to functional languages with session-based communication. The interplay of linearity and dynamic types requires a novel approach to specifying the dynamics of the language.",10.1145/3110282
10.1145/3127499,MARC: A Resource Consumption Modeling Service for Self-Aware Autonomous Agents,"Ferroni, Matteo; Corna, Andrea; Damiani, Andrea; Brondolin, Rolando; Kubiatowicz, John D.; Sciuto, Donatella; Santambrogio, Marco D.",2017,ACM Trans. Auton. Adapt. Syst.,"Autonomicity is a golden feature when dealing with a high level of complexity. This complexity can be tackled partitioning huge systems in small autonomous modules, i.e., agents. Each agent then needs to be capable of extracting knowledge from its environment and to learn from it, in order to fulfill its goals: this could not be achieved without proper modeling techniques that allow each agent to gaze beyond its sensors. Unfortunately, the simplicity of agents and the complexity of modeling do not fit together, thus demanding for a third party to bridge the gap.Given the opportunities in the field, the main contributions of this work are twofold: (1) we propose a general methodology to model resource consumption trends and (2) we implemented it into MARC, a Cloud-service platform that produces Models-as-a-Service, thus relieving self-aware agents from the burden of building their custom modeling framework. In order to validate the proposed methodology, we set up a custom simulator to generate a wide spectrum of controlled traces: this allowed us to verify the correctness of our framework from a general and comprehensive point of view.",10.1145/3127499
10.1145/3139290,From DevOps to BizOps: Economic Sustainability for Scalable Cloud Applications,"Fokaefs, Marios; Barna, Cornel; Litoiu, Marin",2017,ACM Trans. Auton. Adapt. Syst.,"Virtualization of resources in cloud computing has enabled developers to commission and recommission resources at will and on demand. This virtualization is a coin with two sides. On one hand, the flexibility in managing virtual resources has enabled developers to efficiently manage their costs; they can easily remove unnecessary resources or add resources temporarily when the demand increases. On the other hand, the volatility of such environment and the velocity with which changes can occur may have a greater impact on the economic position of a stakeholder and the business balance of the overall ecosystem. In this work, we recognise the business ecosystem of cloud computing as an economy of scale and explore the effect of this fact on decisions concerning scaling the infrastructure of web applications to account for fluctuations in demand. The goal is to reveal and formalize opportunities for economically optimal scaling that takes into account not only the cost of infrastructure but also the revenue from service delivery and eventually the profit of the service provider. The end product is a scaling mechanism that makes decisions based on both performance and economic criteria and takes adaptive actions to optimize both performance and profitability for the system.",10.1145/3139290
10.1145/3148149,Auto-Scaling Web Applications in Clouds: A Taxonomy and Survey,"Qu, Chenhao; Calheiros, Rodrigo N.; Buyya, Rajkumar",2018,ACM Comput. Surv.,"Web application providers have been migrating their applications to cloud data centers, attracted by the emerging cloud computing paradigm. One of the appealing features of the cloud is elasticity. It allows cloud users to acquire or release computing resources on demand, which enables web application providers to automatically scale the resources provisioned to their applications without human intervention under a dynamic workload to minimize resource cost while satisfying Quality of Service (QoS) requirements. In this article, we comprehensively analyze the challenges that remain in auto-scaling web applications in clouds and review the developments in this field. We present a taxonomy of auto-scalers according to the identified challenges and key properties. We analyze the surveyed works and map them to the taxonomy to identify the weaknesses in this field. Moreover, based on the analysis, we propose new future directions that can be explored in this area.",10.1145/3148149
10.1145/3150224,"HPC Cloud for Scientific and Business Applications: Taxonomy, Vision, and Research Challenges","Netto, Marco A. S.; Calheiros, Rodrigo N.; Rodrigues, Eduardo R.; Cunha, Renato L. F.; Buyya, Rajkumar",2018,ACM Comput. Surv.,"High performance computing (HPC) clouds are becoming an alternative to on-premise clusters for executing scientific applications and business analytics services. Most research efforts in HPC cloud aim to understand the cost benefit of moving resource-intensive applications from on-premise environments to public cloud platforms. Industry trends show that hybrid environments are the natural path to get the best of the on-premise and cloud resources—steady (and sensitive) workloads can run on on-premise resources and peak demand can leverage remote resources in a pay-as-you-go manner. Nevertheless, there are plenty of questions to be answered in HPC cloud, which range from how to extract the best performance of an unknown underlying platform to what services are essential to make its usage easier. Moreover, the discussion on the right pricing and contractual models to fit small and large users is relevant for the sustainability of HPC clouds. This article brings a survey and taxonomy of efforts in HPC cloud and a vision on what we believe is ahead of us, including a set of research challenges that, once tackled, can help advance businesses and scientific discoveries. This becomes particularly relevant due to the fast increasing wave of new HPC applications coming from big data and artificial intelligence.",10.1145/3150224
10.1145/3150227,A Systematic Review of Cloud Modeling Languages,"Bergmayr, Alexander; Breitenb\""{u}cher, Uwe; Ferry, Nicolas; Rossini, Alessandro; Solberg, Arnor; Wimmer, Manuel; Kappel, Gerti; Leymann, Frank",2018,ACM Comput. Surv.,"Modern cloud computing environments support a relatively high degree of automation in service provisioning, which allows cloud service customers (CSCs) to dynamically acquire services required for deploying cloud applications. Cloud modeling languages (CMLs) have been proposed to address the diversity of features provided by cloud computing environments and support different application scenarios, such as migrating existing applications to the cloud, developing new cloud applications, or optimizing them. There is, however, still much debate in the research community on what a CML is, and what aspects of a cloud application and its target cloud computing environment should be modeled by a CML. Furthermore, the distinction between CMLs on a fine-grain level exposing their modeling concepts is rarely made. In this article, we investigate the diverse features currently provided by existing CMLs. We classify and compare them according to a common framework with the goal to support CSCs in selecting the CML that fits the needs of their application scenario and setting. As a result, not only features of existing CMLs are pointed out for which extensive support is already provided but also in which existing CMLs are deficient, thereby suggesting a research agenda.",10.1145/3150227
10.1145/3152421,Participatory Design that Matters—Facing the Big Issues,"B\o{}dker, Susanne; Kyng, Morten",2018,ACM Trans. Comput.-Hum. Interact.,"At a time where computer technology is putting human lives and work under pressure, we discuss how to provide alternatives. We look back at Participatory Design (PD) which was originally about possibilities and alternatives as much as it was about specific solutions. The paper aims to revitalize and revise PD to help people influence big issues. The agenda for this is set through proposing a set of key elements for realizing new, important possibilities. We discuss the possible changes of partnership with users, call for a new role of researchers as activists, debate how to work with demanding visions for lasting impact, and democratic control. We focus on high technological ambitions, on deployment of working prototypes, on alliances, and on scaling up, all seen as important for a PD that matters. We conclude the paper with an invitation to participate in the continued discussion, codesign, and realization of a PD that matters.",10.1145/3152421
10.1145/3158136,Online detection of effectively callback free objects with applications to smart contracts,"Grossman, Shelly; Abraham, Ittai; Golan-Gueta, Guy; Michalevsky, Yan; Rinetzky, Noam; Sagiv, Mooly; Zohar, Yoni",2017,Proc. ACM Program. Lang.,"Callbacks are essential in many programming environments, but drastically complicate program understanding and reasoning because they allow to mutate object's local states by external objects in unexpected fashions, thus breaking modularity. The famous DAO bug in the cryptocurrency framework Ethereum, employed callbacks to steal $150M. We define the notion of Effectively Callback Free (ECF) objects in order to allow callbacks without preventing modular reasoning. An object is ECF in a given execution trace if there exists an equivalent execution trace without callbacks to this object. An object is ECF if it is ECF in every possible execution trace. We study the decidability of dynamically checking ECF in a given execution trace and statically checking if an object is ECF. We also show that dynamically checking ECF in Ethereum is feasible and can be done online. By running the history of all execution traces in Ethereum, we were able to verify that virtually all existing contract executions, excluding these of the DAO or of contracts with similar known vulnerabilities, are ECF. Finally, we show that ECF, whether it is verified dynamically or statically, enables modular reasoning about objects with encapsulated state.",10.1145/3158136
10.1145/3173572,Introduction to the Special Issue on Emerging Software Technologies for Internet-Based Systems: Internetware and DevOps,"Xie, Tao; van Hoorn, Andre; Wang, Huaimin; Weber, Ingo",2018,ACM Trans. Internet Technol.,,10.1145/3173572
10.1145/3199665,Democratizing Authority in the Built Environment,"Andersen, Michael P.; Kolb, John; Chen, Kaifei; Fierro, Gabe; Culler, David E.; Katz, Randy",2018,ACM Trans. Sen. Netw.,"Operating systems and applications in the built environment have relied upon central authorization and management mechanisms that restrict their scalability, especially with respect to administrative overhead. We propose a new set of primitives encompassing syndication, security, and service execution that unifies the management of applications and services across the built environment, while enabling participants to individually delegate privilege across multiple administrative domains with no loss of security or manageability. We show how to leverage a decentralized authorization syndication platform to extend the design of building operating systems beyond the single administrative domain of a building. The authorization system leveraged is based on blockchain smart contracts to permit decentralized and democratized delegation of authorization without central trust. Upon this, a publish/subscribe syndication tier and a containerized service execution environment are constructed. Combined, these mechanisms solve problems of delegation, federation, device protection and service execution that arise throughout the built environment. We leverage a high-fidelity city-scale emulation to verify the scalability of the authorization tier, and briefly describe a prototypical democratized operating system for the built environment using this foundation.This is an extension of work presented in Ref.&nbsp;[3].",10.1145/3199665
10.1145/3226644,A Unified Model for the Mobile-Edge-Cloud Continuum,"Baresi, L.; Mendon\c{c}a, D. F.; Garriga, M.; Guinea, S.; Quattrocchi, G.",2019,ACM Trans. Internet Technol.,"Technologies such as mobile, edge, and cloud computing have the potential to form a computing continuum for new, disruptive applications. At runtime, applications can choose to execute parts of their logic on different infrastructures that constitute the continuum, with the goal of minimizing latency and battery consumption and maximizing availability. In this article, we propose A3-E, a unified model for managing the life cycle of continuum applications. In particular, A3-E exploits the Functions-as-a-Service model to bring computation to the continuum in the form of microservices. Furthermore, A3-E selects where to execute a certain function based on the specific context and user requirements. The article also presents a prototype framework that implements the concepts behind A3-E. Results show that A3-E is capable of dynamically deploying microservices and routing the application’s requests, reducing latency by up to 90\% when using edge instead of cloud resources, and battery consumption by 74\% when computation has been offloaded.",10.1145/3226644
10.1145/3230713,A Meta-Model for Information Systems Quality: A Mixed Study of the Financial Sector,"Russo, Daniel; Ciancarini, Paolo; Falasconi, Tommaso; Tomasi, Massimo",2018,ACM Trans. Manage. Inf. Syst.,"Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models.",10.1145/3230713
10.1145/3236332,"Quantifying Cloud Performance and Dependability: Taxonomy, Metric Design, and Emerging Challenges","Herbst, Nikolas; Bauer, Andr\'{e}; Kounev, Samuel; Oikonomou, Giorgos; Eyk, Erwin Van; Kousiouris, George; Evangelinou, Athanasia; Krebs, Rouven; Brecht, Tim; Abad, Cristina L.; Iosup, Alexandru",2018,ACM Trans. Model. Perform. Eval. Comput. Syst.,"In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii)&nbsp;performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii)&nbsp;availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks.",10.1145/3236332
10.1145/3241737,A Manifesto for Future Generation Cloud Computing: Research Directions for the Next Decade,"Buyya, Rajkumar; Srirama, Satish Narayana; Casale, Giuliano; Calheiros, Rodrigo; Simmhan, Yogesh; Varghese, Blesson; Gelenbe, Erol; Javadi, Bahman; Vaquero, Luis Miguel; Netto, Marco A. S.; Toosi, Adel Nadjaran; Rodriguez, Maria Alejandra; Llorente, Ignacio M.; Vimercati, Sabrina De Capitani Di; Samarati, Pierangela; Milojicic, Dejan; Varela, Carlos; Bahsoon, Rami; Assuncao, Marcos Dias De; Rana, Omer; Zhou, Wanlei; Jin, Hai; Gentzsch, Wolfgang; Zomaya, Albert Y.; Shen, Haiying",2018,ACM Comput. Surv.,"The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.",10.1145/3241737
10.1145/3267468,Test-Based Security Certification of Composite Services,"Anisetti, Marco; Ardagna, Claudio; Damiani, Ernesto; Polegri, Gianluca",2018,ACM Trans. Web,"The diffusion of service-based and cloud-based systems has created a scenario where software is often made available as services, offered as commodities over corporate networks or the global net. This scenario supports the definition of business processes as composite services, which are implemented via either static or runtime composition of offerings provided by different suppliers. Fast and accurate evaluation of services’ security properties becomes then a fundamental requirement and is nowadays part of the software development process. In this article, we show how the verification of security properties of composite services can be handled by test-based security certification and built to be effective and efficient in dynamic composition scenarios. Our approach builds on existing security certification schemes for monolithic services and extends them towards service compositions. It virtually certifies composite services, starting from certificates awarded to the component services. We describe three heuristic algorithms for generating runtime test-based evidence of the composite service holding the properties. These algorithms are compared with the corresponding exhaustive algorithm to evaluate their quality and performance. We also evaluate the proposed approach in a real-world industrial scenario, which considers ENGpay online payment system of Engineering Ingegneria Informatica S.p.A. The proposed industrial evaluation presents the utility and generality of the proposed approach by showing how certification results can be used as a basis to establish compliance to Payment Card Industry Data Security Standard.",10.1145/3267468
10.1145/3276522,Reactive caching for composed services: polling at the speed of push,"Burckhardt, Sebastian; Coppieters, Tim",2018,Proc. ACM Program. Lang.,"Sometimes, service clients repeat requests in a polling loop in order to refresh their view. However, such polling may be slow to pick up changes, or may increase the load unacceptably, in particular for composed services that disperse over many components. We present an alternative reactive polling API and reactive caching algorithm that combines the conceptual simplicity of polling with the efficiency of push-based change propagation. A reactive cache contains a summary of a distributed read-only operation and maintains a connection to its dependencies so changes can be propagated automatically. We first formalize the setting using an abstract calculus for composed services. Then we present a fault-tolerant distributed algorithm for reactive caching that guarantees eventual consistency. Finally, we implement and evaluate our solution by extending the Orleans actor framework, and perform experiments on two benchmarks in a distributed cloud deployment. The results show that our solution provides superior performance compared to polling, at a latency that comes close to hand-written change notifications.",10.1145/3276522
10.1145/3290342,Distributed programming using role-parametric session types in go: statically-typed endpoint APIs for dynamically-instantiated communication structures,"Castro, David; Hu, Raymond; Jongmans, Sung-Shik; Ng, Nicholas; Yoshida, Nobuko",2019,Proc. ACM Program. Lang.,"This paper presents a framework for the static specification and safe programming of message passing protocols where the number and kinds of participants are dynamically instantiated. We develop the first theory of distributed multiparty session types (MPST) to support parameterised protocols with indexed roles—our framework statically infers the different kinds of participants induced by a protocol definition as role variants, and produces decoupled endpoint projections of the protocol onto each variant. This enables safe MPST-based programming of the parameterised endpoints in distributed settings: each endpoint can be implemented separately by different programmers, using different techniques (or languages). We prove the decidability of role variant inference and well-formedness checking, and the correctness of projection. We implement our theory as a toolchain for programming such role-parametric MPST protocols in Go. Our approach is to generate API families of lightweight, protocol- and variant-specific type wrappers for I/O. The APIs ensure a well-typed Go endpoint program (by native Go type checking) will perform only compliant I/O actions w.r.t. the source protocol. We leverage the abstractions of MPST to support the specification and implementation of Go applications involving multiple channels, possibly over mixed transports (e.g., Go channels, TCP), and channel passing via a unified programming interface. We evaluate the applicability and run-time performance of our generated APIs using microbenchmarks and real-world applications.",10.1145/3290342
10.1145/3290365,Adventures in monitorability: from branching to linear time and back again,"Aceto, Luca; Achilleos, Antonis; Francalanza, Adrian; Ing\'{o}lfsd\'{o}ttir, Anna; Lehtinen, Karoliina",2019,Proc. ACM Program. Lang.,"This paper establishes a comprehensive theory of runtime monitorability for Hennessy-Milner logic with recursion, a very expressive variant of the modal µ-calculus. It investigates the monitorability of that logic with a linear-time semantics and then compares the obtained results with ones that were previously presented in the literature for a branching-time setting. Our work establishes an expressiveness hierarchy of monitorable fragments of Hennessy-Milner logic with recursion in a linear-time setting and exactly identifies what kinds of guarantees can be given using runtime monitors for each fragment in the hierarchy. Each fragment is shown to be complete, in the sense that it can express all properties that can be monitored under the corresponding guarantees. The study is carried out using a principled approach to monitoring that connects the semantics of the logic and the operational semantics of monitors. The proposed framework supports the automatic, compositional synthesis of correct monitors from monitorable properties.",10.1145/3290365
10.1145/3293455,RESTful API Automated Test Case Generation with EvoMaster,"Arcuri, Andrea",2019,ACM Trans. Softw. Eng. Methodol.,"RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.",10.1145/3293455
10.1145/3295662,"A Survey of Opinion Mining in Arabic: A Comprehensive System Perspective Covering Challenges and Advances in Tools, Resources, Models, Applications, and Visualizations","Badaro, Gilbert; Baly, Ramy; Hajj, Hazem; El-Hajj, Wassim; Shaban, Khaled Bashir; Habash, Nizar; Al-Sallab, Ahmad; Hamdi, Ali",2019,ACM Trans. Asian Low-Resour. Lang. Inf. Process.,"Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.",10.1145/3295662
10.1145/3301443,Fog Computing for the Internet of Things: A Survey,"Puliafito, Carlo; Mingozzi, Enzo; Longo, Francesco; Puliafito, Antonio; Rana, Omer",2019,ACM Trans. Internet Technol.,"Research in the Internet of Things (IoT) conceives a world where everyday objects are connected to the Internet and exchange, store, process, and collect data from the surrounding environment. IoT devices are becoming essential for supporting the delivery of data to enable electronic services, but they are not sufficient in most cases to host application services directly due to their intrinsic resource constraints. Fog Computing (FC) can be a suitable paradigm to overcome these limitations, as it can coexist and cooperate with centralized Cloud systems and extends the latter toward the network edge. In this way, it is possible to distribute resources and services of computing, storage, and networking along the Cloud-to-Things continuum. As such, FC brings all the benefits of Cloud Computing (CC) closer to end (user) devices. This article presents a survey on the employment of FC to support IoT devices and services. The principles and literature characterizing FC are described, highlighting six IoT application domains that may benefit from the use of this paradigm. The extension of Cloud systems towards the network edge also creates new challenges and can have an impact on existing approaches employed in Cloud-based deployments. Research directions being adopted by the community are highlighted, with an indication of which of these are likely to have the greatest impact. An overview of existing FC software and hardware platforms for the IoT is also provided, along with the standardisation efforts in this area initiated by the OpenFog Consortium (OFC).",10.1145/3301443
10.1145/3302258,Derecho: Fast State Machine Replication for Cloud Services,"Jha, Sagar; Behrens, Jonathan; Gkountouvas, Theo; Milano, Mae; Song, Weijia; Tremel, Edward; Renesse, Robbert Van; Zink, Sydney; Birman, Kenneth P.",2019,ACM Trans. Comput. Syst.,"Cloud computing services often replicate data and may require ways to coordinate distributed actions. Here we present Derecho, a library for such tasks. The API provides interfaces for structuring applications into patterns of subgroups and shards, supports state machine replication within them, and includes mechanisms that assist in restart after failures. Running over 100Gbps RDMA, Derecho can send millions of events per second in each subgroup or shard and throughput peaks at 16GB/s, substantially outperforming prior solutions. Configured to run purely on TCP, Derecho is still substantially faster than comparable widely used, highly-tuned, standard tools. The key insight is that on modern hardware (including non-RDMA networks), data-intensive protocols should be built from non-blocking data-flow components.",10.1145/3302258
10.1145/3305268,A Multi-Vocal Review of Security Orchestration,"Islam, Chadni; Babar, Muhammad Ali; Nepal, Surya",2019,ACM Comput. Surv.,"Organizations use diverse types of security solutions to prevent cyber-attacks. Multiple vendors provide security solutions developed using heterogeneous technologies and paradigms. Hence, it is a challenging rather impossible to easily make security solutions to work an integrated fashion. Security orchestration aims at smoothly integrating multivendor security tools that can effectively and efficiently interoperate to support security staff of a Security Operation Centre (SOC). Given the increasing role and importance of security orchestration, there has been an increasing amount of literature on different aspects of security orchestration solutions. However, there has been no effort to systematically review and analyze the reported solutions. We report a Multivocal Literature Review that has systematically selected and reviewed both academic and grey (blogs, web pages, white papers) literature on different aspects of security orchestration published from January 2007 until July 2017. The review has enabled us to provide a working definition of security orchestration and classify the main functionalities of security orchestration into three main areas—unification, orchestration, and automation. We have also identified the core components of a security orchestration platform and categorized the drivers of security orchestration based on technical and socio-technical aspects. We also provide a taxonomy of security orchestration based on the execution environment, automation strategy, deployment type, mode of task and resource type. This review has helped us to reveal several areas of further research and development in security orchestration.",10.1145/3305268
10.1145/3309705,"Enabling Workload Engineering in Edge, Fog, and Cloud Computing through OpenStack-based Middleware","Merlino, Giovanni; Dautov, Rustem; Distefano, Salvatore; Bruneo, Dario",2019,ACM Trans. Internet Technol.,"To enable and support smart environments, a recent ICT trend promotes pushing computation from the remote Cloud as close to data sources as possible, resulting in the emergence of the Fog and Edge computing paradigms. Together with Cloud computing, they represent a stacked architecture, in which raw datasets are first pre-processed locally at the Edge and then vertically offloaded to the Fog and/or the Cloud. However, as hardware is becoming increasingly powerful, Edge devices are seen as candidates for offering data processing capabilities, able to pool and share computing resources to achieve better performance at a lower network latency—a pattern that can be also applied to Fog nodes. In these circumstances, it is important to enable efficient, intelligent, and balanced allocation of resources, as well as their further orchestration, in an elastic and transparent manner. To address such a requirement, this article proposes an OpenStack-based middleware platform through which resource containers at the Edge, Fog, and Cloud levels can be discovered, combined, and provisioned to end users and applications, thereby facilitating and orchestrating offloading processes. As demonstrated through a proof of concept on an intelligent surveillance system, by converging the Edge, Fog, and Cloud, the proposed architecture has the potential to enable faster data processing, as compared to processing at the Edge, Fog, or Cloud levels separately. This also allows architects to combine different offloading patterns in a flexible and fine-grained manner, thus providing new workload engineering patterns. Measurements demonstrated the effectiveness of such patterns, even outperforming edge clusters.",10.1145/3309705
10.1145/3319404,"Guest Editors’ Introduction to the Special Issue on Fog, Edge, and Cloud Integration for Smart Environments","Longo, Francesco; Puliafito, Antonio; Rana, Omer",2019,ACM Trans. Internet Technol.,,10.1145/3319404
10.1145/3329786,Dynamic Malware Analysis in the Modern Era—A State of the Art Survey,"Or-Meir, Ori; Nissim, Nir; Elovici, Yuval; Rokach, Lior",2019,ACM Comput. Surv.,"Although malicious software (malware) has been around since the early days of computers, the sophistication and innovation of malware has increased over the years. In particular, the latest crop of ransomware has drawn attention to the dangers of malicious software, which can cause harm to private users as well as corporations, public services (hospitals and transportation systems), governments, and security institutions. To protect these institutions and the public from malware attacks, malicious activity must be detected as early as possible, preferably before it conducts its harmful acts. However, it is not always easy to know what to look for—especially when dealing with new and unknown malware that has never been seen. Analyzing a suspicious file by static or dynamic analysis methods can provide relevant and valuable information regarding a file's impact on the hosting system and help determine whether the file is malicious or not, based on the method's predefined rules. While various techniques (e.g., code obfuscation, dynamic code loading, encryption, and packing) can be used by malware writers to evade static analysis (including signature-based anti-virus tools), dynamic analysis is robust to these techniques and can provide greater understanding regarding the analyzed file and consequently can lead to better detection capabilities. Although dynamic analysis is more robust than static analysis, existing dynamic analysis tools and techniques are imperfect, and there is no single tool that can cover all aspects of malware behavior. The most recent comprehensive survey performed in this area was published in 2012. Since that time, the computing environment has changed dramatically with new types of malware (ransomware, cryptominers), new analysis methods (volatile memory forensics, side-channel analysis), new computing environments (cloud computing, IoT devices), new machine-learning algorithms, and more. The goal of this survey is to provide a comprehensive and up-to-date overview of existing methods used to dynamically analyze malware, which includes a description of each method, its strengths and weaknesses, and its resilience against malware evasion techniques. In addition, we include an overview of prominent studies presenting the usage of machine-learning methods to enhance dynamic malware analysis capabilities aimed at detection, classification, and categorization.",10.1145/3329786
10.1145/3331149,A Conceptual Framework and Content Model for Next Generation Presentation Solutions,"Roels, Reinout; Signer, Beat",2019,Proc. ACM Hum.-Comput. Interact.,"Mainstream presentation tools such as Microsoft PowerPoint were originally built to mimic physical media like photographic slides and still exhibit the same characteristics. However, the state of the art in presentation tools shows that more recent solutions start to go beyond the classic presentation paradigms. For instance, presentations are becoming increasingly non-linear, content is quickly evolving beyond simple text and images and the way we author our presentations is becoming more collaborative. Nevertheless, existing presentation content models are often based on assumptions that do not apply to the current state of presentations any more, making them incompatible for some use cases and limiting the potential of end-user presentation solutions. In order to support state-of-the-art presentation functionality, we rethink the concept of a presentation and introduce a conceptual framework for presentation content. We then present a new content model for presentation solutions based on the Resource-Selector-Link (RSL) hypermedia metamodel. We further discuss an implementation of our model and show some example use cases. We conclude by outlining how design choices in the model address currently unmet needs with regards to extensibility, content reuse, collaboration, semantics, user access management, non-linearity, and context awareness, resulting in better support for the corresponding end-user functionality in presentation tools.",10.1145/3331149
10.1145/3341145,Machine Learning Methods for Reliable Resource Provisioning in Edge-Cloud Computing: A Survey,"Duc, Thang Le; Leiva, Rafael Garc\'{\i}a; Casari, Paolo; \""{O}stberg, Per-Olov",2019,ACM Comput. Surv.,"Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use.This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article.",10.1145/3341145
10.1145/3342103,"Cloud Pricing Models: Taxonomy, Survey, and Interdisciplinary Challenges","Wu, Caesar; Buyya, Rajkumar; Ramamohanarao, Kotagiri",2019,ACM Comput. Surv.,"This article provides a systematic review of cloud pricing in an interdisciplinary approach. It examines many historical cases of pricing in practice and tracks down multiple roots of pricing in research. The aim is to help both cloud service provider (CSP) and cloud customers to capture the essence of cloud pricing when they need to make a critical decision either to achieve competitive advantages or to manage cloud resource effectively. Currently, the number of available pricing schemes in the cloud market is overwhelming. It is an intricate issue to understand these schemes and associated pricing models clearly due to involving several domains of knowledge, such as cloud technologies, microeconomics, operations research, and value theory. Some earlier studies have introduced this topic unsystematically. Their approaches inevitably lead to much confusion for many cloud decision-makers. To address their weaknesses, we present a comprehensive taxonomy of cloud pricing, which is driven by a framework of three fundamental pricing strategies that are built on nine cloud pricing categories. These categories can be further mapped onto a total of 60 pricing models. Many of the pricing models have been already adopted by CSPs. Others have been widespread across in other industries. We give descriptions of these model categories and highlight both advantages and disadvantages. Moreover, this article offers an extensive survey of many cloud pricing models that were proposed by many researchers during the past decade. Based on the survey, we identify four trends of cloud pricing and the general direction, which is moving from intrinsic value per physical box to extrinsic value per serverless sandbox. We conclude that hyper-converged cloud resources pool supported by cloud orchestration, virtual machine, Open Application Programming Interface, and serverless sandbox will drive the future of cloud pricing.",10.1145/3342103
10.1145/3359981,A Survey of DevOps Concepts and Challenges,"Leite, Leonardo; Rocha, Carla; Kon, Fabio; Milojicic, Dejan; Meirelles, Paulo",2019,ACM Comput. Surv.,"DevOpsis a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.",10.1145/3359981
10.1145/3360497,Understanding the Knowledge Gaps of Software Engineers: An Empirical Analysis Based on SWEBOK,"Garousi, Vahid; Giray, Gorkem; Tuzun, Eray",2019,ACM Trans. Comput. Educ.,"Context: Knowledge level and productivity of the software engineering (SE) workforce are the subject of regular discussions among practitioners, educators, and researchers. There have been many efforts to measure and improve the knowledge gap between SE education and industrial needs.Objective: Although the existing efforts for aligning SE education and industrial needs have provided valuable insights, there is a need for analyzing the SE topics in a more “fine-grained” manner; i.e., knowing that SE university graduates should know more about requirements engineering is important, but it is more valuable to know the exact topics of requirements engineering that are most important in the industry.Method: We achieve the above objective by assessing the knowledge gaps of software engineers by designing and executing an opinion survey on levels of knowledge learned in universities versus skills needed in industry. We designed the survey by using the SE knowledge areas (KAs) from the latest version of the Software Engineering Body of Knowledge (SWEBOK v3), which classifies the SE knowledge into 12 KAs, which are themselves broken down into 67 subareas (sub-KAs) in total. Our analysis is based on (opinion) data gathered from 129 practitioners, who are mostly based in Turkey.Results: Based on our findings, we recommend that educators should include more materials on software maintenance, software configuration management, and testing in their SE curriculum. Based on the literature as well as the current trends in industry, we provide actionable suggestions to improve SE curriculum to decrease the knowledge gap.",10.1145/3360497
10.1145/3360575,Formal foundations of serverless computing,"Jangda, Abhinav; Pinckney, Donald; Brun, Yuriy; Guha, Arjun",2019,Proc. ACM Program. Lang.,"Serverless computing (also known as functions as a service) is a new cloud computing abstraction that makes it easier to write robust, large-scale web services. In serverless computing, programmers write what are called serverless functions, which are programs that respond to external events. When demand for the serverless function spikes, the platform automatically allocates additional hardware and manages load-balancing; when demand falls, the platform silently deallocates idle resources; and when the platform detects a failure, it transparently retries affected requests. In 2014, Amazon Web Services introduced the first serverless platform, AWS Lambda, and similar abstractions are now available on all major cloud computing platforms. Unfortunately, the serverless computing abstraction exposes several low-level operational details that make it hard for programmers to write and reason about their code. This paper sheds light on this problem by presenting λλ, an operational semantics of the essence of serverless computing. Despite being a small (half a page) core calculus, λλ models all the low-level details that serverless functions can observe. To show that λλ is useful, we present three applications. First, to ease reasoning about code, we present a simplified naive semantics of serverless execution and precisely characterize when the naive semantics and λλ coincide. Second, we augment λλ with a key-value store to allow reasoning about stateful serverless functions. Third, since a handful of serverless platforms support serverless function composition, we show how to extend λλ with a composition language and show that our implementation can outperform prior work.",10.1145/3360575
10.1145/3360610,"Initialize once, start fast: application initialization at build time","Wimmer, Christian; Stancu, Codrut; Hofer, Peter; Jovanovic, Vojin; W\""{o}gerer, Paul; Kessler, Peter B.; Pliss, Oleg; W\""{u}rthinger, Thomas",2019,Proc. ACM Program. Lang.,"Arbitrary program extension at run time in language-based VMs, e.g., Java's dynamic class loading, comes at a startup cost: high memory footprint and slow warmup. Cloud computing amplifies the startup overhead. Microservices and serverless cloud functions lead to small, self-contained applications that are started often. Slow startup and high memory footprint directly affect the cloud hosting costs, and slow startup can also break service-level agreements. Many applications are limited to a prescribed set of pre-tested classes, i.e., use a closed-world assumption at deployment time. For such Java applications, GraalVM Native Image offers fast startup and stable performance. GraalVM Native Image uses a novel iterative application of points-to analysis and heap snapshotting, followed by ahead-of-time compilation with an optimizing compiler. Initialization code can run at build time, i.e., executables can be tailored to a particular application configuration. Execution at run time starts with a pre-populated heap, leveraging copy-on-write memory sharing. We show that this approach improves the startup performance by up to two orders of magnitude compared to the Java HotSpot VM, while preserving peak performance. This allows Java applications to have a better startup performance than Go applications and the V8 JavaScript VM.",10.1145/3360610
10.1145/3362031,"A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms: Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet","Ren, Ju; Zhang, Deyu; He, Shiwen; Zhang, Yaoxue; Li, Tao",2019,ACM Comput. Surv.,"Sending data to the cloud for analysis was a prominent trend during the past decades, driving cloud computing as a dominant computing paradigm. However, the dramatically increasing number of devices and data traffic in the Internet-of-Things (IoT) era are posing significant burdens on the capacity-limited Internet and uncontrollable service delay. It becomes difficult to meet the delay-sensitive and context-aware service requirements of IoT applications by using cloud computing alone. Facing these challenges, computing paradigms are shifting from the centralized cloud computing to distributed edge computing. Several new computing paradigms, including Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet, have emerged to leverage the distributed resources at network edge to provide timely and context-aware services. By integrating end devices, edge servers, and cloud, they form a hierarchical IoT architecture, i.e., End-Edge-Cloud orchestrated architecture to improve the performance of IoT systems. This article presents a comprehensive survey of these emerging computing paradigms from the perspective of end-edge-cloud orchestration. Specifically, we first introduce and compare the architectures and characteristics of different computing paradigms. Then, a comprehensive survey is presented to discuss state-of-the-art research in terms of computation offloading, caching, security, and privacy. Finally, some potential research directions are envisioned for fostering continuous research efforts.",10.1145/3362031
10.1145/3368036,Multiple Workflows Scheduling in Multi-tenant Distributed Systems: A Taxonomy and Future Directions,"Hilman, Muhammad H.; Rodriguez, Maria A.; Buyya, Rajkumar",2020,ACM Comput. Surv.,"Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.",10.1145/3368036
10.1145/3371038,"Fast Packet Processing with eBPF and XDP: Concepts, Code, Challenges, and Applications","Vieira, Marcos A. M.; Castanho, Matheus S.; Pac\'{\i}fico, Racyus D. G.; Santos, Elerson R. S.; J\'{u}nior, Eduardo P. M. C\^{a}mara; Vieira, Luiz F. M.",2020,ACM Comput. Surv.,"Extended Berkeley Packet Filter (eBPF) is an instruction set and an execution environment inside the Linux kernel. It enables modification, interaction, and kernel programmability at runtime. eBPF can be used to program the eXpress Data Path (XDP), a kernel network layer that processes packets closer to the NIC for fast packet processing. Developers can write programs in C or P4 languages and then compile to eBPF instructions, which can be processed by the kernel or by programmable devices (e.g., SmartNICs). Since its introduction in 2014, eBPF has been rapidly adopted by major companies such as Facebook, Cloudflare, and Netronome. Use cases include network monitoring, network traffic manipulation, load balancing, and system profiling. This work aims to present eBPF to an inexpert audience, covering the main theoretical and fundamental aspects of eBPF and XDP, as well as introducing the reader to simple examples to give insight into the general operation and use of both technologies.",10.1145/3371038
10.1145/3375633,Visualizing Distributed System Executions,"Beschastnikh, Ivan; Liu, Perry; Xing, Albert; Wang, Patty; Brun, Yuriy; Ernst, Michael D.",2020,ACM Trans. Softw. Eng. Methodol.,"Distributed systems pose unique challenges for software developers. Understanding the system’s communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts’ logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1)&nbsp;understanding the relative ordering of events, (2)&nbsp;searching for specific patterns of interaction between hosts, and (3)&nbsp;identifying structural similarities and differences between pairs of executions. Our approach consists of XVector, which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz, which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size.",10.1145/3375633
10.1145/3379469,Set the Configuration for the Heart of the OS: On the Practicality of Operating System Kernel Debloating,"Kuo, Hsuan-Chi; Chen, Jianyan; Mohan, Sibin; Xu, Tianyin",2020,Proc. ACM Meas. Anal. Comput. Syst.,"This paper presents a study on the practicality of operating system (OS) kernel debloating---reducing kernel code that is not needed by the target applications---in real-world systems. Despite their significant benefits regarding security (attack surface reduction) and performance (fast boot times and reduced memory footprints), the state-of-the-art OS kernel debloating techniques are seldom adopted in practice, especially in production systems. We identify the limitations of existing kernel debloating techniques that hinder their practical adoption, including both accidental and essential limitations. To understand these limitations, we build an advanced debloating framework named tool which enables us to conduct a number of experiments on different types of OS kernels (including Linux and the L4 microkernel) with a wide variety of applications (including HTTPD, Memcached, MySQL, NGINX, PHP and Redis). Our experimental results reveal the challenges and opportunities towards making kernel debloating techniques practical for real-world systems. The main goal of this paper is to share these insights and our experiences to shed light on addressing the limitations of kernel debloating in future research and development efforts.",10.1145/3379469
10.1145/3381452,Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems,"Tsigkanos, Christos; Garriga, Martin; Baresi, Luciano; Ghezzi, Carlo",2020,ACM Trans. Internet Technol.,"Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.",10.1145/3381452
10.1145/3385896,A Survey and Classification of Software-Defined Storage Systems,"Macedo, Ricardo; Paulo, Jo\~{a}o; Pereira, Jos\'{e}; Bessani, Alysson",2020,ACM Comput. Surv.,"The exponential growth of digital information is imposing increasing scale and efficiency demands on modern storage infrastructures. As infrastructure complexity increases, so does the difficulty in ensuring quality of service, maintainability, and resource fairness, raising unprecedented performance, scalability, and programmability challenges. Software-Defined Storage (SDS) addresses these challenges by cleanly disentangling control and data flows, easing management, and improving control functionality of conventional storage systems. Despite its momentum in the research community, many aspects of the paradigm are still unclear, undefined, and unexplored, leading to misunderstandings that hamper the research and development of novel SDS technologies. In this article, we present an in-depth study of SDS systems, providing a thorough description and categorization of each plane of functionality. Further, we propose a taxonomy and classification of existing SDS solutions according to different criteria. Finally, we provide key insights about the paradigm and discuss potential future research directions for the field.",10.1145/3385896
10.1145/3386319,APL since 1978,"Hui, Roger K. W.; Kromberg, Morten J.",2020,Proc. ACM Program. Lang.,"The Evolution of APL, the HOPL I paper by Falkoff and Iverson on APL, recounted the fundamental design principles which shaped the implementation of the APL language in 1966, and the early uses and other influences which shaped its first decade of enhancements.In the 40 years that have elapsed since HOPL I, several dozen APL implementations have come and gone. In the first decade or two, interpreters were typically born and buried along with the hardware or operating system that they were created for. More recently, the use of C as an implementation language provided APL interpreters with greater longevity and portability.APL started its life on IBM mainframes which were time-shared by multiple users. As the demand for computing resources grew and costs dropped, APL first moved in-house to mainframes, then to mini- and micro-computers. Today, APL runs on PCs and tablets, Apples and Raspberry Pis, smartphones and watches.The operating systems, and the software application platforms that APL runs on, have evolved beyond recognition. Tools like database systems have taken over many of the tasks that were initially implemented in APL or provided by the APL system, and new capabilities like parallel hardware have also changed the focus of design and implementation efforts through the years.The first wave of significant language enhancements occurred shortly after HOPL I, resulting in so-called second-generation APL systems. The most important feature of the second generation is the addition of general arrays—in which any item of an array can be another array—and a number of new functions and operators aligned with, if not always motivated by, the new data structures.The majority of implementations followed IBM’s path with APL2 “floating” arrays; others aligned themselves with SHARP APL and “grounded” arrays. While the APL2 style of APL interpreters came to dominate the mainstream of the APL community, two new cousins of APL descended from the SHARP APL family tree: J (created by Iverson and Hui) and k (created by Arthur Whitney).We attempt to follow a reasonable number of threads through the last 40 years, to identify the most important factors that have shaped the evolution of APL. We will discuss the details of what we believe are the most significant language features that made it through the occasionally unnatural selection imposed by the loss of habitats that disappeared with hardware, software platforms, and business models.The history of APL now spans six decades. It is still the case, as Falkoff and Iverson remarked at the end of the HOPL I paper, that:Although this is not the place to discuss the future, it should be remarked that the evolution of APL is far from finished.",10.1145/3386319
10.1145/3386321,A history of Clojure,"Hickey, Rich",2020,Proc. ACM Program. Lang.,"Clojure was designed to be a general-purpose, practical functional language, suitable for use by professionals wherever its host language, e.g., Java, would be. Initially designed in 2005 and released in 2007, Clojure is a dialect of Lisp, but is not a direct descendant of any prior Lisp. It complements programming with pure functions of immutable data with concurrency-safe state management constructs that support writing correct multithreaded programs without the complexity of mutex locks. Clojure is intentionally hosted, in that it compiles to and runs on the runtime of another language, such as the JVM. This is more than an implementation strategy; numerous features ensure that programs written in Clojure can leverage and interoperate with the libraries of the host language directly and efficiently. In spite of combining two (at the time) rather unpopular ideas, functional programming and Lisp, Clojure has since seen adoption in industries as diverse as finance, climate science, retail, databases, analytics, publishing, healthcare, advertising and genomics, and by consultancies and startups worldwide, much to the career-altering surprise of its author. Most of the ideas in Clojure were not novel, but their combination puts Clojure in a unique spot in language design (functional, hosted, Lisp). This paper recounts the motivation behind the initial development of Clojure and the rationale for various design decisions and language constructs. It then covers its evolution subsequent to release and adoption.",10.1145/3386321
10.1145/3386326,A history of the Groovy programming language,"King, Paul",2020,Proc. ACM Program. Lang.,"This paper describes the history of the Groovy programming language. At the time of Groovy’s inception, Java was a dominant programming language with a wealth of useful libraries. Despite this, it was perceived by some to be evolving slowing and to have shortcomings for scripting, rapid prototyping and when trying to write minimalistic code. Other languages seemed to be innovating faster than Java and, while overcoming some of Java’s shortcomings, used syntax that was less familiar to Java developers. Integration with Java libraries was also non-optimal. Groovy was created as a complementary language to Java—its dynamic counterpart. It would look and feel like Java but focus on extensibility and rapid innovation. Groovy would borrow ideas from dynamic languages like Ruby, Python and Smalltalk where needed to provide compelling JVM solutions for some of Java’s shortcomings. Groovy supported innovation through its runtime and compile-time metaprogramming capabilities. It supported simple operator overloading, had a flexible grammar and was extensible. These characteristics made it suitable for growing the language to have new commands (verbs) and properties (nouns) specific to a particular domain, a so called Domain Specific Language (DSL). While still intrinsically linked with Java, over time Groovy has evolved from a niche dynamic scripting language into a compelling mainstream language. After many years as a principally dynamically-typed language, a static nature was added to Groovy. Code could be statically type checked or when dynamic features weren’t needed, they could be turned off entirely for Java-like performance. A number of nuances to the static nature came about to support the style of coding used by Groovy developers. Many choices made by Groovy in its design, later appeared in other languages (Swift, C#, Kotlin, Ceylon, PHP, Ruby, Coffeescript, Scala, Frege, TypeScript and Java itself). This includes Groovy’s dangling closure, Groovy builders, null-safe navigation, the Elvis operator, ranges, the spaceship operator, and flow typing. For most languages, we don’t know to what extent Groovy played a part in their choices. We do know that Kotlin took inspiration from Groovy’s dangling closures, builder concept, default it parameter for closures, templates and interpolated strings, null-safe navigation and the Elvis operator. The leadership, governance and sponsorship arrangements of Groovy have evolved over time, but Groovy has always been a successful highly collaborative open source project driven more by the needs of the community than by a vision of a particular company or person.",10.1145/3386326
10.1145/3391533,Handling SQL Databases in Automated System Test Generation,"Arcuri, Andrea; Galeotti, Juan P.",2020,ACM Trans. Softw. Eng. Methodol.,"Automated system test generation for web/enterprise systems requires either a sequence of actions on a GUI (e.g., clicking on HTML links and form buttons) or direct HTTP calls when dealing with web services (e.g., REST and SOAP). When doing white-box testing of such systems, their code can be analyzed, and the same type of heuristics (e.g., the branch distance) used in search-based unit testing can be employed to improve performance. However, web/enterprise systems do often interact with a database. To obtain higher coverage and find new faults, the state of the databases needs to be taken into account when generating white-box tests. In this work, we present a novel heuristic to enhance search-based software testing of web/enterprise systems, which takes into account the state of the accessed databases. Furthermore, we enable the generation of SQL data directly from the test cases. This is useful when it is too difficult or time consuming to generate the right sequence of events to put the database in the right state. Also, it is useful when dealing with databases that are “read-only” for the system under test, and the actual data are generated by other services. We implemented our technique as an extension of EVOMASTER, where system tests are generated in the JUnit format. Experiments on six RESTful APIs (five open-source and one industrial) show that our novel techniques improve coverage significantly (up to +16.5\%), finding seven new faults in those systems.",10.1145/3391533
10.1145/3396374,"Foundations, Properties, and Security Applications of Puzzles: A Survey","Ali, Isra Mohamed; Caprolu, Maurantonio; Pietro, Roberto Di",2020,ACM Comput. Surv.,"Cryptographic algorithms have been used not only to create robust ciphertexts but also to generate cryptograms that, contrary to the classic goal of cryptography, are meant to be broken. These cryptograms, generally called puzzles, require the use of a certain amount of resources to be solved, hence introducing a cost that is often regarded as a time delay—though it could involve other metrics as well, such as bandwidth. These powerful features have made puzzles the core of many security protocols, acquiring increasing importance in the IT security landscape. The concept of a puzzle has subsequently been extended to other types of schemes that do not use cryptographic functions, such as CAPTCHAs, which are used to discriminate humans from machines. Overall, puzzles have experienced a renewed interest with the advent of Bitcoin, which uses a CPU-intensive puzzle as proof of work. In this article, we provide a comprehensive study of the most important puzzle construction schemes available in the literature, categorizing them according to several attributes, such as resource type, verification type, and applications. We have redefined the term puzzle by collecting and integrating the scattered notions used in different works, to cover all the existing applications. Moreover, we provide an overview of the possible applications, identifying key requirements and different design approaches. Finally, we highlight the features and limitations of each approach, providing a useful guide for the future development of new puzzle schemes.",10.1145/3396374
10.1145/3397022,"Paving the Way for NFV Acceleration: A Taxonomy, Survey and Future Directions","Fei, Xincai; Liu, Fangming; Zhang, Qixia; Jin, Hai; Hu, Hongxin",2020,ACM Comput. Surv.,"As a recent innovation, network functions virtualization (NFV)—with its core concept of replacing hardware middleboxes with software network functions (NFs) implemented in commodity servers—promises cost savings and flexibility benefits. However, transitioning NFs from special-purpose hardware to commodity servers has turned out to be more challenging than expected, as it inevitably incurs performance penalties due to bottlenecks in both software and hardware. To achieve performance comparable to hardware middleboxes, there is a strong demand for a speedup in NF processing, which plays a crucial role in the success of NFV. In this article, we study the performance challenges that exist in general-purpose servers and simultaneously summarize the typical performance bottlenecks in NFV. Through reviewing the progress in the field of NFV acceleration, we present a new taxonomy of the state-of-the-art efforts according to various acceleration approaches. We discuss the surveyed works and identify the respective advantages and disadvantages in each category. We then discuss the products, solutions, and projects emerged in industry. We also present a gap analysis to improve current solutions and highlight promising research trends that can be explored in the future.",10.1145/3397022
10.1145/3397160,Incentive-Driven Computation Offloading in Blockchain-Enabled E-Commerce,"Deng, Shuiguang; Cheng, Guanjie; Zhao, Hailiang; Gao, Honghao; Yin, Jianwei",2020,ACM Trans. Internet Technol.,"Blockchain is regarded as one of the most promising technologies to upgrade e-commerce. This article analyzes the challenges that current e-commerce is facing and introduces a new scenario of e-commerce enabled by blockchain. A framework is proposed for mining tasks in this scenario offloaded onto edge servers based on mobile edge computing. Then, the offloading issue is modeled as a multi-constrained optimization problem, and evolutionary algorithms are utilized and re-designed as solvers. The experimental results validate the efficiency of the framework and algorithms and also show that the lower bound of computation resources exists to obtain the maximum overall revenue.",10.1145/3397160
10.1145/3397495,A Survey of Multitier Programming,"Weisenburger, Pascal; Wirth, Johannes; Salvaneschi, Guido",2020,ACM Comput. Surv.,"Multitier programming deals with developing the components that pertain to different tiers in the system (e.g., client and server), mixing them in the same compilation unit. In this paradigm, the code for different tiers is then either generated at run time or it results from the compiler splitting the codebase into components that belong to different tiers based on user annotations, static analysis, types, or a combination of these. In the Web context, multitier languages aim at reducing the distinction between client and server code, by translating the code that is to be executed on the clients to JavaScript or by executing JavaScript on the server, too. Ultimately, the goal of the multitier approach is to improve program comprehension, simplify maintenance and enable formal reasoning about the properties of the whole distributed application.A number of multitier research languages have been proposed over the last decade, which support various degrees of multitier programming and explore different design tradeoffs. In this article, we provide an overview of the existing solutions, discuss their positioning in the design space, and outline open research problems.",10.1145/3397495
10.1145/3400031,Parallel Genetic Algorithms: A Useful Survey,"Harada, Tomohiro; Alba, Enrique",2020,ACM Comput. Surv.,"In this article, we encompass an analysis of the recent advances in parallel genetic algorithms (PGAs). We have selected these algorithms because of the deep interest in many research fields for techniques that can face complex applications where running times and other computational resources are greedily consumed by present solvers, and PGAs act then as efficient procedures that fully use modern computational platforms at the same time that allow the resolution of cutting-edge open problems. We have faced this survey on PGAs with the aim of helping newcomers or busy researchers who want to have a wide vision on the field. Then, we discuss the most well-known models and their implementations from a recent (last six years) and useful point of view: We discuss on highly cited articles, keywords, the venues where they can be found, a very comprehensive (and new) taxonomy covering different research domains involved in PGAs, and a set of recent applications. We also introduce a new vision on open challenges and try to give hints that guide practitioners and specialized researchers. Our conclusion is that there are many advantages to using these techniques and lots of potential interactions to other evolutionary algorithms; as well, we contribute to creating a body of knowledge in PGAs by summarizing them in a structured way, so the reader can find this article useful for practical research, graduate teaching, and as a pedagogical guide to this exciting domain.",10.1145/3400031
10.1145/3408314,Big Data Systems: A Software Engineering Perspective,"Davoudian, Ali; Liu, Mengchi",2020,ACM Comput. Surv.,"Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.",10.1145/3408314
10.1145/3412378,An Empirical Study of Developer Discussions in the Gitter Platform,"Ehsan, Osama; Hassan, Safwat; Mezouar, Mariam El; Zou, Ying",2021,ACM Trans. Softw. Eng. Methodol.,"Developer chatrooms (e.g., the Gitter platform) are gaining popularity as a communication channel among developers. In developer chatrooms, a developer (asker) posts questions and other developers (respondents) respond to the posted questions. The interaction between askers and respondents results in a discussion thread. Recent studies show that developers use chatrooms to inquire about issues, discuss development ideas, and help each other. However, prior work focuses mainly on analyzing individual messages of a chatroom without analyzing the discussion thread in a chatroom. Developer chatroom discussions are context-sensitive, entangled, and include multiple participants that make it hard to accurately identify threads. Therefore, prior work has limited capability to show the interactions among developers within a chatroom by analyzing only individual messages.In this article, we perform an in-depth analysis of the Gitter platform (i.e., developer chatrooms) by analyzing 6,605,248 messages of 709 chatrooms. To analyze the characteristics of the posted questions and the impact on the response behavior (e.g., whether the posted questions get responses), we propose an approach that identifies discussion threads in chatrooms with high precision (i.e., 0.81 F-score). Our results show that inactive members responded more often and unique questions take longer discussion time than simple questions. We also find that clear and concise questions are more likely to be responded to than poorly written questions.We further manually analyze a randomly selected sample of 384 threads to examine how respondents resolve the raised questions. We observe that more than 80\% of the studied threads are resolved. Advanced-level/beginner-level questions along with the edited questions are the mostly resolved questions. Our results can help the project maintainers understand the nature of the discussion threads (e.g., the topic trends). Project maintainers can also benefit from our thread identification approach to spot the common repeated threads and use these threads as frequently asked questions (FAQs) to improve the documentation of their projects.",10.1145/3412378
10.1145/3412381,Automated Orchestration of Online Educational Collaboration in Cloud-based Environments,"Czekierda, \L{}ukasz; Zieli\'{n}ski, Krzysztof; Zieli\'{n}ski, S\l{}awomir",2021,ACM Trans. Multimedia Comput. Commun. Appl.,"Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Ma\l{}opolska Educational Cloud (MEC) project conducted by AGH University and its partners.The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management.In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns.The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.",10.1145/3412381
10.1145/3415580,"Cost-effective, Energy-efficient, and Scalable Storage Computing for Large-scale AI Applications","Do, Jaeyoung; Ferreira, Victor C.; Bobarshad, Hossein; Torabzadehkashi, Mahdi; Rezaei, Siavash; Heydarigorji, Ali; Souza, Diego; Goldstein, Brunno F.; Santiago, Leandro; Kim, Min Soo; Lima, Priscila M. V.; Fran\c{c}a, Felipe M. G.; Alves, Vladimir",2020,ACM Trans. Storage,"The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.",10.1145/3415580
10.1145/3418899,A Black-box Monitoring Approach to Measure Microservices Runtime Performance,"Brondolin, Rolando; Santambrogio, Marco D.",2020,ACM Trans. Archit. Code Optim.,"Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.",10.1145/3418899
10.1145/3425866,Operating Systems for Resource-adaptive Intelligent Software: Challenges and Opportunities,"Liu, Xuanzhe; Wang, Shangguang; Ma, Yun; Zhang, Ying; Mei, Qiaozhu; Liu, Yunxin; Huang, Gang",2021,ACM Trans. Internet Technol.,"The past decades witnessed the fast and wide deployment of Internet. The Internet has bred the ubiquitous computing environment that is spanning the cloud, edge, mobile devices, and IoT. Software running over such a ubiquitous computing environment environment is eating the world. A recently emerging trend of Internet-based software systems is “resource adaptive,” i.e., software systems should be robust and intelligent enough to the changes of heterogeneous resources, both physical and logical, provided by their running environment. To keep pace of such a trend, we argue that some considerations should be taken into account for the future operating system design and implementation. From the structural perspective, rather than the “monolithic OS” that manages the aggregated resources on the single machine, the OS should be dynamically composed over the distributed resources and flexibly adapt to the resource and environment changes. Meanwhile, the OS should leverage advanced machine/deep learning techniques to derive configurations and policies and automatically learn to tune itself and schedule resources. This article envisions our recent thinking of the new OS abstraction, namely, ServiceOS, for future resource-adaptive intelligent software systems. The idea of ServiceOS is inspired by the delivery model of “Software-as-a-Service” that is supported by the Service-Oriented Architecture (SOA). The key principle of ServiceOS is based on resource disaggregation, resource provisioning as a service, and learning-based resource scheduling and allocation. The major goal of this article is not providing an immediately deployable OS. Instead, we aim to summarize the challenges and potentially promising opportunities and try to provide some practical implications for researchers and practitioners.",10.1145/3425866
10.1145/3434178,Opportunistic Collective Experiences: Identifying Shared Situations and Structuring Shared Activities at Distance,"Louie, Ryan; Garg, Kapil; Werner, Jennie; Sun, Allison; Gergle, Darren; Zhang, Haoqi",2021,Proc. ACM Hum.-Comput. Interact.,"Despite many available social technologies for connecting at a distance, we don't always find opportunities to actively engage in shared experiences and activities with friends and loved ones, even though this kind of interaction is associated with increased social closeness. To better support active engagement in shared experiences and activities while also making it convenient to find opportunities for interacting in this way, our work explores the design of Opportunistic Collective Experiences (OCEs), or social experiences powered by computer programs that identify opportune moments when users share situations across distance and structure shared activities in those situations. To support interacting with, programming, and executing OCEs, we developed Cerebro, a computational platform that consists of a mobile app that supports users? social interaction, an API for expressing the situations and activities that make up the interactional opportunity, and an opportunistic execution engine that checks for interactional opportunities and executes them when possible. Through a 20 day deployment study tested with groups of geographically-distributed college alumni (N=21), we found that OCEs promoted opportunities for active engagement; facilitated interactions that were socially connecting by structuring ways to engage in shared experiences and activities; and made actively engaging easier by identifying situations appropriate for interacting and structuring how to engage in activities in these situations. We contribute to CSCW (1) a novel interaction that facilitates engaging in shared experiences and activities at distance during coincidental moments; and (2) the design of systems to interact with, program, and execute these kinds of interactions.",10.1145/3434178
10.1145/3439769,Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites,"Uddin, Gias; Khomh, Foutse; Roy, Chanchal K.",2021,ACM Trans. Softw. Eng. Methodol.,"The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80\% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.",10.1145/3439769
10.1145/3444692,A Survey on Edge Performance Benchmarking,"Varghese, Blesson; Wang, Nan; Bermbach, David; Hong, Cheol-Ho; Lara, Eyal De; Shi, Weisong; Stewart, Christopher",2021,ACM Comput. Surv.,"Edge computing is the next Internet frontier that will leverage computing resources located near users, sensors, and data stores to provide more responsive services. Therefore, it is envisioned that a large-scale, geographically dispersed, and resource-rich distributed system will emerge and play a key role in the future Internet. However, given the loosely coupled nature of such complex systems, their operational conditions are expected to change significantly over time. In this context, the performance characteristics of such systems will need to be captured rapidly, which is referred to as performance benchmarking, for application deployment, resource orchestration, and adaptive decision-making. Edge performance benchmarking is a nascent research avenue that has started gaining momentum over the past five years. This article first reviews articles published over the past three decades to trace the history of performance benchmarking from tightly coupled to loosely coupled systems. It then systematically classifies previous research to identify the system under test, techniques analyzed, and benchmark runtime in edge performance benchmarking.",10.1145/3444692
10.1145/3447868,"The Programmable Data Plane: Abstractions, Architectures, Algorithms, and Applications","Michel, Oliver; Bifulco, Roberto; R\'{e}tv\'{a}ri, G\'{a}bor; Schmid, Stefan",2021,ACM Comput. Surv.,"Programmable data plane technologies enable the systematic reconfiguration of the low-level processing steps applied to network packets and are key drivers toward realizing the next generation of network services and applications. This survey presents recent trends and issues in the design and implementation of programmable network devices, focusing on prominent abstractions, architectures, algorithms, and applications proposed, debated, and realized over the past years. We elaborate on the trends that led to the emergence of this technology and highlight the most important pointers from the literature, casting different taxonomies for the field, and identifying avenues for future research.",10.1145/3447868
10.1145/3448976,A Survey of Software Log Instrumentation,"Chen, Boyuan; Jiang, Zhen Ming (Jack)",2021,ACM Comput. Surv.,"Log messages have been used widely in many software systems for a variety of purposes during software development and field operation. There are two phases in software logging: log instrumentation and log management. Log instrumentation refers to the practice that developers insert logging code into source code to record runtime information. Log management refers to the practice that operators collect the generated log messages and conduct data analysis techniques to provide valuable insights of runtime behavior. There are many open source and commercial log management tools available. However, their effectiveness highly depends on the quality of the instrumented logging code, as log messages generated by high-quality logging code can greatly ease the process of various log analysis tasks (e.g., monitoring, failure diagnosis, and auditing). Hence, in this article, we conducted a systematic survey on state-of-the-art research on log instrumentation by studying 69 papers between 1997 and 2019. In particular, we have focused on the challenges and proposed solutions used in the three steps of log instrumentation: (1) logging approach; (2) logging utility integration; and (3) logging code composition. This survey will be useful to DevOps practitioners and researchers who are interested in software logging.",10.1145/3448976
10.1145/3456630,On the Use of Intelligent Models towards Meeting the Challenges of the Edge Mesh,"Oikonomou, Panagiotis; Karanika, Anna; Anagnostopoulos, Christos; Kolomvatsos, Kostas",2021,ACM Comput. Surv.,"Nowadays, we are witnessing the advent of the Internet of Things (IoT) with numerous devices performing interactions between them or with their environment. The huge number of devices leads to huge volumes of data that demand the appropriate processing. The “legacy” approach is to rely on Cloud where increased computational resources can realize any desired processing. However, the need for supporting real-time applications requires a reduced latency in the provision of outcomes. Edge Computing (EC) comes as the “solver” of the latency problem. Various processing activities can be performed at EC nodes having direct connection with IoT devices. A number of challenges should be met before we conclude a fully automated ecosystem where nodes can cooperate or understand their status to efficiently serve applications. In this article, we perform a survey of the relevant research activities towards the vision of Edge Mesh (EM), i.e., a “cover” of intelligence upon the EC. We present the necessary hardware and discuss research outcomes in every aspect of EC/EM nodes functioning. We present technologies and theories adopted for data, tasks, and resource management while discussing how machine learning and optimization can be adopted in the domain.",10.1145/3456630
10.1145/3460013,SWARM: Adaptive Load Balancing in Distributed Streaming Systems for Big Spatial Data,"Daghistani, Anas; Aref, Walid G.; Ghafoor, Arif; Mahmood, Ahmed R.",2021,ACM Trans. Spatial Algorithms Syst.,"The proliferation of GPS-enabled devices has led to the development of numerous location-based services. These services need to process massive amounts of streamed spatial data in real-time. The current scale of spatial data cannot be handled using centralized systems. This has led to the development of distributed spatial streaming systems. Existing systems are using static spatial partitioning to distribute the workload. In contrast, the real-time streamed spatial data follows non-uniform spatial distributions that are continuously changing over time. Distributed spatial streaming systems need to react to the changes in the distribution of spatial data and queries. This article introduces SWARM, a lightweight adaptivity protocol that continuously monitors the data and query workloads across the distributed processes of the spatial data streaming system and redistributes and rebalances the workloads as soon as performance bottlenecks get detected. SWARM is able to handle multiple query-execution and data-persistence models. A distributed streaming system can directly use SWARM to adaptively rebalance the system’s workload among its machines with minimal changes to the original code of the underlying spatial application. Extensive experimental evaluation using real and synthetic datasets illustrate that, on average, SWARM achieves 2 improvement in throughput over a static grid partitioning that is determined based on observing a limited history of the data and query workloads. Moreover, SWARM reduces execution latency on average 4 compared with the other technique.",10.1145/3460013
10.1145/3460345,A Survey on Automated Log Analysis for Reliability Engineering,"He, Shilin; He, Pinjia; Chen, Zhuangbin; Yang, Tianyi; Su, Yuxin; Lyu, Michael R.",2021,ACM Comput. Surv.,"Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems, because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits and datasets. Based on the discussion of the recent advances, we present several promising future directions toward real-world and next-generation automated log analysis.",10.1145/3460345
10.1145/3460818,Cloud-based Network Virtualization in IoT with OpenStack,"Benomar, Zakaria; Longo, Francesco; Merlino, Giovanni; Puliafito, Antonio",2021,ACM Trans. Internet Technol.,"In Cloud computing deployments, specifically in the Infrastructure-as-a-Service (IaaS) model, networking is one of the core enabling facilities provided for the users. The IaaS approach ensures significant flexibility and manageability, since the networking resources and topologies are entirely under users’ control. In this context, considerable efforts have been devoted to promoting the Cloud paradigm as a suitable solution for managing IoT environments. Deep and genuine integration between the two ecosystems, Cloud and IoT, may only be attainable at the IaaS level. In light of extending the IoT domain capabilities’ with Cloud-based mechanisms akin to the IaaS Cloud model, network virtualization is a fundamental enabler of infrastructure-oriented IoT deployments. Indeed, an IoT deployment without networking resilience and adaptability makes it unsuitable to meet user-level demands and services’ requirements. Such a limitation makes the IoT-based services adopted in very specific and statically defined scenarios, thus leading to limited plurality and diversity of use cases. This article presents a Cloud-based approach for network virtualization in an IoT context using the de-facto standard IaaS middleware, OpenStack, and its networking subsystem, Neutron. OpenStack is being extended to enable the instantiation of virtual/overlay networks between Cloud-based instances (e.g., virtual machines, containers, and bare metal servers) and/or geographically distributed IoT nodes deployed at the network edge.",10.1145/3460818
10.1145/3461011,Software Architectural Migration: An Automated Planning Approach,"Chondamrongkul, Nacha; Sun, Jing; Warren, Ian",2021,ACM Trans. Softw. Eng. Methodol.,"Software architectural designs are usually changed over time to support emerging technologies and to adhere to new principles. Architectural migration is an important activity that helps to transform the architectural styles applied during a system’s design with the result of modernising the system. If not performed correctly, this process could lead to potential system failures. This article presents an automated approach to refactoring architectural design and to planning the evolution process. With our solution, the architectural design can be refactored, ensuring that system functionality is preserved. Furthermore, the architectural migration process allows the system to be safely and incrementally transformed. We have evaluated our approach with five real-world software applications. The results prove the effectiveness of our approach and identify factors that impact the performance of architectural verification and migration planning. An interesting finding is that planning algorithms generate migration plans that differ in term of their relative efficiency.",10.1145/3461011
10.1145/3464305,Evaluation of Software Architectures under Uncertainty: A Systematic Literature Review,"Sobhy, Dalia; Bahsoon, Rami; Minku, Leandro; Kazman, Rick",2021,ACM Trans. Softw. Eng. Methodol.,"Context: Evaluating software architectures in uncertain environments raises new challenges, which require continuous approaches. We define continuous evaluation as multiple evaluations of the software architecture that begins at the early stages of the development and is periodically and repeatedly performed throughout the lifetime of the software system. Numerous approaches have been developed for continuous evaluation; to handle dynamics and uncertainties at run-time, over the past years, these approaches are still very few, limited, and lack maturity. Objective: This review surveys efforts on architecture evaluation and provides a unified terminology and perspective on the subject. Method: We conducted a systematic literature review to identify and analyse architecture evaluation approaches for uncertainty including continuous and non-continuous, covering work published between 1990–2020. We examined each approach and provided a classification framework for this field. We present an analysis of the results and provide insights regarding open challenges. Major results and conclusions: The survey reveals that most of the existing architecture evaluation approaches typically lack an explicit linkage between design-time and run-time. Additionally, there is a general lack of systematic approaches on how continuous architecture evaluation can be realised or conducted. To remedy this lack, we present a set of necessary requirements for continuous evaluation and describe some examples.",10.1145/3464305
10.1145/3464940,Adaptive Hypermutation for Search-Based System Test Generation: A Study on REST APIs with EvoMaster,"Zhang, Man; Arcuri, Andrea",2021,ACM Trans. Softw. Eng. Methodol.,"REST web services are widely popular in industry, and search techniques have been successfully used to automatically generate system-level test cases for those systems. In this article, we propose a novel mutation operator which is designed specifically for test generation at system-level, with a particular focus on REST APIs. In REST API testing, and often in system testing in general, an individual can have a long and complex chromosome. Furthermore, there are two specific issues: (1) fitness evaluation in system testing is highly costly compared with the number of objectives (e.g., testing targets) to optimize for; and (2) a large part of the genotype might have no impact on the phenotype of the individuals (e.g., input data that has no impact on the execution flow in the tested program). Due to these issues, it might be not suitable to apply a typical low mutation rate like 1/n (where n is the number of genes in an individual), which would lead to mutating only one gene on average. Therefore, in this article, we propose an adaptive weight-based hypermutation, which is aware of the different characteristics of the mutated genes. We developed adaptive strategies that enable the selection and mutation of genes adaptively based on their fitness impact and mutation history throughout the search. To assess our novel proposed mutation operator, we implemented it in the EvoMaster tool, integrated in the MIO algorithm, and further conducted an empirical study with three artificial REST APIs and four real-world REST APIs. Results show that our novel mutation operator demonstrates noticeable improvements over the default MIO. It provides a significant improvement in performance for six out of the seven case studies, where the relative improvement is up to +12.09\% for target coverage, +12.69\% for line coverage, and +32.51\% for branch coverage.",10.1145/3464940
10.1145/3465630,Optimizing the Performance of Containerized Cloud Software Systems Using Adaptive PID Controllers,"Sabuhi, Mikael; Mahmoudi, Nima; Khazaei, Hamzeh",2021,ACM Trans. Auton. Adapt. Syst.,"Control theory has proven to be a practical approach for the design and implementation of controllers, which does not inherit the problems of non-control theoretic controllers due to its strong mathematical background. State-of-the-art auto-scaling controllers suffer from one or more of the following limitations: (1) lack of a reliable performance model, (2) using a performance model with low scalability, tractability, or fidelity, (3) being application- or architecture-specific leading to low extendability, and (4) no guarantee on their efficiency. Consequently, in this article, we strive to mitigate these problems by leveraging an adaptive controller, which is composed of a neural network as the performance model and a Proportional-Integral-Derivative (PID) controller as the scaling engine. More specifically, we design, implement, and analyze different flavours of these adaptive and non-adaptive controllers, and we compare and contrast them against each other to find the most suitable one for managing containerized cloud software systems at runtime. The controller’s objective is to maintain the response time of the controlled software system in a pre-defined range, and meeting the Service-level Agreements, while leading to efficient resource provisioning.",10.1145/3465630
10.1145/3466696,Autonomic Security Management for IoT Smart Spaces,"Lin, Changyuan; Khazaei, Hamzeh; Walenstein, Andrew; Malton, Andrew",2021,ACM Trans. Internet Things,"Embedded sensors and smart devices have turned the environments around us into smart spaces that could automatically evolve, depending on the needs of users, and adapt to the new conditions. While smart spaces are beneficial and desired in many aspects, they could be compromised and expose privacy, security, or render the whole environment a hostile space in which regular tasks cannot be accomplished anymore. In fact, ensuring the security of smart spaces is a very challenging task due to the heterogeneity of devices, vast attack surface, and device resource limitations. The key objective of this study is to minimize the manual work in enforcing the security of smart spaces by leveraging the autonomic computing paradigm in the management of IoT environments. More specifically, we strive to build an autonomic manager that can monitor the smart space continuously, analyze the context, plan and execute countermeasures to maintain the desired level of security, and reduce liability and risks of security breaches. We follow the microservice architecture pattern and propose a generic ontology named Secure Smart Space Ontology (SSSO) for describing dynamic contextual information in security-enhanced smart spaces. Based on SSSO, we build an autonomic security manager with four layers that continuously monitors the managed spaces, analyzes contextual information and events, and automatically plans and implements adaptive security policies.As the evaluation, focusing on a current BlackBerry customer problem, we deployed the proposed autonomic security manager to maintain the security of a smart conference room with 32 devices and 66 services. The high performance of the proposed solution was also evaluated on a large-scale deployment with over 1.8 million triples.",10.1145/3466696
10.1145/3468521,A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter,"Yang, Juncheng; Yue, Yao; Rashmi, K. V.",2021,ACM Trans. Storage,"Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.",10.1145/3468521
10.1145/3469440,Applying Machine Learning in Self-adaptive Systems: A Systematic Literature Review,"Gheibi, Omid; Weyns, Danny; Quin, Federico",2021,ACM Trans. Auton. Adapt. Syst.,"Recently, we have been witnessing a rapid increase in the use of machine learning techniques in self-adaptive systems. Machine learning has been used for a variety of reasons, ranging from learning a model of the environment of a system during operation to filtering large sets of possible configurations before analyzing them. While a body of work on the use of machine learning in self-adaptive systems exists, there is currently no systematic overview of this area. Such an overview is important for researchers to understand the state of the art and direct future research efforts. This article reports the results of a systematic literature review that aims at providing such an overview. We focus on self-adaptive systems that are based on a traditional Monitor-Analyze-Plan-Execute (MAPE)-based feedback loop. The research questions are centered on the problems that motivate the use of machine learning in self-adaptive systems, the key engineering aspects of learning in self-adaptation, and open challenges in this area. The search resulted in 6,709 papers, of which 109 were retained for data collection. Analysis of the collected data shows that machine learning is mostly used for updating adaptation rules and policies to improve system qualities, and managing resources to better balance qualities and resources. These problems are primarily solved using supervised and interactive learning with classification, regression, and reinforcement learning as the dominant methods. Surprisingly, unsupervised learning that naturally fits automation is only applied in a small number of studies. Key open challenges in this area include the performance of learning, managing the effects of learning, and dealing with more complex types of goals. From the insights derived from this systematic literature review, we outline an initial design process for applying machine learning in self-adaptive systems that are based on MAPE feedback loops.",10.1145/3469440
10.1145/3470658,Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability,"Avasalcai, Cosmin; Tsigkanos, Christos; Dustdar, Schahram",2021,ACM Trans. Internet Technol.,"Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.",10.1145/3470658
10.1145/3472753,A Survey on Data-driven Network Intrusion Detection,"Chou, Dylan; Jiang, Meng",2021,ACM Comput. Surv.,"Data-driven network intrusion detection (NID) has a tendency towards minority attack classes compared to normal traffic. Many datasets are collected in simulated environments rather than real-world networks. These challenges undermine the performance of intrusion detection machine learning models by fitting machine learning models to unrepresentative “sandbox” datasets. This survey presents a taxonomy with eight main challenges and explores common datasets from 1999 to 2020. Trends are analyzed on the challenges in the past decade and future directions are proposed on expanding NID into cloud-based environments, devising scalable models for large network data, and creating labeled datasets collected in real-world networks.",10.1145/3472753
10.1145/3472958,BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications,"Damiani, Andrea; Fiscaletti, Giorgia; Bacis, Marco; Brondolin, Rolando; Santambrogio, Marco D.",2022,ACM Trans. Reconfigurable Technol. Syst.,"“Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.",10.1145/3472958
10.1145/3473575,Of JavaScript AOT compilation performance,"Serrano, Manuel",2021,Proc. ACM Program. Lang.,"The fastest JavaScript production implementations use just-in-time (JIT) compilation and the vast majority of academic publications about implementations of dynamic languages published during the last two decades focus on JIT compilation. This does not imply that static compilers (AoT) cannot be competitive; as comparatively little effort has been spent creating fast AoT JavaScript compilers, a scientific comparison is lacking. This paper presents the design and implementation of an AoT JavaScript compiler, focusing on a performance analysis. The paper reports on two experiments, one based on standard JavaScript benchmark suites and one based on new benchmarks chosen for their diversity of styles, authors, sizes, provenance, and coverage of the language. The first experiment shows an advantage to JIT compilers, which is expected after the decades of effort that these compilers have paid to these very tests. The second shows more balanced results, as the AoT compiler generates programs that reach competitive speeds and that consume significantly less memory. The paper presents and evaluates techniques that we have either invented or adapted from other systems, to improve AoT JavaScript compilation.",10.1145/3473575
10.1145/3474554,A Survey on Privacy Preservation in Fog-Enabled Internet of Things,"Sarwar, Kinza; Yongchareon, Sira; Yu, Jian; Ur Rehman, Saeed",2021,ACM Comput. Surv.,"Despite the rapid growth and advancement in the Internet of Things (IoT), there are critical challenges that need to be addressed before the full adoption of the IoT. Data privacy is one of the hurdles towards the adoption of IoT as there might be potential misuse of users’ data and their identity in IoT applications. Several researchers have proposed different approaches to reduce privacy risks. However, most of the existing solutions still suffer from various drawbacks, such as huge bandwidth utilization and network latency, heavyweight cryptosystems, and policies that are applied on sensor devices and in the cloud. To address these issues, fog computing has been introduced for IoT network edges providing low latency, computation, and storage services. In this survey, we comprehensively review and classify privacy requirements for an in-depth understanding of privacy implications in IoT applications. Based on the classification, we highlight ongoing research efforts and limitations of the existing privacy-preservation techniques and map the existing IoT schemes with Fog-enabled IoT schemes to elaborate on the benefits and improvements that Fog-enabled IoT can bring to preserve data privacy in IoT applications. Lastly, we enumerate key research challenges and point out future research directions.",10.1145/3474554
10.1145/3475991,Horizontal Auto-Scaling for Multi-Access Edge Computing Using Safe Reinforcement Learning,"Ray, Kaustabha; Banerjee, Ansuman",2021,ACM Trans. Embed. Comput. Syst.,"Multi-Access Edge Computing (MEC) has emerged as a promising new paradigm allowing low latency access to services deployed on edge servers to avert network latencies often encountered in accessing cloud services. A key component of the MEC environment is an auto-scaling policy which is used to decide the overall management and scaling of container instances corresponding to individual services deployed on MEC servers to cater to traffic fluctuations. In this work, we propose a Safe Reinforcement Learning (RL)-based auto-scaling policy agent that can efficiently adapt to traffic variations to ensure adherence to service specific latency requirements. We model the MEC environment using a Markov Decision Process (MDP). We demonstrate how latency requirements can be formally expressed in Linear Temporal Logic (LTL). The LTL specification acts as a guide to the policy agent to automatically learn auto-scaling decisions that maximize the probability of satisfying the LTL formula. We introduce a quantitative reward mechanism based on the LTL formula to tailor service specific latency requirements. We prove that our reward mechanism ensures convergence of standard Safe-RL approaches. We present experimental results in practical scenarios on a test-bed setup with real-world benchmark applications to show the effectiveness of our approach in comparison to other state-of-the-art methods in literature. Furthermore, we perform extensive simulated experiments to demonstrate the effectiveness of our approach in large scale scenarios.",10.1145/3475991
10.1145/3476035,"An Archive of Interfaces: Exploring the Potential of Emulation for Software Research, Pedagogy, and Design","Cardoso-Llach, Daniel; Kaltman, Eric; Erdolu, Emek; Furste, Zachary",2021,Proc. ACM Hum.-Comput. Interact.,"This paper explores the potential of distributed emulation networks to support research and pedagogy into historical and sociotechnical aspects of software. Emulation is a type of virtualization that re-creates the conditions for a piece of legacy software to operate on a modern system. The paper first offers a review of Computer-Supported Cooperative Work (CSCW), Human-Computer Interaction (HCI), and Science and Technology Studies (STS) literature engaging with software as historical and sociotechnical artifacts, and with emulation as a vehicle of scholarly inquiry. It then documents the novel use of software emulations as a pedagogical resource and research tool for legacy software systems analysis. This is accomplished through the integration of the Emulation as a Service Infrastructure (EaaSI) distributed emulation network into a university-level course focusing on computer-aided design (CAD). The paper offers a detailed case study of a pedagogical experience oriented to incorporate emulations into software research and learning. It shows how emulations allow for close, user-centered analyses of software systems that highlight both their historical evolution and core interaction concepts, and how they shape the work practices of their users.",10.1145/3476035
10.1145/3476513,Underwater Sensor Multi-Parameter Scheduling for Heterogenous Computing Nodes,"Elhoseny, Mohamed; Lakhan, Abdullah; Rashid, Ahmed; Mohammed, Mazin; Abdulkareem, Karrar",2022,ACM Trans. Sen. Netw.,"Sensor-aware distributed workflow applications are becoming increasingly popular underwater. The apps are marine operations that generate data and process it based on its characteristics. Mobile-fog-cloud paradigms, as well as computing such as sensor nodes, have emerged. As previously stated, the nodes can be combined into a single system to achieve several goals. Many factors are considered, including network contents, workload fluctuation, variable execution durations, deadlines, and bandwidth. As a result, scheduling mobile workflow systems with multiple parameters might be challenging. The study suggests a novel content-efficient decision-aware task scheduling (CATSA) method for defining and adapting to complicated environmental changes. The CATSA consists of several components that work together to perform various benchmarks in the system, including a decision planner, sequencing, and scheduling. As evidenced by test findings during evaluation, the suggested architecture outperforms current studies regarding workflow execution quality of services and improved the makespan 30\% and deadline meeting 40\% in the study.",10.1145/3476513
10.1145/3477271,Enhancing Search-based Testing with Testability Transformations for Existing APIs,"Arcuri, Andrea; Galeotti, Juan P.",2021,ACM Trans. Softw. Eng. Methodol.,"Search-based software testing (SBST) has been shown to be an effective technique to generate test cases automatically. Its effectiveness strongly depends on the guidance of the fitness function. Unfortunately, a common issue in SBST is the so-called flag problem, where the fitness landscape presents a plateau that provides no guidance to the search. In this article, we provide a series of novel testability transformations aimed at providing guidance in the context of commonly used API calls (e.g., strings that need to be converted into valid date/time objects). We also provide specific transformations aimed at helping the testing of REST Web Services. We implemented our novel techniques as an extension to EvoMaster, an SBST tool that generates system-level test cases. Experiments on nine open-source REST web services, as well as an industrial web service, show that our novel techniques improve performance significantly.",10.1145/3477271
10.1145/3478680,"Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities","Siqueira, Frank; Davis, Joseph G.",2021,ACM Comput. Surv.,"Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.",10.1145/3478680
10.1145/3483424,A Survey of AIOps Methods for Failure Management,"Notaro, Paolo; Cardoso, Jorge; Gerndt, Michael",2021,ACM Trans. Intell. Syst. Technol.,"Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.",10.1145/3483424
10.1145/3485474,Much ADO about failures: a fault-aware model for compositional verification of strongly consistent distributed systems,"Honor\'{e}, Wolf; Kim, Jieung; Shin, Ji-Yong; Shao, Zhong",2021,Proc. ACM Program. Lang.,"Despite recent advances, guaranteeing the correctness of large-scale distributed applications without compromising performance remains a challenging problem. Network and node failures are inevitable and, for some applications, careful control over how they are handled is essential. Unfortunately, existing approaches either completely hide these failures behind an atomic state machine replication (SMR) interface, or expose all of the network-level details, sacrificing atomicity. We propose a novel, compositional, atomic distributed object (ADO) model for strongly consistent distributed systems that combines the best of both options. The object-oriented API abstracts over protocol-specific details and decouples high-level correctness reasoning from implementation choices. At the same time, it intentionally exposes an abstract view of certain key distributed failure cases, thus allowing for more fine-grained control over them than SMR-like models. We demonstrate that proving properties even of composite distributed systems can be straightforward with our Coq verification framework, Advert, thanks to the ADO model. We also show that a variety of common protocols including multi-Paxos and Chain Replication refine the ADO semantics, which allows one to freely choose among them for an application's implementation without modifying ADO-level correctness proofs.",10.1145/3485474
10.1145/3485510,Durable functions: semantics for stateful serverless,"Burckhardt, Sebastian; Gillum, Chris; Justo, David; Kallas, Konstantinos; McMahon, Connor; Meiklejohn, Christopher S.",2021,Proc. ACM Program. Lang.,"Serverless, or Functions-as-a-Service (FaaS), is an increasingly popular paradigm for application development, as it provides implicit elastic scaling and load based billing. However, the weak execution guarantees and intrinsic compute-storage separation of FaaS create serious challenges when developing applications that require persistent state, reliable progress, or synchronization. This has motivated a new generation of serverless frameworks that provide stateful abstractions. For instance, Azure's Durable Functions (DF) programming model enhances FaaS with actors, workflows, and critical sections. As a programming model, DF is interesting because it combines task and actor parallelism, which makes it suitable for a wide range of serverless applications. We describe DF both informally, using examples, and formally, using an idealized high-level model based on the untyped lambda calculus. Next, we demystify how the DF runtime can (1) execute in a distributed unreliable serverless environment with compute-storage separation, yet still conform to the fault-free high-level model, and (2) persist execution progress without requiring checkpointing support by the language runtime. To this end we define two progressively more complex execution models, which contain the compute-storage separation and the record-replay, and prove that they are equivalent to the high-level model.",10.1145/3485510
10.1145/3487043,Software Engineering for AI-Based Systems: A Survey,"Mart\'{\i}nez-Fern\'{a}ndez, Silverio; Bogner, Justus; Franch, Xavier; Oriol, Marc; Siebert, Julien; Trendowicz, Adam; Vollmer, Anna Maria; Wagner, Stefan",2022,ACM Trans. Softw. Eng. Methodol.,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",10.1145/3487043
10.1145/3487292,AT-DIFC+: Toward Adaptive and Trust-Aware Decentralized Information Flow Control,"Skandylas, Charilaos; Khakpour, Narges; Andersson, Jesper",2021,ACM Trans. Auton. Adapt. Syst.,"Modern software systems and their corresponding architectures are increasingly decentralized, distributed, and dynamic. As a consequence, decentralized mechanisms are required to ensure security in such architectures. Decentralized Information Flow Control (DIFC) is a mechanism to control information flow in distributed systems. This article presents and discusses several improvements to an adaptive decentralized information flow approach that incorporates trust for decentralized systems to provide security. Adaptive Trust-Aware Decentralized Information Flow (AT-DIFC+) combines decentralized information flow control mechanisms, trust-based methods, and decentralized control architectures to control and enforce information flow in an open, decentralized system. We strengthen our approach against newly discovered attacks and provide additional information about its reconfiguration, decentralized control architectures, and reference implementation. We evaluate the effectiveness and performance of AT-DIFC+ on two case studies and perform additional experiments and to gauge the mitigations’ effectiveness against the identified attacks.",10.1145/3487292
10.1145/3488247,A Systematic Review on Osmotic Computing,"Neha, Benazir; Panda, Sanjaya Kumar; Sahu, Pradip Kumar; Sahoo, Kshira Sagar; Gandomi, Amir H.",2022,ACM Trans. Internet Things,"Osmotic computing in association with related computing paradigms (cloud, fog, and edge) emerges as a promising solution for handling bulk of security-critical as well as latency-sensitive data generated by the digital devices. It is a growing research domain that studies deployment, migration, and optimization of applications in the form of microservices across cloud/edge infrastructure. It presents dynamically tailored microservices in technology-centric environments by exploiting edge and cloud platforms. Osmotic computing promotes digital transformation and furnishes benefits to transportation, smart cities, education, and healthcare. In this article, we present a comprehensive analysis of osmotic computing through a systematic literature review approach. To ensure high-quality review, we conduct an advanced search on numerous digital libraries to extracting related studies. The advanced search strategy identifies 99 studies, from which 29 relevant studies are selected for a thorough review. We present a summary of applications in osmotic computing build on their key features. On the basis of the observations, we outline the research challenges for the applications in this research field. Finally, we discuss the security issues resolved and unresolved in osmotic computing.",10.1145/3488247
10.1145/3488585,Fog Computing Platforms for Smart City Applications: A Survey,"Da Silva, Thiago Pereira; Batista, Thais; Lopes, Frederico; Neto, Aluizio Rocha; Delicato, Fl\'{a}via C.; Pires, Paulo F.; Da Rocha, Atslands R.",2022,ACM Trans. Internet Technol.,"Emerging IoT applications with stringent requirements on latency and data processing have posed many challenges to cloud-centric platforms for Smart Cities. Recently, Fog Computing has been advocated as a promising approach to support such new applications and handle the increasing volume of IoT data and devices. The Fog Computing paradigm is characterized by a horizontal system-level architecture where devices close to end-users and IoT devices are used for processing, storage, and networking functions. Fog Computing platforms aim to facilitate the development of applications and systems for Smart Cities by providing services and abstractions designed to integrate data from IoT devices and various information systems deployed in the city. Despite the potential of the Fog Computing paradigm, the literature still lacks a broad, comprehensive overview of what has been investigated on the use of such paradigm in platforms for Smart Cities and open issues to be addressed in future research and development. In this paper, a systematic mapping study was performed and we present a comprehensive understanding of the use of the Fog Computing paradigm in Smart Cities platforms, providing an overview of the current state of research on this topic, and identifying important gaps in the existing approaches and promising research directions.",10.1145/3488585
10.1145/3492762,Continuous and Proactive Software Architecture Evaluation: An IoT Case,"Sobhy, Dalia; Minku, Leandro; Bahsoon, Rami; Kazman, Rick",2022,ACM Trans. Softw. Eng. Methodol.,"Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.",10.1145/3492762
10.1145/3497807,NLUBroker: A QoE-driven Broker System for Natural Language Understanding Services,"Xu, Lanyu; Iyengar, Arun; Shi, Weisong",2022,ACM Trans. Internet Technol.,"Cloud-based Natural Language Understanding (NLU) services are becoming more popular with the development of artificial intelligence. More applications are integrated with cloud-based NLU services to enhance the way people communicate with machines. However, with NLU services provided by different companies powered by unrevealed AI technology, how to choose the best one is a problem for developers. Existing tools that can provide guidance to developers and make recommendations based on their needs are severely limited. This article comprehensively evaluates multiple state-of-the-art NLU services, and the results indicate that there is no absolute winner for different usage requirements. Motivated by this observation, we provide several insights and propose&nbsp;NLUBroker, a Quality of Experience-driven (QoE-driven) broker system, to select the proper service according to the environment. NLUBroker senses the client and service status and leverages a solution to the multi-armed bandit problem to conduct online learning, aiming to achieve maximum expected QoE. The performance of&nbsp;NLUBroker is evaluated in both simulation and real-world environments, and the evaluation results demonstrate that&nbsp;NLUBroker is an efficient solution for selecting NLU services. It is adaptive to changes in the environment, outperforms three baseline methods we evaluated and improves overall QoE up to 1.5\texttimes{} for the evaluated state-of-the-art NLU services.",10.1145/3497807
10.1145/3498336,Deployment Archetypes for Cloud Applications,"Berenberg, Anna; Calder, Brad",2022,ACM Comput. Surv.,"This is a survey article that explores six Cloud-based deployment archetypes for Cloud applications and the tradeoffs between them to achieve high availability, low end-user latency, and acceptable costs. These are (1) Zonal, (2) Regional, (3) Multi-regional, (4) Global, (5) Hybrid, and (6) Multi-cloud deployment archetypes. The goal is to classify cloud applications into a set of deployment archetypes and deployment models that tradeoff their needs around availability, latency, and geographical constraints with a focus on serving applications. This enables application owners to better examine the tradeoffs of each deployment model and what is needed for achieving the availability and latency goals for their application.",10.1145/3498336
10.1145/3498338,"Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects","Li, Huan; Lu, Hua; Jensen, Christian S.; Tang, Bo; Cheema, Muhammad Aamir",2022,ACM Comput. Surv.,"With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.",10.1145/3498338
10.1145/3501297,Anomaly Detection and Failure Root Cause Analysis in (Micro) Service-Based Cloud Applications: A Survey,"Soldani, Jacopo; Brogi, Antonio",2022,ACM Comput. Surv.,"The proliferation of services and service interactions within microservices and cloud-native applications, makes it harder to detect failures and to identify their possible root causes, which is, on the other hand crucial to promptly recover and fix applications. Various techniques have been proposed to promptly detect failures based on their symptoms, viz., observing anomalous behaviour in one or more application services, as well as to analyse logs or monitored performance of such services to determine the possible root causes for observed anomalies. The objective of this survey is to provide a structured overview and qualitative analysis of currently available techniques for anomaly detection and root cause analysis in modern multi-service applications. Some open challenges and research directions stemming out from the analysis are also discussed.",10.1145/3501297
10.1145/3501813,Federated Learning for Healthcare: Systematic Review and Architecture Proposal,"Antunes, Rodolfo Stoffel; Andr\'{e} da Costa, Cristiano; K\""{u}derle, Arne; Yari, Imrana Abdullahi; Eskofier, Bj\""{o}rn",2022,ACM Trans. Intell. Syst. Technol.,"The use of machine learning (ML) with electronic health records (EHR) is growing in popularity as a means to extract knowledge that can improve the decision-making process in healthcare. Such methods require training of high-quality learning models based on diverse and comprehensive datasets, which are hard to obtain due to the sensitive nature of medical data from patients. In this context, federated learning (FL) is a methodology that enables the distributed training of machine learning models with remotely hosted datasets without the need to accumulate data and, therefore, compromise it. FL is a promising solution to improve ML-based systems, better aligning them to regulatory requirements, improving trustworthiness and data sovereignty. However, many open questions must be addressed before the use of FL becomes widespread. This article aims at presenting a systematic literature review on current research about FL in the context of EHR data for healthcare applications. Our analysis highlights the main research topics, proposed solutions, case studies, and respective ML methods. Furthermore, the article discusses a general architecture for FL applied to healthcare data based on the main insights obtained from the literature review. The collected literature corpus indicates that there is extensive research on the privacy and confidentiality aspects of training data and model sharing, which is expected given the sensitive nature of medical data. Studies also explore improvements to the aggregation mechanisms required to generate the learning model from distributed contributions and case studies with different types of medical data.",10.1145/3501813
10.1145/3502724,Dynamic Evaluation of Microservice Granularity Adaptation,"Hassan, Sara; Bahsoon, Rami; Minku, Leandro; Ali, Nour",2022,ACM Trans. Auton. Adapt. Syst.,"Microservices have gained acceptance in software industries as an emerging architectural style for autonomic, scalable, and more reliable computing. Among the critical microservice architecture design decisions is when to adapt the granularity of a microservice architecture by merging/decomposing microservices. No existing work investigates the following question: How can we reason about the trade-off between predicted benefits and cost of pursuing microservice granularity adaptation under uncertainty? To address this question, we provide a novel formulation of the decision problem to pursue granularity adaptation as a real options problem. We propose a novel evaluation process for dynamically evaluating granularity adaptation design decisions under uncertainty. Our process is based on a novel combination of real options and the concept of Bayesian surprises. We show the benefits of our evaluation process by comparing it to four representative industrial microservice runtime monitoring tools, which can be used for retrospective evaluation for granularity adaptation decisions. Our comparison shows that our process can supersede and/or complement these tools. We implement a microservice application—Filmflix—using Amazon Web Service Lambda and use this implementation as a case study to show the unique benefit of our process compared to traditional application of real options analysis.",10.1145/3502724
10.1145/3503508,Accessibility in Software Practice: A Practitioner’s Perspective,"Bi, Tingting; Xia, Xin; Lo, David; Grundy, John; Zimmermann, Thomas; Ford, Denae",2022,ACM Trans. Softw. Eng. Methodol.,"Being able to access software in daily life is vital for everyone, and thus accessibility is a fundamental challenge for software development. However, given the number of accessibility issues reported by many users, e.g., in app reviews, it is not clear if accessibility is widely integrated into current software projects and how software projects address accessibility issues. In this article, we report a study of the critical challenges and benefits of incorporating accessibility into software development and design. We applied a mixed qualitative and quantitative approach for gathering data from 15 interviews and 365 survey respondents from 26 countries across five continents to understand how practitioners perceive accessibility development and design in practice. We got 44 statements grouped into eight topics on accessibility from practitioners’ viewpoints and different software development stages. Our statistical analysis reveals substantial gaps between groups, e.g., practitioners have Direct vs. Indirect accessibility relevant work experience when they reviewed the summarized statements. These gaps might hinder the quality of accessibility development and design, and we use our findings to establish a set of guidelines to help practitioners be aware of accessibility challenges and benefit factors. We suggest development teams put accessibility as a first-class consideration throughout the software development process, and we also propose some remedies to resolve the gaps between groups and to highlight key future research directions to incorporate accessibility into software design and development.",10.1145/3503508
10.1145/3505228,Integration of DevOps Practices on a Noise Monitor System with CircleCI and Terraform,"Romero, Esteban Elias; Camacho, Carlos David; Montenegro, Carlos Enrique; Acosta, \'{O}scar Esneider; Crespo, Rub\'{e}n Gonz\'{a}lez; Gaona, Elvis Eduardo; Mart\'{\i}nez, Marcelo Herrera",2022,ACM Trans. Manage. Inf. Syst.,"Lowering pollution levels is one of the main principles of Sustainable Development goals dictated by the United Nations. Consequently, developments on noise monitoring contribute in great manner to this purpose, since they give the opportunity to governments and institutions to maintain track on the matter. While developing a software product for this purpose, with the growth in terms of functional and non-functional requirements, elements such as infrastructure, source code, and others also scale up. Consequently if there are not good practices to face the new challenges of the software product, then it could become more complex to refactor, maintain, and scale, causing a decrease on delivery rate and the quality of the product. DevOps is an emerging concept but still hazy, which involves a set of practices that helps organizations to speed up delivery time, improve software quality and collaboration between teams. The aim of this article is to document the implementation of some DevOps practices such as IaC, continuous integration and deployment, code quality control, and collaboration on a noise monitor system to increase the product quality and automation of deployment. The final result is a set of automated pipelines that represents the entire integration and deployment cycle of the software integrated with platforms to improve quality and maintainability of the software components.",10.1145/3505228
10.1145/3506713,The Future of FPGA Acceleration in Datacenters and the Cloud,"Bobda, Christophe; Mbongue, Joel Mandebi; Chow, Paul; Ewais, Mohammad; Tarafdar, Naif; Vega, Juan Camilo; Eguro, Ken; Koch, Dirk; Handagala, Suranga; Leeser, Miriam; Herbordt, Martin; Shahzad, Hafsah; Hofste, Peter; Ringlein, Burkhard; Szefer, Jakub; Sanaullah, Ahmed; Tessier, Russell",2022,ACM Trans. Reconfigurable Technol. Syst.,"In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.",10.1145/3506713
10.1145/3508042,Understanding I/O Direct Cache Access Performance for End Host Networking,"Wang, Minhu; Xu, Mingwei; Wu, Jianping",2022,Proc. ACM Meas. Anal. Comput. Syst.,"Direct Cache Access (DCA) enables a network interface card (NIC) to load and store data directly on the processor cache, as conventional Direct Memory Access (DMA) is no longer suitable as the bridge between NIC and CPU in the era of 100 Gigabit Ethernet. As numerous I/O devices and cores compete for scarce cache resources, making the most of DCA for networking applications with varied objectives and constraints is a challenge, especially given the increasing complexity of modern cache hardware and I/O stacks. In this paper, we reverse engineer details of one commercial implementation of DCA, Intel's Data Direct I/O (DDIO), to explicate the importance of hardware-level investigation into DCA. Based on the learned knowledge of DCA and network I/O stacks, we (1) develop an analytical framework to predict the effectiveness of DCA (i.e., its hit rate) under certain hardware specifications, system configurations, and application properties; (2) measure penalties of the ineffective use of DCA (i.e., its miss penalty) to characterize its benefits; and (3) show that our reverse engineering, measurement, and model contribute to a deeper understanding of DCA, which in turn helps diagnose, optimize, and design end-host networking.",10.1145/3508042
10.1145/3508360,The Serverless Computing Survey: A Technical Primer for Design Architecture,"Li, Zijun; Guo, Linsong; Cheng, Jiagan; Chen, Quan; He, Bingsheng; Guo, Minyi",2022,ACM Comput. Surv.,"The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.",10.1145/3508360
10.1145/3510411,A Survey of Techniques for Fulfilling the Time-Bound Requirements of Time-Sensitive IoT Applications,"Korala, Harindu; Georgakopoulos, Dimitrios; Jayaraman, Prem Prakash; Yavari, Ali",2022,ACM Comput. Surv.,"This article surveys existing techniques for meeting the time-bound requirements of time-sensitive applications in the Internet of Things (IoT). To provide the foundation for identifying and classifying relevant techniques, we present three sample time-sensitive IoT applications and their time-bound requirements, describe the main computation and network resources in IoT that can be used to process such applications, and identify the main challenges in meeting their time-bound requirements. Based on these, the article presents a comprehensive literature review of existing techniques and tools that can help meet application-specific time-bound requirements in IoT. The article also includes a gap analysis in existing research outcomes and proposes research directions for bridging the remaining research gaps in supporting time-sensitive IoT applications.",10.1145/3510411
10.1145/3510412,A Holistic View on Resource Management in Serverless Computing Environments: Taxonomy and Future Directions,"Mampage, Anupama; Karunasekera, Shanika; Buyya, Rajkumar",2022,ACM Comput. Surv.,"Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include rapid auto-scaling, strong isolation, fine-grained billing options, and access to a massive service ecosystem, which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Effective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring, and scaling has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements that influence these aspects, encompassing characteristics of system design, workload attributes, and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog, and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.",10.1145/3510412
10.1145/3510415,Machine Learning-based Orchestration of Containers: A Taxonomy and Future Directions,"Zhong, Zhiheng; Xu, Minxian; Rodriguez, Maria Alejandra; Xu, Chengzhong; Buyya, Rajkumar",2022,ACM Comput. Surv.,"Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted.",10.1145/3510415
10.1145/3510611,"Serverless Computing: A Survey of Opportunities, Challenges, and Applications","Shafiei, Hossein; Khonsari, Ahmad; Mousavi, Payam",2022,ACM Comput. Surv.,"The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this article, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.",10.1145/3510611
10.1145/3511094,"Energy Efficient Computing Systems: Architectures, Abstractions and Modeling to Techniques and Standards","Muralidhar, Rajeev; Borovica-Gajic, Renata; Buyya, Rajkumar",2022,ACM Comput. Surv.,"Computing systems have undergone a tremendous change in the last few decades with several inflexion points. While Moore’s law guided the semiconductor industry to cram more and more transistors and logic into the same volume, the limits of instruction-level parallelism (ILP) and the end of Dennard’s scaling drove the industry towards multi-core chips. More recently, we have entered the era of domain-specific architectures (DSA) and chips for new workloads like artificial intelligence (AI) and machine learning (ML). These trends continue, arguably with other limits, along with challenges imposed by tighter integration, extreme form factors and increasingly diverse workloads, making systems more complex to architect, design, implement and optimize from an energy efficiency perspective. Energy efficiency has now become a first order design parameter and constraint across the entire spectrum of computing devices.Many research surveys have gone into different aspects of energy efficiency techniques implemented in hardware and microarchitecture across devices, servers, HPC/cloud, data center systems along with improved software, algorithms, frameworks, and modeling energy/thermals. Somewhat in parallel, the semiconductor industry has developed techniques and standards around specification, modeling/simulation, benchmarking and verification of complex chips; these areas have not been addressed in detail by previous research surveys. This survey aims to bring these domains holistically together, present the latest in each of these areas, highlight potential gaps and challenges, and discuss opportunities for the next generation of energy efficient systems. The survey is composed of a systematic categorization of key aspects of building energy efficient systems - (1) specification - the ability to precisely specify the power intent, attributes or properties at different layers (2) modeling and simulation of the entire system or subsystem (hardware or software or both) so as to be able to experiment with possible options and perform what-if analysis, (3) techniques used for implementing energy efficiency at different levels of the stack, (4) verification techniques used to provide guarantees that the functionality of complex designs are preserved, and (5) energy efficiency benchmarks, standards and consortiums that aim to standardize different aspects of energy efficiency, including cross-layer optimizations.",10.1145/3511094
10.1145/3517197,Privacy-Preserving Decision Trees Training and Prediction,"Akavia, Adi; Leibovich, Max; Resheff, Yehezkel S.; Ron, Roey; Shahar, Moni; Vald, Margarita",2022,ACM Trans. Priv. Secur.,"In the era of cloud computing and machine learning, data has become a highly valuable resource. Recent history has shown that the benefits brought forth by this data driven culture come at a cost of potential data leakage. Such breaches have a devastating impact on individuals and industry, and lead the community to seek privacy preserving solutions. A promising approach is to utilize Fully Homomorphic Encryption ( ( mathsf {FHE } ) ) to enable machine learning over encrypted data, thus providing resiliency against information leakage. However, computing over encrypted data incurs a high computational overhead, thus requiring the redesign of algorithms, in an “ ( mathsf {FHE } ) -friendly” manner, to maintain their practicality.In this work we focus on the ever-popular tree based methods, and propose a new privacy-preserving solution to training and prediction for trees over data encrypted with homomorphic encryption. Our solution employs a low-degree approximation for the step-function together with a lightweight interactive protocol, to replace components of the vanilla algorithm that are costly over encrypted data. Our protocols for decision trees achieve practical usability demonstrated on standard UCI datasets encrypted with fully homomorphic encryption. In addition, the communication complexity of our protocols is independent of the tree size and dataset size in prediction and training, respectively, which significantly improves on prior works.1",10.1145/3517197
10.1145/3524106,A Survey on Requirements of Future Intelligent Networks: Solutions and Future Research Directions,"Husen, Arif; Chaudary, Muhammad Hasanain; Ahmad, Farooq",2022,ACM Comput. Surv.,"The context of this study examines the requirements of Future Intelligent Networks (FIN), solutions, and current research directions through a survey technique. The background of this study is hinged on the applications of Machine Learning (ML) in the networking field. Through careful analysis of literature and real-world reports, we noted that ML has significantly expedited decision-making processes, enhanced intelligent automation, and helped resolve complex problems economically in different fields of life. Various researchers have also envisioned future networks incorporating intelligent functions and operations with ML. Several efforts have been made to automate individual functions and operations in the networking domain; however, most of the existing ML models proposed in the literature lack several vital requirements. Hence, this study aims to present a comprehensive summary of the requirements of FIN and propose a taxonomy of different network functionalities that needs to be equipped with ML techniques. The core objectives of this study are to provide a taxonomy of requirements envisioned for end-to-end FIN, relevant ML techniques, and their analysis to find research gaps, open issues, and future research directions. The real benefit of ML applications in any domain can only be ensured if intelligent capabilities cover all of its components. We observed that future generations of networks are heterogeneous, multi-vendor, and multidimensional, and ML can provide optimal results only if intelligent capabilities are used on a holistic scale. Realizing intelligence on a holistic scale is only possible if the ML algorithms can solve heterogeneous problems in a multi-vendor and multidimensional environment. ML models must be reliable and efficient, support, and possess the capability to learn and share the knowledge across the network layers and administrative domains to solve issues. First, this study ascertains the requirements of the FIN and proposes their taxonomy through reviews on envisioned ideas by various researchers and articles gathered from reputed conferences and standard developing organizations using keyword queries. Second, we have reviewed existing studies on ML applications focusing on coverage, heterogeneity, distributed architecture, and cross-domain knowledge learning and sharing. Our study observed that in the past, ML applications were focused mainly on an individual/isolated level only, and aspects of global and deep holistic learning with cross-layer/cross-domain knowledge sharing with agile ML operations are not explored at large. We recommend that the issues mentioned previously be addressed with improved ML architecture and agile operations and propose an ML pipeline based architecture for FIN. The significant contribution of this study is the impetus for researchers to seek ML models suitable for a modular, distributed, multi-domain, and multi-layer environment and provide decision making on a global or holistic rather than an individual function level.",10.1145/3524106
10.1145/3524616,A Pressure-Aware Policy for Contention Minimization on Multicore Systems,"Kundan, Shivam; Marinakis, Theodoros; Anagnostopoulos, Iraklis; Kagaris, Dimitri",2022,ACM Trans. Archit. Code Optim.,"Modern Chip Multiprocessors (CMPs) are integrating an increasing amount of cores to address the continually growing demand for high-application performance. The cores of a CMP share several components of the memory hierarchy, such as Last-Level Cache (LLC) and main memory. This allows for considerable gains in multithreaded applications while also helping to maintain architectural simplicity. However, sharing resources can also result in performance bottleneck due to contention among concurrently executing applications. In this work, we formulate a fine-grained application characterization methodology that leverages Performance Monitoring Counters (PMCs) and Cache Monitoring Technology (CMT) in Intel processors. We utilize this characterization methodology to develop two contention-aware scheduling policies, one static and one dynamic, that co-schedule applications based on their resource-interference profiles. Our approach focuses on minimizing contention on both the main-memory bandwidth and the LLC by monitoring the pressure that each application inflicts on these resources. We achieve performance benefits for diverse workloads, outperforming Linux and three state-of-the-art contention-aware schedulers in terms of system throughput and fairness for both single and multithreaded workloads. Compared with Linux, our policy achieves up to 16\% greater throughput for single-threaded and up to 40\% greater throughput for multithreaded applications. Additionally, the policies increase fairness by up to 65\% for single-threaded and up to 130\% for multithreaded ones.",10.1145/3524616
10.1145/3524885,Semi-Synchronous Federated Learning for Energy-Efficient Training and Accelerated Convergence in Cross-Silo Settings,"Stripelis, Dimitris; Thompson, Paul M.; Ambite, Jos\'{e} Luis",2022,ACM Trans. Intell. Syst. Technol.,"There are situations where data relevant to machine learning problems are distributed across multiple locations that cannot share the data due to regulatory, competitiveness, or privacy reasons. Machine learning approaches that require data to be copied to a single location are hampered by the challenges of data sharing. Federated Learning (FL) is a promising approach to learn a joint model over all the available data across silos. In many cases, the sites participating in a federation have different data distributions and computational capabilities. In these heterogeneous environments existing approaches exhibit poor performance: synchronous FL protocols are communication efficient, but have slow learning convergence and high energy cost; conversely, asynchronous FL protocols have faster convergence with lower energy cost, but higher communication. In this work, we introduce a novel energy-efficient Semi-Synchronous Federated Learning protocol that mixes local models periodically with minimal idle time and fast convergence. We show through extensive experiments over established benchmark datasets in the computer-vision domain as well as in real-world biomedical settings that our approach significantly outperforms previous work in data and computationally heterogeneous environments.",10.1145/3524885
10.1145/3527312,Complexity-guided container replacement synthesis,"Wang, Chengpeng; Yao, Peisen; Tang, Wensheng; Shi, Qingkai; Zhang, Charles",2022,Proc. ACM Program. Lang.,"Containers, such as lists and maps, are fundamental data structures in modern programming languages. However, improper choice of container types may lead to significant performance issues. This paper presents Cres, an approach that automatically synthesizes container replacements to improve runtime performance. The synthesis algorithm works with static analysis techniques to identify how containers are utilized in the program, and attempts to select a method with lower time complexity for each container method call. Our approach can preserve program behavior and seize the opportunity of reducing execution time effectively for general inputs. We implement Cres and evaluate it on 12 real-world Java projects. It is shown that Cres synthesizes container replacements for the projects with 384.2 KLoC in 14 minutes and discovers six categories of container replacements, which can achieve an average performance improvement of 8.1\%.",10.1145/3527312
10.1145/3530692,DECENT: A Decentralized Configurator for Controlling Elasticity in Dynamic Edge Networks,"Murturi, Ilir; Dustdar, Schahram",2022,ACM Trans. Internet Technol.,"Recent advancements in distributed systems have enabled deploying low-latency and highly resilient edge applications close to the IoT domain at the edge of the network. The broad range of edge application requirements combined with heterogeneous, resource-constrained, and dynamic edge networks make it particularly challenging to configure and deploy them. Besides that, missing elastic capabilities on the edge makes it difficult to operate such applications under dynamic workloads. To this end, this article proposes a lightweight, self-adaptive, and decentralized mechanism (DECENT) for (1) deploying edge applications on edge resources and on premises of Edge-Cloud infrastructure and (2) controlling elasticity requirements. DECENT enables developers to characterize their edge applications by specifying elasticity requirements, which are automatically captured, interpreted, and enforced by our decentralized elasticity interpreters. In response to dynamic workloads, edge applications automatically adapt in compliance with their elasticity requirements. We discuss the architecture, processes of the approach, and the experiment conducted on a real-world testbed to validate its feasibility on low-powered edge devices. Furthermore, we show performance and adaptation aspects through an edge safety application and its evolution in elasticity space (i.e., cost, resource, and quality).",10.1145/3530692
10.1145/3530814,A Systematic Survey on Android API Usage for Data-driven Analytics with Smartphones,"Lee, Hansoo; Park, Joonyoung; Lee, Uichin",2022,ACM Comput. Surv.,"Recent industrial and academic research has focused on data-driven analytics with smartphones by collecting user interaction, context, and device systems data through Application Programming Interfaces (APIs) and sensors. The Android operating system provides various APIs to collect such mobile usage and sensor data for third-party developers. Usage Statistics API (US API) and Accessibility Service API (AS API) are representative Android APIs for collecting app usage data and are used for various research purposes, as they can collect fine-grained interaction data (e.g., app usage history and user interaction type). Furthermore, other sensor APIs help to collect a user’s context and device state data, along with AS/US APIs. This review investigates mobile usage and sensor data-driven research using AS/US APIs by categorizing the research purposes and the data types. In this article, the surveyed studies are classified as follows: five themes and 21 subthemes and a four-layer hierarchical data classification structure. This allows us to identify a data usage trend and derive insight into data collection according to research purposes. Several limitations and future research directions of mobile usage and sensor data-driven analytics research are discussed, including the impact of changes in the Android API versions on research, the privacy and data quality issues, and the mitigation of reproducibility risks with standardized data typology.",10.1145/3530814
10.1145/3530892,WISEFUSE: Workload Characterization and DAG Transformation for Serverless Workflows,"Mahgoub, Ashraf; Yi, Edgardo Barsallo; Shankar, Karthick; Minocha, Eshaan; Elnikety, Sameh; Bagchi, Saurabh; Chaterji, Somali",2022,Proc. ACM Meas. Anal. Comput. Syst.,"We characterize production workloads of serverless DAGs at a major cloud provider. Our analysis highlights two major factors that limit performance: (a) lack of efficient communication methods between the serverless functions in the DAG, and (b) stragglers when a DAG stage invokes a set of parallel functions that must complete before starting the next DAG stage. To address these limitations, we propose WISEFUSE, an automated approach to generate an optimized execution plan for serverless DAGs for a user-specified latency objective or budget. We introduce three optimizations: (1) Fusion combines in-series functions together in a single VM to reduce the communication overhead between cascaded functions. (2) Bundling executes a group of parallel invocations of a function in one VM to improve resource sharing among the parallel workers to reduce skew. (3) Resource Allocation assigns the right VM size to each function or function bundle in the DAG to reduce the E2E latency and cost. We implement WISEFUSE to evaluate it experimentally using three popular serverless applications with different DAG structures, memory footprints, and intermediate data sizes. Compared to competing approaches and other alternatives, WISEFUSE shows significant improvements in E2E latency and cost. Specifically, for a machine learning pipeline, WISEFUSE achieves P95 latency that is 67\% lower than Photons, 39\% lower than Faastlane, and 90\% lower than SONIC without increasing the cost.",10.1145/3530892
10.1145/3531327,Scientific Workflows in IoT Environments: A Data Placement Strategy Based on Heterogeneous Edge-Cloud Computing,"Du, Xin; Tang, Songtao; Lu, Zhihui; Gai, Keke; Wu, Jie; Hung, Patrick C. K.",2022,ACM Trans. Manage. Inf. Syst.,"In Industry 4.0 and Internet of Things (IoT) environments, the heterogeneous edge-cloud computing paradigm can provide a more proper solution to deploy scientific workflows compared to cloud computing or other traditional distributed computing. Owing to the different sizes of scientific datasets and the privacy issue concerning some of these datasets, it is essential to find a data placement strategy that can minimize data transmission time. Some state-of-the-art data placement strategies combine edge computing and cloud computing to distribute scientific datasets. However, the dynamic distribution of newly generated datasets to appropriate datacenters and exiting the spent datasets are still a challenge during workflows execution. To address this challenge, this study not only constructs a data placement model that includes shared datasets within the individual and among multiple workflows across various geographical regions, but also proposes a data placement strategy (DYM-RL-DPS) based on algorithms of two stages. First, during the build-time stage of workflows, we use the discrete particle swarm optimization algorithm with differential evolution to pre-allocate initial datasets to proper datacenters. Then, we reformulate the dynamic datasets distribution problem as a Markov decision process and provide a reinforcement learning–based approach to learn the data placement strategy in the runtime stage of scientific workflows. Through using the heterogeneous edge-cloud computing architecture to simulate IoT environments, we designed comprehensive experiments to demonstrate the superiority of DYM-RL-DPS. The results of our strategy can effectively reduce the data transmission time as compared to other strategies.",10.1145/3531327
10.1145/3532183,"Microservice Security Metrics for Secure Communication, Identity Management, and Observability","Zdun, Uwe; Queval, Pierre-Jean; Simhandl, Georg; Scandariato, Riccardo; Chakravarty, Somik; Jelic, Marjan; Jovanovic, Aleksandar",2023,ACM Trans. Softw. Eng. Methodol.,"Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems’ dataset through statistical methods.",10.1145/3532183
10.1145/3534520,Facilitating Asynchronous Collaboration in Scientific Workflow Composition Using Provenance,"AbediniAla, Mostafa; Roy, Banani",2022,Proc. ACM Hum.-Comput. Interact.,"Advances in scientific domains are led to an increase in the complexity of the experiments. To address this growing complexity, scientists from different domains require to work collaboratively. Scientific Workflow Management Systems (SWfMSs) are popular tools for data-intensive experiments. To the best of our knowledge, very few of the existing SWfMSs support collaboration, and it is not efficient in many cases. Researchers share a single version of the workflow in existing collaborative data analysis systems, which increases the chance of interference as the number of collaborators grows. Moreover, for effective collaboration, contributors require a clear view of the project's status, the information that existing SWfMSs do not provide. Another significant problem is most scientists are not capable of adding collaborative tools to existing SWfMSs, and they need software engineers to take on this responsibility. Even for software engineers such tasks could be challenging and time consuming. In this paper, we attempted to address this crucial issue in scientific workflow composition and doing so in a collaborative setting. Hence, we propose a tool to facilitate collaborative workflow composition. This tool provides branching and versioning, which are standard version control system features to allow multiple researchers to contribute to the project asynchronously. We also suggest some visualizations and a variety of reports to increase group awareness and help the scientists to realize the project's status and issues. As a proof of concept, we developed an API to capture the provenance data and provide collaborative tools. This API is developed as an example for software engineers to help them understand how to integrate collaborative tools into any SWfMS. We collect provenance information during workflow composition and then employ it to track workflow versions using the proposed collaborative tool. Prior to implementing the visualizations, we surveyed to discover how much the proposed visualizations could contribute to group awareness. Moreover, in the survey we investigated to what extent the proposed version control system could help address shortcomings in collaborative experiments. The survey participants provided us with valuable feedback. In future, we will use the survey responses to enhance the proposed version control system and visualizations.",10.1145/3534520
10.1145/3539605,Provenance-based Intrusion Detection Systems: A Survey,"Zipperle, Michael; Gottwalt, Florian; Chang, Elizabeth; Dillon, Tharam",2022,ACM Comput. Surv.,"Traditional Intrusion Detection Systems (IDS) cannot cope with the increasing number and sophistication of cyberattacks such as Advanced Persistent Threats (APT). Due to their high false-positive rate and the required effort of security experts to validate them, incidents can remain undetected for up to several months. As a result, enterprises suffer from data loss and severe financial damage. Recent research explored data provenance for Host-based Intrusion Detection Systems (HIDS) as one promising data source to tackle this issue. Data provenance represents information flows between system entities as Direct Acyclic Graph (DAG). Provenance-based Intrusion Detection Systems (PIDS) utilize data provenance to enhance the detection performance of intrusions and reduce false-alarm rates compared to traditional IDS. This survey demonstrates the potential of PIDS by providing a detailed evaluation of recent research in the field, proposing a novel taxonomy for PIDS, discussing current issues, and potential future research directions. This survey aims to help and motivate researchers to get started in the field of PIDS by tackling issues of data collection, graph summarization, intrusion detection, and developing real-world benchmark datasets.",10.1145/3539605
10.1145/3539606,"Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges","Carri\'{o}n, Carmen",2022,ACM Comput. Surv.,"Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.",10.1145/3539606
10.1145/3542821,Exploiting Nil-external Interfaces for Fast Replicated Storage,"Ganesan, Aishwarya; Alagappan, Ramnatthan; Rebello, Anthony; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.",2022,ACM Trans. Storage,"Do some storage interfaces enable higher performance than others? Can one identify and exploit such interfaces to realize high performance in storage systems? This article answers these questions in the affirmative by identifying nil-externality, a property of storage interfaces. A nil-externalizing (nilext) interface may modify state within a storage system but does not externalize its effects or system state immediately to the outside world. As a result, a storage system can apply nilext operations lazily, improving performance.In this article, we take advantage of nilext interfaces to build high-performance replicated storage. We implement Skyros, a nilext-aware replication protocol that offers high performance by deferring ordering and executing operations until their effects are externalized. We show that exploiting nil-externality offers significant benefit: For many workloads, Skyros provides higher performance than standard consensus-based replication. For example, Skyros offers 3\texttimes{} lower latency while providing the same high throughput offered by throughput-optimized Paxos.",10.1145/3542821
10.1145/3542823,Code and Data Synthesis for Genetic Improvement in Emergent Software Systems,"Rainford, Penny Faulkner; Porter, Barry",2022,ACM Trans. Evol. Learn. Optim.,"Emergent software systems are assembled from a collection of small code blocks, where some of those blocks have alternative implementation variants; they optimise at run-time by learning which compositions of alternative blocks best suit each deployment environment encountered.In this paper we study the automated synthesis of new implementation variants for a running system using genetic improvement (GI). Typical GI approaches, however, rely on large amounts of data for accurate training and large code bases from which to source genetic material. In emergent systems we have neither asset, with sparsely sampled runtime data and small code volumes in each building block.We therefore examine two approaches to more effective GI under these constraints: the synthesis of data from sparse samples to construct statistically representative larger training corpora; and the synthesis of code to counter the relative lack of genetic material in our starting population members.Our results demonstrate that a mixture of synthesised and existing code is a viable optimisation strategy, and that phases of increased synthesis can make GI more robust to deleterious mutations. On synthesised data, we find that we can produce equivalent optimisation compared to GI methods using larger data sets, and that this optimisation can produce both useful specialists and generalists.",10.1145/3542823
10.1145/3544791,Nudge: Accelerating Overdue Pull Requests toward Completion,"Maddila, Chandra; Upadrasta, Sai Surya; Bansal, Chetan; Nagappan, Nachiappan; Gousios, Georgios; van Deursen, Arie",2023,ACM Trans. Softw. Eng. Methodol.,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The&nbsp;key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60\% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73\% of these notifications as positive. We&nbsp;observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.",10.1145/3544791
10.1145/3544836,Scheduling IoT Applications in Edge and Fog Computing Environments: A Taxonomy and Future Directions,"Goudarzi, Mohammad; Palaniswami, Marimuthu; Buyya, Rajkumar",2022,ACM Comput. Surv.,"Fog computing, as a distributed paradigm, offers cloud-like services at the edge of the network with low latency and high-access bandwidth to support a diverse range of IoT application scenarios. To fully utilize the potential of this computing paradigm, scalable, adaptive, and accurate scheduling mechanisms and algorithms are required to efficiently capture the dynamics and requirements of users, IoT applications, environmental properties, and optimization targets. This article presents a taxonomy of recent literature on scheduling IoT applications in Fog computing. Based on our new classification schemes, current works in the literature are analyzed, research gaps of each category are identified, and respective future directions are described.",10.1145/3544836
10.1145/3549539,Focused Layered Performance Modelling by Aggregation,"Islam, Farhana; Petriu, Dorina; Woodside, Murray",2022,ACM Trans. Model. Perform. Eval. Comput. Syst.,"Performance models of server systems, based on layered queues, may be very complex. This is particularly true for cloud-based systems based on microservices, which may have hundreds of distinct components, and for models derived by automated data analysis. Often only a few of these many components determine the system performance, and a smaller simplified model is all that is needed. To assist an analyst, this work describes a focused model that includes the important components (the focus) and aggregates the rest in groups, called dependency groups. The method Focus-based Simplification with Preservation of Tasks described here fills an important gap in a previous method by the same authors. The use of focused models for sensitivity predictions is evaluated empirically in the article on a large set of randomly generated models. It is found that the accuracy depends on a “saturation ratio” (SR) between the highest utilization value in the model and the highest value of a component excluded from the focus; evidence suggests that SR must be at least 2 and must be larger to evaluate larger model changes. This dependency was captured in an “Accurate Sensitivity Hypothesis” based on SR, which can be used to indicate trustable sensitivity results.",10.1145/3549539
10.1145/3554760,A Scalable Trustworthy Infrastructure for Collaborative Container Repositories,"Wei, Franklin; Tate, Stephen; Ramkumar, Mahalingam; Mohanty, Somya",2022,Distrib. Ledger Technol.,"Within cloud computing containerization has become ubiquitous. As the availability of pre-built containers increases there is a need for methods capable of efficiently securing large repositories of software containers. We present a “Trustworthy Container Repository” (TCR) system which provides security assurances (confidentiality, integrity, and authenticity) regarding such a repository in a scalable manner. Trust within the TCR architecture is rooted in a low-complexity, tamper-resistant trusted module, which leverages index-ordered Merkle trees (IOMTs) to efficiently track a large number of container images and provide assurances of repository integrity to its users. The key contributions of the study are, identification of the required security model, a novel TCR data-structure, and verifiable algorithms to operate on it. Through experiment, we observe closely logarithmic time complexity of the proposed system up to a high container count (N = 225 ≈ 107).",10.1145/3554760
10.1145/3555354,From False-Free to Privacy-Oriented Communitarian Microblogging Social Networks,"Marqu\`{e}s Puig, Joan Manuel; Rif\`{a}-Pous, Helena; Oukemeni, Samia",2023,ACM Trans. Multimedia Comput. Commun. Appl.,"Online Social Networks (OSNs) have gained enormous popularity in recent years. They provide a dynamic platform for sharing content (text messages or multimedia) and for facilitating communication between friends and acquaintances. Microblogging services are a popular form of OSNs. They allow sending small messages in a one-to-many messaging model so that users can communicate with their favorite celebrity, brand, politician, or other regular users without the obligation of a pre-existing social relationship. A chain of privacy-related scandals linked to questionable data handling practices in microblogging services has arisen in the last past few years. Most current microblogging service providers offer centralized services and their business model is based on monitoring, analyzing, and selling users’ activity and patterns. In the end, the personal information shared by the users to benefit from the free-of-charge services is used for the underlying payment in such systems. In this paper, we present Garlanet, a privacy-aware censorship-resistant microblogging social network that does not rely on a centralized service provider as all data is hosted in computers voluntarily contributed by the users of the system. Garlanet provides microblogging functionalities while protecting privacy and preserving the confidentiality and integrity of users and data. It ensures that users’ identities and their social graphs are hidden from the system and adversaries and it provides availability and scalability of the services. We also evaluate the privacy level of Garlanet and we compare it with the privacy level of eight other microblogging systems.",10.1145/3555354
10.1145/3558771,"DiVA: A Scalable, Interactive and Customizable Visual Analytics Platform for Information Diffusion on Large Networks","Sehnan, Dhruv; Goel, Vasu; Masud, Sarah; Jain, Chhavi; Goyal, Vikram; Chakraborty, Tanmoy",2023,ACM Trans. Knowl. Discov. Data,"With an increasing outreach of digital platforms in our lives, researchers have taken a keen interest in studying different facets of social interactions. Analyzing the spread of information (aka diffusion) has brought forth multiple research areas such as modelling user engagement, determining emerging topics, forecasting the virality of online posts and predicting information cascades. Despite such ever-increasing interest, there remains a vacuum among easy-to-use interfaces for large-scale visualization of diffusion models. In this article, we introduce DiVA—Diffusion Visualization and Analysis, a tool that provides a scalable web interface and extendable APIs to analyze various diffusion trends on networks. DiVA uniquely offers support for simultaneous comparison of two competing diffusion models and even the comparison with the ground-truth results, which help develop a coherent understanding of real-world scenarios. Along with performing an exhaustive feature comparison and system evaluation of DiVA against publicly-available web interfaces for information diffusion, we conducted a user study to understand the strengths and limitations of DiVA. We noticed that evaluators had a seamless user experience, especially when analyzing diffusion on large networks.",10.1145/3558771
10.1145/3561818,GraphQL: A Systematic Mapping Study,"Qui\~{n}a-Mera, Antonio; Fernandez, Pablo; Garc\'{\i}a, Jos\'{e} Mar\'{\i}a; Ruiz-Cort\'{e}s, Antonio",2023,ACM Comput. Surv.,"GraphQL is a query language and execution engine for web application programming interfaces (APIs) proposed as an alternative to improve data access problems and versioning of representational state transfer APIs. In this article, we thoroughly study the GraphQL field, first describing the GraphQL paradigm and its conceptual framework, and then conducting a systematic mapping study of 84 primary studies selected from an original set of 3,185. Our work analyzes trends or knowledge gaps about GraphQL by general classification of the studies and specific classification of this research topic. The study’s main conclusions show that GraphQL adoption is growing in the community as a strong alternative to implement APIs. However, we identified the need to strengthen the amount and rigor of empirical evidence collection in applied industry and government studies. In addition, we revealed the opportunity for specific studies on most GraphQL components, especially the consumption of GraphQL API services.",10.1145/3561818
10.1145/3565800,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,"Wang, Chengpeng; Wang, Wenyang; Yao, Peisen; Shi, Qingkai; Zhou, Jinguo; Xiao, Xiao; Zhang, Charles",2023,ACM Trans. Softw. Eng. Methodol.,"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.This work presents a fast and precise value-flow analysis framework called Anchor for the programs using containers. We introduce the notion of anchored containers and propose the memory orientation analysis to construct a precise value-flow graph. Specifically, we establish a combined domain to identify anchored containers and apply strong updates to container memory layouts. Anchor finally conducts a demand-driven reachability analysis in the value-flow graph for a client. Experiments show that it removes 17.1\% spurious statements from thin slices and discovers 20 null pointer exceptions with 9.1\% as its false-positive ratio, while the smashing-based analysis reports 66.7\% false positives. Anchor scales to millions of lines of code and checks the program with around 5.12 MLoC within 5 hours.",10.1145/3565800
10.1145/3568170,Enhancing IoT Project Success through Agile Best Practices,"Moedt van Bolhuis, Wouter; Bernsteiner, Reinhard; Hall, Margeret; Fruhling, Ann",2023,ACM Trans. Internet Things,"Worldwide spending on Internet of Things (IoT) applications is forecasted to surpass $1 trillion by 2022. To stay competitive in this growing technological industry segment, lowering costs while increasing productivity and shortening time-to-market will become increasingly important. Adopting Agile Software Development practices for IoT projects may provide this competitive advantage, as it enables organizations to respond to change, while being dynamic and innovative. Applying a mixed-methods approach, agile IoT practitioners around the world and from diverse industries were surveyed and interviewed. Our study recommends that Agile Software Development team makeup, practices, and methods should be tailored to the specific industry, culture, people, and IT application of an organization. People play an important role in the success of agile projects; therefore, our research focuses on identifying the critical attributes of agile teams to maximize success. Our study identified the five critical agile practices: Collective Code Ownership, Continuous Integration, Single Team, Dedicated Customer, and Sprint Planning and found that both technical and soft skills are essential for successful IoT development.",10.1145/3568170
10.1145/3570611,Malcolm: Multi-agent Learning for Cooperative Load Management at Rack Scale,"Abyaneh, Ali Hossein Abbasi; Liao, Maizi; Zahedi, Seyed Majid",2022,Proc. ACM Meas. Anal. Comput. Syst.,"We consider the problem of balancing the load among servers in dense racks for microsecond-scale workloads. To balance the load in such settings tens of millions of scheduling decisions have to be made per second. Achieving this throughput while providing microsecond-scale latency and high availability is extremely challenging. To address this challenge, we design a fully decentralized load-balancing framework. In this framework, servers collectively balance the load in the system. We model the interactions among servers as a cooperative stochastic game. To find the game's parametric Nash equilibrium, we design and implement a decentralized algorithm based on multi-agent-learning theory. We empirically show that our proposed algorithm is adaptive and scalable while outperforming state-of-the art alternatives. In homogeneous settings, Malcolm performs as well as the best alternative among other baselines. In heterogeneous settings, compared to other baselines, for lower loads, Malcolm improves tail latency by up to a factor of four. And for the same tail latency, Malcolm achieves up to 60\% more throughput compared to the best alternative among other baselines.",10.1145/3570611
10.1145/3570924,Estimating Multiclass Service Demand Distributions Using Markovian Arrival Processes,"Wang, Runan; Casale, Giuliano; Filieri, Antonio",2023,ACM Trans. Model. Comput. Simul.,"Building performance models for software services in DevOps is costly and error-prone. Accurate service demand distribution estimation is critical to precisely modeling queueing behaviors and performance prediction. However, current estimation methods focus on capturing the mean service demand, disregarding higher-order moments of the distribution that still can largely affect prediction accuracy. To address this limitation, we propose to estimate higher moments of the service demand distribution for a microservice from monitoring traces. We first generate a closed queueing model to abstract software performance and use it to model the departure process of requests completed by the software service as a Markovian arrival process (MAP). This allows formulating the estimation of service demand into an optimization problem, which aims to find the first multiple moments of the service demand distribution that maximize the likelihood of the MAP using generated the measured inter-departure times. We then estimate the service demand distribution for different classes of service with a maximum likelihood algorithm and novel heuristics to mitigate the computational cost of the optimization process for scalability. We apply our method to real traces from a microservice-based application and demonstrate that its estimations lead to greater prediction accuracy than exponential distributions assumed in traditional service demand estimation approaches for software services.",10.1145/3570924
10.1145/3571206,"Executing Microservice Applications on Serverless, Correctly","Kallas, Konstantinos; Zhang, Haoran; Alur, Rajeev; Angel, Sebastian; Liu, Vincent",2023,Proc. ACM Program. Lang.,"While serverless platforms substantially simplify the provisioning, configuration, and management of cloud applications, implementing correct services on top of these platforms can present significant challenges to programmers. For example, serverless infrastructures introduce a host of failure modes that are not present in traditional deployments. Individual serverless instances can fail while others continue to make progress, correct but slow instances can be killed by the cloud provider as part of resource management, and providers will often respond to such failures by re-executing requests. For functions with side-effects, these scenarios can create behaviors that are not observable in serverful deployments. In this paper, we propose mu2sls, a framework for implementing microservice applications on serverless using standard Python code with two extra primitives: transactions and asynchronous calls. Our framework orchestrates user-written services to address several challenges, such as failures and re-executions, and provides formal guarantees that the generated serverless implementations are correct. To that end, we present a novel service specification abstraction and formalization of serverless implementations that facilitate reasoning about the correctness of a given application’s serverless implementation. This formalization forms the basis of the mu2sls prototype, which we then use to develop a few real-world microservice applications and show that the performance of the generated serverless implementations achieves significant scalability (3-5\texttimes{} the throughput of a sequential implementation) while providing correctness guarantees in the context of faults, re-execution, and concurrency.",10.1145/3571206
10.1145/3572901,Could Tierless Languages Reduce IoT Development Grief?,"Lubbers, Mart; Koopman, Pieter; Ramsingh, Adrian; Singer, Jeremy; Trinder, Phil",2023,ACM Trans. Internet Things,"Internet of Things (IoT) software is notoriously complex, conventionally comprising multiple tiers. Traditionally an IoT developer must use multiple programming languages and ensure that the components interoperate correctly. A novel alternative is to use a single tierless language with a compiler that generates the code for each component and ensures their correct interoperation.We report a systematic comparative evaluation of two tierless language technologies for IoT stacks: one for resource-rich sensor nodes (Clean with iTask) and one for resource-constrained sensor nodes (Clean with iTask and mTask). The evaluation is based on four implementations of a typical smart campus application: two tierless and two Python-based tiered.(1) We show that tierless languages have the potential to significantly reduce the development effort for IoT systems, requiring 70\% less code than the tiered implementations. Careful analysis attributes this code reduction to reduced interoperation (e.g., two embedded domain-specific languages and one paradigm versus seven languages and two paradigms), automatically generated distributed communication, and powerful IoT programming abstractions. (2) We show that tierless languages have the potential to significantly improve the reliability of IoT systems, describing how Clean iTask/mTask maintains type safety, provides higher-order failure management, and simplifies maintainability. (3) We report the first comparison of a tierless IoT codebase for resource-rich sensor nodes with one for resource-constrained sensor nodes. The comparison shows that they have similar code size (within 7\%), and functional structure. (4) We present the first comparison of two tierless IoT languages, one for resource-rich sensor nodes and the other for resource-constrained sensor nodes.",10.1145/3572901
10.1145/3573009,Performance Interference of Virtual Machines: A Survey,"Lin, Weiwei; Xiong, Chennian; Wu, Wentai; Shi, Fang; Li, Keqin; Xu, Minxian",2023,ACM Comput. Surv.,"The rapid development of cloud computing with virtualization technology has benefited both academia and industry. For any cloud data center at scale, one of the primary challenges is how to effectively orchestrate a large number of virtual machines (VMs) in a performance-aware and cost-effective manner. A key problem here is that the performance interference between VMs can significantly undermine the efficiency of cloud data centers, leading to performance degradation and additional operation cost. To address this issue, extensive studies have been conducted to investigate the problem from different aspects. In this survey, we make a comprehensive investigation into the causes of VM interference and provide an in-depth review of existing research and solutions in the literature. We first categorize existing studies on interference models according to their modeling objectives, metrics used, and modeling methods. Then we revisit interference-aware strategies for scheduling optimization as well as co-optimization-based approaches. Finally, the survey identifies open challenges with respect to VM interference in data centers and discusses possible research directions to provide insights for future research in the area.",10.1145/3573009
10.1145/3573125,VioLinn: Proximity-aware Edge Placementwith Dynamic and Elastic Resource Provisioning,"Tocz\'{e}, Klervie; Fahs, Ali J.; Pierre, Guillaume; Nadjm-Tehrani, Simin",2023,ACM Trans. Internet Things,"Deciding where to handle services and tasks, as well as provisioning an adequate amount of computing resources for this handling, is a main challenge of edge computing systems. Moreover, latency-sensitive services constrain the type and location of edge devices that can provide the needed resources. When available resources are scarce there is a possibility that some resource allocation requests are denied.In this work, we propose the VioLinn system to tackle the joint problems of task placement, service placement, and edge device provisioning. Dealing with latency-sensitive services is achieved through proximity-aware algorithms that ensure the tasks are handled close to the end-user. Moreover, the concept of spare edge device is introduced to handle sudden load variations in time and space without having to continuously overprovision. Several spare device selection algorithms are proposed with different cost/performance tradeoffs.Evaluations are performed both in a Kubernetes-based testbed and using simulations and show the benefit of using spare devices for handling localized load spikes with higher quality of service (QoS) and lower computing resource usage. The study of the different algorithms shows that it is possible to achieve this increase in QoS with different tradeoffs against cost and performance.",10.1145/3573125
10.1145/3573206,A Flexible and Modular Architecture for Edge Digital Twin: Implementation and Evaluation,"Picone, Marco; Mamei, Marco; Zambonelli, Franco",2023,ACM Trans. Internet Things,"IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.",10.1145/3573206
10.1145/3574323,Principled Schedulability Analysis for Distributed Storage Systems Using Thread Architecture Models,"Yang, Suli; Liu, Jing; Arpaci-Dusseau, Andrea; Arpaci-Dusseau, Remzi",2023,ACM Trans. Storage,"In this article, we present an approach to systematically examine the schedulability of distributed storage systems, identify their scheduling problems, and enable effective scheduling in these systems. We use Thread Architecture Models (TAMs) to describe the behavior and interactions of different threads in a system, and show both how to construct TAMs for existing systems and utilize TAMs to identify critical scheduling problems. We specify three schedulability conditions that a schedulable TAM should satisfy: completeness, local enforceability, and independence; meeting these conditions enables a system to easily support different scheduling policies. We identify five common problems that prevent a system from satisfying the schedulability conditions, and show that these problems arise in existing systems such as HBase, Cassandra, MongoDB, and Riak, making it difficult or impossible to realize various scheduling disciplines. We demonstrate how to address these schedulability problems using both direct and indirect solutions, with different trade-offs. To show how to apply our approach to enable scheduling in realistic systems, we develop Tamed-HBase and Muzzled-HBase, sets of modifications to HBase that can realize the desired scheduling disciplines, including fairness and priority scheduling, even when presented with challenging workloads.",10.1145/3574323
10.1145/3579353,"Macroprogramming: Concepts, State of the Art, and Opportunities of Macroscopic Behaviour Modelling","Casadei, Roberto",2023,ACM Comput. Surv.,"Macroprogramming refers to the theory and practice of expressing the macro(scopic) behaviour of a collective system using a single program. Macroprogramming approaches are motivated by the need of effectively capturing global/system-level aspects and the collective behaviour of multiple computational components, while abstracting over low-level details. Previously, this programming style had been primarily adopted to describe the data-processing logic in sensor networks; recently, research forums on spatial computing, collective systems, and the Internet of Things have provided renewed interest in macro approaches. However, related contributions are still fragmented and lack conceptual consistency. Therefore, to foster principled research, an integrated view of the field is provided, together with opportunities and challenges.",10.1145/3579353
10.1145/3579643,Rise of the Planet of Serverless Computing: A Systematic Review,"Wen, Jinfeng; Chen, Zhenpeng; Jin, Xin; Liu, Xuanzhe",2023,ACM Trans. Softw. Eng. Methodol.,"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.",10.1145/3579643
10.1145/3581759,A Survey on Edge Intelligence and Lightweight Machine Learning Support for Future Applications and Services,"Hoffpauir, Kyle; Simmons, Jacob; Schmidt, Nikolas; Pittala, Rachitha; Briggs, Isaac; Makani, Shanmukha; Jararweh, Yaser",2023,J. Data and Information Quality,"As the number of devices connected to the Internet has grown larger, so too has the intensity of the tasks that these devices need to perform. Modern networks are more frequently working to perform computationally intensive tasks on low-power devices and low-end hardware. Current architectures and platforms tend towards centralized and resource-rich cloud computing approaches to address these deficits. However, edge computing presents a much more viable and flexible alternative. Edge computing refers to a distributed and decentralized network architecture in which demanding tasks such as image recognition, smart city services, and high-intensity data processing tasks can be distributed over a number of integrated network devices. In this article, we provide a comprehensive survey for emerging edge intelligence applications, lightweight machine learning algorithms, and their support for future applications and services. We start by analyzing the rise of cloud computing, discuss its weak points, and identify situations in which edge computing provides advantages over traditional cloud computing architectures. We then divulge details of the survey: the first section identifies opportunities and domains for edge computing growth, the second identifies algorithms and approaches that can be used to enhance edge intelligence implementations, and the third specifically analyzes situations in which edge intelligence can be enhanced using any of the aforementioned algorithms or approaches. In this third section, lightweight machine learning approaches are detailed. A more in-depth analysis and discussion of future developments follows. The primary discourse of this article is in service of an effort to ensure that appropriate approaches are applied adequately to artificial intelligence implementations in edge systems, mainly, the lightweight machine learning approaches.",10.1145/3581759
10.1145/3582081,Situational Factor Determinants of the Allocation of Decision Rights to Edge Computers,"Chua, Cecil Eng Huang; Niederman, Fred",2023,ACM Trans. Manage. Inf. Syst.,"Internet of Things (IoT) designers frequently must determine whether action-oriented decisions should be made by edge computers or whether they should be made only by central servers combining input from all edge computers. An important example of this design problem occurs in fire protection IoT, where individual edge computers attached to sensors might be empowered to make decisions (have decision rights) about how to manage the fire. Alternatively, decision rights could be held exclusively by a central server isolated from the fire, because the designer is concerned damage to edge computers could cause them to act unreliably. This research models this allocation of decision rights to identify the relative influence of various decision factors. We first model the allocation of decision rights under the following assumptions: (1) The central server cannot make an error the edge computer cannot make; (2) the central server cannot update the edge computer with its information in a timely manner; and (3) the central server cannot reverse an action initiated by the edge computer to explore the factors impacting decision rights conferral. We then relax each of these three assumptions. We show how relaxing each assumption radically changes the factors impacting decision rights conferral. We also show that allowing the central server to update information on the edge computer or reverse the edge computer's decision making can result in overall lower system performance. We then perform a series of numerical experiments to understand how changing various parameters affect the problem. We show for the general real-world scenario, the key factor influencing the decision is the ability of the edge computer to detect false alarms. We also show magnitude of loss and ratio of real to false incidents have a linear and logarithmic relationship to the reliability of the edge computer.",10.1145/3582081
10.1145/3582695,Realizing Strong Determinism Contract on Log-Structured Merge Key-Value Stores,"Kwon, Miryeong; Lee, Seungjun; Choi, Hyunkyu; Hwang, Jooyoung; Jung, Myoungsoo",2023,ACM Trans. Storage,"We propose Vigil-KV, a hardware and software co-designed framework that eliminates long-tail latency almost perfectly by introducing strong latency determinism. To make Get latency deterministic, Vigil-KV first enables a predictable latency mode (PLM) interface on a real datacenter-scale NVMe SSD, having knowledge about the nature of the underlying flash technologies. Vigil-KV at the system-level then hides the non-deterministic time window (associated with SSD’s internal tasks and/or write services) by internally scheduling the different device states of PLM across multiple physical functions. Vigil-KV further schedules compaction/flush operations and client requests being aware of PLM’s restrictions thereby integrating strong latency determinism into LSM KVs. We implement Vigil-KV upon a 1.92TB NVMe SSD prototype and Linux 4.19.91, but other LSM KVs can adopt its concept. We evaluate diverse Facebook and Yahoo scenarios with Vigil-KV, and the results show that Vigil-KV can reducethe tail latency of a baseline KV system by 3.19\texttimes{} while reducing the average latency by 34\%, on average.",10.1145/3582695
10.1145/3583563,Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment,"Camilli, Matteo; Colarusso, Carmine; Russo, Barbara; Zimeo, Eugenio",2023,ACM Trans. Softw. Eng. Methodol.,"The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.",10.1145/3583563
10.1145/3585009,White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study,"Zhang, Man; Arcuri, Andrea; Li, Yonggang; Liu, Yang; Xue, Kaiming",2023,ACM Trans. Softw. Eng. Methodol.,"Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation.",10.1145/3585009
10.1145/3586053,Randomized Testing of Byzantine Fault Tolerant Algorithms,"Winter, Levin N.; Buse, Florena; de Graaf, Daan; von Gleissenthall, Klaus; Kulahcioglu Ozkan, Burcu",2023,Proc. ACM Program. Lang.,"Byzantine fault-tolerant algorithms promise agreement on a correct value, even if a subset of processes can deviate from the algorithm arbitrarily. While these algorithms provide strong guarantees in theory, in practice, protocol bugs and implementation mistakes may still cause them to go wrong. This paper introduces ByzzFuzz, a simple yet effective method for automatically finding errors in implementations of Byzantine fault-tolerant algorithms through randomized testing. ByzzFuzz detects fault-tolerance bugs by injecting randomly generated network and process faults into their executions. To navigate the space of possible process faults, ByzzFuzz introduces small-scope message mutations which mutate the contents of the protocol messages by applying small changes to the original message either in value (e.g., by incrementing the round number) or in time (e.g., by repeating a proposal value from a previous message). We find that small-scope mutations, combined with insights from the testing and fuzzing literature, are effective at uncovering protocol logic and implementation bugs in real-world fault-tolerant systems. We implemented ByzzFuzz and applied it to test the production implementations of two popular blockchain systems, Tendermint and Ripple, and an implementation of the seminal PBFT protocol. ByzzFuzz detected several bugs in the implementation of PBFT, a potential liveness violation in Tendermint, and materialized two theoretically described vulnerabilities in Ripple’s XRP Ledger Consensus Algorithm. Moreover, we discovered a previously unknown fault-tolerance bug in the production implementation of Ripple, which is confirmed by the developers and fixed.",10.1145/3586053
10.1145/3588769,Intelligent English Language Translation And Grammar Learning Based On Internet Of Things Technology,"Chen, Yuanyuan",2023,ACM Trans. Asian Low-Resour. Lang. Inf. Process.,,10.1145/3588769
10.1145/3589227,Self-Adaptation in Industry: A Survey,"Weyns, Danny; Gerostathopoulos, Ilias; Abbas, Nadeem; Andersson, Jesper; Biffl, Stefan; Brada, Premek; Bures, Tomas; Di Salle, Amleto; Galster, Matthias; Lago, Patricia; Lewis, Grace; Litoiu, Marin; Musil, Angelika; Musil, Juergen; Patros, Panos; Pelliccione, Patrizio",2023,ACM Trans. Auton. Adapt. Syst.,"Computing systems form the backbone of many areas in our society, from manufacturing to traffic control, healthcare, and financial systems. When software plays a vital role in the design, construction, and operation, these systems are referred to as software-intensive systems. Self-adaptation equips a software-intensive system with a feedback loop that either automates tasks that otherwise need to be performed by human operators or deals with uncertain conditions. Such feedback loops have found their way to a variety of practical applications; typical examples are an elastic cloud to adapt computing resources and automated server management to respond quickly to business needs. To gain insight into the motivations for applying self-adaptation in practice, the problems solved using self-adaptation and how these problems are solved, and the difficulties and risks that industry faces in adopting self-adaptation, we performed a large-scale survey. We received 184 valid responses from practitioners spread over 21 countries. Based on the analysis of the survey data, we provide an empirically grounded overview the of state of the practice in the application of self-adaptation. From that, we derive insights for researchers to check their current research with industrial needs, and for practitioners to compare their current practice in applying self-adaptation. These insights also provide opportunities for applying self-adaptation in practice and pave the way for future industry-research collaborations.",10.1145/3589227
10.1145/3589262,DARQ Matter Binds Everything: Performant and Composable Cloud Programming via Resilient Steps,"Li, Tianyu; Chandramouli, Badrish; Burckhardt, Sebastian; Madden, Samuel",2023,Proc. ACM Manag. Data,"Providing strong fault-tolerant guarantees for the modern cloud is difficult, as application developers must coordinate between independent stateful services and ephemeral compute and handle various failure-induced anomalies. We propose Composable Resilient Steps (CReSt), a new abstraction for resilient cloud applications. CReSt uses fault-tolerant steps as its core building block, which allows participants to receive, process, and send messages as a single uninterruptible atomic unit. Composability and reliability are orthogonally achieved by reusable CReSt implementations, for example, leveraging reliable message queues. Thus, CReSt application builders focus solely on translating application logic into steps, and infrastructure builders focus on efficient CReSt implementations. We propose one such implementation called DARQ (for Deduplicated Asynchronously Recoverable Queues). At its core, DARQ is a storage service that encapsulates CReSt participant state and enforces CReSt semantics; developers attach ephemeral compute nodes to DARQ instances to implement stateful distributed components. Services built with DARQ are resilient by construction, and CReSt-compatible services naturally compose without loss of resilience. For performance, we propose a novel speculative execution scheme to execute CReSt steps without waiting for message persistence in DARQ, effectively eliding cloud persistence overheads; our scheme maintains CReSt's fault-tolerance guarantees and automatically restores to a consistent system state upon failure. We showcase the generality of CReSt and DARQ using two applications: cloud streaming and workflow processing. Experiments show that DARQ is able to achieve extremely low latency and high throughput across these use cases, often beating state-of-the-art customized solutions.",10.1145/3589262
10.1145/3589306,Using Cloud Functions as Accelerator for Elastic Data Analytics,"Bian, Haoqiong; Sha, Tiannan; Ailamaki, Anastasia",2023,Proc. ACM Manag. Data,"Cloud function (CF) services, such as AWS Lambda, have been applied as the new computing infrastructure in implementing analytical query engines. For bursty and sparse workloads, CF-based query engine is more elastic than the traditional query engines running in servers, i.e., virtual machines (VMs), and might provide a higher performance/price ratio. However, it is still controversial whether CF services are good suites for general analytical workloads, in respect of the limitations of CFs in storage, network, and lifetime, as well as the much higher resource unit prices than VMs.In this paper, we first present micro-benchmark evaluations of the features of CF and VM. We reveal that for query processing, though CF is more elastic than VM, it is less scalable and is more expensive for continuous workloads. Then, to get the best of both worlds, we propose Pixels-Turbo - a hybrid query engine that processes queries in a scalable VM cluster by default and invokes CFs to accelerate the processing of unpredictable workload spikes. In the query engine, we propose several optimizations to improve the performance and scalability of the CF-based operators and a cost-based optimizer to select the appropriate algorithm and parallelism for the physical query plan. Evaluations on TPC-H and real-world workload show that our query engine has a 1-2 orders of magnitude higher performance/price ratio than state-of-the-art serverless query engines for sustained workloads while not compromising the elasticity for workload spikes.",10.1145/3589306
10.1145/3591273,Reliable Actors with Retry Orchestration,"Tardieu, Olivier; Grove, David; Bercea, Gheorghe-Teodor; Castro, Paul; Cwiklik, Jaroslaw; Epstein, Edward",2023,Proc. ACM Program. Lang.,"Cloud developers have to build applications that are resilient to failures and interruptions. We advocate for a fault-tolerant programming model for the cloud based on actors, retry orchestration, and tail calls. This model builds upon persistent data stores and message queues readily available on the cloud. Retry orchestration not only guarantees that (1) failed actor invocations will be retried but also that (2) completed invocations are never repeated and (3) it preserves a strict happen-before relationship across failures within call stacks. Tail calls can break complex tasks into simple steps to minimize re-execution during recovery. We review key application patterns and failure scenarios. We formalize a process calculus to precisely capture the mechanisms of fault tolerance in this model. We briefly describe our implementation. Using an application inspired by a typical enterprise scenario, we validate the functional correctness of our implementation and assess the impact of fault preparedness and recovery on performance.",10.1145/3591273
10.1145/3592598,Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions,"Pallewatta, Samodha; Kostakos, Vassilis; Buyya, Rajkumar",2023,ACM Comput. Surv.,"The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.",10.1145/3592598
10.1145/3593021,Intel Software Guard Extensions Applications: A Survey,"Will, Newton C.; Maziero, Carlos A.",2023,ACM Comput. Surv.,"Data confidentiality is a central concern in modern computer systems and services, as sensitive data from users and companies are being increasingly delegated to such systems. Several hardware-based mechanisms have been recently proposed to enforce security guarantees of sensitive information. Hardware-based isolated execution environments are a class of such mechanisms, in which the operating system and other low-level components are removed from the trusted computing base. One of such mechanisms is the Intel Software Guard Extensions (Intel SGX), which creates the concept of enclave to encapsulate sensitive components of applications and their data. Despite being largely applied in several computing areas, SGX has limitations and performance issues that must be addressed for the development of secure solutions. This text brings a categorized literature review of the ongoing research on the Intel SGX architecture, discussing its applications and providing a classification of the solutions that take advantage of SGX mechanisms. We analyze and categorize 293 papers that rely on SGX to provide integrity, confidentiality, and privacy to users and data, regarding different contexts and goals. We also discuss research challenges and provide future directions in the field of enclaved execution, particularly when using SGX.",10.1145/3593021
10.1145/3593055,Jointly Optimizing Job Assignment and Resource Partitioning for Improving System Throughput in Cloud Datacenters,"Chen, Ruobing; Shi, Haosen; Wu, Jinping; Li, Yusen; Liu, Xiaoguang; Wang, Gang",2023,ACM Trans. Archit. Code Optim.,"Colocating multiple jobs on the same server has been widely applied for improving resource utilization in cloud datacenters. However, the colocated jobs would contend for the shared resources, which could lead to significant performance degradation. An efficient approach for eliminating performance interference is to partition the shared resources among the colocated jobs. However, this makes the resource management in datacenters very challenging. In this paper, we propose JointOPT, the first resource management framework that optimizes job assignment and resource partitioning jointly for improving the throughput of cloud datacenters. JointOPT uses a local search based algorithm to find the near optimal job assignment configuration, and uses a deep reinforcement learning (DRL) based approach to dynamically partition the shared resources among the colocated jobs. In order to reduce the interaction overhead with real systems, it leverages deep learning to estimate job performance without running them on real servers. We conduct extensive experiments to evaluate JointOPT and the results show that JointOPT significantly outperforms the state-of-the-art baselines, with an advantage from 13.3\% to 47.7\%.",10.1145/3593055
10.1145/3593427,INFRA-ART: An Open Access Spectral Library of Art-related Materials as a Digital Support Tool for Cultural Heritage Science,"Cortea, I. M.; Chiro\c{s}ca, A.; Anghelu\c{t}\u{a}, L. M.; Seri\c{t}an, G.",2023,J. Comput. Cult. Herit.,"Easily accessible characterization techniques such as X-ray fluorescence (XRF), Fourier Transform Infrared Spectroscopy (FTIR), or Raman spectroscopy, are at this moment the most commonly used analytical tools in heritage and conservation science. Materials identification in works of art is a fundamental step for understanding an object's history or an artist's technique. Comprehensive characterization and diagnosis of the various constituent materials in artworks can provide valuable information on the artist's working methods, as well as significant evidence for dating, provenance attribution, or forgery detection. The development of databases with high-quality data on the pure substances used as artists’ materials is of utmost importance for the identification and characterization of unknown samples. However, there are relatively few open access spectra libraries dedicated exclusively to the cultural heritage field. To address this need, within the frame of the postdoctoral project INFRA-ART, an open access spectral library of art-related materials has been developed. The database is an ongoing compilation of spectra that contains at this moment over 1,000 high-quality attenuated total reflection–FTIR, Raman, and XRF spectra associated with over 500 known reference materials. In this article, a summary of the database structure and design, functionality, and use is presented, in view of the dissemination of this new open access spectral library to the scientific community.",10.1145/3593427
10.1145/3595376,Component-based Distributed Software Reconfiguration:A Verification-oriented Survey,"Coullon, H\'{e}l\'{e}ne; Henrio, Ludovic; Loulergue, Fr\'{e}d\'{e}ric; Robillard, Simon",2023,ACM Comput. Surv.,"Distributed software built from components has become a mainstay of service-oriented applications, which frequently undergo reconfigurations to adapt to changes in their operating environment or their functional requirements. Given the complexity of distributed software and the adverse effects of incorrect reconfigurations, a suitable methodology is needed to ensure the correctness of reconfigurations in component-based systems. This survey gives the reader a global perspective over existing formal techniques that pursue this goal. It distinguishes different ways in which formal methods can improve the reliability of reconfigurations, and lists techniques that contribute to solving each of these particular scientific challenges.",10.1145/3595376
10.1145/3597205,Open Problems in Fuzzing RESTful APIs: A Comparison of Tools,"Zhang, Man; Arcuri, Andrea",2023,ACM Trans. Softw. Eng. Methodol.,"RESTful APIs are a type of web service that are widely used in industry. In the past few years, a lot of effort in the research community has been spent in designing novel techniques to automatically fuzz those APIs to find faults in them. Many real faults were automatically found in a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no analysis of what is actually covered in these systems is done. Therefore, although these fuzzers are clearly useful for practitioners, we do not know their current limitations and actual effectiveness. Solving this is a necessary step to be able to design better, more efficient, and effective techniques. To address this issue, in this article we compare seven state-of-the-art fuzzers on 18 open source—1 industrial and 1 artificial—RESTful APIs. We then analyze the source code for which parts of these APIs the fuzzers fail to generate tests. This analysis points to clear limitations of these current fuzzers, listing concrete follow-up challenges for the research community.",10.1145/3597205
10.1145/3597420,Citizen-centric Design of Consumable Services for Smart Cities,"Voelz, Alexander; Muck, Christian; Amlashi, Danial M.; Karagiannis, Dimitris",2023,Digit. Gov.: Res. Pract.,"The ongoing growth of city populations has brought new challenges and increased complexity to everyday city life. In response, numerous cities are making efforts to adopt new information technologies to become more efficient and transform themselves into smart cities. A large body of research has underlined that the success of these smart city initiatives depends on putting citizens at the center of the design process. In contrast to the techno-centric view applied in the past, citizen-centric approaches focus on prioritizing the needs of citizens over technology and aim at involving them directly in the development of smart city services. In this article, we describe this involvement as citizen development by introducing a model-based architecture that enables citizens to develop smart city services in a low-code fashion. Consequently, we focus on citizen developers, a group of citizens engaged with the technological development of services. Furthermore, the conceptualization of a citizen development architecture is discussed, as well as the ways citizens can interact with it. An important pillar in this approach are citizen workshops, which aim at stimulating innovative ideas and collaboration among stakeholders by using design thinking methodology. Our approach empowers citizens to become active producers within the smart city ecosystem, leading to more inclusive and citizen-centric smart cities.",10.1145/3597420
10.1145/3603707,Context-aware Big Data Quality Assessment: A Scoping Review,"Fadlallah, Hadi; Kilany, Rima; Dhayne, Houssein; El Haddad, Rami; Haque, Rafiqul; Taher, Yehia; Jaber, Ali",2023,J. Data and Information Quality,"The term data quality refers to measuring the fitness of data regarding the intended usage. Poor data quality leads to inadequate, inconsistent, and erroneous decisions that could escalate the computational cost, cause a decline in profits, and cause customer churn. Thus, data quality is crucial for researchers and industry practitioners.Different factors drive the assessment of data quality. Data context is deemed one of the key factors due to the contextual diversity of real-world use cases of various entities such as people and organizations. Data used in a specific context (e.g., an organization policy) may need to be more efficacious for another context. Hence, implementing a data quality assessment solution in different contexts is challenging.Traditional technologies for data quality assessment reached the pinnacle of maturity. Existing solutions can solve most of the quality issues. The data context in these solutions is defined as validation rules applied within the ETL (extract, transform, load) process, i.e., the data warehousing process. In contrast to traditional data quality management, it is impossible to specify all the data semantics beforehand for big data. We need context-aware data quality rules to detect semantic errors in a massive amount of heterogeneous data generated at high speed. While many researchers tackle the quality issues of big data, they define the data context from a specific standpoint. Although data quality is a longstanding research issue in academia and industries, it remains an open issue, especially with the advent of big data, which has fostered the challenge of data quality assessment more than ever.This article provides a scoping review to study the existing context-aware data quality assessment solutions, starting with the existing big data quality solutions in general and then covering context-aware solutions. The strength and weaknesses of such solutions are outlined and discussed. The survey showed that none of the existing data quality assessment solutions could guarantee context awareness with the ability to handle big data. Notably, each solution dealt only with a partial view of the context. We compared the existing quality models and solutions to reach a comprehensive view covering the aspects of context awareness when assessing data quality. This led us to a set of recommendations framed in a methodological framework shaping the design and implementation of any context-aware data quality service for big data. Open challenges are then identified and discussed.",10.1145/3603707
10.1145/3608479,Reinforcement Learning for Adaptive Video Compressive Sensing,"Lu, Sidi; Yuan, Xin; Katsaggelos, Aggelos K.; Shi, Weisong",2023,ACM Trans. Intell. Syst. Technol.,"We apply reinforcement learning to video compressive sensing to adapt the compression ratio. Specifically, video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera is considered in this work, in which multiple (B) video frames can be reconstructed from a snapshot measurement. One research gap in previous studies is how to adapt B in the video SCI system for different scenes. In this article, we fill this gap utilizing reinforcement learning (RL). An RL model, as well as various convolutional neural networks for reconstruction, are learned to achieve adaptive sensing of video SCI systems. Furthermore, the performance of an object detection network using directly the video SCI measurements without reconstruction is also used to perform RL-based adaptive video compressive sensing. Our proposed adaptive SCI method can thus be implemented in low cost and real time. Our work takes the technology one step further towards real applications of video SCI.",10.1145/3608479
10.1145/3609427,Random Testing and Evolutionary Testing for Fuzzing GraphQL APIs,"Belhadi, Asma; Zhang, Man; Arcuri, Andrea",2024,ACM Trans. Web,"The Graph Query Language (GraphQL) is a powerful language for application programming interface (API) manipulation in web services. It has been recently introduced as an alternative solution for addressing the limitations of RESTful APIs. This article introduces an automated solution for GraphQL API testing. We present a full framework for automated API testing, from the schema extraction to test case generation. In addition, we consider two kinds of testing: white-box and black-box testing. The white-box testing is performed when the source code of the GraphQL API is available. Our approach is based on evolutionary search. Test cases are evolved to intelligently explore the solution space while maximizing code coverage and fault-finding criteria. The black-box testing does not require access to the source code of the GraphQL API. It is therefore of more general applicability, albeit it has worse performance. In this context, we use a random search to generate GraphQL data. The proposed framework is implemented and integrated into the open source EvoMaster tool. With enabled white-box heuristics (i.e., white-box mode), experiments on 7 open source GraphQL APIs and three search algorithms show statistically significant improvement of the evolutionary approach compared to the baseline random search. In addition, experiments on 31 online GraphQL APIs reveal the ability of the black-box mode to detect real faults.",10.1145/3609427
10.1145/3609504,Service Caching and Computation Reuse Strategies at the Edge: A Survey,"Barrios, Carlos; Kumar, Mohan",2023,ACM Comput. Surv.,"With the proliferation of connected devices including smartphones, novel network connectivity and management methods are needed to meet user Quality of Experience (QoE) and computational demands of contemporary applications. Service caching and computation reuse techniques are being employed to alleviate challenges due to scalability, interoperability, and mobility, as well as to reduce application latency by enabling caching at the edge. This survey provides a taxonomy for service caching and computation reuse and describes the current state of the research and its challenges. This is the first survey that provides a comprehensive analysis and suggests future research directions on this topic.",10.1145/3609504
10.1145/3611312,Strega: An HTTP Server for FPGAs,"Maschi, Fabio; Alonso, Gustavo",2024,ACM Trans. Reconfigurable Technol. Syst.,"The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega, an open source1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7&nbsp;M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.",10.1145/3611312
10.1145/3612918,A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT,"Sun, Danfeng; Hu, Junjie; Wu, Huifeng; Wu, Jia; Yang, Jian; Sheng, Quan Z.; Dustdar, Schahram",2023,ACM Comput. Surv.,"The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.",10.1145/3612918
10.1145/3614426,Secure and Trustworthy Artificial Intelligence-extended Reality (AI-XR) for Metaverses,"Qayyum, Adnan; Butt, Muhammad Atif; Ali, Hassan; Usman, Muhammad; Halabi, Osama; Al-Fuqaha, Ala; Abbasi, Qammer H.; Imran, Muhammad Ali; Qadir, Junaid",2024,ACM Comput. Surv.,"Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users’ privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.",10.1145/3614426
10.1145/3617123,Providing Realtime Support for Containerized Edge Services,"Zhang, Wenzhao; Gao, Yi; Dong, Wei",2023,ACM Trans. Internet Technol.,"Containers have emerged as a popular technology for edge computing platforms. Although there are varieties of container orchestration frameworks, e.g., Kubernetes to provide high-reliable services for cloud infrastructure, providing real-time support at the containerized edge systems (CESs) remains a challenge. In this paper, we propose EdgeMan, a holistic edge service management framework for CESs, which consists of (1) a model-assisted event-driven lightweight online scheduling algorithm to provide request-level execution plans; (2) a bottleneck-metric-aware progressive resource allocation mechanism to improve resource efficiency. We then build a testbed that installed three containerized services with different latency sensitivities for concrete evaluation. Additionally, we adopt real-world data traces from Alibaba and Twitter for large-scale emulations. Extensive experiments demonstrate that the deadline miss ratio of time-sensitive services run with EdgeMan&nbsp;is reduced by 85.9\%&nbsp;on average compared with that of existing methods in both industry and academia.",10.1145/3617123
10.1145/3617175,Testing RESTful APIs: A Survey,"Golmohammadi, Amid; Zhang, Man; Arcuri, Andrea",2023,ACM Trans. Softw. Eng. Methodol.,"In industry, RESTful APIs are widely used to build modern Cloud Applications. Testing them is challenging, because not only do they rely on network communications, but also they deal with external services like databases. Therefore, there has been a large amount of research sprout in recent years on how to automatically verify this kind of web services. In this article, we present a comprehensive review of the current state-of-the-art in testing RESTful APIs based on the analysis of 92 scientific articles. These articles were gathered by utilizing search queries formulated around the concept of RESTful API testing on seven popular databases. We eliminated irrelevant articles based on our predefined criteria and conducted a snowballing phase to minimize the possibility of missing any relevant paper. This survey categorizes and summarizes the existing scientific work on testing RESTful APIs and discusses the current challenges in the verification of RESTful APIs. This survey clearly shows an increasing interest among researchers in this field, from 2017 onward. However, there are still a lot of open research challenges to overcome.",10.1145/3617175
10.1145/3617507,SensiX++: Bringing MLOps and Multi-tenant Model Serving to Sensory Edge Devices,"Min, Chulhong; Mathur, Akhil; Acer, Utku G\""{u}nay; Montanari, Alessandro; Kawsar, Fahim",2023,ACM Trans. Embed. Comput. Syst.,"We present SensiX++, a multi-tenant runtime for adaptive model execution with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT sensors. SensiX++ operates on two fundamental principles: highly modular componentisation to externalise data operations with clear abstractions and document-centric manifestation for system-wide orchestration. First, a data coordinator manages the lifecycle of sensors and serves models with correct data through automated transformations. Next, a resource-aware model server executes multiple models in isolation through model abstraction, pipeline automation, and feature sharing. An adaptive scheduler then orchestrates the best-effort executions of multiple models across heterogeneous accelerators, balancing latency and throughput. Finally, microservices with REST APIs serve synthesised model predictions, system statistics, and continuous deployment. Collectively, these components enable SensiX++ to serve multiple models efficiently with fine-grained control on edge devices while minimising data operation redundancy, managing data and device heterogeneity, and reducing resource contention. We benchmark SensiX++ with 10 different vision and acoustics models across various multi-tenant configurations on different edge accelerators (Jetson AGX and Coral TPU) designed for sensory devices. We report on the overall throughput and quantified benefits of various automation components of SensiX++ and demonstrate its efficacy in significantly reducing operational complexity and lowering the effort to deploy, upgrade, reconfigure, and serve embedded models on edge devices.",10.1145/3617507
10.1145/3618001,Human–machine Teaming with Small Unmanned Aerial Systems in a MAPE-K Environment,"Cleland-Huang, Jane; Chambers, Theodore; Zudaire, Sebastian; Chowdhury, Muhammed Tawfiq; Agrawal, Ankit; Vierhauser, Michael",2024,ACM Trans. Auton. Adapt. Syst.,"The Human Machine Teaming (HMT) paradigm focuses on supporting partnerships between humans and autonomous machines. HMT describes requirements for transparency, augmented cognition, and coordination that enable far richer partnerships than those found in typical human-on-the-loop and human-in-the-loop systems. Autonomous, self-adaptive systems in domains such as autonomous driving, robotics, and Cyber-Physical Systems, are often implemented using the MAPE-K feedback loop as the primary reference model. However, while MAPE-K enables fully autonomous behavior, it does not explicitly address the interactions that occur between humans and autonomous machines as intended by HMT. In this article, we, therefore, present the MAPE-KHMT framework, which utilizes runtime models to augment the monitoring, analysis, planning, and execution phases of the MAPE-K loop to support HMT despite the different operational cadences of humans and machines. We draw on examples from our own emergency response system of interactive, autonomous, small unmanned aerial systems to illustrate the application of MAPE-KHMT in both a simulated and physical environment, and we discuss how the various HMT models are connected and can be integrated into a MAPE-K solution.",10.1145/3618001
10.1145/3622784,Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild,"Martinez-Maldonado, Roberto; Echeverria, Vanessa; Fernandez-Nieto, Gloria; Yan, Lixiang; Zhao, Linxuan; Alfredo, Riordan; Li, Xinyu; Dix, Samantha; Jaggard, Hollie; Wotherspoon, Rosie; Osborne, Abra; Shum, Simon Buckingham; Ga\v{s}evi\'{c}, Dragan",2023,ACM Trans. Comput.-Hum. Interact.,"Multimodal Learning Analytics (MMLA) innovations make use of rapidly evolving sensing and artificial intelligence algorithms to collect rich data about learning activities that unfold in physical spaces. The analysis of these data is opening exciting new avenues for both studying and supporting learning. Yet, practical and logistical challenges commonly appear while deploying MMLA innovations “in-the-wild”. These can span from technical issues related to enhancing the learning space with sensing capabilities, to the increased complexity of teachers’ tasks. These practicalities have been rarely investigated. This article addresses this gap by presenting a set of lessons learnt from a 2-year human-centred MMLA in-the-wild study conducted with 399 students and 17 educators in the context of nursing education. The lessons learnt were synthesised into topics related to (i) technological/physical aspects of the deployment; (ii) multimodal data and interfaces; (iii) the design process; (iv) participation, ethics and privacy; and (v) sustainability of the deployment.",10.1145/3622784
10.1145/3622787,MicroProf: Code-level Attribution of Unnecessary Data Transfer in Microservice Applications,"Tariq, Syed Salauddin Mohammad; Menard, Lance; Su, Pengfei; Roy, Probir",2023,ACM Trans. Archit. Code Optim.,"The microservice architecture style has gained popularity due to its ability to fault isolation, ease of scaling applications, and developer’s agility. However, writing applications in the microservice design style has its challenges. Due to the loosely coupled nature, services communicate with others through standard communication APIs. This incurs significant overhead in the application due to communication protocol and data transformation. An inefficient service communication at the microservice application logic can further overwhelm the application. We perform a grey literature review showing that unnecessary data transfer is a real challenge in the industry. To the best of our knowledge, no effective tool is currently available to accurately identify the origins of unnecessary microservice communications that lead to significant performance overhead and provide guidance for optimization.To bridge the knowledge gap, we propose MicroProf, a dynamic program analysis tool to detect unnecessary data transfer in Java-based microservice applications. At the implementation level, MicroProf proposes novel techniques such as remote object sampling and hardware debug registers to monitor remote object usage. MicroProf reports the unnecessary data transfer at the application source code level. Furthermore, MicroProf pinpoints the opportunities for communication API optimization. MicroProf is evaluated on four well-known applications involving two real-world applications and two benchmarks, identifying five inefficient remote invocations. Guided by MicroProf, API optimization achieves an 87.5\% reduction in the number of fields within REST API responses. The empirical evaluation further reveals that the optimized services experience a speedup of up to 4.59\texttimes{}.",10.1145/3622787
10.1145/3622833,A Cocktail Approach to Practical Call Graph Construction,"Cai, Yuandao; Zhang, Charles",2023,Proc. ACM Program. Lang.,"After decades of research, constructing call graphs for modern C-based software remains either imprecise or inefficient when scaling up to the ever-growing complexity. The main culprit is the difficulty of resolving function pointers, as precise pointer analyses are cubic in nature and become exponential when considering calling contexts. This paper takes a practical stance by first conducting a comprehensive empirical study of function pointer manipulations in the wild. By investigating 5355 indirect calls in five popular open-source systems, we conclude that, instead of the past uniform treatments for function pointers, a cocktail approach can be more effective in “squeezing” the number of difficult pointers to a minimum using a potpourri of cheap methods. In particular, we decompose the costs of constructing highly precise call graphs of big code by tailoring several increasingly precise algorithms and synergizing them into a concerted workflow. As a result, many indirect calls can be precisely resolved in an efficient and principled fashion, thereby reducing the final, expensive refinements. This is, in spirit, similar to the well-known cocktail medical therapy. The results are encouraging — our implemented prototype called Coral can achieve similar precision versus the previous field-, flow-, and context-sensitive Andersen-style call graph construction, yet scale up to millions of lines of code for the first time, to the best of our knowledge. Moreover, by evaluating the produced call graphs through the lens of downstream clients (i.e., use-after-free detection, thin slicing, and directed grey-box fuzzing), the results show that Coral can dramatically improve their effectiveness for better vulnerability hunting, understanding, and reproduction. More excitingly, we found twelve confirmed bugs (six impacted by indirect calls) in popular systems (e.g., MariaDB), spreading across multiple historical versions.",10.1145/3622833
10.1145/3622858,Inference of Resource Management Specifications,"Shadab, Narges; Gharat, Pritam; Tiwari, Shrey; Ernst, Michael D.; Kellogg, Martin; Lahiri, Shuvendu K.; Lal, Akash; Sridharan, Manu",2023,Proc. ACM Program. Lang.,"A resource leak occurs when a program fails to free some finite resource after it is no longer needed. Such leaks are a significant cause of real-world crashes and performance problems. Recent work proposed an approach to prevent resource leaks based on checking resource management specifications. A resource management specification expresses how the program allocates resources, passes them around, and releases them; it also tracks the ownership relationship between objects and resources, and aliasing relationships between objects. While this specify-and-verify approach has several advantages compared to prior techniques, the need to manually write annotations presents a significant barrier to its practical adoption. This paper presents a novel technique to automatically infer a resource management specification for a program, broadening the applicability of specify-and-check verification for resource leaks. Inference in this domain is challenging because resource management specifications differ significantly in nature from the types that most inference techniques target. Further, for practical effectiveness, we desire a technique that can infer the resource management specification intended by the developer, even in cases when the code does not fully adhere to that specification. We address these challenges through a set of inference rules carefully designed to capture real-world coding patterns, yielding an effective fixed-point-based inference algorithm. We have implemented our inference algorithm in two different systems, targeting programs written in Java and C#. In an experimental evaluation, our technique inferred 85.5\% of the annotations that programmers had written manually for the benchmarks. Further, the verifier issued nearly the same rate of false alarms with the manually-written and automatically-inferred annotations.",10.1145/3622858
10.1145/3624556,Introduction to the Special Issue on Citizen Centricity in Smart Cities,"Anthopoulos, Leonidas; Janssen, Marijn; Weerakkody, Vishanth",2023,Digit. Gov.: Res. Pract.,"City digital transformation is being performed across the globe and it is being supported by several initiatives and policy groups. Nevertheless, this transformation is mainly technology oriented. Although there have been several interesting and impactful outcomes, like solutions for enhancing municipal efficiency and local living, the results lag behind expectations. Real transformation should also change structures and address the whole range from social to technical aspects. People-centricity in this transformation is an emerging topic that is attracting attention and some initiatives have launched. People-centric transformation can be defined as a multi-stakeholder approach to digital transformation that realizes sustainability, inclusiveness, prosperity, and human rights for the benefit of all. The aim of this special issue is to better understand all aspects of citizen centricity and how it can change the orientation of city digital transformation process. The special issue highlights the main elements of people-centricity in cities: inclusiveness, openness, engagement and empowerment via securing citizen awareness, data privacy, service simplification and transparency, and technological availability and observability.",10.1145/3624556
10.1145/3624739,Understanding the Helpfulness of Stale Bot for Pull-Based Development: An Empirical Study of 20 Large Open-Source Projects,"Khatoonabadi, Sayedhassan; Costa, Diego Elias; Mujahid, Suhaib; Shihab, Emad",2023,ACM Trans. Softw. Eng. Methodol.,"Pull Requests (PRs) that are neither progressed nor resolved clutter the list of PRs, making it difficult for the maintainers to manage and prioritize unresolved PRs. To automatically track, follow up, and close such inactive PRs, Stale bot was introduced by GitHub. Despite its increasing adoption, there are ongoing debates on whether using Stale bot alleviates or exacerbates the problem of inactive PRs. To better understand if and how Stale bot helps projects in their pull-based development workflow, we perform an empirical study of 20 large and popular open source projects. We find that Stale bot can help deal with a backlog of unresolved PRs, as the projects closed more PRs within the first few months of adoption. Moreover, Stale bot can help improve the efficiency of the PR review process as the projects reviewed PRs that ended up merged and resolved PRs that ended up closed faster after the adoption. However, Stale bot can also negatively affect the contributors, as the projects experienced a considerable decrease in their number of active contributors after the adoption. Therefore, relying solely on Stale bot to deal with inactive PRs may lead to decreased community engagement and an increased probability of contributor abandonment.",10.1145/3624739
10.1145/3626723,Demystifying the QoS and QoE of Edge-hosted Video Streaming Applications in the Wild with SNESet,"Li, Yanan; Deng, Guangqing; Bai, Changming; Yang, Jingyu; Wang, Gang; Zhang, Hao; Bai, Jin; Yuan, Haitao; Xu, Mengwei; Wang, Shangguang",2023,Proc. ACM Manag. Data,"Video streaming applications (VSAs) are increasingly being deployed on large-scale edge platforms, which have the potential to significantly improve the quality of service (QoS) and end-user experience (QoE), ultimately maximizing business outcomes. However, there is currently very little understanding of how QoS, QoE, and the impact of QoS on QoE for VSAs on edge platforms in the wild and at scale. To close the knowledge gap, we collect SNESet, an active measurement dataset comprising QoS and QoE telemetry metrics of 8 VSAs over four months, covering end-users from 798 edge sites,30 cities, and 3 ISPs in one country.We characterize and compare the QoS and QoE metrics in SNESet with existing publicly available datasets, highlighting that SNESet includes a significantly greater number of metrics (horizontal diversity and vertical hierarchy) and provides more comprehensive coverage of specific metrics.Moreover, we qualitatively and quantitatively analyze the impact of QoS on QoE in both domain-general and domain-specific scenarios. Our findings can inform the system design decisions that different entities in the video ecosystem (content providers, video player designers, third-party optimizers, edge vendors) make to maximize end-users experience and ultimately maximize the business outcomes. We hope SNESet can attract more research efforts in the data management community, computer network community, and beyond.",10.1145/3626723
10.1145/3626785,HEAL: Performance Troubleshooting Deep inside Data Center Hosts,"Pan, Yicheng; Zhang, Yang; Bi, Tingzhu; Han, Linlin; Zhang, Yu; Ma, Meng; Shen, Xiangzhuang; Jiang, Xinrui; Wang, Feng; Liu, Xian; Wang, Ping",2023,Proc. ACM Meas. Anal. Comput. Syst.,"This study demonstrates the salient facts and challenges of host failure operations in hyperscale data centers. A host incident can involve hundreds of distinct host-level metrics, covering broad aspects. The faulting mechanism inside the host connects these heterogeneous metrics through direct and indirect correlation, making it extremely difficult to sort out the propagation procedures and the root cause from these intertwined indicators. To deeply understand the failure mechanism inside the host, we develop HEAL -- a novel host metrics analysis toolkit. HEAL synergistically discovers dynamic causality in sparse heterogeneous host metrics by combining the strengths of both time series and random variable analysis. It can also proactively extract causal directional hints from causality's asymmetry and historical knowledge. Together, these breakthroughs help HEAL produce accurate results given undesirable inputs. Extensive experiments in our production environment verify that HEAL provides significantly better result accuracy and full-process interpretability than the SOTA baselines. With these advantages, HEAL successfully serves our data center and worldwide product operations and impressively contributes to many other workflows.",10.1145/3626785
10.1145/3627163,Self-Adaptive Testing in the Field,"Silva, Samira; Pelliccione, Patrizio; Bertolino, Antonia",2024,ACM Trans. Auton. Adapt. Syst.,"We are increasingly surrounded by systems connecting us with the digital world and facilitating our life by supporting our work, leisure, activities at home, health, and so on. These systems are pressed by two forces. On the one side, they operate in environments that are increasingly challenging due to uncertainty and uncontrollability. On the other side, they need to evolve, often in a continuous fashion, to meet changing needs, to offer new functionalities, or also to fix emerging failures. To make the picture even more complex, these systems rarely work in isolation and often need to collaborate with other systems, as well as humans. All such facets call for moving their validation during operation, as offered by approaches called testing in the field.In this article, we observe that even the field-based testing approaches should change over time to follow and adapt to the changes and evolution of collaborating systems or environments or users’ behaviors. We provide a taxonomy of this new category of testing that we call self-adaptive testing in the field (SATF), together with a reference architecture for SATF approaches. To achieve this objective, we surveyed the literature and collected feedback and contributions from experts in the domain via a questionnaire and interviews.",10.1145/3627163
10.1145/3629141,EXPLORA: AI/ML EXPLainability for the Open RAN,"Fiandrino, Claudio; Bonati, Leonardo; D'Oro, Salvatore; Polese, Michele; Melodia, Tommaso; Widmer, Joerg",2023,Proc. ACM Netw.,"The Open Radio Access Network (RAN) paradigm is transforming cellular networks into a system of disaggregated, virtualized, and software-based components. These self-optimize the network through programmable, closed-loop control, leveraging Artificial Intelligence (AI) and Machine Learning (ML) routines. In this context, Deep Reinforcement Learning (DRL) has shown great potential in addressing complex resource allocation problems. However, DRL-based solutions are inherently hard to explain, which hinders their deployment and use in practice. In this paper, we propose EXPLORA, a framework that provides explainability of DRL-based control solutions for the Open RAN ecosystem. EXPLORA synthesizes network-oriented explanations based on an attributed graph that produces a link between the actions taken by a DRL agent (i.e., the nodes of the graph) and the input state space (i.e., the attributes of each node). This novel approach allows EXPLORA to explain models by providing information on the wireless context in which the DRL agent operates. EXPLORA is also designed to be lightweight for real-time operation. We prototype EXPLORA and test it experimentally on an O-RAN-compliant near-real-time RIC deployed on the Colosseum wireless network emulator. We evaluate EXPLORA for agents trained for different purposes and showcase how it generates clear network-oriented explanations. We also show how explanations can be used to perform informative and targeted intent-based action steering and achieve median transmission bitrate improvements of 4\% and tail improvements of 10\%.",10.1145/3629141
10.1145/3630006,Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing,"Zhao, Laiping; Cui, Yushuai; Yang, Yanan; Zhou, Xiaobo; Qiu, Tie; Li, Keqiu; Bao, Yungang",2024,ACM Trans. Comput. Syst.,"Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3\%, CPU utilization by 38.6\%, and memory bandwidth utilization by 45.4\% while guaranteeing the tail latency requirement.",10.1145/3630006
10.1145/3631607,Optimizing Resource Management for Shared Microservices: A Scalable System Design,"Luo, Shutian; Lin, Chenyu; Ye, Kejiang; Xu, Guoyao; Zhang, Liping; Yang, Guodong; Xu, Huanle; Xu, Chengzhong",2024,ACM Trans. Comput. Syst.,"A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements.We present an efficient resource management system, namely Erms, for guaranteeing SLAs with high probability in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5\texttimes{} and more importantly, lead to a reduction in resource usage by 1.6\texttimes{}, compared to state-of-the-art approaches.",10.1145/3631607
10.1145/3632398,Choral: Object-oriented Choreographic Programming,"Giallorenzo, Saverio; Montesi, Fabrizio; Peressotti, Marco",2024,ACM Trans. Program. Lang. Syst.,"Choreographies are coordination plans for concurrent and distributed systems, which define the roles of the involved participants and how they are supposed to work together. In the paradigm of choreographic programming, choreographies are programs that can be compiled into executable implementations.In this article, we present Choral, the first choreographic programming language based on mainstream abstractions. The key idea in Choral is a new notion of data type, which allows for expressing that data is distributed over different roles. We use this idea to reconstruct the paradigm of choreographic programming through object-oriented abstractions. Choreographies are classes, and instances of choreographies are objects with states and behaviours implemented collaboratively by roles.Choral comes with a compiler that, given a choreography, generates an implementation for each of its roles. These implementations are libraries in pure Java, whose types are under the control of the Choral programmer. Developers can then modularly compose these libraries in their programs, to participate correctly in choreographies. Choral is the first incarnation of choreographic programming offering such modularity, which finally connects more than a decade of research on the paradigm to practical software development.The integration of choreographic and object-oriented programming yields other powerful advantages, where the features of one paradigm benefit the other in ways that go beyond the sum of the parts. On the one hand, the high-level abstractions and static checks from the world of choreographies can be used to write concurrent and distributed object-oriented software more concisely and correctly. On the other hand, we obtain a much more expressive choreographic language from object-oriented abstractions than in previous work. This expressivity allows for writing more reusable and flexible choreographies. For example, object passing makes Choral the first higher-order choreographic programming language, whereby choreographies can be parameterised over other choreographies without any need for central coordination. We also extend method overloading to a new dimension: specialisation based on data location. Together with subtyping and generics, this allows Choral to elegantly support user-defined communication mechanisms and middleware.",10.1145/3632398
10.1145/3632922,Indexed Types for a Statically Safe WebAssembly,"Geller, Adam T.; Frank, Justin; Bowman, William J.",2024,Proc. ACM Program. Lang.,"We present Wasm-prechk, a superset of WebAssembly (Wasm) that uses indexed types to express and check simple constraints over program values. This additional static reasoning enables safely removing dynamic safety checks from Wasm, such as memory bounds checks. We implement Wasm-prechk as an extension of the Wasmtime compiler and runtime, evaluate the run-time and compile-time performance of Wasm-prechk vs WebAssembly configurations with explicit dynamic checks, and find an average run-time performance gain of 1.71x faster in the widely used PolyBenchC benchmark suite, for a small overhead in binary size (7.18\% larger) and type-checking time (1.4\% slower). We also prove type and memory safety of Wasm-prechk, prove Wasm safely embeds into Wasm-prechk ensuring backwards compatibility, prove Wasm-prechk type-erases to Wasm, and discuss design and implementation trade-offs.",10.1145/3632922
10.1145/3634750,NEPTUNE: A Comprehensive Framework for Managing Serverless Functions at the Edge,"Baresi, Luciano; Hu, Davide Yi Xian; Quattrocchi, Giovanni; Terracciano, Luca",2024,ACM Trans. Auton. Adapt. Syst.,"Applications that are constrained by low-latency requirements can hardly be executed on cloud infrastructures, given the high network delay required to reach remote servers. Multi-access Edge Computing (MEC) is the reference architecture for executing applications on nodes that are located close to users (i.e., at the edge of the network). This way, the network overhead is reduced but new challenges emerge. The resources available on edge nodes are limited, workloads fluctuate since users can rapidly change location, and complex tasks are becoming widespread (e.g., machine learning inference). To address these issues, this article presents NEPTUNE, a serverless-based framework that automates the management of large-scale MEC infrastructures. In particular, NEPTUNE provides (i) the placement of serverless functions on MEC nodes according to users’ location, (ii) the resolution of resource contention scenarios by avoiding that single nodes be saturated, and (iii) the dynamic allocation of CPUs and GPUs to meet foreseen execution times. To assess NEPTUNE, we built a prototype based on K3S, an edge-dedicated version of Kubernetes, and executed a comprehensive set of experiments. Results show that NEPTUNE obtains a significant reduction in terms of response time, network overhead, and resource consumption compared with five state-of-the-art solutions.",10.1145/3634750
10.1145/3635709,Causality-driven Testing of Autonomous Driving Systems,"Giamattei, Luca; Guerriero, Antonio; Pietrantuono, Roberto; Russo, Stefano",2024,ACM Trans. Softw. Eng. Methodol.,"Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART (CAusal-Reasoning-driven Testing), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.",10.1145/3635709
10.1145/3636515,Automated Grading and Feedback Tools for Programming Education: A Systematic Review,"Messer, Marcus; Brown, Neil C. C.; K\""{o}lling, Michael; Shi, Miaojing",2024,ACM Trans. Comput. Educ.,"We conducted a systematic literature review on automated grading and feedback tools for programming education. We analysed 121 research papers from 2017 to 2021 inclusive and categorised them based on skills assessed, approach, language paradigm, degree of automation, and evaluation techniques. Most papers assess the correctness of assignments in object-oriented languages. Typically, these tools use a dynamic technique, primarily unit testing, to provide grades and feedback to the students or static analysis techniques to compare a submission with a reference solution or with a set of correct student submissions. However, these techniques’ feedback is often limited to whether the unit tests have passed or failed, the expected and actual output, or how they differ from the reference solution. Furthermore, few tools assess the maintainability, readability, or documentation of the source code, with most using static analysis techniques, such as code quality metrics, in conjunction with grading correctness. Additionally, we found that most tools offered fully automated assessment to allow for near-instantaneous feedback and multiple resubmissions, which can increase student satisfaction and provide them with more opportunities to succeed. In terms of techniques used to evaluate the tools’ performance, most papers primarily use student surveys or compare the automatic assessment tools to grades or feedback provided by human graders. However, because the evaluation dataset is frequently unavailable, it is more difficult to reproduce results and compare tools to a collection of common assignments.",10.1145/3636515
10.1145/3638553,Ad Hoc Transactions through the Looking Glass: An Empirical Study of Application-Level Transactions in Web Applications,"Wang, Zhaoguo; Tang, Chuzhe; Zhang, Xiaodong; Yu, Qianmian; Zang, Binyu; Guan, Haibing; Chen, Haibo",2024,ACM Trans. Database Syst.,"Many transactions in web applications are constructed ad hoc in the application code. For example, developers might explicitly use locking primitives or validation procedures to coordinate critical code fragments. We refer to database operations coordinated by application code as ad hoc transactions. Until now, little is known about them. This paper presents the first comprehensive study on ad hoc transactions. By studying 91 ad hoc transactions among eight popular open-source web applications, we found that (i) every studied application uses ad hoc transactions (up to 16 per application), 71 of which play critical roles; (ii) compared with database transactions, concurrency control of ad hoc transactions is much more flexible; (iii) ad hoc transactions are error-prone—53 of them have correctness issues, and 33 of them are confirmed by developers; and (iv) ad hoc transactions have the potential for improving performance in contentious workloads by utilizing application semantics such as access patterns. Based on these findings, we discuss the implications of ad hoc transactions to the database research community.",10.1145/3638553
10.1145/3639404,Diciclo: Flexible User-level Services for Efficient Multitenant Isolation,"Kappes, Giorgos; Anastasiadis, Stergios V.",2024,ACM Trans. Comput. Syst.,"Containers are a mainstream virtualization technique for running stateful workloads over persistent storage. In highly utilized multitenant hosts, resource contention at the system kernel leads to inefficient container input/output (I/O) handling. Although there are interesting techniques to address this issue, they incur high implementation complexity and execution overhead. As a cost-effective alternative, we introduce the Diciclo architecture with our assumptions, goals, and principles. For each tenant, Diciclo isolates the control and data I/O path at user level and runs dedicated storage systems. Diciclo includes the libservice unified user-level abstraction of system services and the node structure design pattern for the application and server side. We prototyped a toolkit of user-level components that comprise the library to invoke the standard I/O calls, the I/O communication mechanism, and the I/O services. Based on Diciclo, we built Danaus, a filesystem client that integrates a union filesystem with a Ceph distributed filesystem client and configurable shared cache. Across different host configurations, workloads, and systems, Danaus achieves improved performance stability, because it handles I/O with reserved per-tenant resources and avoids intensive kernel locking. Based on having built and evaluated Danaus, we share valuable lessons about resource contention, file management, service separation, and performance stability in multitenant systems.",10.1145/3639404
10.1145/3640818,EffCause: Discover Dynamic Causal Relationships Efficiently from Time-Series,"Pan, Yicheng; Zhang, Yifan; Jiang, Xinrui; Ma, Meng; Wang, Ping",2024,ACM Trans. Knowl. Discov. Data,"Since the proposal of Granger causality, many researchers have followed the idea and developed extensions to the original algorithm. The classic Granger causality test aims to detect the existence of the static causal relationship. Notably, a fundamental assumption underlying most previous studies is the stationarity of causality, which requires the causality between variables to keep stable. However, this study argues that it is easy to break in real-world scenarios. Fortunately, our paper presents an essential observation: if we consider a sufficiently short window when discovering the rapidly changing causalities, they will keep approximately static and thus can be detected using the static way correctly. In light of this, we develop EffCause, bringing dynamics into classic Granger causality. Specifically, to efficiently examine the causalities on different sliding window lengths, we design two optimization schemes in EffCause and demonstrate the advantage of EffCause through extensive experiments on both simulated and real-world datasets. The results validate that EffCause achieves state-of-the-art accuracy in continuous causal discovery tasks while achieving faster computation. Case studies from cloud system failure analysis and traffic flow monitoring show that EffCause effectively helps us understand real-world time-series data and solve practical problems.",10.1145/3640818
10.1145/3643029,Trinity: High-Performance and Reliable Mobile Emulation through Graphics Projection,"Lin, Hao; Li, Zhenhua; Gao, Di; Liu, Yunhao; Qian, Feng; Xu, Tianyin; Xiao, Bo; Qin, Xiaokang",2024,ACM Trans. Comput. Syst.,"Mobile emulation, which creates full-fledged software mobile devices on a physical PC/server, is pivotal to the mobile ecosystem. Unfortunately, existing mobile emulators perform poorly on graphics-intensive apps in terms of efficiency and compatibility. To address this, we introduce graphics projection, a novel graphics virtualization mechanism that adds a small-size projection space inside the guest memory, which processes graphics operations involving control contexts and resource handles without host interactions. While enhancing performance, the decoupled and asynchronous guest/host control flows introduced by graphics projection can significantly complicate emulators’ reliability issue diagnosis when faced with a variety of uncommon or non-standard app behaviors in the wild, hindering practical deployment in production. To overcome this drawback, we develop an automatic reliability issue analysis pipeline that distills the critical code paths across the guest and host control flows by runtime quarantine and state introspection. The resulting new Android emulator, dubbed Trinity, exhibits an average of 97\% native hardware performance and 99.3\% reliable app support, in some cases outperforming other emulators by more than an order of magnitude.",10.1145/3643029
10.1145/3643728,ChangeRCA: Finding Root Causes from Software Changes in Large Online Systems,"Yu, Guangba; Chen, Pengfei; He, Zilong; Yan, Qiuyu; Luo, Yu; Li, Fangyuan; Zheng, Zibin",2024,Proc. ACM Softw. Eng.,"In large-scale online service systems, the occurrence of software changes is inevitable and frequent. Despite rigorous pre-deployment testing practices, the presence of defective software changes in the online environment cannot be completely eliminated. Consequently, there is a pressing need for automated techniques that can effectively identify these defective changes. However, the current abnormal change detection (ACD) approaches fall short in accurately pinpointing defective changes, primarily due to their disregard for the propagation of faults. To address the limitations of ACD, we propose a novel concept called root cause change analysis (RCCA) to identify the underlying root causes of change-inducing incidents. In order to apply the RCCA concept to practical scenarios, we have devised an intelligent RCCA framework named ChangeRCA. This framework aims to localize the defective change associated with change-inducing incidents among multiple changes. To assess the effectiveness of ChangeRCA, we have conducted an extensive evaluation utilizing a real-world dataset from WeChat and a simulated dataset encompassing 81 diverse defective changes. The evaluation results demonstrate that ChangeRCA outperforms the state-of-the-art ACD approaches, achieving an impressive Top-1 Hit Rate of 85\% and significantly reducing the time required to identify defective changes.",10.1145/3643728
10.1145/3643748,TraStrainer: Adaptive Sampling for Distributed Traces with System Runtime State,"Huang, Haiyu; Zhang, Xiaoyu; Chen, Pengfei; He, Zilong; Chen, Zhiming; Yu, Guangba; Chen, Hongyang; Sun, Chen",2024,Proc. ACM Softw. Eng.,"Distributed tracing has been widely adopted in many microservice systems and plays an important role in monitoring and analyzing the system. However, trace data often come in large volumes, incurring substantial computational and storage costs. To reduce the quantity of traces, trace sampling has become a prominent topic of discussion, and several methods have been proposed in prior work. To attain higher-quality sampling outcomes, biased sampling has gained more attention compared to random sampling. Previous biased sampling methods primarily considered the importance of traces based on diversity, aiming to sample more edge-case traces and fewer common-case traces. However, we contend that relying solely on trace diversity for sampling is insufficient, system runtime state is another crucial factor that needs to be considered, especially in cases of system failures. In this study, we introduce TraStrainer, an online sampler that takes into account both system runtime state and trace diversity. TraStrainer employs an interpretable and automated encoding method to represent traces as vectors. Simultaneously, it adaptively determines sampling preferences by analyzing system runtime metrics. When sampling, it combines the results of system-bias and diversity-bias through a dynamic voting mechanism. Experimental results demonstrate that TraStrainer can achieve higher quality sampling results and significantly improve the performance of downstream root cause analysis (RCA) tasks. It has led to an average increase of 32.63\% in Top-1 RCA accuracy compared to four baselines in two datasets.",10.1145/3643748
10.1145/3643819,An End-to-end High-performance Deduplication Scheme for Docker Registries and Docker Container Storage Systems,"Zhao, Nannan; Lin, Muhui; Albahar, Hadeel; Paul, Arnab K.; Huan, Zhijie; Abraham, Subil; Chen, Keren; Tarasov, Vasily; Skourtis, Dimitrios; Anwar, Ali; Butt, Ali",2024,ACM Trans. Storage,"The wide adoption of Docker containers for supporting agile and elastic enterprise applications has led to a broad proliferation of container images. The associated storage performance and capacity requirements place a high pressure on the infrastructure of container registries that store and distribute images and container storage systems on the Docker client side that manage image layers and store ephemeral data generated at container runtime. The storage demand is worsened by the large amount of duplicate data in images. Moreover, container storage systems that use Copy-on-Write (CoW) file systems as storage drivers exacerbate the redundancy. Exploiting the high file redundancy in real-world images is a promising approach to drastically reduce the growing storage requirements of container registries and improve the space efficiency of container storage systems. However, existing deduplication techniques significantly degrade the performance of both registries and container storage systems because of data reconstruction overhead as well as the deduplication cost.We propose DupHunter, an end-to-end deduplication scheme that deduplicates layers for both Docker registries and container storage systems while maintaining a high image distribution speed and container I/O performance. DupHunter is divided into three tiers: registry tier, middle tier, and client tier. Specifically, we first build a high-performance deduplication engine at the registry tier that not only natively deduplicates layers for space savings but also reduces layer restore overhead. Then, we use deduplication offloading at the middle tier to eliminate the redundant files from the client tier and avoid bringing deduplication overhead to the clients. To further reduce the data duplicates caused by CoWs and improve the container I/O performance, we utilize a container-aware storage system at the client tier that reserves space for each container and arranges the placement of files and their modifications on the disk to preserve locality. Under real workloads, DupHunter reduces storage space by up to 6.9\texttimes{} and reduces the GET layer latency up to 2.8\texttimes{} compared to the state-of-the-art. Moreover, DupHunter can improve the container I/O performance by up to 93\% for reads and 64\% for writes.",10.1145/3643819
10.1145/3645092,A Systematic Literature Review on Maintenance of Software Containers,"Malhotra, Ruchika; Bansal, Anjali; Kessentini, Marouane",2024,ACM Comput. Surv.,"Nowadays, cloud computing is gaining tremendous attention to deliver information via the internet. Virtualization plays a major role in cloud computing as it deploys multiple virtual machines on the same physical machine and thus results in improving resource utilization. Hypervisor-based virtualization and containerization are two commonly used approaches in operating system virtualization. In this article, we provide a systematic literature review on various phases in maintenance of containers including container image detection, container scheduling, container security measures, and performance evaluation of containers. We have selected 145 primary studies out of which 24\% of studies are related to container performance evaluation, 42\% of studies are related to container scheduling techniques, 22\% of studies are related to container security measures, and 12\% of studies are related to container image detection process. A few studies are related to container image detection process and evaluation of container security measures. Resource utilization is the most considered performance objective in almost all container scheduling techniques. We conclude that there is a need to introduce new tagging approaches, smell detection approaches, and also new approaches to detect and resolve threat issues in containers so that we can maintain the security of containers.",10.1145/3645092
10.1145/3649453,Anunnaki: A Modular Framework for Developing Trusted Artificial Intelligence,"Langford, Michael Austin; Zilberman, Sol; Cheng, Betty",2024,ACM Trans. Auton. Adapt. Syst.,"Trustworthy artificial intelligence (Trusted AI) is of utmost importance when learning-enabled components (LECs) are used in autonomous, safety-critical systems. When reliant on deep learning, these systems need to address the reliability, robustness, and interpretability of learning models. In addition to developing strategies to address these concerns, appropriate software architectures are needed to coordinate LECs and ensure they deliver acceptable behavior even under uncertain conditions. This work describes Anunnaki, a model-driven framework comprising loosely-coupled modular services designed to monitor and manage LECs with respect to Trusted AI assurance concerns when faced with different sources of uncertainty. More specifically, the Anunnaki framework supports the composition of independent, modular services to assess and improve the resilience and robustness of AI systems. The design of Annunaki was guided by several key software engineering principles (e.g., modularity, composability, and reusability) in order to facilitate its use and maintenance to support different aggregate monitoring and assurance analysis tools for LESs and their respective data sets. We demonstrate Anunnaki on two autonomous platforms, a terrestrial rover, and an unmanned aerial vehicle. Our studies show how Anunnaki can be used to manage the operations of different autonomous learning-enabled systems with vision-based LECs while exposed to uncertain environmental conditions.",10.1145/3649453
10.1145/3649834,Design and Implementation of an Aspect-Oriented C Programming Language,"Chen, Zhe; Zhu, Yunlong; Wang, Zhemin",2024,Proc. ACM Program. Lang.,"Aspect-Oriented Programming (AOP) is a programming paradigm that implements crosscutting concerns in a modular way. People have witnessed the prosperity of AOP languages for Java and C++, such as AspectJ and AspectC++, which has propelled AOP to become an important programming paradigm with many interesting application scenarios, e.g., runtime verification. In contrast, the AOP languages for C are still poor and lack compiler support. In this paper, we design a new general-purpose and expressive aspect-oriented C programming language, namely Aclang, and implement a compiler for it, which brings fully-fledged AOP support into the C domain. We have evaluated the effectiveness and performance of our compiler against two state-of-the-art tools, ACC and AspectC++. In terms of effectiveness, Aclang outperforms ACC and AspectC++. In terms of performance, Aclang outperforms ACC in execution time and outperforms AspectC++ in both execution time and memory consumption.",10.1145/3649834
10.1145/3650110,Camouflage: Utility-Aware Obfuscation for Accurate Simulation of Sensitive Program Traces,"Pal, Asmita; Desai, Keerthana; Chatterjee, Rahul; San Miguel, Joshua",2024,ACM Trans. Archit. Code Optim.,"Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility, while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8\%, which is better than traces generated from the state-of-the-art.",10.1145/3650110
10.1145/3651618,Accurate Localization in LOS/NLOS Channel Coexistence Scenarios Based on Heterogeneous Knowledge Graph Inference,"Zhang, Bojun; Liu, Xiulong; Xie, Xin; Tong, Xinyu; Jia, Yungang; Shi, Tuo; Qu, Wenyu",2024,ACM Trans. Sen. Netw.,"Accurate localization is one of the basic requirements for smart cities and smart factories. In wireless cellular network localization, the straight-line propagation of electromagnetic waves between base stations and users is called line-of-sight (LOS) wireless propagation. In some cases, electromagnetic wave signals cannot propagate in a straight line due to obstruction by buildings or trees, and these scenarios are usually called non-LOS (NLOS) wireless propagation. Traditional localization algorithms such as TDOA, AOA, etc., are based on LOS channels, which are no longer applicable in environments where NLOS propagation is dominant, and in most scenarios, the number of base stations with LOS channels containing users is often small, resulting in traditional localization algorithms being unable to satisfy the accuracy demand of high-precision localization. In addition, some nonideal factors may be included in the actual system, all of which can lead to localization accuracy degradation. Therefore, the approach developed in this paper uses knowledge graph and graph neural network (GNN) technology to model communication data as knowledge graphs, and it adopts the knowledge graph inference technique based on a heterogeneous graph attention mechanism to infer unknown data representations in complex scenarios based on the known data and the relationships between the data to achieve high-precision localization in scenarios with LOS/NLOS channel coexistence. We experimentally demonstrate a spatial 2D localization accuracy level of approximately 10 meters on multiple datasets and find that our proposed algorithm has higher accuracy and stronger robustness than the state-of-the-art algorithms.",10.1145/3651618
10.1145/3652157,Advanced White-Box Heuristics for Search-Based Fuzzing of REST APIs,"Arcuri, Andrea; Zhang, Man; Galeotti, Juan",2024,ACM Trans. Softw. Eng. Methodol.,"Due to its importance and widespread use in industry, automated testing of REST APIs has attracted major interest from the research community in the last few years. However, most of the work in the literature has been focused on black-box fuzzing. Although existing fuzzers have been used to automatically find many faults in existing APIs, there are still several open research challenges that hinder the achievement of better results (e.g., in terms of code coverage and fault finding). For example, under-specified schemas are a major issue for black-box fuzzers. Currently, EvoMaster is the only existing tool that supports white-box fuzzing of REST APIs. In this paper, we provide a series of novel white-box heuristics, including for example how to deal with under-specified constrains in API schemas, as well as under-specified schemas in SQL databases. Our novel techniques are implemented as an extension to our open-source, search-based fuzzer EvoMaster. An empirical study on 14 APIs from the EMB corpus, plus one industrial API, shows clear improvements of the results in some of these APIs.",10.1145/3652157
10.1145/3652508,DevOps Metrics and KPIs: A Multivocal Literature Review,"Amaro, Ricardo; Pereira, R\'{u}ben; Mira da Silva, Miguel",2024,ACM Comput. Surv.,"Context: Information Technology organizations are aiming to implement DevOps capabilities to fulfill market, customer, and internal needs. While many are successful with DevOps implementation, others still have difficulty measuring DevOps success in their organization. As a result, the effectiveness of assessing DevOps remains erratic. This emphasizes the need to withstand management in measuring the implementation process with suitable DevOps Metrics. But what are these metrics?Objective: This research seeks to provide relevant DevOps Metrics to facilitate the efficiency of DevOps adoption and better analyze DevOps performance in enterprises.Method: A Multivocal Literature Review (MLR) is conducted, with 139 documents gathered and thoroughly examined from throughout the community, including books, scientific articles, white papers, conferences, among others.Results: This article conducts an extensive and rigorous MLR, contributing with a definition of DevOps Metrics, 22 main metrics, their definitions, importance, and categorization in sets of Key Performance Indicators, as well as exposing clear indicators on how to improve them. It is also discussed how metrics could be put into practice and what constitutes a change in the context of DevOps Metrics. The study’s outcomes will assist researchers and practitioners understand DevOps Metrics and how to better implement them.",10.1145/3652508
10.1145/3652515,A Differential Evolution Offloading Strategy for Latency and Privacy Sensitive Tasks with Federated Local-edge-cloud Collaboration,"Chen, Yishan; Li, Wei; Huang, Junhong; Gao, Honghao; Deng, Shuiguang",2024,ACM Trans. Sen. Netw.,"Due to an explosive growth in mobile devices and the rapid evolution of wireless communication technologies, local-edge-cloud computing is becoming an attractive solution for providing a higher-quality service by exploiting the multi-computation power of mobile devices, edge servers and cloud. However, as the tasks are latency and privacy sensitive, highly credible task offloading becomes a crucial problem in a local-edge-cloud orchestrated computing system. In this paper, we study the computation offloading problem for latency and privacy sensitive tasks in a hierarchical local-edge-cloud network by using federated learning method. Our goal is to minimize the operational time of latency-sensitive tasks requested by mobile devices that have data privacy concerns, while each task can be executed under local, edge or cloud computing mode with no need to rely on privacy data. We first build system models to analyze the latency incurred under different computing modes, and then develop a constrained optimization problem to minimize the latency consumed by the federated offloading collaboration. A Hierarchical Federated Averaging method based on Differential Evolution algorithm (HierFAVG-DE) is proposed for solving the problem in-hand, and extensive simulations are conducted to verify the superiority of our approach.",10.1145/3652515
10.1145/3652862,A compositional simulation framework for Abstract State Machine models of Discrete Event Systems,"Bonfanti, Silvia; Gargantini, Angelo; Riccobene, Elvinia; Scandurra, Patrizia",2024,Form. Asp. Comput.,"Modeling complex system requirements often requires specifying system components in separate models, which can be validated and verified in isolation from each other, and then integrating all components’ behavior in order to validate the operation of the whole system. If models are executable, as for state-based formal specifications, engines to orchestrate the simulation of separate component operational models are extremely useful. This paper presents an approach for the co-simulation, according to predefined orchestration schemas, of state-based models of separate components of a Discrete Event System. More precisely, we exploit the Abstract State Machine (ASM) formal method as state-based formalism, and we (i) define a set of operators to compose ASMs that communicate with each other through I/O events, and (ii) present an engine to execute the compositional simulation of the ASMs as a whole assembly. As proof of concepts, we use a set of model examples of Discrete Event Systems of increasing complexity to show the application of our approach and to evaluate its effectiveness in co-simulating models of real systems.",10.1145/3652862
10.1145/3653072,Boki: Towards Data Consistency and Fault Tolerance with Shared Logs in Stateful Serverless Computing,"Jia, Zhipeng; Witchel, Emmett",2024,ACM Trans. Comput. Syst.,"Bokiis a new serverless runtime that exports a shared log API to serverless functions. Boki shared logs enable stateful serverless applications to manage their state with durability, consistency, and fault tolerance. Boki shared logs achieve high throughput and low latency. The key enabler is the metalog, a novel mechanism that allows Boki to address ordering, consistency and fault tolerance independently. The metalog orders shared log records with high throughput and it provides read consistency while allowing service providers to optimize the write and read path of the shared log in different ways. To demonstrate the value of shared logs for stateful serverless applications, we build Boki support libraries that implement fault-tolerant workflows, durable object storage, and message queues. Our evaluation shows that shared logs can speed up important serverless workloads by up to 4.2\texttimes{}.",10.1145/3653072
10.1145/3656405,Jacdac: Service-Based Prototyping of Embedded Systems,"Ball, Thomas; de Halleux, Peli; Devine, James; Hodges, Steve; Moskal, Micha\l{}",2024,Proc. ACM Program. Lang.,"The traditional approach to programming embedded systems is monolithic: firmware on a microcontroller contains both application code and the drivers needed to communicate with sensors and actuators, using low-level protocols such as I2C, SPI, and RS232. In comparison, software development for the cloud has moved to a service-based development and operation paradigm: a service provides a discrete unit of functionality that can be accessed remotely by an application, or other service, but is independently managed and updated. We propose, design, implement, and evaluate a service-based approach to prototyping embedded systems called Jacdac. Jacdac defines a service specification language, designed especially for embedded systems, along with a host of specifications for a variety of sensors and actuators. With Jacdac, each sensor/actuator in a system is paired with a low-cost microcontroller that advertises the services that represent the functionality of the underlying hardware over an efficient and low-cost single-wire bus protocol. A separate microcontroller executes the user's application program, which is a client of the Jacdac services on the bus. Our evaluation shows that Jacdac supports a service-based abstraction for sensors/actuators at low cost and reasonable performance, with many benefits for prototyping: ease of use via the automated discovery of devices and their capabilities, substitution of same-service devices for each other, as well as high-level programming, monitoring, and debugging. We also report on the experience of bringing Jacdac to commercial availability via third-party manufacturers.",10.1145/3656405
10.1145/3660794,Predicting Failures of Autoscaling Distributed Applications,"Denaro, Giovanni; El Moussa, Noura; Heydarov, Rahim; Lomio, Francesco; Pezz\`{e}, Mauro; Qiu, Ketai",2024,Proc. ACM Softw. Eng.,"Predicting failures in production environments allows service providers to activate countermeasures that prevent harming the users of the applications. The most successful approaches predict failures from error states that the current approaches identify from anomalies in time series of fixed sets of KPI values collected at runtime. They cannot handle time series of KPI sets with size that varies over time. Thus these approaches work with applications that run on statically configured sets of components and computational nodes, and do not scale up to the many popular cloud applications that exploit autoscaling. This paper proposes Preface, a novel approach to predict failures in cloud applications that exploit autoscaling. Preface originally augments the neural-network-based failure predictors successfully exploited to predict failures in statically configured applications, with a Rectifier layer that handles KPI sets of highly variable size as the ones collected in cloud autoscaling applications, and reduces those KPIs to a set of rectified-KPIs of fixed size that can be fed to the neural-network predictor. The Preface Rectifier computes the rectified-KPIs as descriptive statistics of the original KPIs, for each logical component of the target application. The descriptive statistics shrink the highly variable sets of KPIs collected at different timestamps to a fixed set of values compatible with the input nodes of the neural-network failure predictor. The neural network can then reveal anomalies that correspond to error states, before they propagate to failures that harm the users of the applications. The experiments on both a commercial application and a widely used academic exemplar confirm that Preface can indeed predict many harmful failures early enough to activate proper countermeasures.",10.1145/3660794
10.1145/3660805,BARO: Robust Root Cause Analysis for Microservices via Multivariate Bayesian Online Change Point Detection,"Pham, Luan; Ha, Huong; Zhang, Hongyu",2024,Proc. ACM Softw. Eng.,"Detecting failures and identifying their root causes promptly and accurately is crucial for ensuring the availability of microservice systems. A typical failure troubleshooting pipeline for microservices consists of two phases: anomaly detection and root cause analysis. While various existing works on root cause analysis require accurate anomaly detection, there is no guarantee of accurate estimation with anomaly detection techniques. Inaccurate anomaly detection results can significantly affect the root cause localization results. To address this challenge, we propose BARO, an end-to-end approach that integrates anomaly detection and root cause analysis for effectively troubleshooting failures in microservice systems. BARO leverages the Multivariate Bayesian Online Change Point Detection technique to model the dependency within multivariate time-series metrics data, enabling it to detect anomalies more accurately. BARO also incorporates a novel nonparametric statistical hypothesis testing technique for robustly identifying root causes, which is less sensitive to the accuracy of anomaly detection compared to existing works. Our comprehensive experiments conducted on three popular benchmark microservice systems demonstrate that BARO consistently outperforms state-of-the-art approaches in both anomaly detection and root cause analysis.",10.1145/3660805
10.1145/3660812,A Weak Supervision-Based Approach to Improve Chatbots for Code Repositories,"Farhour, Farbod; Abdellatif, Ahmad; Mansour, Essam; Shihab, Emad",2024,Proc. ACM Softw. Eng.,"Software chatbots are growing in popularity and have been increasingly used in software projects due to their benefits in saving time, cost, and effort. At the core of every chatbot is a Natural Language Understanding (NLU) component that enables chatbots to comprehend the users' queries. Prior work shows that chatbot practitioners face challenges in training the NLUs because the labeled training data is scarce. Consequently, practitioners resort to user queries to enhance chatbot performance. They annotate these queries and use them for NLU training. However, such training is done manually and prohibitively expensive. Therefore, we propose AlphaBot to automate the query annotation process for SE chatbots. Specifically, we leverage weak supervision to label users' queries posted to a software repository-based chatbot. To evaluate the impact of using AlphaBot on the NLU's performance, we conducted a case study using a dataset that comprises 749 queries and 52 intents. The results show that using AlphaBot improves the NLU's performance in terms of F1-score, with improvements ranging from 0.96\% to 35\%. Furthermore, our results show that applying more labeling functions improves the NLU's classification of users' queries. Our work enables practitioners to focus on their chatbots' core functionalities rather than annotating users' queries.",10.1145/3660812
10.1145/3661314,"ACM Transactions on Autonomous and Adaptive Systems (ACM TAAS): Editorial Welcome and Update on State of the Journal, Vision and Ongoing Developments","Bahsoon, Rami",2024,ACM Trans. Auton. Adapt. Syst.,,10.1145/3661314
10.1145/3663482,Addressing Data Challenges to Drive the Transformation of Smart Cities,"Gilman, Ekaterina; Bugiotti, Francesca; Khalid, Ahmed; Mehmood, Hassan; Kostakos, Panos; Tuovinen, Lauri; Ylipulli, Johanna; Su, Xiang; Ferreira, Denzil",2024,ACM Trans. Intell. Syst. Technol.,"Cities serve as vital hubs of economic activity and knowledge generation and dissemination. As such, cities bear a significant responsibility to uphold environmental protection measures while promoting the welfare and living comfort of their residents. There are diverse views on the development of smart cities, from integrating Information and Communication Technologies into urban environments for better operational decisions to supporting sustainability, wealth, and comfort of people. However, for all these cases, data are the key ingredient and enabler for the vision and realization of smart cities. This article explores the challenges associated with smart city data. We start with gaining an understanding of the concept of a smart city, how to measure that the city is a smart one, and what architectures and platforms exist to develop one. Afterwards, we research the challenges associated with the data of the cities, including availability, heterogeneity, management, analysis, privacy, and security. Finally, we discuss ethical issues. This article aims to serve as a “one-stop shop” covering data-related issues of smart cities with references for diving deeper into particular topics of interest.",10.1145/3663482
10.1145/3664200,Adaptation in Edge Computing: A Review on Design Principles and Research Challenges,"Golpayegani, Fateneh; Chen, Nanxi; Afraz, Nima; Gyamfi, Eric; Malekjafarian, Abdollah; Sch\""{a}fer, Dominik; Krupitzer, Christian",2024,ACM Trans. Auton. Adapt. Syst.,"Edge computing places the computational services and resources closer to the user proximity, to reduce latency, and ensure the quality of service and experience. Low latency, context awareness and mobility support are the major contributors to edge-enabled smart systems. Such systems require handling new situations and change on the fly and ensuring the quality of service while only having access to constrained computation and communication resources and operating in mobile, dynamic and ever-changing environments. Hence, adaptation and self-organisation are crucial for such systems to maintain their performance, and operability while accommodating new changes in their environment.This article reviews the current literature in the field of adaptive edge computing systems. We use a widely accepted taxonomy, which describes the important aspects of adaptive behaviour implementation in computing systems. This taxonomy discusses aspects such as adaptation reasons, the various levels an adaptation strategy can be implemented, the time of reaction to a change, categories of adaptation technique and control of the adaptive behaviour. In this article, we discuss how these aspects are addressed in the literature and identify the open research challenges and future direction in adaptive edge computing systems.The results of our analysis show that most of the identified approaches target adaptation at the application level, and only a few focus on middleware, communication infrastructure and context. Adaptations that are required to address the changes in the context, changes caused by users or in the system itself are also less explored. Furthermore, most of the literature has opted for reactive adaptation, although proactive adaptation is essential to maintain the edge computing systems’ performance and interoperability by anticipating the required adaptations on the fly. Additionally, most approaches apply a centralised adaptation control, which does not perfectly fit the mostly decentralised/distributed edge computing settings.",10.1145/3664200
10.1145/3664805,Technical Debt Monitoring Decision Making with Skin in the Game,"Fungprasertkul, Suwichak; Bahsoon, Rami; Kazman, Rick",2024,ACM Trans. Softw. Eng. Methodol.,"Technical Debt Management (TDM) can suffer from unpredictability, communication gaps and the inaccessibility of relevant information, which hamper the effectiveness of its decision making. These issues can stem from division among decision-makers which takes root in unfair consequences of decisions among different decision-makers. One mitigation route is Skin in the Game thinking, which enforces transparency, fairness and shared responsibility during collective decision-making under uncertainty. This article illustrates characteristics which require Skin in the Game thinking in Technical Debt (TD) identification, measurement, prioritisation and monitoring. We point out crucial problems in TD monitoring rooted in asymmetric information and asymmetric payoff between different factions of decision-makers. A systematic TD monitoring method is presented to mitigate the said problems. The method leverages Replicator Dynamics and Behavioural Learning. The method supports decision-makers with automated TD monitoring decisions; it informs decision-makers when human interventions are required. Two publicly available industrial projects with a non-trivial number of TD and timestamps are utilised to evaluate the application of our method. Mann–Whitney U hypothesis tests are conducted on samples of decisions from our method and the baseline. The statistical evidence indicates that our method can produce cost-effective and contextual TD monitoring decisions.",10.1145/3664805
10.1145/3665223,"IMAGE: An Open-Source, Extensible Framework for Deploying Accessible Audio and Haptic Renderings of Web Graphics","Regimbal, Juliette; Blum, Jeffrey R.; Kuo, Cyan; Cooperstock, Jeremy R.",2024,ACM Trans. Access. Comput.,"For accessibility practitioners, creating and deploying novel multimedia interactions for people with disabilities is a nontrivial task. As a result, many projects aiming to support such accessibility needs come and go or never make it to a public release. To reduce the overhead involved in deploying and maintaining a system that transforms web content into multimodal renderings, we created an open source, modular microservices architecture as part of the IMAGE project. This project aims to design richer means of interacting with web graphics than is afforded by a screen reader and text descriptions alone. To benefit the community of accessibility software developers, we discuss this architecture and explain how it provides support for several multimodal processing pipelines. Beyond illustrating the initial use case that motivated this effort, we further describe two use cases outside the scope of our project to explain how a team could use the architecture to develop and deploy accessible solutions for their own work. We then discuss our team’s experience working with the IMAGE architecture, informed by discussions with six project members, and provide recommendations to other practitioners considering applying the framework to their own accessibility projects.",10.1145/3665223
10.1145/3665795,Self-Supervised Machine Learning Framework for Online Container Security Attack Detection,"Tunde-Onadele, Olufogorehan; Lin, Yuhang; Gu, Xiaohui; He, Jingzhu; Latapie, Hugo",2024,ACM Trans. Auton. Adapt. Syst.,"Container security has received much research attention recently. Previous work has proposed to apply various machine learning techniques to detect security attacks in containerized applications. On one hand, supervised machine learning schemes require sufficient labeled training data to achieve good attack detection accuracy. On the other hand, unsupervised machine learning methods are more practical by avoiding training data labeling requirements, but they often suffer from high false alarm rates. In this article, we present a generic self-supervised hybrid learning (SHIL) framework for achieving efficient online security attack detection in containerized systems. SHIL can effectively combine both unsupervised and supervised learning algorithms but does not require any manual data labeling. We have implemented a prototype of SHIL and conducted experiments over 46 real-world security attacks in 29 commonly used server applications. Our experimental results show that SHIL can reduce false alarms by 33\%–93\% compared to existing supervised, unsupervised, or semi-supervised machine learning schemes while achieving a higher or similar detection rate.",10.1145/3665795
10.1145/3673897,A Toolset for Predicting Performance of Legacy Real-Time Software Based on the RAST Approach,"Tomak, Juri; Gorlatch, Sergei",2024,ACM Trans. Model. Comput. Simul.,"Simulating and predicting the performance of a distributed software system that works under stringent real-time constraints poses significant challenges, particularly when dealing with legacy systems being in production use, where any disruption is intolerable. This challenge is exacerbated in the context of a System Under Evaluation (SUE) that operates within a resource-sharing environment, running concurrently with numerous other software components. In this paper, we introduce an innovative toolset designed for predicting the performance of such complex and time-critical software systems. Our toolset builds upon the RAST (Regression Analysis, Simulation, and load Testing) approach, significantly enhanced in this paper compared to its initial version. While current state-of-the-art methods for performance prediction often rely on data collected by Application Performance Monitoring (APM), the unavailability of APM tools for existing systems and the complexities associated with integrating them into legacy software necessitate alternative approaches. Our toolset, therefore, utilizes readily accessible system request logs as a substitute for APM data. We describe the enhancements made to the original RAST approach, we outline the design and implementation of our RAST-based toolset, and we showcase its simulation accuracy and effectiveness using the publicly available TeaStore benchmarking system. To ensure the reproducibility of our experiments, we provide open access to our toolset’s implementation and the utilized TeaStore model.",10.1145/3673897
10.1145/3674726,HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Zhu, Zhouruixing; Lee, Cheryl; Tang, Xiaoying; He, Pinjia",2024,ACM Trans. Softw. Eng. Methodol.,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",10.1145/3674726
10.1145/3674734,Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency,"Antoniou, Georgia; Bartolini, Davide; Volos, Haris; Kleanthous, Marios; Wang, Zhe; Kalaitzidis, Kleovoulos; Rollet, Tom; Li, Ziwei; Mutlu, Onur; Sazeides, Yiannakis; Haj Yahya, Jawad",2024,ACM Trans. Archit. Code Optim.,"Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70\% with limited performance degradation (at most 2\%).",10.1145/3674734
10.1145/3674972,DeGONet: Decentralized Group-Oriented Interconnection Network for IoT-enabled Metaverse,"Jiang, Sining; Cheng, Xu; Dai, Hong-Ning; Zhao, Shuo; Lan, Yujun; Xie, Haoran; Tao, Xiaohui; Guo, Zhongwen",2024,ACM Trans. Internet Technol.,"As a transformative technology across various industries, the metaverse has emerged to connect the physical world with the virtual world. During this process, the Internet of Things (IoT) has played a critical role in achieving effective cyber-physical interaction. However, its prevalent centralized interconnection architectures encounter challenges related to interoperability and data privacy, consequently limiting their full potential in human-to-human interactions. To address these challenges, this paper introduces a novel decentralized group-oriented data interconnection network for IoT systems, abbreviated as DeGONet. We propose a group-based trust management model to facilitate user adaptability in data-sharing practices. Additionally, we present a new interaction paradigm based on smart contracts and oracles. Recognizing the potential latency and scalability limitations of existing blockchain structures in large-scale data integration, we devise a novel blockchain structure called Direct Acyclic Graph Tree (DAG-Tree) and a novel consensus mechanism, Proof-of-Verification. These contributions enhance data security while mitigating the hardware and development costs associated with decentralized systems. Through rigorous performance analysis and comparative experiments, we validate the effectiveness and efficiency of our proposed framework in large-scale data interconnection scenarios.",10.1145/3674972
10.1145/3676149,Introduction to the Special Issue on W4A’22,"Rauschenberger, Maria; Eraslan, Sukru",2024,ACM Trans. Access. Comput.,,10.1145/3676149
10.1145/3676167,Dynamically Balancing Load with Overload Control for Microservices,"Bhattacharya, Ratnadeep; Gao, Yuan; Wood, Timothy",2024,ACM Trans. Auton. Adapt. Syst.,"The microservices architecture simplifies application development by breaking monolithic applications into manageable microservices. However, this distributed microservice “service mesh” leads to new challenges due to the more complex application topology. Particularly, each service component scales up and down independently creating load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this article, we propose a self-managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes. BLOC uses overload control approaches to provide feedback to the load balancers. We show that this performs significantly better in solving the incast problem in microservice architectures. A critical component of BLOC is the dynamic capacity estimation algorithm. We show that a well-tuned capacity estimate can outperform even join-the-shortest-queue, a nearly optimal algorithm, while a reasonable dynamic estimate still outperforms Least Connection, a distributed implementation of join-the-shortest-queue. Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 –4 times and the tail, 99th percentile, latency by 2 times.",10.1145/3676167
10.1145/3680463,An Empirical Study of Testing Machine Learning in the Wild,"Openja, Moses; Khomh, Foutse; Foundjem, Armstrong; Jiang, Zhen Ming (Jack); Abidi, Mouna; Hassan, Ahmed E.",2024,ACM Trans. Softw. Eng. Methodol.,"Background: Recently, machine and deep learning (ML/DL) algorithms have been increasingly adopted in many software systems. Due to their inductive nature, ensuring the quality of these systems remains a significant challenge for the research community. Traditionally, software systems were constructed deductively, by writing explicit rules that govern the behavior of the system as program code. However, ML/DL systems infer rules from training data i.e., they are generated inductively. Recent research in ML/DL quality assurance has adapted concepts from traditional software testing, such as mutation testing, to improve reliability. However, it is unclear if these proposed testing techniques are adopted in practice, or if new testing strategies have emerged from real-world ML deployments. There is little empirical evidence about the testing strategies.Aims: To fill this gap, we perform the first fine-grained empirical study on ML testing in the wild to identify the ML properties being tested, the testing strategies, and their implementation throughout the ML workflow.Method: We conducted a mixed-methods study to understand ML software testing practices. We analyzed test files and cases from 11 open-source ML/DL projects on GitHub. Using open coding, we manually examined the testing strategies, tested ML properties, and implemented testing methods to understand their practical application in building and releasing ML/DL software systems.Results: Our findings reveal several key insights: (1) The most common testing strategies, accounting for less than 40\%, are Grey-box and White-box methods, such as Negative Testing, Oracle Approximation, and Statistical Testing. (2) A wide range of (17) ML properties are tested, out of which only 20\% to 30\% are frequently tested, including Consistency, Correctness, and Efficiency. (3) Bias and Fairness is more tested in Recommendation (6\%) and Computer Vision (CV) (3.9\%) systems, while Security and Privacy is tested in CV (2\%), Application Platforms (0.9\%), and NLP (0.5\%). (4) We identified 13 types of testing methods, such as Unit Testing, Input Testing, and Model Testing.Conclusions: This study sheds light on the current adoption of software testing techniques and highlights gaps and limitations in existing ML testing practices.",10.1145/3680463
10.1145/3682076,FedRL: A Reinforcement Learning Federated Recommender System for Efficient Communication Using Reinforcement Selector and Hypernet Generator,"Di, Yicheng; Shi, Hongjian; Ma, Ruhui; Gao, Honghao; Liu, Yuan; Wang, Weiyu",2024,ACM Trans. Recomm. Syst.,"The field of recommender systems aims to predict users' latent interests by analyzing their preferences and behaviors. However, privacy concerns about user data collection lead to challenges such as incomplete initial information and data sparsity. Federated learning has emerged to address these privacy issues in recommender systems. However, federated recommender systems face heterogeneity among edge devices regarding data features and sample sizes. Moreover, differences in computational and storage capabilities introduce communication overhead and processing delays during parameter aggregation at the third-party server. This paper introduces a framework named A Reinforcement Learning Federated Recommender System for Efficient Communication Using Reinforcement Selector and Hypernet Generator (FedRL) to address the proposed issues. The Reinforcement Selector (RLS) dynamically selects participating edge devices and helps maximize their use of local data resources. Meanwhile, the Hypernet Generator (HNG) optimizes communication bandwidth consumption during the federated learning parameter transmission, enabling rapid deployment and updates of new model architectures or hyperparameters. Furthermore, the framework incorporates item attributes as content embeddings in edge devices' recommender models, enriching them with global information. Real-world dataset experiments demonstrate that the proposed solution balances recommender quality and communication efficiency. The code for this work is publicly available on GitHub: .",10.1145/3682076
10.1145/3685694,Exploring Indoor Air Quality Dynamics in Developing Nations: A Perspective from India,"Karmakar, Prasenjit; Pradhan, Swadhin; Chakraborty, Sandip",2024,ACM J. Comput. Sustain. Soc.,"Indoor air pollution is a major issue in developing countries such as India and Bangladesh, exacerbated by factors such as traditional cooking methods, insufficient ventilation, and cramped living conditions, all of which elevate the risk of health issues such as lung infections and cardiovascular diseases. With the World Health Organization associating around 3.2 million annual deaths globally to household air pollution, the gravity of the problem is clear. Yet, extensive empirical studies exploring these unique patterns and indoor pollution’s extent are missing. To fill this gap, we carried out a 6-months long field study involving over 30 households, uncovering the complexity of indoor air pollution in developing countries, such as the longer lingering time of volatile organic compounds (VOCs) in the air or the significant influence of air circulation on the spatiotemporal distribution of pollutants. We introduced an innovative Internet of Things (IoT) air quality sensing platform, the Distributed Air QuaLiTy MONitor (DALTON), explicitly designed to meet the needs of these nations, considering factors such as cost, sensor type, accuracy, network connectivity, power, and usability. As a result of a multi-device deployment, the platform identifies pollution hot spots in low- and middle-income households in developing nations. It identifies best practices to minimize daily indoor pollution exposure. Our extensive qualitative survey estimates an overall system usability score of 2.04, indicating an efficient system for air quality monitoring.",10.1145/3685694
10.1145/3685930,Cost-Efficient Deep Neural Network Placement in Edge Intelligence-Enabled Internet of Things,"Tian, Hao; Xu, Xiaolong; Wu, Hongyue; Zhao, Qingzhan; Dai, Jianguo; Khan, Maqbool",2024,ACM Trans. Sen. Netw.,"Edge intelligence (EI) integrates edge computing and artificial intelligence empowering service providers to deploy deep neural networks (DNNs) on edge servers in proximity to users to provision intelligent applications (e.g., autonomous driving) for ubiquitous Internet of Things (IoT) in smart cities, which facilitates the quality of experience (QoE) of users and improves the processing and energy efficiency. However, considering DNN is typically computational-intensive and resource-hungry, conventional placement approaches ignore the influence of multi-dimensional resource requirements (processor, memory, etc.), which may degrade the real-time performance. Moreover, with the increasing scale of geo-distributed edge servers, centralized decision-making is still challenging to find the optimal strategies effectively. To overcome these shortcomings, in this paper we propose a game theoretic DNN placement approach in EI-enabled IoT. First, a DNN placement optimization problem is formulated to maximize system benefits, which is proven to be (mathcal {N}mathcal {P} ) -hard and model the original problem as an exact potential game (EPG). Moreover, an EPG-based DNN model placement algorithm, named EPOL, is designed for edge servers to make sub-optimal strategies independently and theoretical analysis is possessed to guarantee the performance of EPOL. Finally, real-world dataset based experimental results corroborate the superiority and effectiveness of EPOL.",10.1145/3685930
10.1145/3686154,A Reversible Perspective on Petri Nets and Event Structures,"Melgratti, Hern\'{a}n; Mezzina, Claudio Antares; Pinna, G. Michele",2024,ACM Trans. Comput. Logic,"Event structures have emerged as a foundational model for concurrent computation, explaining computational processes by outlining the events and the relationships that dictate their execution. They play a pivotal role in the study of key aspects of concurrent computation models, such as causality and independence, and have found applications across a broad range of languages and models, spanning realms like persistence, probabilities, and quantum computing. Recently, event structures have been extended to address reversibility, where computational processes can undo previous computations. In this context, reversible event structures provide abstract representations of processes capable of both forward and backward steps in a computation. Since their introduction, event structures have played a crucial role in bridging operational models, traditionally exemplified by Petri nets and process calculi, with denotational ones, i.e., algebraic domains. In this context, we revisit the standard connection between Petri nets and event structures under the lenses of reversibility. Specifically, we introduce a subset of contextual Petri nets, dubbed reversible causal nets, that precisely correspond to reversible prime event structures. The distinctive feature of reversible causal nets lies in deriving causality from inhibitor arcs, departing from the conventional dependence on the overlap between the postset and preset of transitions. In this way, we are able to operationally explain the full model of reversible prime event structures.",10.1145/3686154
10.1145/3686161,EdgeStreaming: Secure Computation Intelligence in Distributed Edge Networks for Streaming Analytics,"Ye, Peigen; Wang, Wenfeng; Mi, Bing; Chen, Kongyang",2024,ACM Trans. Multimedia Comput. Commun. Appl.,"In modern information systems, real-time streaming data are generated in various vertical application scenarios, such as industrial security cameras, household intelligent devices, mobile robots and among others. However, these low-end devices can hardly provide real-time and accurate data analysis functionalities due to their limited on-board performances. Traditional centralized server computing also suffers from its prolonged transmission latency, resulting in huge response time. To deal with this problem, this paper presents a novel distributed computation intelligent system with nearby edge devices, abbreviated as EdgeStreaming, to facilitate rapid and accurate analysis of streaming data. Firstly, we thoroughly explore the available edge devices surrounding the terminal to generate an internally interconnected edge network. This edge network real-time perceives and updates the internal resource status of each edge device, such as computational and storage resources. Dynamic allocation of external computational or storage demands can be made based on the current load of individual edge devices. Consequently, the streaming data perceived by external terminal devices can be transmitted in real-time to any edge gateway. The edge network employs a well-designed task scheduling strategy to partition and allocate streaming data processing demands to one or multiple edge devices. Additionally, it customizes computational requirements judiciously, for instance, by utilizing model compression to expedite computation speed. We deployed an edge network comprising multiple Raspberry Pis, NVIDIA Jetson Nano, and Jetson NVIDIA TX2 devices, successfully achieving real-time analysis and detection of video streaming data. We believe our work provides new technological support for the real-time processing of streaming data.",10.1145/3686161
10.1145/3686803,Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap,"Li, Jialong; Zhang, Mingyue; Li, Nianyu; Weyns, Danny; Jin, Zhi; Tei, Kenji",2024,ACM Trans. Auton. Adapt. Syst.,"Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this article aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI’s within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.†",10.1145/3686803
10.1145/3687265,Performance Modeling of Distributed Data Processing in Microservice Applications,"Gao, Yicheng; Casale, Giuliano; Singhal, Rekha",2024,ACM Trans. Model. Perform. Eval. Comput. Syst.,"Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.",10.1145/3687265
10.1145/3687299,"A Systematic Literature Review on the Influence of Enhanced Developer Experience on Developers' Productivity: Factors, Practices, and Recommendations","Razzaq, Abdul; Buckley, Jim; Lai, Qin; Yu, Tingting; Botterweck, Goetz",2024,ACM Comput. Surv.,"Context and Motivation – Developer eXperience (Dev-X) is a recent research area that focuses on developers perceptions, feelings, and values with respect to software development and software quality. Research suggests that factors and practices related to Dev-X can have a substantial impact on developer productivity (Dev-P). However, despite a large and diverse body of literature on factors that can impact Dev-P in general, there is no coherent and comprehensive characterization of how Dev-X-specific insights can influence developer productivity.Aims – In the presented research, we aim to provide a coherent, comprehensive characterization of factors and practices related to Dev-X, with a particular focus on those factors and practices that potentially affect Dev-P.Method – To this end, we performed a systematic literature review and identified 218 relevant papers in this area. We characterize the papers based on the related frameworks and concepts common to Dev-X and Dev-P as presented in the existing literature. Dev-X factors such as “work fragmentation” and practices such as “collaboration with owner-developer” are identified using a grounded-in-the-literature, content-analysis method, guided by the theory. For each Dev-X factor, we identify attributes that might be used to assess/ measure the current status (of an organization or project) regarding that factor and how that factor and its effects on productivity have been evidenced in the literature (mentioned vs. considered in questionnaires vs. substantiated with a more positivist evaluation).Results – We identify 33 Dev-X-related factors and 41 Dev-X-related practices, which are organized into 10 themes to summarize their influence. The results suggest that the availability of required resources, relevant expertise re the allocated tasks, and fewer interruptions are among the top positively impacting factors. Conversely, factors such as code complexity, heterogeneous contexts of tasks, and non-adherence to standardization harm Dev-X and Dev-P. Top industrial practices employed to mitigate the negative influence of factors include characterization-based task assignments, mental model support, and the timely evolution of technologies.Conclusions – Overall, this research suggests that organizations can influence Dev-P through improved Dev-X, incorporating suitable practices to mediate relevant factors in their context. Important in this regard are practices such as fragmenting large tasks, highlighting the utility of proposed tasks/changes to the developers, and promoting (developer) ownership of artefacts. Finally, our results point to areas where further research seems appropriate, i.e., where Dev-X factors/practices have been proposed as being impactful on Dev-P but not yet fully substantiated or explored as such (factors like “Nature of Activity” and practices like choosing practices/protocols appropriately).",10.1145/3687299
10.1145/3687301,Data Mesh: A Systematic Gray Literature Review,"Goedegebuure, Abel; Kumara, Indika; Driessen, Stefan; Van Den Heuvel, Willem-Jan; Monsieur, Geert; Tamburri, Damian Andrew; Nucci, Dario Di",2024,ACM Comput. Surv.,"Data mesh is an emerging domain-driven decentralized data architecture that aims to minimize or avoid operational bottlenecks associated with centralized, monolithic data architectures in enterprises. The topic has piqued the practitioners’ interest, and considerable gray literature exists. At the same time, we observe a lack of academic attempts at defining and building upon the concept. Hence, in this article, we aim to start from the foundations and characterize the data mesh architecture regarding its design principles, architectural components, capabilities, and organizational roles. We systematically collected, analyzed, and synthesized 114 industrial gray literature articles. The resulting review provides insights into practitioners’ perspectives on the four key principles of data mesh: data as a product, domain ownership of data, self-serve data platform, and federated computational governance. Moreover, due to the comparability of data mesh and SOA (service-oriented architecture), we mapped the findings from the gray literature into the reference architectures from the SOA academic literature to create the reference architectures for describing three key dimensions of data mesh: organization of capabilities and roles, development, and runtime. Finally, we discuss open research issues in data mesh, partially based on the findings from the gray literature.",10.1145/3687301
10.1145/3687472,Model-Driven Development Towards Distributed Intelligent Systems,"Barriga, Arturo; Barriga, Jos\'{e} A.; P\'{e}rez-Toledano, Miguel A.; Clemente, Pedro J.",2024,ACM Trans. Internet Technol.,"A Distributed Intelligent System (DIS) encompasses a set of intelligent subsystems and components that collaborate to perform tasks and solve problems. Given the advancements of paradigms such as the Internet of Things, along with the advancements of technologies such as Machine Learning and Digital Twins, DISs are on the rise. These systems are increasingly integrating components that perform intelligent functions, and these intelligent functions are increasingly heterogeneous and varied. Moreover, there is no standardized framework to help researchers and practitioners adequately address DISs. As a result, the complexity, interoperability issues, and development time and costs of these systems are growing. However, Model-Driven Development (MDD) can help to address these challenges by providing a Domain-Specific Language (DSL) for developing DISs. In this work, a DSL for the design, validation, generation, and deployment of DISs is proposed. Firstly, the proposed DSL captures in a metamodel the key and high-level abstract concepts of the distinct DISs documented in the literature. Then, it allows modeling of DISs conforming to this metamodel. Subsequently, the DSL enables formal validation of the modeled systems. Lastly, it allows the generation and deployment of all DISs into production. Therefore, the work undertaken in this communication provides a methodological, formal, and standardized approach to defining and developing DISs from a high level of abstraction. This work allows users to address DISs by facilitating agility, minimizing manual tasks, and reducing the number of defects introduced in their development. To illustrate the applicability of the proposed DSL, a real case study of an agricultural digital twin is presented.",10.1145/3687472
10.1145/3688841,An Exploratory Study on Machine Learning Model Management,"Latendresse, Jasmine; Abedu, Samuel; Abdellatif, Ahmad; Shihab, Emad",2024,ACM Trans. Softw. Eng. Methodol.,"Effective model management is crucial for ensuring performance and reliability in Machine Learning (ML) systems, given the dynamic nature of data and operational environments. However, standard practices are lacking, often resulting in ad hoc approaches. To address this, our research provides a clear definition of ML model management activities, processes, and techniques. Analyzing 227 ML repositories, we propose a taxonomy of 16 model management activities and identify 12 unique challenges. We find that 57.9\% of the identified activities belong to the maintenance category, with activities like refactoring (20.5\%) and documentation (18.3\%) dominating. Our findings also reveal significant challenges in documentation maintenance (15.3\%) and bug management (14.9\%), emphasizing the need for robust versioning tools and practices in the ML pipeline. Additionally, we conducted a survey that underscores a shift toward automation, particularly in data, model, and documentation versioning, as key to managing ML models effectively. Our contributions include a detailed taxonomy of model management activities, a mapping of challenges to these activities, practitioner-informed solutions for challenge mitigation, and a publicly available dataset of model management activities and challenges. This work aims to equip ML developers with knowledge and best practices essential for the robust management of ML models.",10.1145/3688841
10.1145/3689722,Iris-MSWasm: Elucidating and Mechanising the Security Invariants of Memory-Safe WebAssembly,"Legoupil, Maxime; Rousseau, June; Georges, A\""{\i}na Linn; Pichon-Pharabod, Jean; Birkedal, Lars",2024,Proc. ACM Program. Lang.,"WebAssembly offers coarse-grained encapsulation guarantees via its module system, but does not support fine-grained sharing of its linear memory. MSWasm is a recent proposal which extends WebAssembly with fine-grained memory sharing via handles, a type of capability that guarantees spatial and temporal safety, and thus enables an expressive yet safe style of programming with flexible sharing. In this paper, we formally validate the pen-and-paper design of MSWasm. To do so, we first define MSWasmCert, a mechanisation of MSWasm that makes it a fully-defined, conservative extension of WebAssembly 1.0, including the module system. We then develop Iris-MSWasm, a foundational reasoning framework for MSWasm composed of a separation logic to reason about known code, and a logical relation to reason about unknown, potentially adversarial code. Iris-MSWasm thereby makes explicit a key aspect of the implicit universal contract of MSWasm: robust capability safety. We apply Iris-MSWasm to reason about key use cases of handles, in which the effect of calling an unknown function is bounded by robust capability safety. Iris-MSWasm thus works as a framework to prove complex security properties of MSWasm programs, and provides a foundation to evaluate the language-level guarantees of MSWasm.",10.1145/3689722
10.1145/3689742,Plume: Efficient and Complete Black-Box Checking of Weak Isolation Levels,"Liu, Si; Gu, Long; Wei, Hengfeng; Basin, David",2024,Proc. ACM Program. Lang.,"Modern databases embrace weak isolation levels to cater for highly available transactions. However, weak isolation bugs have recently manifested in many production databases. This raises the concern of whether database implementations actually deliver their promised isolation guarantees in practice. In this paper we present Plume, the first efficient, complete, black-box checker for weak isolation levels. Plume builds on modular, fine-grained, transactional anomalous patterns, with which we establish sound and complete characterizations of representative weak isolation levels, including read committed, read atomicity, and transactional causal consistency. Plume leverages a novel combination of two techniques, vectors and tree clocks, to accelerate isolation checking. Our extensive assessment shows that Plume can reproduce all known violations in a large collection of anomalous database execution histories, detect new isolation bugs in three production databases along with informative counterexamples, find more weak isolation anomalies than the state-of-the-art checkers, and efficiently validate isolation guarantees under a wide variety of workloads.",10.1145/3689742
10.1145/3689759,MEA2: A Lightweight Field-Sensitive Escape Analysis with Points-to Calculation for Golang,"Ding, Boyao; Li, Qingwei; Zhang, Yu; Tang, Fugen; Chen, Jinbao",2024,Proc. ACM Program. Lang.,"Escape analysis plays a crucial role in garbage-collected languages as it enables the allocation of non-escaping variables on the stack by identifying the dynamic lifetimes of objects and pointers. This helps in reducing heap allocations and alleviating garbage collection pressure. However, Go, as a garbage-collected language, employs a fast yet conservative escape analysis, which is field-insensitive and omits point-to-set calculation to expedite compilation. This results in more variables being allocated on the heap. Empirical statistics reveal that field access and indirect memory access are prevalent in real-world Go programs, suggesting potential opportunities for escape analysis to enhance program performance. In this paper, we propose MEA2, an escape analysis framework atop GoLLVM (an LLVM-based Go compiler), which combines field sensitivity and points-to analysis. Moreover, a novel generic function summary representation is designed to facilitate fast inter-procedural analysis. We evaluated it by using MEA2 to perform stack allocation in 12 wildly-use open-source projects. The results show that, compared to Go’s escape analysis, MEA2 can reduce heap allocation sites by 7.9",10.1145/3689759
10.1145/3689784,Automated Verification of Parametric Channel-Based Process Communication,"Saioc, Georgian-Vlad; Lange, Julien; M\o{}ller, Anders",2024,Proc. ACM Program. Lang.,"A challenge of writing concurrent message passing programs is ensuring the absence of partial deadlocks, which can cause severe memory leaks in long running systems. Several static analysis techniques have been proposed for automatically detecting partial deadlocks in Go programs. For a large enterprise code base, we found these tools too imprecise to reason about process communication that is parametric, i.e., where the number of channel communication operations or the channel capacities are determined at runtime. We present a novel approach to automatically verify the absence of partial deadlocks in Go program fragments with such parametric process communication. The key idea is to translate Go fragments to a core language that is sufficiently expressive to represent real-world parametric communication patterns and can be encoded into Dafny programs annotated with postconditions enforcing partial deadlock freedom. In situations where a fragment is partial deadlock free only when the concurrency parameters satisfy certain conditions, a suitable precondition can often be inferred. Experimental results on a real-world code base containing 583 program fragments that are beyond the reach of existing techniques have shown that the approach can verify the absence of partial deadlocks in 145 cases. For an additional 228 cases, a nontrivial precondition is inferred that the surrounding code must satisfy to ensure partial deadlock freedom.",10.1145/3689784
10.1145/3689793,Making Sense of Multi-threaded Application Performance at Scale with NonSequitur,"Wong, Augustine; Bucci, Paul; Beschastnikh, Ivan; Fedorova, Alexandra",2024,Proc. ACM Program. Lang.,"Modern multi-threaded systems are highly complex. This makes their behavior difficult to understand. Developers frequently capture behavior in the form of program traces and then manually inspect these traces. Existing tools, however, fail to scale to traces larger than a million events. In this paper we present an approach to compress multi-threaded traces in order to allow developers to visually explore these traces at scale. Our approach is able to compress traces that contain millions of events down to a few hundred events. We use this approach to design and implement a tool called NonSequitur. We present three case studies which demonstrate how we used NonSequitur to analyze real-world performance issues with Meta's storage engine RocksDB and MongoDB's storage engine WiredTiger, two complex database backends. We also evaluate NonSequitur with 42 participants on traces from RocksDB and WiredTiger. We demonstrate that, in some cases, participants on average scored 11 times higher when performing performance analysis tasks on large execution traces. Additionally, for some performance analysis tasks, the participants spent on average three times longer with other tools than with NonSequitur.",10.1145/3689793
10.1145/3691338,Deep Learning for Time Series Anomaly Detection: A Survey,"Zamanzadeh Darban, Zahra; Webb, Geoffrey I.; Pan, Shirui; Aggarwal, Charu; Salehi, Mahsa",2024,ACM Comput. Surv.,"Time series anomaly detection is important for a wide range of research fields and applications, including financial markets, economics, earth sciences, manufacturing, and healthcare. The presence of anomalies can indicate novel or unexpected events, such as production faults, system defects, and heart palpitations, and is therefore of particular interest. The large size and complexity of patterns in time series data have led researchers to develop specialised deep learning models for detecting anomalous patterns. This survey provides a structured and comprehensive overview of state-of-the-art deep learning for time series anomaly detection. It provides a taxonomy based on anomaly detection strategies and deep learning models. Aside from describing the basic anomaly detection techniques in each category, their advantages and limitations are also discussed. Furthermore, this study includes examples of deep anomaly detection in time series across various application domains in recent years. Finally, it summarises open issues in research and challenges faced while adopting deep anomaly detection models to time series data.",10.1145/3691338
10.1145/3691630,On the Understandability of Design-Level Security Practices in Infrastructure-as-Code Scripts and Deployment Architectures,"Ntentos, Evangelos; Lueger, Nicole Elisabeth; Simhandl, Georg; Zdun, Uwe; Schneider, Simon; Scandariato, Riccardo; Ferreyra, Nicol\'{a}s E. D\'{\i}az",2024,ACM Trans. Softw. Eng. Methodol.,"Infrastructure as Code (IaC) automates IT infrastructure deployment, which is particularly beneficial for continuous releases, for instance, in the context of microservices and cloud systems. Despite its flexibility in application architecture, neglecting security can lead to vulnerabilities. The lack of comprehensive architectural security guidelines for IaC poses challenges in adhering to best practices. We studied how developers interpret IaC scripts (source code) in two IaC technologies, Ansible and Terraform, compared to semi-formal IaC deployment architecture models and metrics regarding design-level security understanding. In a controlled experiment involving ninety-four participants, we assessed the understandability of IaC-based deployment architectures through source code inspection compared to semi-formal representations in models and metrics.We hypothesized that providing semi-formal IaC deployment architecture models and metrics as supplementary material would significantly improve the comprehension of IaC security-related practices, as measured by task correctness. Our findings suggest that semi-formal IaC deployment architecture models and metrics as supplementary material enhance the understandability of IaC security-related practices without significantly increasing duration. We also observed a significant correlation between task correctness and duration when models and metrics were provided.",10.1145/3691630
10.1145/3695999,Interpretable Failure Localization for Microservice Systems Based on Graph Autoencoder,"Sun, Yongqian; Lin, Zihan; Shi, Binpeng; Zhang, Shenglin; Ma, Shiyu; Jin, Pengxiang; Zhong, Zhenyu; Pan, Lemeng; Guo, Yicheng; Pei, Dan",2024,ACM Trans. Softw. Eng. Methodol.,"Accurate and efficient localization of root cause instances in large-scale microservice systems is of paramount importance. Unfortunately, prevailing methods face several limitations. Notably, some recent methods rely on supervised learning which necessitates a substantial amount of labeled data. However, labeling root cause instances is time-consuming and laborious, especially with multiple modalities of data including logs, traces, metrics, etc. Moreover, some approaches favor deep learning for localization but lack interpretability and continuous improvement mechanisms.To address the above challenges, we propose DeepHunt, a novel root cause localization method based on multimodal data analysis. Firstly, DeepHunt introduces Root Cause Score (RCS) by integrating reconstruction errors and failure propagation patterns (upstream-downstream relationships), imparting interpretability to the localization of root causes. Then, it embraces Graph Autoencoder (GAE) to address the limitation imposed by scarce labeled data. It employs data augmentation to mitigate the adverse effects of insufficient historical training samples. We evaluate DeepHunt on two open-source datasets, and it outperforms existing methods when facing a zero-label cold start. DeepHunt can be further improved by continuously fine-tuning through a feedback mechanism.",10.1145/3695999
10.1145/3696405,SPARC: Spatio-Temporal Adaptive Resource Control for Multi-site Spectrum Management in NextG Cellular Networks,"Ghosh, Ushasi; Chiejina, Azuka; Stephenson, Nathan; Shah, Vijay K; Shakkottai, Srinivas; Bharadia, Dinesh",2024,Proc. ACM Netw.,"This work presents SPARC (Spatio-Temporal Adaptive Resource Control), a novel approach for multi-site spectrum management in NextG cellular networks. SPARC addresses the challenge of limited licensed spectrum in dynamic environments. We leverage the O-RAN architecture to develop a multi-timescale RAN Intelligent Controller (RIC) framework, featuring an xApp for near-real-time interference detection and localization, and a xApp for real-time intelligent resource allocation. By utilizing base stations as spectrum sensors, SPARC enables efficient and fine-grained dynamic resource allocation across multiple sites, enhancing signal-to-noise ratio (SNR) by up to 7dB, spectral efficiency by up to 15\%, and overall system throughput by up to 20\%. Comprehensive evaluations, including emulations and over-the-air experiments, demonstrate the significant performance gains achieved through SPARC, showcasing it as a promising solution for optimizing resource efficiency and network performance in NextG cellular networks.",10.1145/3696405
10.1145/3697350,The Role of Multi-Agents in Digital Twin Implementation: Short Survey,"Kalyani, Yogeswaranathan; Collier, Rem",2024,ACM Comput. Surv.,"In recent years, Digital Twin (DT) technology has emerged as a significant technological advancement. A digital twin is a digital representation of a physical asset that mirrors its data model, behaviour, and interactions with other physical assets. Digital Twin aims at achieving adaptability, seamless data integration, modelling, simulation, automation, and real-time data management. The primary goal of this article is to explore the role of agents in DT implementations, seeking to understand their predominant usage scenarios and purposes. From our perspective, agents serving as intelligent entities play a role in realising the features of DTs. This article also discusses the gaps in DT, highlights future directions, and analyses various technologies integrated with multi-agent systems technologies in DT implementations. Finally, the article briefly discusses an overview of an architecture to implement a DT for smart agriculture with multi-agents.",10.1145/3697350
10.1145/3697352,A Portable Linux-based Firmware for NVMe Computational Storage Devices,"Wertenbroek, Rick; Thoma, Yann; Dassatti, Alberto",2024,ACM Trans. Storage,"Abstract: Over the years, interest in computational storage devices has been growing steadily. This is largely due to the rise of data-intensive applications, such as machine learning, online video distribution, astrophysics, and genomics. Moving compute operations closer to the data provides benefits in terms of scaling possibilities and energy efficiency. The development of computational storage devices has been limited by the need for specialized and complex hardware. In this work we propose a portable Linux-based firmware framework for the development of NVMe computational storage devices. Our firmware runs on a variety of hardware platforms ranging from expensive FPGA solutions to inexpensive off-the-shelf single board computers. The firmware leverages the vast Linux software ecosystem to facilitate the development and prototyping of novel computational storage devices. We benchmark our firmware on multiple hardware platforms and demonstrate its versatility through several computational examples including a content-aware disk image search engine based on natural language processing and AI-driven image recognition.",10.1145/3697352
10.1145/3699514,"A Survey on Architectures, Hardware Acceleration and Challenges for In-Network Computing","Nickel, Matthias; G\""{o}hringer, Diana",2024,ACM Trans. Reconfigurable Technol. Syst.,"By moving data and computation away from the end user to more powerful servers in the cloud or to cloudlets at the edge, end user devices only need to compute locally for small amounts of data and when low latency is required. However, with the advent of 6G and Internet-of-Everything, the demand for more powerful networks continues to grow. The introduction of Software-Defined Networking and Network Function Virtualization has allowed us to rethink networks and use them for more than just routing data to servers. In addition, the use of more powerful network devices is bringing new life to the concept of active networks in the form of in-network computing. In-Network Computing provides the ability to move applications into the network and process data on programmable network devices as they are transmitted. In this work, we provide an overview of in-network computing and its enabling technologies. We take a look at the programmability and different hardware architectures for SmartNICs and switches, focusing primarily on accelerators such as FPGAs. We discuss the state of the art and challenges in this area, and look at CGRAs, a class of hardware accelerators that have not been widely discussed in this context.",10.1145/3699514
10.1145/3699520,An Entanglement-Aware Middleware for Digital Twins,"Bellavista, Paolo; Bicocchi, Nicola; Fogli, Mattia; Giannelli, Carlo; Mamei, Marco; Picone, Marco",2024,ACM Trans. Internet Things,"The development of the Digital Twin (DT) approach is tilting research from initial approaches that aim at promoting early adoption to sophisticated attempts to develop, deploy, and maintain applications based on DTs. In this context, we propose a highly dynamic and distributed ecosystem where containerized DTs co-evolve with an orchestration middleware. DTs provide digitalized representations of the targeted physical systems, while the orchestration middleware monitors and re-configures the deployed DTs in light of application constraints, available resources, and the quality of cyber-physical entanglement. First, we lay out the reference scenario. Then, we discuss the limitations of current approaches and identify a set of requirements that shape both DTs and the orchestration middleware. Subsequently, we describe a blueprint architecture that meets those requirements. Finally, we report empirical evidence on both the feasibility and the effectiveness of a proof-of-concept implementation of the proposed ecosystem.",10.1145/3699520
10.1145/3699954,A Survey on IoT Programming Platforms: A Business-Domain Experts Perspective,"Hannou, Fatma-Zohra; Lefran\c{c}ois, Maxime; Jouvelot, Pierre; Charpenay, Victor; Zimmermann, Antoine",2024,ACM Comput. Surv.,"The vast growth and digitalization potential offered by the Internet of Things (IoT) is hindered by substantial barriers in accessibility, interoperability, and complexity, mainly affecting small organizations and non-technical entities. This survey article provides a detailed overview of the landscape of IoT programming platforms, focusing specifically on the development support they offer for varying end user profiles, ranging from developers with IoT expertise to business experts willing to take advantage of IoT solutions to automate their organization processes. The survey examines a range of IoT platforms, classified according to their programming approach between general-purpose programming solutions, model-driven programming, mashups, and end-user programming. Necessary IoT and programming backgrounds are described to empower non-technical readers with a comprehensive field summary. In addition, the article compares the features of the most representative platforms and provides decision insights and guidelines to support end users in selecting appropriate IoT platforms for their use cases. This work contributes to narrowing the knowledge gap between IoT specialists and end users, breaking accessibility barriers and further promoting the integration of IoT technologies in various domains.1",10.1145/3699954
10.1145/3700144,A Trust Establishment and Key Management Architecture for Hospital-at-Home,"\r{A}kesson, Alfred; Gehrmann, Christian; Hedin, G\""{o}rel; Johnsson, Bj\""{o}rn A.; Magnusson, Boris; Nordahl, Mattias; Ramezanian, Sara; Stankovski Wagner, Paul",2024,ACM Trans. Comput. Healthcare,"The landscape of healthcare is experiencing a digitalization shift, transferring many medical activities to the patients’ homes, a phenomenon commonly referred to as Hospital-at-Home. While Internet of Things (IoT) devices facilitate the building of such systems, there is a need for powerful middleware that encapsulates device-to-device communication, and enables the construction of user-friendly, secure, and robust Hospital-at-Home systems. A key challenge for such middleware is to build a trustworthy and lightweight key management system allowing different devices in the system to exchange messages securely. In this paper we present a simple, easily manageable and scalable such architecture which, in addition, supports long term data protection using post-quantum cryptographic primitives. Our proposed solution utilizes a Merkle tree to enable the IoT devices to establish trust between each other automatically, even in the absence of Internet connection. We have implemented the architecture and present performance figures as well as a security analysis of our approach.",10.1145/3700144
10.1145/3700436,The Tale of Errors in Microservices,"Lee, I-Ting Angelina; Zhang, Zhizhou; Parwal, Abhishek; Chabbi, Milind",2024,Proc. ACM Meas. Anal. Comput. Syst.,"Microservice architecture is the computing paradigm of choice for large, service-oriented software catering to real-time requests. Individual programs in such a system perform Remote Procedure Calls (RPCs) to other microservices to accomplish sub-tasks. Microservices are designed to be robust; top-level requests can succeed despite errors returned from RPC sub-tasks, referred to as non-fatal errors. Because of this design, the top-level microservices tend to ''live with'' non-fatal errors. Hence, a natural question to ask is ''how prevalent are non-fatal errors and what impact do they have on the exposed latency of top-level requests?''In this paper, we present a large-scale study of errors in microservices. We answer the aforementioned question by analyzing 11 Billion RPCs covering 1,900 user-facing endpoints at the Uber serving requests of hundreds of millions of active users. To assess the latency impact of non-fatal errors, we develop a methodology that projects potential latency savings for a given request as if the time spent on failing APIs were eliminated. This estimator allows ranking and bubbling up those APIs that are worthy of further investigations, where the non-fatal errors likely resulted in operational inefficiencies. Finally, we employ our error detection and impact estimation techniques to pinpoint operational inefficiencies, which a) result in a tail latency reduction of a critical endpoint by 30\% and b) offer insights into common inefficiency-introducing patterns.",10.1145/3700436
10.1145/3700792,A Survey on Securing Image-Centric Edge Intelligence,"Tang, Li; Hu, Haibo; Gabbouj, Moncef; Ye, Qingqing; Xiang, Yang; Li, Jin; Li, Lang",2024,ACM Trans. Multimedia Comput. Commun. Appl.,"Facing enormous data generated at the network edge, Edge Intelligence (EI) emerges as the fusion of Edge Computing and Artificial Intelligence, revolutionizing edge data processing and intelligent decision-making. Nonetheless, this emergent mode presents a complex array of security challenges, particularly prominent in image-centric applications due to the sheer volume of visual data and its direct connection to user privacy. These challenges include safeguarding model/image privacy and ensuring model integrity against various security threats, such as model poisoning. Essentially, those threats originate from data attacks, suggesting data protection as a promising solution. Although data protection measures are well-established in other domains, image-centric EI necessitates focused research. This survey examines the security issues inherent to image-centric EI and outlines the protection efforts, providing a comprehensive overview of the landscape. We begin by introducing EI, detailing its operational mechanics and associated security issues. We then explore the technologies facilitating security enhancement (e.g., differential privacy) and edge intelligence (e.g., compact networks and distributed learning frameworks). Next, we categorize security strategies by their application in data preparation, training, and inference, with a focus on image-based contexts. Despite these efforts on security, our investigation identifies research gaps. We also outline promising research directions to bridge these gaps, bolstering security frameworks in image-centric EI applications.",10.1145/3700792
10.1145/3700875,"Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions","Golec, Muhammed; Walia, Guneet Kaur; Kumar, Mohit; Cuadrado, Felix; Gill, Sukhpal Singh; Uhlig, Steve",2024,ACM Comput. Surv.,"Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on cold start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence/Machine Learning-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.",10.1145/3700875
10.1145/3702978,Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features,"Gu, Wenwei; Liu, Jinyang; Chen, Zhuangbin; Zhang, Jianping; Su, Yuxin; Gu, Jiazhen; Feng, Cong; Yang, Zengyin; Yang, Yongqiang; Lyu, Michael R.",2024,ACM Trans. Softw. Eng. Methodol.,"Cloud systems, typically comprised of various components (e.g., microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e., key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.",10.1145/3702978
10.1145/3704436,Democratizing Container Live Migration for Enhanced Future Networks - A Survey,"Soussi, Wissem; G\""{u}r, G\""{u}rkan; Stiller, Burkhard",2024,ACM Comput. Surv.,"Emerging cloud-centric networks span from edge clouds to large-scale datacenters with shared infrastructure among multiple tenants and applications with high availability, isolation, fault tolerance, security, and energy efficiency demands. Live migration (LiMi) plays an increasingly critical role in these environments by enabling seamless application mobility covering the edge-to-cloud continuum and maintaining these requirements. This paper presents a comprehensive survey of recent advancements that democratize LiMi, making it more applicable to a broader range of scenarios and network environments both for virtual machines (VMs) and containers, and analyzes LiMi’s technical underpinnings and optimization techniques. It also delves into the issue of connections handover, presenting a taxonomy to categorize methods of traffic redirection synthesized from the existing literature. Finally, it identifies technical challenges and paves the way for future research directions in this key technology.",10.1145/3704436
10.1145/3705896,Unlocking AutoML: Enhancing Data with Deep Learning Algorithms for Medical Imaging,"Ribeiro Jesus, Rui Filipe; Rodrigues, Ana; Costa, Carlos",2024,J. Data and Information Quality,"Deep learning algorithms have become increasingly popular over the years, having proved their efficiency in input-output functions for distinct types of data. This technology is particularly useful in medical imaging, where complex image structures often generate disagreements between medical staff. These technologies can streamline the diagnostic process by performing automatic image analysis, which results in more accurate and reproducible diagnoses. Additionally, these technologies can enhance content retrieval systems by automatically labeling the images based on the structures they possess. Despite the benefits, the mathematical complexity of deep learning algorithms and their training optimizations can be challenging. Automated machine learning provides a solution to this challenge by offering tools that automate the development and training of these algorithms. This makes it possible for users with limited programming experience to take advantage of these powerful technologies to quickly develop and prototype analysis algorithms for their specific needs. This article presents a management platform for deep learning services on the cloud that provides a code-free experience through automated machine learning. The evaluation was done in one of the most demanding scenarios, where the service was integrated into a research pathology PACS to annotate mitotic cells in breast cancer tissue automatically. The annotations are processed by an open-source PACS archive and stored directly on the files, enhancing the image metadata and consequently content retrieval systems. The results of the developed algorithms were compared to the state-of-the-art to evaluate the competitiveness of the solution.",10.1145/3705896
10.1145/3707651,On Collaboration and Automation in the Context of Threat Detection and Response with Privacy-Preserving Features,"Nitz, Lasse; Gurabi, Mehdi Akbari; Cermak, Milan; Zadnik, Martin; Karpuk, David; Drichel, Arthur; Sch\""{a}fer, Sebastian; Holmes, Benedikt; Mandal, Avikarsha",2024,Digital Threats,"Organizations and their security operation centers often struggle to detect and respond effectively to an extensive quantity of ever-evolving cyberattacks. While collaboration, such as threat intelligence sharing between security teams, and response automation are often discussed in the cybersecurity community, issues like data sensitivity and confidence in detection may hinder their adoption. This work investigates the potentials and challenges of collaboration and automation to enhance incident response processes. We propose a reference architecture for data sharing in threat detection and response, aiming to boost collaborative and automated efforts across organizations while also considering privacy-preserving features. To address these challenges and potentials, we discuss how such a framework could enhance current response processes within and between organizations, validated with results in local attack detection, incident response, and data sharing.",10.1145/3707651
10.1145/3708476,DistMeasure: A Framework for Run-Time Characterization and Quality Assessment of Distributed Software via Interprocess Communications,"Fu, Xiaoqin; Zaman, Asif; Cai, Haipeng",2024,ACM Trans. Softw. Eng. Methodol.,"A defining, unique aspect of distributed systems lies in interprocess communication (IPC) through which distributed components interact and collaborate toward the holistic system behaviors. This highly decoupled construction intuitively contributes to the scalability, performance, and resiliency advantages of distributed software, but also adds largely to their greater complexity, compared to centralized software. Yet despite the importance of IPC in distributed systems, little is known about how to quantify IPC-induced behaviors in these systems through IPC measurement and how such behaviors may be related to the quality of distributed software. To answer these questions, in this paper, we present DistMeasure, a framework for measuring distributed software systems via the lens of IPC hence enabling the study of its correlation with distributed system quality. Underlying DistMeasure is a novel set of IPC metrics that focus on gauging the coupling and cohesion of distributed processes. Through these metrics, DistMeasure quantifies relevant run-time characteristics of distributed systems and their quality relevance, covering a range of quality aspects each via respective direct quality metrics. Further, DistMeasure enables predictive assessment of distributed system quality in those aspects via learning-based anomaly detection with respect to the corresponding quality metrics based on their significant correlations with related IPC metrics. Using DistMeasure, we demonstrated the practicality and usefulness of IPC measurement against 11 real-world distributed systems and their diverse execution scenarios. Among other findings, our results revealed that IPC has a strong correlation with distributed system complexity, performance efficiency, and security. Higher IPC coupling between distributed processes tended to be negatively indicative of distributed software quality, while more cohesive processes have positive quality implications. Yet overall IPC-induced behaviors are largely independent of the system scale, and higher (lower) process coupling does not necessarily come with lower (higher) process cohesion. We also show promising merits (with 98\% precision/recall/F1) of IPC measurement (e.g., class-level coupling and process-level cohesion) for predictive anomaly assessment of various aspects (e.g., attack surface and performance efficiency) of distributed system quality.",10.1145/3708476
10.1145/3708497,Towards Trustworthy Machine Learning in Production: An Overview of the Robustness in MLOps Approach,"Bayram, Firas; Ahmed, Bestoun S.",2024,ACM Comput. Surv.,"Artificial intelligence (AI), and especially its sub-field of Machine Learning (ML), are impacting the daily lives of everyone with their ubiquitous applications. In recent years, AI researchers and practitioners have introduced principles and guidelines to build systems that make reliable and trustworthy decisions. From a practical perspective, conventional ML systems process historical data to extract the features that are consequently used to train ML models that perform the desired task. However, in practice, a fundamental challenge arises when the system needs to be operationalized and deployed to evolve and operate in real-life environments continuously. To address this challenge, Machine Learning Operations (MLOps) have emerged as a potential recipe for standardizing ML solutions in deployment. Although MLOps demonstrated great success in streamlining ML processes, thoroughly defining the specifications of robust MLOps approaches remains of great interest to researchers and practitioners. In this paper, we provide a comprehensive overview of the trustworthiness property of MLOps systems. Specifically, we highlight technical practices to achieve robust MLOps systems. In addition, we survey the existing research approaches that address the robustness aspects of ML systems in production. We also review the tools and software available to build MLOps systems and summarize their support to handle the robustness aspects. Finally, we present the open challenges and propose possible future directions and opportunities within this emerging field. The aim of this paper is to provide researchers and practitioners working on practical AI applications with a comprehensive view to adopt robust ML solutions in production environments.",10.1145/3708497
10.1145/3708527,"Contemporary Software Modernization: Strategies, Driving Forces, and Research Opportunities","Assun\c{c}\~{a}o, Wesley K. G.; Marchezan, Luciano; Arkoh, Lawrence; Egyed, Alexander; Ramler, Rudolf",2024,ACM Trans. Softw. Eng. Methodol.,"Software modernization is a common activity in software engineering, since technologies advance, requirements change, and business models evolve. Differently from conventional software evolution (e.g., adding new features, enhancing performance, or adapting to new requirements), software modernization involves re-engineering entire legacy systems (e.g., changing the technology stack, migrating to a new architecture style, or programming paradigms). Given the pervasive nature of software today, modernizing legacy systems is paramount to provide customers with competitive and innovative products and services, while keeping companies profitable. Despite the prevalent discussion of software modernization in gray literature, and the many papers in the literature, there is no work presenting a “big picture” of contemporary software modernization, describing challenges, and providing a well-defined research agenda. The goal of this work is to describe the state of the art in software modernization in the past 10 years. We collect the state of the art by performing a rapid review (searching five digital libraries), identifying potential 3,460 studies, leading to a final set of 127. We analyzed these studies to understand which strategies are employed, the driving forces that lead organizations to modernize their systems, and the challenges that need to be addressed. The results show that studies in the last 10 years have explored eight strategies for modernizing legacy systems, namely cloudification, architecture redesign, moving to a new programming language, targeting reuse optimization, software modernization for new hardware integration, practices to leverage automation, database modernization, and digital transformation. Modernization is triggered by 14 driving forces, with the most common ones being reducing operational costs, improving performance and scalability, and reducing complexity. In addition, based on the analysis of existing literature, we present a detailed discussion of research opportunities in this field. The main challenges are providing tooling support, followed by defining a modernization process and considering better evaluation metrics. The main contribution of our work is to equip practitioners and researchers with knowledge of the current state of contemporary software modernization so that they are aware of practices and challenges to be addressed when deciding to modernize legacy systems.",10.1145/3708527
10.1145/3708996,Project Silica: Towards Sustainable Cloud Archival Storage in Glass,"Anderson, Patrick; Aranas, Erika; Assaf, Youssef; Behrendt, Raphael; Black, Richard; Caballero, Marco; Cameron, Pashmina; Canakci, Burcu; Chatzieleftheriou, Andromachi; Clarke, Rebekah; Clegg, James; Cletheroe, Daniel; Cooper, Bridgette; De Carvalho, Thales; Deegan, Tim; Donnelly, Austin; Drevinskas, Rokas; Gaunt, Alexander; Gkantsidis, Christos; Gomez Diaz, Ariel; Haller, Istvan; Hong, Freddie; Ilieva, Teodora; Joshi, Shashidhar; Joyce, Russell; Kunkel, William; Lara, David; Legtchenko, Sergey; Liu, Fanglin; Magalhaes, Bruno; Marzoev, Alana; McNett, Marvin; Mohan, Jayashree; Myrah, Michael; Nguyen, Truong; Nowozin, Sebastian; Ogus, Aaron; Overweg, Hiske; Rowstron, Antony; Sah, Maneesh; Sakakura, Masaaki; Scholtz, Peter; Schreiner, Nina; Sella, Omer; Smith, Adam; Stefanovici, Ioan; Sweeney, David; Thomsen, Benn; Verkes, Govert; Wainman, Phil; Westcott, Jonathan; Weston, Luke; Whittaker, Charles; Wilke Berenguer, Pablo; Williams, Hugh; Winkler, Thomas; Winzeck, Stefan",2025,ACM Trans. Storage,"Sustainable and cost-effective long-term storage remains an unsolved problem. The most widely used storage technologies today are magnetic (hard disk drives and tape). They use media that degrades over time and has a limited lifetime, which leads to inefficient, wasteful, and costly solutions for long-lived data. This paper presents Silica: the first cloud storage system for archival data underpinned by quartz glass, an extremely resilient media that allows data to be left in situ indefinitely. The hardware and software of Silica have been co-designed and co-optimized from the media up to the service level with sustainability as a primary objective. The design follows a cloud-first, data-driven methodology underpinned by principles derived from analyzing the archival workload of a large public cloud service. Silica can support a wide range of archival storage workloads and ushers in a new era of sustainable, cost-effective storage.",10.1145/3708996
10.1145/3709353,From Today’s Code to Tomorrow’s Symphony: The AI Transformation of Developer’s Routine by 2030,"Qiu, Ketai; Puccinelli, Niccol\`{o}; Ciniselli, Matteo; Di Grazia, Luca",2024,ACM Trans. Softw. Eng. Methodol.,"In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI’s ChatGPT, which have become a crucial element for coding, debugging, and software design. In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers’ roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers’ capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity.",10.1145/3709353
10.14778/3137765.3137783,Colt: concept lineage tool for data flow metadata capture and analysis,"Aggour, Kareem S.; Williams, Jenny Weisenberg; McHugh, Justin; Kumar, Vijay S.",2017,Proc. VLDB Endow.,"Most organizations are becoming increasingly data-driven, often processing data from many different sources to enable critical business operations. Beyond the well-addressed challenge of storing and processing large volumes of data, financial institutions in particular are increasingly subject to federal regulations requiring high levels of accountability for the accuracy and lineage of this data. For companies like GE Capital, which maintain data across a globally interconnected network of thousands of systems, it is becoming increasingly challenging to capture an accurate understanding of the data flowing between those systems. To address this problem, we designed and developed a concept lineage tool allowing organizational data flows to be modeled, visualized and interactively explored. This tool has novel features that allow a data flow network to be contextualized in terms of business-specific metadata such as the concept, business, and product for which it applies. Key analysis features have been implemented, including the ability to trace the origination of particular datasets, and to discover all systems where data is found that meets some user-defined criteria. This tool has been readily adopted by users at GE Capital and in a short time has already become a business-critical application, with over 2,200 data systems and over 1,000 data flows captured.",10.14778/3137765.3137783
10.14778/3352063.3352092,Stateful functions as a service in action,"Akhter, Adil; Fragkoulis, Marios; Katsifodimos, Asterios",2019,Proc. VLDB Endow.,"In the serverless model, users upload application code to a cloud platform and the cloud provider undertakes the deployment, execution and scaling of the application, relieving users from all operational aspects. Although very popular, current serverless offerings offer poor support for the management of local application state, the main reason being that managing state and keeping it consistent at large scale is very challenging. As a result, the serverless model is inadequate for executing stateful, latency-sensitive applications. In this paper we present a high-level programming model for developing stateful functions and deploying them in the cloud. Our programming model allows functions to retain state as well as call other functions. In order to deploy stateful functions in a cloud infrastructure, we translate functions and their data exchanges into a stateful dataflow graph. With this paper we aim at demonstrating that using a modified version of an open-source dataflow engine as a runtime for stateful functions, we can deploy scalable and stateful services in the cloud with surprisingly low latency and high throughput.",10.14778/3352063.3352092
10.14778/3352063.3352144,Performance in the spotlight,"Colyer, Adrian",2019,Proc. VLDB Endow.,"Performance in its various guises features prominently in research evaluations, and rightly so. Without adequate performance a system is not fit for purpose. That doesn't necessarily mean we should pursue performance at all costs though. In this talk we'll explore a variety of additional evaluation criteria, with a focus on those that are most important to practitioners, and ask whether or not considering them can open up interesting avenues of research.",10.14778/3352063.3352144
10.14778/3377369.3377370,A.M.B.R.O.S.I.A: providing performant virtual resiliency for distributed applications,"Goldstein, Jonathan; Abdelhamid, Ahmed; Barnett, Mike; Burckhardt, Sebastian; Chandramouli, Badrish; Gehring, Darren; Lebeck, Niel; Meiklejohn, Christopher; Minhas, Umar Farooq; Newton, Ryan; Peshawaria, Rahee Ghosh; Zaccai, Tal; Zhang, Irene",2020,Proc. VLDB Endow.,"When writing today's distributed programs, which frequently span both devices and cloud services, programmers are faced with complex decisions and coding tasks around coping with failure, especially when these distributed components are stateful. If their application can be cast as pure data processing, they benefit from the past 40--50 years of work from the database community, which has shown how declarative database systems can completely isolate the developer from the possibility of failure in a performant manner. Unfortunately, while there have been some attempts at bringing similar functionality into the more general distributed programming space, a compelling general-purpose system must handle non-determinism, be performant, support a variety of machine types with varying resiliency goals, and be language agnostic, allowing distributed components written in different languages to communicate. This paper introduces Ambrosia, the first system to satisfy all these requirements. We coin the term ""virtual resiliency"", analogous to virtual memory, for the platform feature which allows failure oblivious code to run in a failure resilient manner. We also introduce novel programming language constructs for resiliently handling non-determinism. Of further interest is the effective reapplication of much database performance optimization technology to make Ambrosia more performant than many of today's non-resilient cloud solutions.",10.14778/3377369.3377370
10.14778/3415478.3415545,POLARIS: the distributed SQL engine in azure synapse,"Aguilar-Saborit, Josep; Ramakrishnan, Raghu; Srinivasan, Krish; Bocksrocker, Kevin; Alagiannis, Ioannis; Sankara, Mahadevan; Shafiei, Moe; Blakeley, Jose; Dasarathy, Girish; Dash, Sumeet; Davidovic, Lazar; Damjanic, Maja; Djunic, Slobodan; Djurkic, Nemanja; Feddersen, Charles; Galindo-Legaria, Cesar; Halverson, Alan; Kovacevic, Milana; Kicovic, Nikola; Lukic, Goran; Maksimovic, Djordje; Manic, Ana; Markovic, Nikola; Mihic, Bosko; Milic, Ugljesa; Milojevic, Marko; Nayak, Tapas; Potocnik, Milan; Radic, Milos; Radivojevic, Bozidar; Rangarajan, Srikumar; Ruzic, Milan; Simic, Milan; Sosic, Marko; Stanko, Igor; Stikic, Maja; Stanojkov, Sasa; Stefanovic, Vukasin; Sukovic, Milos; Tomic, Aleksandar; Tomic, Dragan; Toscano, Steve; Trifunovic, Djordje; Vasic, Veljko; Verona, Tomer; Vujic, Aleksandar; Vujic, Nikola; Vukovic, Marko; Zivanovic, Marko",2020,Proc. VLDB Endow.,"In this paper, we describe the Polaris distributed SQL query engine in Azure Synapse. It is the result of a multi-year project to re-architect the query processing framework in the SQL DW parallel data warehouse service, and addresses two main goals: (i) converge data warehousing and big data workloads, and (ii) separate compute and state for cloud-native execution.From a customer perspective, these goals translate into many useful features, including the ability to resize live workloads, deliver predictable performance at scale, and to efficiently handle both relational and unstructured data. Achieving these goals required many innovations, including a novel ""cell"" data abstraction, and flexible, fine-grained, task monitoring and scheduling capable of handling partial query restarts and PB-scale execution. Most importantly, while we develop a completely new scale-out framework, it is fully compatible with T-SQL and leverages decades of investment in the SQL Server single-node runtime and query optimizer. The scalability of the system is highlighted by a 1PB scale run of all 22 TPC-H queries; to our knowledge, this is the first reported run with scale larger than 100TB.",10.14778/3415478.3415545
10.14778/3436905.3436926,Scalable structural index construction for JSON analytics,"Jiang, Lin; Qiu, Junqiao; Zhao, Zhijia",2020,Proc. VLDB Endow.,"JavaScript Object Notation (JSON) and its variants have gained great popularity in recent years. Unfortunately, the performance of their analytics is often dragged down by the expensive JSON parsing. To address this, recent work has shown that building bitwise indices on JSON data, called structural indices, can greatly accelerate querying. Despite its promise, the existing structural index construction does not scale well as records become larger and more complex, due to its (inherently) sequential construction process and the involvement of costly memory copies that grow as the nesting level increases.To address the above issues, this work introduces Pison - a more memory-efficient structural index constructor with supports of intra-record parallelism. First, Pison features a redesign of the bottleneck step in the existing solution. The new design is not only simpler but more memory-efficient. More importantly, Pison is able to build structural indices for a single bulky record in parallel, enabled by a group of customized parallelization techniques. Finally, Pison is also optimized for better data locality, which is especially critical in the scenario of bulky record processing. Our evaluation using real-world JSON datasets shows that Pison achieves 9.8X speedup (on average) over the existing structural index construction solution for bulky records and 4.6X speedup (on average) of end-to-end performance (indexing plus querying) over a state-of-the-art SIMD-based JSON parser on a 16-core machine.",10.14778/3436905.3436926
10.14778/3461535.3461537,Database isolation by scheduling,"Gaffney, Kevin P.; Claus, Robert; Patel, Jignesh M.",2021,Proc. VLDB Endow.,"Transaction isolation is conventionally achieved by restricting access to the physical items in a database. To maximize performance, isolation functionality is often packaged with recovery, I/O, and data access methods in a monolithic transactional storage manager. While this design has historically afforded high performance in online transaction processing systems, industry trends indicate a growing need for a new approach in which intertwined components of the transactional storage manager are disaggregated into modular services. This paper presents a new method to modularize the isolation component. Our work builds on predicate locking, an isolation mechanism that enables this modularization by locking logical rather than physical items in a database. Predicate locking is rarely used as the core isolation mechanism because of its high theoretical complexity and perceived overhead. However, we show that this overhead can be substantially reduced in practice by optimizing for common predicate structures.We present DIBS, a transaction scheduler that employs our predicate locking optimizations to guarantee isolation as a modular service. We evaluate the performance of DIBS as the sole isolation mechanism in a data processing system. In this setting, DIBS scales up to 10.5 million transactions per second on a TATP workload. We also explore how DIBS can be applied to existing database systems to increase transaction throughput. DIBS reduces per-transaction file system writes by 90\% on TATP in SQLite, resulting in a 3X improvement in throughput. Finally, DIBS reduces row contention on YCSB in MySQL, providing serializable isolation with a 1.4X improvement in throughput.",10.14778/3461535.3461537
10.14778/3476311.3476360,Query-driven video event processing for the internet of multimedia things,"Yadav, Piyush; Salwala, Dhaval; Pontes, Felipe Arruda; Dhingra, Praneet; Curry, Edward",2021,Proc. VLDB Endow.,"Advances in Deep Neural Network (DNN) techniques have revolutionized video analytics and unlocked the potential for querying and mining video event patterns. This paper details GNOSIS, an event processing platform to perform near-real-time video event detection in a distributed setting. GNOSIS follows a serverless approach where its component acts as independent microservices and can be deployed at multiple nodes. GNOSIS uses a declarative query-driven approach where users can write customize queries for spatiotemporal video event reasoning. The system converts the incoming video streams into a continuous evolving graph stream using machine learning (ML) and DNN models pipeline and applies graph matching for video event pattern detection. GNOSIS can perform both stateful and stateless video event matching. To improve Quality of Service (QoS), recent work in GNOSIS incorporates optimization techniques like adaptive scheduling, energy efficiency, and content-driven windows. This paper demonstrates the Occupational Health and Safety query use cases to show the GNOSIS efficacy.",10.14778/3476311.3476360
10.14778/3476311.3476387,Hazelcast jet: low-latency stream processing at the 99.99th percentile,"Gencer, Can; Topolnik, Marko; \v{D}urina, Viliam; Demirci, Emin; Kahveci, Ensar B.; G\""{u}rb\""{u}z, Ali; Luk\'{a}\v{s}, Ond\v{r}ej; Bart\'{o}k, J\'{o}zsef; Gierlach, Grzegorz; Hartman, Franti\v{s}ek; Y\i{}lmaz, Ufuk; Do\u{g}an, Mehmet; Mandouh, Mohamed; Fragkoulis, Marios; Katsifodimos, Asterios",2021,Proc. VLDB Endow.,"Jet is an open source, high performance, distributed stream processor built at Hazelcast during the last five years. Jet was engineered with millisecond latency on the 99.99th percentile as its primary design goal. Originally Jet's purpose was to be an execution engine that performs complex business logic on top of streams generated by Hazelcast's In-memory Data Grid (IMDG): a set of in-memory, partitioned and replicated data structures. With time, Jet evolved into a full-fledged, scale-out stream processor that can handle out-of-order streams and provide exactly-once processing guarantees. Jet's end-to-end latency lies in the order of milliseconds, and its throughput in the order of millions of events per CPU-core. This paper presents the main design decisions we made in order to maximize the performance per CPU-core, alongside lessons learned, and an empirical performance evaluation.",10.14778/3476311.3476387
10.14778/3484224.3484232,"Data management in microservices: state of the practice, challenges, and research directions","Laigner, Rodrigo; Zhou, Yongluan; Salles, Marcos Antonio Vaz; Liu, Yijian; Kalinowski, Marcos",2021,Proc. VLDB Endow.,"Microservices have become a popular architectural style for data-driven applications, given their ability to functionally decompose an application into small and autonomous services to achieve scalability, strong isolation, and specialization of database systems to the workloads and data formats of each service. Despite the accelerating industrial adoption of this architectural style, an investigation of the state of the practice and challenges practitioners face regarding data management in microservices is lacking. To bridge this gap, we conducted a systematic literature review of representative articles reporting the adoption of microservices, we analyzed a set of popular open-source microservice applications, and we conducted an online survey to cross-validate the findings of the previous steps with the perceptions and experiences of over 120 experienced practitioners and researchers.Through this process, we were able to categorize the state of practice of data management in microservices and observe several foundational challenges that cannot be solved by software engineering practices alone, but rather require system-level support to alleviate the burden imposed on practitioners. We discuss the shortcomings of state-of-the-art database systems regarding microservices and we conclude by devising a set of features for microservice-oriented database systems.",10.14778/3484224.3484232
10.14778/3523210.3523219,OnlineSTL: scaling time series decomposition by 100x,"Mishra, Abhinav; Sriharsha, Ram; Zhong, Sichen",2022,Proc. VLDB Endow.,"Decomposing a complex time series into trend, seasonality, and remainder components is an important primitive that facilitates time series anomaly detection, change point detection, and forecasting. Although numerous batch algorithms are known for time series decomposition, none operate well in an online scalable setting where high throughput and real-time response are paramount. In this paper, we propose OnlineSTL, a novel online algorithm for time series decomposition which is highly scalable and is deployed for real-time metrics monitoring on high-resolution, high-ingest rate data. Experiments on different synthetic and real world time series datasets demonstrate that OnlineSTL achieves orders of magnitude speedups (100x) for large seasonalities while maintaining quality of decomposition.",10.14778/3523210.3523219
10.14778/3547305.3547313,Optimizing inference serving on serverless platforms,"Ali, Ahsan; Pinciroli, Riccardo; Yan, Feng; Smirni, Evgenia",2022,Proc. VLDB Endow.,"Serverless computing is gaining popularity for machine learning (ML) serving workload due to its autonomous resource scaling, easy to use and pay-per-use cost model. Existing serverless platforms work well for image-based ML inference, where requests are homogeneous in service demands. That said, recent advances in natural language processing could not fully benefit from existing serverless platforms as their requests are intrinsically heterogeneous.Batching requests for processing can significantly increase ML serving efficiency while reducing monetary cost, thanks to the pay-per-use pricing model adopted by serverless platforms. Yet, batching heterogeneous ML requests leads to additional computation overhead as small requests need to be ""padded"" to the same size as large requests within the same batch. Reaching effective batching decisions (i.e., which requests should be batched together and why) is non-trivial: the padding overhead coupled with the serverless auto-scaling forms a complex optimization problem.To address this, we develop Multi-Buffer Serving (MBS), a framework that optimizes the batching of heterogeneous ML inference serving requests to minimize their monetary cost while meeting their service level objectives (SLOs). The core of MBS is a performance and cost estimator driven by analytical models supercharged by a Bayesian optimizer. MBS is prototyped and evaluated on AWS using bursty workloads. Experimental results show that MBS preserves SLOs while outperforming the state-of-the-art by up to 8 x in terms of cost savings while minimizing the padding overhead by up to 37 x with 3 x less number of serverless function invocations.",10.14778/3547305.3547313
10.14778/3554821.3554839,Magma: a high data density storage engine used in couchbase,"Lakshman, Sarath; Gupta, Apaar; Suri, Rohan; Lashley, Scott; Liang, John; Duvuru, Srinath; Mayuram, Ravi",2022,Proc. VLDB Endow.,"We present Magma, a write-optimized high data density key-value storage engine used in the Couchbase NoSQL distributed document database. Today's write-heavy data-intensive applications like ad-serving, internet-of-things, messaging, and online gaming, generate massive amounts of data. As a result, the requirement for storing and retrieving large volumes of data has grown rapidly. Distributed databases that can scale out horizontally by adding more nodes can be used to serve the requirements of these internet-scale applications. To maintain a reasonable cost of ownership, we need to improve storage efficiency in handling large data volumes per node, such that we don't have to rely on adding more nodes. Our current generation storage engine, Couchstore is based on a log-structured append-only copy-on-write B+Tree architecture. To make substantial improvements to support higher data density and write throughput, we needed a storage engine architecture that lowers write amplification and avoids compaction operations that rewrite the whole database files periodically.We introduce Magma, a hybrid key-value storage engine that combines LSM Trees and a segmented log approach from log-structured file systems. We present a novel approach to performing garbage collection of stale document versions avoiding index lookup during log segment compaction. This is the key to achieving storage efficiency for Magma and eliminates the need for random I/Os during compaction. Magma offers significantly lower write amplification, scalable incremental compaction, and lower space amplification while not regressing the read amplification. Through the efficiency improvements, we improved the single machine data density supported by the Couchbase Server by 3.3x and lowered the memory requirement by 10x, thereby reducing the total cost of ownership up to 10x. Our evaluation results show that Magma outperforms Couchstore and RocksDB in write-heavy workloads.",10.14778/3554821.3554839
10.14778/3554821.3554843,Manu: a cloud native vector database management system,"Guo, Rentong; Luan, Xiaofan; Xiang, Long; Yan, Xiao; Yi, Xiaomeng; Luo, Jigao; Cheng, Qianya; Xu, Weizhi; Luo, Jiarui; Liu, Frank; Cao, Zhenshan; Qiao, Yanliang; Wang, Ting; Tang, Bo; Xie, Charles",2022,Proc. VLDB Endow.,"With the development of learning-based embedding models, embedding vectors are widely used for analyzing and searching unstructured data. As vector collections exceed billion-scale, fully managed and horizontally scalable vector databases are necessary. In the past three years, through interaction with our 1200+ industry users, we have sketched a vision for the features that next-generation vector databases should have, which include long-term evolvability, tunable consistency, good elasticity, and high performance.We present Manu, a cloud native vector database that implements these features. It is difficult to integrate all these features if we follow traditional DBMS design rules. As most vector data applications do not require complex data models and strong data consistency, our design philosophy is to relax the data model and consistency constraints in exchange for the aforementioned features. Specifically, Manu firstly exposes the write-ahead log (WAL) and binlog as backbone services. Secondly, write components are designed as log publishers while all read-only analytic and search components are designed as independent subscribers to the log services. Finally, we utilize multi-version concurrency control (MVCC) and a delta consistency model to simplify the communication and cooperation among the system components. These designs achieve a low coupling among the system components, which is essential for elasticity and evolution. We also extensively optimize Manu for performance and usability with hardware-aware implementations and support for complex search semantics. Manu has been used for many applications, including, but not limited to, recommendation, multimedia, language, medicine and security. We evaluated Manu in three typical application scenarios to demonstrate its efficiency, elasticity, and scalability.",10.14778/3554821.3554843
10.14778/3554821.3554892,Machine programming: turning data into programmer productivity,"Wasay, Abdul; Tatbul, Nesime; Gottschlich, Justin",2022,Proc. VLDB Endow.,"Machine programming is an emerging research area that improves the software development life cycle from design through deployment. We present a tutorial on machine programming research highlighting aspects relevant to the data systems community. We divide this tutorial into three parts: We begin with an introduction to machine programming introducing its three pillars: intention, invention, and adaptation. Then, we provide an overview of the data ecosystem central to all machine programming systems, highlighting challenges and novel opportunities relevant to the data systems community. Finally, we describe recent advances in machine programming research and how these directions use various data sets to improve the ease of creating and maintaining performant software systems.",10.14778/3554821.3554892
10.14778/3587136.3587139,InfiniStore: Elastic Serverless Cloud Storage,"Zhang, Jingyuan; Wang, Ao; Ma, Xiaolong; Carver, Benjamin; Newman, Nicholas John; Anwar, Ali; Rupprecht, Lukas; Tarasov, Vasily; Skourtis, Dimitrios; Yan, Feng; Cheng, Yue",2023,Proc. VLDB Endow.,"Cloud object storage such as AWS S3 is cost-effective and highly elastic but relatively slow, while high-performance cloud storage such as AWS ElastiCache is expensive and provides limited elasticity. We present a new cloud storage service called ServerlessMemory, which stores data using the memory of serverless functions. ServerlessMemory employs a sliding-window-based memory management strategy inspired by the garbage collection mechanisms used in the programming language to effectively segregate hot/cold data and provides fine-grained elasticity, good performance, and a pay-per-access cost model with extremely low cost.We then design and implement InfiniStore, a persistent and elastic cloud storage system, which seamlessly couples the function-based ServerlessMemory layer with a persistent, inexpensive cloud object store layer. InfiniStore enables durability despite function failures using a fast parallel recovery scheme built on the auto-scaling functionality of a FaaS (Function-as-a-Service) platform. We evaluate InfiniStore extensively using both microbenchmarking and two real-world applications. Results show that InfiniStore has more performance benefits for objects larger than 10 MB compared to AWS ElastiCache and Anna, and InfiniStore achieves 26.25\% and 97.24\% tenant-side cost reduction compared to InfiniCache and ElastiCache, respectively.",10.14778/3587136.3587139
10.14778/3611479.3611484,Epoxy: ACID Transactions across Diverse Data Stores,"Kraft, Peter; Li, Qian; Zhou, Xinjing; Bailis, Peter; Stonebraker, Michael; Zaharia, Matei; Yu, Xiangyao",2023,Proc. VLDB Endow.,"Developers are increasingly building applications that incorporate multiple data stores, for example to manage heterogeneous data. Often, these require transactional safety for operations across stores, but few systems support such guarantees. To solve this problem, we introduce Epoxy, a protocol for providing transactions across heterogeneous data stores. We make two contributions. First, we adapt multi-version concurrency control to a cross-data store setting, storing versioning information in record metadata and filtering reads with predicates on metadata so they only see record versions in a global transaction snapshot. Second, we show our design enables an atomic commit protocol that does not require data stores implement the participant protocol of two-phase commit, requiring only durable writes. We implement Epoxy for five data stores: Postgres, Elasticsearch, MongoDB, Google Cloud Storage, and MySQL. We evaluate it by adapting TPC-C and microservice workloads to a multi-data store environment. We find it has comparable performance to the distributed transaction protocol XA on TPC-C while providing stronger guarantees like isolation, and has overhead of &lt;10\% compared to a non-transactional baseline on read-mostly microservice workloads and 72\% on write-heavy workloads.",10.14778/3611479.3611484
10.14778/3611479.3611510,R3: Record-Replay-Retroaction for Database-Backed Applications,"Li, Qian; Kraft, Peter; Cafarella, Michael; Demiralp, \c{C}a\u{g}atay; Graefe, Goetz; Kozyrakis, Christos; Stonebraker, Michael; Suresh, Lalith; Yu, Xiangyao; Zaharia, Matei",2023,Proc. VLDB Endow.,"Developers would benefit greatly from time travel: being able to faithfully replay past executions and retroactively execute modified code on past events. Currently, replay and retroaction are impractical because they require expensively capturing fine-grained timing information to reproduce concurrent accesses to shared state. In this paper, we propose practical time travel for database-backed applications, an important class of distributed applications that access shared state through transactions.We present R3, a novel Record-Replay-Retroaction tool. R3 implements a lightweight interceptor to record concurrency information for applications at transaction-level granularity, enabling replay and retroaction with minimal overhead. We address key challenges in both replay and retroaction. First, we design a novel algorithm for faithfully reproducing application requests running with snapshot isolation, allowing R3 to support most production DBMSs. Second, we develop a retroactive execution mechanism that provides high fidelity with the original trace while supporting nearly arbitrary code modifications. We demonstrate how R3 simplifies debugging for real, hard-to-reproduce concurrency bugs from popular open-source web applications. We evaluate R3 using TPC-C and microservice workloads and show that R3 always-on recording has a small performance overhead (&lt;25\% for point queries but &lt;0.1\% for complex transactions like in TPC-C) during normal application execution and that R3 can retroactively execute bugfixed code over recorded traces within 0.11--0.78\texttimes{} of the original execution time.",10.14778/3611479.3611510
10.14778/3611479.3611530,Tigger: A Database Proxy That Bounces with User-Bypass,"Butrovich, Matthew; Ramanathan, Karthik; Rollinson, John; Lim, Wan Shen; Zhang, William; Sherry, Justine; Pavlo, Andrew",2023,Proc. VLDB Endow.,"Developers often deploy database-specific network proxies whereby applications connect transparently to the proxy instead of directly connecting to the database management system (DBMS). This indirection improves system performance through connection pooling, load balancing, and other DBMS-specific optimizations. Instead of simply forwarding packets, these proxies implement DBMS protocol logic (i.e., at the application layer) to achieve this behavior. Consequently, existing proxies are user-space applications that process requests as they arrive on network sockets and forward them to the appropriate destinations. This approach incurs inefficiencies as the kernel repeatedly copies buffers between user-space and kernel-space, and the associated system calls add CPU overhead.This paper presents user-bypass, a technique to eliminate these overheads by leveraging modern operating system features that support custom code execution. User-bypass pushes application logic into kernel-space via Linux's eBPF infrastructure. To demonstrate its benefits, we implemented Tigger, a PostgreSQL-compatible DBMS proxy using user-bypass to eliminate the overheads of traditional proxy design. We compare Tigger's performance against other state-of-the-art proxies widely used in real-world deployments. Our experiments show that Tigger outperforms other proxies --- in one scenario achieving both the lowest transaction latencies (up to 29\% reduction) and lowest CPU utilization (up to 42\% reduction). The results show that user-bypass implementations like Tigger are well-suited to DBMS proxies' unique requirements.",10.14778/3611479.3611530
10.14778/3611540.3611562,PolarDB-SCC: A Cloud-Native Database Ensuring Low Latency for Strongly Consistent Reads,"Yang, Xinjun; Zhang, Yingqiang; Chen, Hao; Sun, Chuan; Li, Feifei; Zhou, Wenchao",2023,Proc. VLDB Endow.,"A classic design of cloud-native databases adopts an architecture that consists of one read/write (RW) node and one or more read-only (RO) nodes. In such a design, the propagation of write-ahead logs (WALs) from the RW node to the RO node(s) is typically performed asynchronously. Consequently, system designers either have to accept a loose consistency guarantee, where a read from the RO node may return stale data, or tolerate significant performance degradation in terms of read latency, as it then needs to wait for the log to be propagated and applied. Most commercial cloud-native databases, such as Amazon Aurora, choose performance over strong consistency. As a result, it makes RO nodes useless for many applications requiring read-after-write consistency (a form of strong consistency), and the support for serverless databases (i.e., allowing the RO nodes to be scaled out automatically) is impossible as they require a single endpoint.This paper proposes PolarDB-SCC (PolarDB-Strongly Consistent Cluster), a cloud-native database architecture that guarantees strongly consistent reads with very low latency. The core idea is to eliminate unnecessary waits and reduce the necessary wait time on RO nodes while still supporting strong consistency. To achieve this, it tracks the RW node's modification timestamp at three progressively finer-grained levels. We further design a Linear Lamport timestamp to reduce the RO node's timestamp fetching operations and leverage the RDMA network for all the data transferring (e.g., timestamp fetching and log shipment) to minimize network overhead and extra CPU usage. Our evaluation shows that PolarDB-SCC does not incur any noticeable overhead for ensuring strongly consistent reads compared with the eventually consistent (stale) read policy. To the best of our knowledge, PolarDB-SCC is the first ""read-write splitting"" cloud-native database that supports strongly consistent read with negligible overhead. Compared with a straightforward read-wait design, PolarDB-SCC improves throughput by up to 4.51\texttimes{} and reduces median latency by up to 3.66\texttimes{} in SysBench's read-write workload. PolarDB-SCC is already commercially available at Alibaba Cloud.",10.14778/3611540.3611562
10.14778/3611540.3611563,ScalarDB: Universal Transaction Manager for Polystores,"Yamada, Hiroyuki; Suzuki, Toshihiro; Ito, Yuji; Nemoto, Jun",2023,Proc. VLDB Endow.,"This paper presents ScalarDB, a universal transaction manager that achieves distributed transactions across multiple disparate databases. ScalarDB provides a database-agnostic transaction manager on top of its database abstraction; thus, it achieves transactions spanning various databases without depending on the transactional capability of underlying databases. ScalarDB is based on several research works and extended to provide a strong correctness guarantee (i.e., strict serializability), further performance optimizations, and several critical mechanisms for productization. In this paper, we describe the design and implementation of ScalarDB. We also present evaluation results showing that ScalarDB achieves database-spanning transactions with reasonable performance and near-linear scalability without sacrificing correctness. Finally, we share some case studies and lessons learned while building and running ScalarDB.",10.14778/3611540.3611563
10.14778/3611540.3611565,Eigen: End-to-End Resource Optimization for Large-Scale Databases on the Cloud,"Li, Ji You; Zhang, Jiachi; Zhou, Wenchao; Liu, Yuhang; Zhang, Shuai; Xue, Zhuoming; Xu, Ding; Fan, Hua; Zhou, Fangyuan; Li, Feifei",2023,Proc. VLDB Endow.,"Increasingly, cloud database vendors host large-scale geographically distributed clusters to provide cloud database services. When managing the clusters, we observe that it is challenging to simultaneously maximizing the resource allocation ratio and resource availability. This problem becomes more severe in modern cloud database clusters, where resource allocations occur more frequently and on a greater scale. To improve the resource allocation ratio without hurting resource availability, we introduce Eigen, a large-scale cloud-native cluster management system for large-scale databases on the cloud. Based on a resource flow model, we propose a hierarchical resource management system and three resource optimization algorithms that enable end-to-end resource optimization. Furthermore, we demonstrate the system optimization that promotes user experience by reducing scheduling latencies and improving scheduling throughput. Eigen has been launched in a large-scale public-cloud production environment for 30+ months and served more than 30+ regions (100+ available zones) globally. Based on the evaluation of real-world clusters and simulated experiments, Eigen can improve the allocation ratio by over 27\% (from 60\% to 87.0\%) on average, while the ratio of delayed resource provisions is under 0.1\%.",10.14778/3611540.3611565
10.14778/3611540.3611567,Kora: A Cloud-Native Event Streaming Platform for Kafka,"Povzner, Anna; Mahajan, Prince; Gustafson, Jason; Rao, Jun; Juma, Ismael; Min, Feng; Sridharan, Shriram; Bhatia, Nikhil; Attaluri, Gopi; Chandra, Adithya; Kozlovski, Stanislav; Sivaram, Rajini; Bradstreet, Lucas; Barrett, Bob; Shah, Dhruvil; Jacot, David; Arthur, David; Dagostino, Ron; McCabe, Colin; Obili, Manikumar Reddy; Prakasam, Kowshik; Sancio, Jose Garcia; Singh, Vikas; Nikhil, Alok; Gupta, Kamal",2023,Proc. VLDB Endow.,"Event streaming is an increasingly critical infrastructure service used in many industries and there is growing demand for cloud-native solutions. Confluent Cloud provides a massive scale event streaming platform built on top of Apache Kafka with tens of thousands of clusters running in 70+ regions across AWS, Google Cloud, and Azure. This paper introduces Kora, the cloud-native platform for Apache Kafka at the core of Confluent Cloud. We describe Kora's design that enables it to meet its cloud-native goals, such as reliability, elasticity, and cost efficiency. We discuss Kora's abstractions which allow users to think in terms of their workload requirements and not the underlying infrastructure, and we discuss how Kora is designed to provide consistent, predictable performance across cloud environments with diverse capabilities.",10.14778/3611540.3611567
10.14778/3611540.3611639,Modernization of Databases in the Cloud Era: Building Databases that Run Like Legos,"Li, Feifei",2023,Proc. VLDB Endow.,"Utilizing cloud for common and critical computing infrastructures has already become the norm across the board. The rapid evolvement of the underlying cloud infrastructure and the revolutionary development of AI present both challenges and opportunities for building new database architectures and systems. It is crucial to modernize database systems in the cloud era, so that next generation cloud native databases may run like legos-they are adaptive, flexible, reliable, and smart towards dynamic workloads and varying requirements.That said, we observe four critical trends and requirements for the modernization of cloud databases: embracing cloud-native architecture, full integration with cloud platform and orchestration, co-design for data fabric, and moving towards being AI augmented. Modernizing database systems by adopting these critical trends and addressing key challenges associated with them provide ample opportunities for data management communities from both academia and industry to explore. We will provide an in-depth case study of how we modernize PolarDB with respect to embracing these four trends in the cloud era. Our ultimate goal is to build databases that run just like playing with legos, so that a database system fits for rich and dynamic workloads and requirements in a self-adaptive, performant, easy-/intuitive-to use, reliable, and intelligent manner.",10.14778/3611540.3611639
10.14778/3632093.3632101,ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection,"Chen, Yuhang; Zhang, Chaoyun; Ma, Minghua; Liu, Yudong; Ding, Ruomeng; Li, Bowen; He, Shilin; Rajmohan, Saravan; Lin, Qingwei; Zhang, Dongmei",2023,Proc. VLDB Endow.,"Anomaly detection in multivariate time series data is of paramount importance for large-scale systems. However, accurately detecting anomalies in such data poses significant challenges due to the need for precise data modeling capability. Existing forecasting and reconstruction-based methods struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capture complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process.We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observes a remarkable 11.4\% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.",10.14778/3632093.3632101
10.14778/3648160.3648180,FCBench: Cross-Domain Benchmarking of Lossless Compression for Floating-Point Data,"Chen, Xinyu; Tian, Jiannan; Beaver, Ian; Freeman, Cynthia; Yan, Yan; Wang, Jianguo; Tao, Dingwen",2024,Proc. VLDB Endow.,"While both the database and high-performance computing (HPC) communities utilize lossless compression methods to minimize floating-point data size, a disconnect persists between them. Each community designs and assesses methods in a domain-specific manner, making it unclear if HPC compression techniques can benefit database applications or vice versa. With the HPC community increasingly leaning towards in-situ analysis and visualization, more floating-point data from scientific simulations are being stored in databases like Key-Value Stores and queried using in-memory retrieval paradigms. This trend underscores the urgent need for a collective study of these compression methods' strengths and limitations, not only based on their performance in compressing data from various domains but also on their runtime characteristics. Our study extensively evaluates the performance of eight CPU-based and five GPU-based compression methods developed by both communities, using 33 real-world datasets assembled in the Floating-point Compressor Benchmark (FCBench). Additionally, we utilize the roofline model to profile their runtime bottlenecks. Our goal is to offer insights into these compression methods that could assist researchers in selecting existing methods or developing new ones for integrated database and HPC applications.",10.14778/3648160.3648180
10.14778/3685800.3685819,Transparent Migration from Datastore to Firestore,"Davisson, Ed; Dickopp, Tilo; Gay, David; Karasuda, Eric; Kesavan, Ram; Yushprakh, Vadim",2024,Proc. VLDB Endow.,"Datastore was one of Google's first cloud databases, launched initially as part of App Engine, and built over Google's internal Megastore database system. Firestore was launched in 2019, both a re-implementation of Datastore over Google's Spanner database system and a new, mobile and web-friendly Firestore API. Spanner was chosen as the storage engine of Firestore in particular for technical reasons---it provides unrestricted transaction capabilities, strong consistency guarantees, and other improvements over Megastore.To provide these improvements to all our customers, and simplify our overall system, a non-disruptive, zero-downtime migration was executed of all Datastore databases (stored in Megastore) to Firestore databases (stored in Spanner). This migration took a couple of years to design and plan, and about three to execute. This paper describes both the core engine for migrating databases, and various practical problems that were solved to make this journey successful. As of the writing of this paper, all (over one million) databases have been successfully migrated.",10.14778/3685800.3685819
10.14778/3685800.3685824,X-Stor: A Cloud-Native NoSQL Database Service with Multi-Model Support,"Lei, Hongyu; Li, Chunhua; Zhou, Ke; Zhu, Jianping; Yan, Kezhou; Xiao, Fen; Xie, Ming; Wang, Jiang; Di, Shiyu",2024,Proc. VLDB Endow.,"In recent years at Tencent, we have observed that the use of multiple NoSQL databases for storing business data with diverse models has led to increased programming and deployment costs, as well as inefficient maintenance and underutilized resources. In this paper, we report X-Stor, a cloud-native NoSQL database system that supports multiple data models by extending different storage engines and efficiently managing them through a unified control plane. This design significantly reduces expenses and enables rapid expansion of new models, while seamlessly supporting their complete functionality through storage engine extensions. By consolidating multi-tenant services and data models on the same physical machines, X-Stor significantly enhances the utilization of cluster resources. Additionally, X-Stor introduces a standardized metric called Request Unit (RU) to measure tenant resource consumption for consumption-based pricing purposes. Leveraging this metric, we design RU-based resource management strategies and achieve efficient multi-tenant resource isolation and system load balancing. Currently, X-Stor manages a storage capacity of over 12PB for online operational data, including more than 100,000 tables with multiple data models. It handles 700 billion requests per day with a peak of 30 million requests per second. We evaluate the performance of X-Stor on popular benchmarks and production workloads. The results show that X-Stor performs well under diverse data models.",10.14778/3685800.3685824
10.5555/3282588.3282614,Incorporating devops into undergraduate software engineering courses: a suggested framework,"Bennett, Brian T.; Barrett, Martin L.",2018,J. Comput. Sci. Coll.,"DevOps is a current trend for application development and delivery in the industry. Teaching about DevOps in an undergraduate software engineering course is difficult because the number of new concepts involved is high, and most students lack experience with them. This paper presents our initial experience integrating DevOps into undergraduate software engineering coursework. It discusses DevOps itself and introduces a basic framework for introducing DevOps into a software engineering sequence. Finally, we discuss how we currently approach DevOps instruction and where we need to go to fit within the suggested framework.",
10.5555/3381555.3381567,Design and development of an open private educational cloud storage solution for application development,"Pyatt, Kevin; Lotfy, Mohamed",2019,J. Comput. Sci. Coll.,"CS, IT, and software engineering students need to learn and master web application development and cloud computing skills on an educational full-stack Cloud architecture. In this paper we provide the design of an educational full-stack architecture integrating a private storage cloud. The focus was on how to integrate, configure, secure and deploy web-software applications, microservices and private storage within a full-stack architecture. In this research study we determined the feasibility of designing and developing an open private-cloud storage solution for cloud software. We also share the results of the deployment and testing of phases 0 and 1 of the educational full-stack architecture.",
10.5555/3447307.3447319,Shifting traditional undergraduate software engineering instruction to a DevOps focus,"Bennett, Brian T.",2021,J. Comput. Sci. Coll.,"Classical Software Engineering education often includes traditional methodologies that do not adequately describe today's industry practice. DevOps is a culture that promotes fast delivery, continuous feedback, and an environment of learning. Its non-linear path requires a shift in software engineering pedagogy. This case study describes the redevelopment of a second-semester course in software engineering to focus on DevOps principles. The study evaluates student performance using formative and summative assessment through a team project tracked throughout the semester and final exam results. Results indicated that students developed DevOps skills during the course, but may have needed more reinforcement of some traditional Software Engineering topics.",
10.5555/3575846.3575851,The structure and delivery of an advanced systems administration IT course,"Lotfy, Mohamed; Fredrickson, Christian",2022,J. Comput. Sci. Coll.,"IT program courses should allow students to acquire the needed IT skills to enable them to be job ready by graduation. To prepare the IT graduates to current and future practices of virtualized computing resources, which integrate IT systems and services, DevOps and microservices, systems administration courses need to introduce students to current tools used to administer and manage continuous integration and deployment of infrastructure and services. In this paper, we provide the structure, components, hands-on assignments, and the virtual environment of a senior-level competency-based advanced Unix/Linux systems administration course that is delivered face-to-face and online. The course introduces current organizational practices of continuous integration and delivery of services. Student course evaluation and what helped them learn the most are also presented and discussed.",
10.5555/3637068.3637070,Preparing Students for Software Production with DevOps: A Graduate Course Approach,"Bennett, Brian T.",2023,J. Comput. Sci. Coll.,"Software Engineering education continues to describe classical methods without fully embracing modern practices. DevOps combines Software Engineering practices with the production and operations of the software itself. This study describes a graduate course in software production that primarily focuses on DevOps practices, while minimally discussing Software Engineering. Student performance in each topic was tracked through formative assessment (on quizzes) and summative assessment (on exams). Results showed that students' performance for learning outcomes improved an average of 22\% after participating in lectures, discussions, exercises, and projects, validating the course design.",
