@article{10.1109/TASLP.2020.3015659,
author = {Yu, Kai and Ma, Rao and Shi, Kaiyu and Liu, Qi},
title = {Neural Network Language Model Compression With Product Quantization and Soft Binarization},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3015659},
doi = {10.1109/TASLP.2020.3015659},
abstract = {Large memory consumption of the neural network language models (NN LMs) prohibits their use in many resource-constrained scenarios. Hence, effective NN LM compression approaches that are independent of NN structures are of great interest. However, previous approaches usually achieve a high compression ratio at the cost of obvious performance loss. In this paper, two recently proposed quantization approaches, product quantization (PQ) and soft binarization are effectively combined to address the issue. PQ decomposes word embedding matrices into a Cartesian product of low dimensional subspaces and quantizes each subspace separately. Soft binarization uses a small number of float scalars and the knowledge distillation technique to recover the performance loss during the binarization. Experiments show that the proposed approaches can achieve a high compression ratio, from 70 to over 100, while still maintaining comparable performance to the uncompressed NN LM on both PPL and word error rate criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2438–2449},
numpages = {12}
}

@article{10.1109/TASLP.2020.3030495,
author = {Sun, Xingwei and Gao, Ze-Feng and Lu, Zhong-Yi and Li, Junfeng and Yan, Yonghong},
title = {A Model Compression Method With Matrix Product Operators for Speech Enhancement},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3030495},
doi = {10.1109/TASLP.2020.3030495},
abstract = {The deep neural network (DNN) based speech enhancement approaches have achieved promising performance. However, the number of parameters involved in these methods is usually enormous for the real applications of speech enhancement on the device with the limited resources. This seriously restricts the applications. To deal with this issue, model compression techniques are being widely studied. In this paper, we propose a model compression method based on matrix product operators (MPO) to substantially reduce the number of parameters in DNN models for speech enhancement. In this method, the weight matrices in the linear transformations of neural network model are replaced by the MPO decomposition format before training. In experiment, this process is applied to the causal neural network models, such as the feedforward multilayer perceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM models with/without compression are then utilized to estimate the ideal ratio mask for monaural speech enhancement. The experimental results show that our proposed MPO-based method outperforms the widely-used pruning method for speech enhancement under various compression rates, and further improvement can be achieved with respect to low compression rates. Our proposal provides an effective model compression method for speech enhancement, especially in cloud-free application.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {2837–2847},
numpages = {11}
}

@article{10.1109/TASLP.2022.3202122,
author = {Xu, Kang and Li, Fei and Xie, Dongdong and Ji, Donghong},
title = {Revisiting Aspect-Sentiment-Opinion Triplet Extraction: Detailed Analyses Towards a Simple and Effective Span-Based Model},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3202122},
doi = {10.1109/TASLP.2022.3202122},
abstract = {Aspect-based sentiment information extraction has attracted increasing attention in the research community of natural language processing. Various methods, such as sequence tagging, sequence-to-sequence generation and span-based extraction, have been proposed, which own different advantages and disadvantages. In this article, we revisit the span-based method for aspect-sentiment-opinion triplet extraction, by designing and analyzing a simple yet effective Span-based Model, called SMTFASTE. Our model leverages a tidily three-layer architecture, including a BERT-based encoding layer, a span representation layer and an aspect-sentiment-opinion prediction layer. In the experiments of two widely-used benchmarks (ASTE-V2 and ASOTE-V2), we find that our model outperforms a number of complicated state-of-the-art models in most evaluation metrics. Therefore, we conduct detailed analyses for our model, such as ablation studies of the core components of our model and the benefit of explicitly using context information, and obtain some insightful findings and conclusion. Through this study, we show that a simple span-based model is able to achieve competitive results without much feature and architecture engineering. Our model is easy to follow and we have opened our code to facilitate related research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2918–2927},
numpages = {10}
}

@article{10.1109/TASLP.2023.3293044,
author = {Hasumi, Takuya and Nakamura, Tomohiko and Takamune, Norihiro and Saruwatari, Hiroshi and Kitamura, Daichi and Takahashi, Yu and Kondo, Kazunobu},
title = {PoP-IDLMA: Product-of-Prior Independent Deeply Learned Matrix Analysis for Multichannel Music Source Separation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3293044},
doi = {10.1109/TASLP.2023.3293044},
abstract = {Independent deeply learned matrix analysis (IDLMA) is a state-of-the-art determined audio source separation method based on pretrained deep neural networks (DNNs). Owing to the excellent expression power of DNNs, IDLMA can handle a wider range of sources than conventional source models such as nonegative matrix factorization (NMF). However, owing to its supervised nature, the separation performance of IDLMA often degrades in the presence of timbral mismatches between the training data and the to-be-separated data. In this paper, we propose two source models that encompass the NMF- and DNN-based source models by constructing a prior distribution of the source power spectrogram (product of priors: PoP) on the basis of the product-of-expert concept. Since the NMF-based source model works well for a fully blind situation, the proposed models can handle the timbral mismatch without losing the expression power of DNNs. By introducing the PoP-based source models into IDLMA, we propose IDLMA extensions (PoP-IDLMAs) and derive their efficient parameter estimation algorithms on the basis of the majorization–minimization algorithm. Experimental results demonstrated the effectiveness of the proposed PoP-IDLMAs and that the proposed models greatly improve the source power estimation in frequency bands above 500 Hz.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2680–2694},
numpages = {15}
}

@article{10.1109/TASLP.2023.3328285,
author = {Drgas, Szymon and Bramsl\o{}w, Lars and Politis, Archontis and Naithani, Gaurav and Virtanen, Tuomas},
title = {Dynamic Processing Neural Network Architecture for Hearing Loss Compensation},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3328285},
doi = {10.1109/TASLP.2023.3328285},
abstract = {This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {203–214},
numpages = {12}
}

@article{10.1109/TASLP.2024.3363414,
author = {Zhou, Xuehao and Zhang, Mingyang and Zhou, Yi and Wu, Zhizheng and Li, Haizhou},
title = {Accented Text-to-Speech Synthesis With Limited Data},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3363414},
doi = {10.1109/TASLP.2024.3363414},
abstract = {This paper presents an accented text-to-speech (TTS) synthesis framework with limited training data. We study two aspects concerning accent rendering: phonetic (phoneme difference) and prosodic (pitch pattern and phoneme duration) variations. The proposed accented TTS framework consists of two models: an accented front-end for grapheme-to-phoneme (G2P) conversion and an accented acoustic model with integrated pitch and duration predictors for phoneme-to-Mel-spectrogram prediction. The accented front-end directly models the phonetic variation, while the accented acoustic model explicitly controls the prosodic variation. Specifically, both models are first pre-trained on a large amount of data, then only the accent-related layers are fine-tuned on a limited amount of data for the target accent. In the experiments, speech data of three English accents, i.e., General American English, Irish English, and British English Received Pronunciation, are used for pre-training. The pre-trained models are then fine-tuned with Scottish and General Australian English accents, respectively. Both objective and subjective evaluation results show that the accented TTS front-end fine-tuned with a small accented phonetic lexicon (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$text{5},k$&lt;/tex-math&gt;&lt;/inline-formula&gt; words) effectively handles the phonetic variation of accents, while the accented TTS acoustic model fine-tuned with a limited amount of accented speech data (approximately 3 minutes) effectively improves the prosodic rendering including pitch and duration. The overall accent modeling contributes to improved speech quality and accent similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1699–1711},
numpages = {13}
}

@article{10.1109/TASLP.2024.3459430,
author = {Meng, Weixin and Li, Xiaoyu and Li, Andong and Luo, Xiaoxue and Yan, Shefeng and Li, Xiaodong and Zheng, Chengshi},
title = {Deep Kronecker Product Beamforming for Large-Scale Microphone Arrays},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3459430},
doi = {10.1109/TASLP.2024.3459430},
abstract = {Although deep learning based beamformers have achieved promising performance using small microphone arrays, they suffer from performance degradation in very challenging environments, such as extremely low Signal-to-Noise Ratio (SNR) environments, e.g., SNR &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$le$&lt;/tex-math&gt;&lt;/inline-formula&gt;−10 dB. A large-scale microphone array with dozens or hundreds of microphones can improve the performance of beamformers in these challenging scenarios because of its high spatial resolution. While a dramatic increase in the number of microphones leads to feature redundancy, causing difficulties in feature extraction and network training. As an attempt to improve the performance of deep beamformers for speech extraction in very challenging scenarios, this paper proposes a novel all neural Kronecker product beamforming denoted by ANKP-BF for large-scale microphone arrays by taking the following two aspects into account. Firstly, a larger microphone array can provide higher performance of spatial filtering when compared with a small microphone array, and deep neural networks are introduced for their powerful non-linear modeling capability in the speech extraction task. Secondly, the feature redundancy problem is solved by introducing the Kronecker product rule to decompose the original one high-dimension weight vector into the Kronecker product of two much lower-dimensional weight vectors. The proposed ANKP-BF is designed to operate in an end-to-end manner. Extensive experiments are conducted on simulated large-scale microphone-array signals using the DNS-Challenge corpus and WSJ0-SI84 corpus, and the real recordings in a semi-anechoic room and outdoor scenes are also used to evaluate and compare the performance of different methods. Quantitative results demonstrate that the proposed method outperforms existing advanced baselines in terms of multiple objective metrics, especially in very low SNR environments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4537–4553},
numpages = {17}
}

@article{10.1109/TCBB.2019.2897679,
author = {Zeng, Min and Li, Min and Fei, Zhihui and Wu, Fang-Xiang and Li, Yaohang and Pan, Yi and Wang, Jianxin},
title = {A Deep Learning Framework for Identifying Essential Proteins by Integrating Multiple Types of Biological Information},
year = {2021},
issue_date = {Jan.-Feb. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2897679},
doi = {10.1109/TCBB.2019.2897679},
abstract = {Computational methods including centrality and machine learning-based methods have been proposed to identify essential proteins for understanding the minimum requirements of the survival and evolution of a cell. In centrality methods, researchers are required to design a score function which is based on prior knowledge, yet is usually not sufficient to capture the complexity of biological information. In machine learning-based methods, some selected biological features cannot represent the complete properties of biological information as they lack a computational framework to automatically select features. To tackle these problems, we propose a deep learning framework to automatically learn biological features without prior knowledge. We use node2vec technique to automatically learn a richer representation of protein-protein interaction (PPI) network topologies than a score function. Bidirectional long short term memory cells are applied to capture non-local relationships in gene expression data. For subcellular localization information, we exploit a high dimensional indicator vector to characterize their feature. To evaluate the performance of our method, we tested it on PPI network of S. cerevisiae. Our experimental results demonstrate that the performance of our method is better than traditional centrality methods and is superior to existing machine learning-based methods. To explore which of the three types of biological information is the most vital element, we conduct an ablation study by removing each component in turn. Our results show that the PPI network embedding contributes most to the improvement. In addition, gene expression profiles and subcellular localization information are also helpful to improve the performance in identification of essential proteins.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {296–305},
numpages = {10}
}

@article{10.1109/TCBB.2021.3122294,
author = {Li, Yiming and Zeng, Min and Wu, Yifan and Li, Yaohang and Li, Min},
title = {Accurate Prediction of Human Essential Proteins Using Ensemble Deep Learning},
year = {2021},
issue_date = {Nov.-Dec. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3122294},
doi = {10.1109/TCBB.2021.3122294},
abstract = {Essential proteins are considered the foundation of life as they are indispensable for the survival of living organisms. Computational methods for essential protein discovery provide a fast way to identify essential proteins. But most of them heavily rely on various biological information, especially protein-protein interaction networks, which limits their practical applications. With the rapid development of high-throughput sequencing technology, sequencing data has become the most accessible biological data. However, using only protein sequence information to predict essential proteins has limited accuracy. In this paper, we propose EP-EDL, an ensemble deep learning model using only protein sequence information to predict human essential proteins. EP-EDL integrates multiple classifiers to alleviate the class imbalance problem and to improve prediction accuracy and robustness. In each base classifier, we employ multi-scale text convolutional neural networks to extract useful features from protein sequence feature matrices with evolutionary information. Our computational results show that EP-EDL outperforms the state-of-the-art sequence-based methods. Furthermore, EP-EDL provides a more practical and flexible way for biologists to accurately predict essential proteins. The source code and datasets can be downloaded from &lt;uri&gt;https://github.com/CSUBioGroup/EP-EDL&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {3263–3271},
numpages = {9}
}

@article{10.1109/TCBB.2022.3219114,
author = {Zhang, Hao and Song, Honglei and Xu, Xiaoming and Chang, Qixin and Wang, Mingkai and Wei, Yanjie and Yin, Zekun and Schmidt, Bertil and Liu, Weiguo},
title = {RabbitFX: Efficient Framework for FASTA/Q File Parsing on Modern Multi-Core Platforms},
year = {2022},
issue_date = {May-June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3219114},
doi = {10.1109/TCBB.2022.3219114},
abstract = {The continuous growth of generated sequencing data leads to the development of a variety of associated bioinformatics tools. However, many of them are not able to fully exploit the resources of modern multi-core systems since they are bottlenecked by parsing files leading to slow execution times. This motivates the design of an efficient method for parsing sequencing data that can exploit the power of modern hardware, especially for modern CPUs with fast storage devices. We have developed RabbitFX, a fast, efficient, and easy-to-use framework for processing biological sequencing data on modern multi-core platforms. It can efficiently read FASTA and FASTQ files by combining a lightweight parsing method by means of an optimized formatting implementation. Furthermore, we provide user-friendly and modularized C++ APIs that can be easily integrated into applications in order to increase their file parsing speed. As proof-of-concept, we have integrated RabbitFX into three I/O-intensive applications: fastp, Ktrim, and Mash. Our evaluation shows that the inclusion of RabbitFX leads to speedups of at least 11.6 (6.6), 2.4 (2.4), and 3.7 (3.2) compared to the original versions on plain (gzip-compressed) files, respectively. These case studies demonstrate that RabbitFX can be easily integrated into a variety of NGS analysis tools to significantly reduce associated runtimes. It is open source software available at &lt;uri&gt;https://github.com/RabbitBio/RabbitFX&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {2341–2348},
numpages = {8}
}

@article{10.1109/TCBB.2024.3366240,
author = {K. S., Sheena and Nair, Madhu S.},
title = {GenCoder: A Novel Convolutional Neural Network Based Autoencoder for Genomic Sequence Data Compression},
year = {2024},
issue_date = {May-June 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3366240},
doi = {10.1109/TCBB.2024.3366240},
abstract = {Revolutionary advances in DNA sequencing technologies fundamentally change the nature of genomics. Today's sequencing technologies have opened into an outburst in genomic data volume. These data can be used in various applications where long-term storage and analysis of genomic sequence data are required. Data-specific compression algorithms can effectively manage a large volume of data. In recent times, deep learning has achieved great success in many compression tools and is gradually being used in genomic sequence compression. Significantly, autoencoder has been applied in dimensionality reduction, compact representations of data, and generative model learning. It can use convolutional layers to learn essential features from input data, which is better for image and series data. Autoencoder reconstructs the input data with some loss of information. Since accuracy is critical in genomic data, compressed genomic data must be decompressed without any information loss. We introduce a new scheme to address the loss incurred in the decompressed data of the autoencoder. This paper proposes a novel algorithm called GenCoder for reference-free compression of genomic sequences using a convolutional autoencoder and regenerating the genomic sequences from a latent code produced by the autoencoder, and retrieving original data losslessly. Performance evaluation is conducted on various genomes and benchmarked datasets. The experimental results on the tested data demonstrate that the deep learning model used in the proposed compression algorithm generalizes well for genomic sequence data and achieves a compression gain of 27% over the best state-of-the-art method.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {405–415},
numpages = {11}
}

@article{10.1109/TCBB.2024.3385635,
author = {Ge, Ruiquan and Xia, Yixiao and Jiang, Minchao and Jia, Gangyong and Jing, Xiaoyang and Li, Ye and Cai, Yunpeng},
title = {HybAVPnet: A Novel Hybrid Network Architecture for Antiviral Peptides Prediction},
year = {2024},
issue_date = {Sept.-Oct. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3385635},
doi = {10.1109/TCBB.2024.3385635},
abstract = {Viruses pose a great threat to human production and life, thus the research and development of antiviral drugs is urgently needed. Antiviral peptides play an important role in drug design and development. Compared with the time-consuming and laborious wet chemical experiment methods, it is critical to use computational methods to predict antiviral peptides accurately and rapidly. However, due to limited data, accurate prediction of antiviral peptides is still challenging and extracting effective feature representations from sequences is crucial for creating accurate models. This study introduces a novel two-step approach, named HybAVPnet, to predict antiviral peptides with a hybrid network architecture based on neural networks and traditional machine learning methods. We adopted a stacking-like structure to capture both the long-term dependencies and local evolution information to achieve a comprehensive and diverse prediction using the predicted labels and probabilities. Using an ensemble technique with the different kinds of features can reduce the variance without increasing the bias. The experimental result shows HybAVPnet can achieve better and more robust performance compared with the state-of-the-art methods, which makes it useful for the research and development of antiviral drugs. Meanwhile, it can also be extended to other peptide recognition problems because of its generalization ability.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {1358–1365},
numpages = {8}
}

@article{10.1109/TCBB.2024.3404013,
author = {Gao, Hang and Shen, Wenjun and Li, Rui and Liu, Cheng and Wu, Si},
title = {Collaborative Structure-Preserved Missing Data Imputation for Single-Cell RNA-Seq Clustering},
year = {2024},
issue_date = {Sept.-Oct. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3404013},
doi = {10.1109/TCBB.2024.3404013},
abstract = {Clustering of the single-cell RNA-seq (scRNA-seq) transcriptome profiles is able to identify cell types, which is beneficial to improve the understanding of disease progression. However, in practice, the single-cell expression data often contains a significant number of missing values as a result of technical variability. Missing data is a critical challenge in scRNA-seq clustering analysis since the unknown value does not reflect the underlying true expression level and makes it difficult to discovering cell types by applying clustering algorithms directly. Various approaches have been developed to overcome missing data issue in scRNA-seq clustering. Most of them recover missing expression values by borrowing observed data from similar cells or synthesizing data via generative adversarial networks. Such that the biologically meaningful cluster structure has not been sufficiently exploited. In this work, we introduce &lt;italic&gt;ColImpute&lt;/italic&gt;, a collaborative structure-preserved missing data imputation approach for the scRNA-seq clustering. Specifically, a cluster structure-preserved imputation module and a subspace clustering module, which respectively perform missing data imputation and cell subtypes identification, are integrated into a unified optimization framework to train the two networks in a collaborative manner. Consequently, the clustering module effectively contributes cluster-structure information to guide the trainning process of the missing data imputation module. Simultaneously, the cluster structure-preserved imputation module reciprocally enhances the performance of the clustering module by generating more precise recovered samples. Promising experimental results show that the proposed method is effective for both the data imputation and the cell types identification.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {1480–1491},
numpages = {12}
}

@article{10.1109/TNET.2020.3041235,
author = {Laki, S\'{a}ndor and N\'{a}das, Szilveszter and Gombos, Gerg\H{o} and Fejes, Ferenc and Hudoba, P\'{e}ter and Tur\'{a}nyi, Zolt\'{a}n and Kiss, Zolt\'{a}n and Keszei, Csaba},
title = {Core-Stateless Forwarding With QoS Revisited: Decoupling Delay and Bandwidth Requirements},
year = {2021},
issue_date = {April 2021},
publisher = {IEEE Press},
volume = {29},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.3041235},
doi = {10.1109/TNET.2020.3041235},
abstract = {Network QoS, fairness and resource sharing control are not completely solved problems. Available solutions lack scalability due to maintaining flow state, require re-tuning if traffic changes, focus on a limited set of networking scenarios or require complex, centralized controllers and feedback loops. In this paper, we propose a core-stateless solution for closed network domains like access, enterprise and data center networks that handles resource sharing and provides guarantees for per-hop latency, independently. The proposed method enables controlled resource sharing by encoding the utility function of flows to Packet Value markings. This allows expressing resource sharing policies for all possible congestion situations, while operation is completely flow unaware inside the network. In addition, it also satisfies per-hop delay requirements for traffic flows independently. The separation of the delay requirements of the packets from their importance has not generally been possible by existing methods so far. The performance of the proposed method has thoroughly been analyzed by large number of simulations covering both static and dynamic scenarios and was implemented in a cloud-native virtual router implementing all the policies needed for a Broadband Network Gateway, showing good performance and better scalability than existing weighted queuing-based solutions.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {503–516},
numpages = {14}
}

@article{10.1109/TNET.2022.3147320,
author = {Li, Xiao-Yan and Lin, Wanling and Chang, Jou-Ming and Jia, Xiaohua},
title = {Transmission Failure Analysis of Multi-Protection Routing in Data Center Networks With Heterogeneous Edge-Core Servers},
year = {2022},
issue_date = {Aug. 2022},
publisher = {IEEE Press},
volume = {30},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3147320},
doi = {10.1109/TNET.2022.3147320},
abstract = {The recently proposed RCube network is a cube-based server-centric data center network (DCN), including two types of heterogeneous servers, called core servers and edge servers. Remarkably, it takes the latter as backup servers to deal with server failures and thus achieve high availability. This paper first points out that RCube is suitable as a candidate topology of DCNs for edge computing. Three transmission types are among core and edge servers based on the demand for applications’ computation and instant response. We then employ protection routing to analyze the transmission failure of RCube DCNs. Unlike traditional protection routing, which only tolerates a single link or node failure, we use the multi-protection routing scheme to improve fault-tolerance capability. To configure a protection routing in a network, according to Tapolcai’s suggestion, we need to construct two completely independent spanning trees (CISTs), which are edge-disjoint and inner-vertex-disjoint spanning trees. It is well-known that the problem of determining whether there exists a dual-CIST (i.e., two CISTs) in a network is NP-complete. A logic graph of RCube, denoted by &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$L$ &lt;/tex-math&gt;&lt;/inline-formula&gt;-&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$RCube(n,m,k)$ &lt;/tex-math&gt;&lt;/inline-formula&gt;, is a network with a recursive structure. Each basic building element consists of &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n$ &lt;/tex-math&gt;&lt;/inline-formula&gt; core servers and &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$m$ &lt;/tex-math&gt;&lt;/inline-formula&gt; edge servers, where the order &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; is the number of recursions applied in the structure. In this paper, we provide algorithms to construct &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$min {n,lfloor (n+m)/2rfloor }$ &lt;/tex-math&gt;&lt;/inline-formula&gt; CISTs in &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$L$ &lt;/tex-math&gt;&lt;/inline-formula&gt;-&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$RCube(n,m,k)$ &lt;/tex-math&gt;&lt;/inline-formula&gt; for &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n+mgeqslant 4$ &lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$n&gt;1$ &lt;/tex-math&gt;&lt;/inline-formula&gt;. From a combination of the multiple CISTs, we can configure the desired multi-protection routing. In our simulation, we configure up to 10 protection routings for RCube DCNs. As far as we know, in past research, there were at most three protection routings developed in other network structures. Finally, we summarize some crucial analysis viewpoints about the transmission efficiency of DCNs with heterogeneous edge-core servers from the simulation results.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {1689–1702},
numpages = {14}
}

@article{10.1109/TNET.2022.3156529,
author = {Nasralla, Zaid H. and Elgorashi, Taisir E. H. and Hammadi, Ali and Musa, Mohamed O. I. and Elmirghani, Jaafar M. H.},
title = {Blackout Resilient Optical Core Network},
year = {2022},
issue_date = {Aug. 2022},
publisher = {IEEE Press},
volume = {30},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3156529},
doi = {10.1109/TNET.2022.3156529},
abstract = {A disaster may not necessarily demolish the telecommunications infrastructure, but instead it might affect the national grid and cause blackouts, consequently disrupting the network operation unless there is an alternative power source(s). In disaster-resilient networks, fiber cut, datacenter destruction, and node isolation have been studied before with different scenarios, but the power outage impact has not been investigated before. In this paper, power outages are considered, and the telecommunication network performance is evaluated during a blackout. A mixed Integer Linear Programming (MILP) model is developed to evaluate the network performance for a single node blackout under two scenarios: minimization of blocking and minimization of renewable and battery energy consumption. Insights analyzed from the MILP model results have demonstrated the trade-off between the two evaluated optimization cost functions and shown that the proposed scheme can extend the network lifetime while minimizing the required amount of backup energy.},
journal = {IEEE/ACM Trans. Netw.},
month = mar,
pages = {1795–1806},
numpages = {12}
}

@article{10.1109/TNET.2022.3201545,
author = {Wang, En and Zhang, Mijia and Liu, Wenbin and Xiong, Haoyi and Yang, Bo and Yang, Yongjian and Wu, Jie},
title = {Outlier-Concerned Data Completion Exploiting Intra- and Inter-Data Correlations in Sparse CrowdSensing},
year = {2022},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {31},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3201545},
doi = {10.1109/TNET.2022.3201545},
abstract = {Mobile CrowdSensing (MCS) is a popular data collection paradigm which usually faces the problem of sparse sensed data because of the limited sensing cost. In order to address the situation of sparse data, sparse MCS recruits users to sense important areas and infers completed data by data completion, which is crucial in sparse MCS for urban sensing applications (e.g. enhancing data expression, improving urban analysis, guiding city planning, etc.) To achieve accurate completion results, previous methods usually utilize the universal similarity and conventional tendency while incorporating only a single dataset to infer the full map. However, in real-world scenarios, there may exist many kinds of data (inter-data), that could help to complement each other. Moreover, for each kind of data (intra-data), there usually exist a few but important outliers caused by the special events (e.g., parking peak, traffic congestion, or festival parade), which may behave in a different way as the statistical patterns. These outliers cannot be ignored, while it is difficult to detect and recover them in data completion because of the following challenges: 1) the infrequency and unpredictability of outliers’ occurrence, 2) the large deviations against the means compared to normal values, and 3) the complex spatiotemporal relations among inter-data. To this end, focusing on spatiotemporal data with both intra- and inter-data correlations, we propose a matrix completion method that takes outliers’ effects into consideration and exploits both intra- and inter-data correlations for enhancing performance. Specifically, we first conduct the Deep Matrix Factorization (DMF) with multiple auxiliary Neural Networks, which named Stacked Deep Matrix Factorization (SDMF). Note that the loss function of SDMF is no longer the previous MSE loss function, but replaced with an Outlier Value Loss (OVL) function to effectively detect and recover the outliers. Moreover, a spatiotemporal outlier value memory network is added for further enhancing the outlier inference. Finally, we take extensive qualitative and quantitative experiments on two popular datasets each with two types of sensing data, and the experimental results indicate the advantages of our method that outperforms the state-of-the-art methods.},
journal = {IEEE/ACM Trans. Netw.},
month = sep,
pages = {648–663},
numpages = {16}
}

@article{10.1109/TNET.2022.3216592,
author = {Zhang, Han and Yin, Xia and Shi, Xingang and Wang, Jilong and Wang, Zhiliang and Guo, Yingya and Lan, Tian and Li, Yahui and Zhu, Yongqing and Ruan, Ke and Geng, Haijun},
title = {Achieving High Availability in Inter-DC WAN Traffic Engineering},
year = {2022},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {31},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3216592},
doi = {10.1109/TNET.2022.3216592},
abstract = {Inter-DataCenter Wide Area Network (Inter-DC WAN) that connects geographically distributed data centers is becoming one of the most critical network infrastructures. Due to limited bandwidth and inevitable link failures, it is highly challenging to guarantee network availability for services, especially those with stringent bandwidth demands, over inter-DC WAN. We present &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathsf {TEDAT}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;, a novel Traffic Engineering (TE) framework for Diverse Availability Targets (DAT), where a Service Level Agreement (SLA) is defined to ensure that each bandwidth demand must be satisfied with a stipulated probability, when subjected to the network capacity and possible failures of the inter-DC WAN. &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathsf {TEDAT}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; has two core components, i.e., traffic scheduling and failure recovery, which are crystalized through different mathematical models and theoretically analyzed. They are also extensively compared against state-of-the-art TE schemes, using a testbed as well as real trace driven simulations across different topologies, traffic matrices and failure scenarios. Our evaluations show that, compared with the optimal admission strategy, &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathsf {TEDAT}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; can speed up the online admission control by &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$30times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; at the expense of less than 4% false rejections. On the other hand, compared with the latest TE schemes like FFC and TEAVAR, &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathsf {TEDAT}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; can meet the bandwidth availability SLAs for 23%~60% more demands under normal loads, and when network failure causes SLA violations, it can retain 10%~20% more profit under a pricing and refunding model.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2406–2421},
numpages = {16}
}

@article{10.1109/TNET.2023.3302650,
author = {Yan, Fulong and Deng, Xiong and Yuan, Changshun and Yan, Boyuan and Xie, Chongjin},
title = {On the Performance Investigation of a Recursive Fast Optical Switch-Based High Performance Computing Network Architecture},
year = {2023},
issue_date = {Feb. 2024},
publisher = {IEEE Press},
volume = {32},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3302650},
doi = {10.1109/TNET.2023.3302650},
abstract = {We propose a novel high performance computing (HPC) network architecture &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; based on &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$L$ &lt;/tex-math&gt;&lt;/inline-formula&gt; parallel levels distributed low radix fast optical switches (FOS). We provide a detailed description of the blade, FOS and the operation of the &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; network. The &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; HPC network is highly scalable, and HFOS4 architecture can support an extremely large HPC network of 65,536 blades under distributed FOS with the same radix of 16 in each level. In principle, the &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; HPC network can be built by FOSes with different radices at each level. To find out the best configuration of FOS at each level and solve the energy and cost optimization problem in &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; network, we break down all the components in the FOS and develop the energy and cost models for the FOS. We verify that the energy and cost per radix functions of FOS are convex functions. Given this foundation, the theoretical investigation of the energy and cost optimization problem shows that the &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; network could achieve the minimum energy and cost only when the FOS radices of all levels in &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; network are the same. Besides, the cost and power consumption of &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$mathrm {HFOS}_{L}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; networks are compared with a widely used Leaf-Spine network.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {777–790},
numpages = {14}
}

@article{10.1109/TNET.2023.3323522,
author = {Guo, Xianwei and Huang, Fangwan and Yang, Dingqi and Tu, Chunyu and Yu, Zhiyong and Guo, Wenzhong},
title = {Spatiotemporal Fracture Data Inference in Sparse Mobile Crowdsensing: A Graph- and Attention-Based Approach},
year = {2023},
issue_date = {April 2024},
publisher = {IEEE Press},
volume = {32},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3323522},
doi = {10.1109/TNET.2023.3323522},
abstract = {Mobile Crowdsensing (MCS) is a sensing paradigm that enables large-scale smart city applications, such as environmental sensing and traffic monitoring. However, traditional MCS often suffers from performance degradation due to the limited spatiotemporal coverage of collected data. In this context, Sparse MCS has been proposed, which utilizes data inference algorithms to recover full data from sparse data collected by users. However, existing Sparse MCS approaches often overlook spatiotemporal fractures, where no data is observed either for a sensing subarea across all sensing time slots (temporal fracture), or for a sensing time slot in all sensing subarea (spatial fracture). Such spatiotemporal fractures pose great challenges to the data inference algorithms, as it is difficult to capture the complex spatiotemporal correlations of the sensing data from very limited observations. To address this issue, we propose a Graph- and Attention-based Matrix Completion (GAMC) method for the spatiotemporal fracture data inference problem in Sparse MCS. Specifically, we first pre-fill the general missing values using the classical Matrix Factorization (MF) technique. Then, we propose a neural network architecture based on Graph Attention Networks (GAT) and Transformer to capture complex spatiotemporal dependencies in the sensing data. Finally, we recover the complete data with a projection layer. We conduct extensive experiments on three real-world urban sensing datasets. The experimental results show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Netw.},
month = oct,
pages = {1631–1644},
numpages = {14}
}

@article{10.1109/TNET.2024.3361324,
author = {Chen, Xiang and Liu, Hongyan and Xiao, Qingjiang and Huang, Qun and Zhang, Dong and Zhou, Haifeng and Zhou, Boyang and Wu, Chunming and Liu, Xuan and Yang, Qiang},
title = {Hermes: Low-Overhead Inter-Switch Coordination in Network-Wide Data Plane Program Deployment},
year = {2024},
issue_date = {Aug. 2024},
publisher = {IEEE Press},
volume = {32},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3361324},
doi = {10.1109/TNET.2024.3361324},
abstract = {Network administrators usually realize network functions in data plane programs. They employ the network-wide program deployment that decomposes input programs into match-action tables (MATs) while deploying each MAT on a specific switch. Since MATs may be deployed on different switches, existing solutions propose the inter-switch coordination that uses the per-packet header space to deliver crucial packet processing information among switches. However, such coordination incurs non-trivial per-packet byte overhead, leading to end-to-end performance degradation. We propose Hermes, a framework that aims to minimize the per-packet byte overhead. The key idea is to formulate network-wide program deployment as a mixed-integer programming (MIP) problem with the objective of minimizing the per-packet byte overhead. Also, Hermes offers a greedy-based heuristic that solves the problem in a near-optimal and timely manner. We have implemented Hermes on Tofino switches. Compared to existing frameworks, Hermes decreases the per-packet byte overhead by 156bytes while preserving end-to-end performance in terms of flow completion time and goodput.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {2842–2857},
numpages = {16}
}

@article{10.1109/TNET.2024.3365815,
author = {Nguyen, Chi-Hieu and Saputra, Yuris Mulya and Hoang, Dinh Thai and Nguyen, Diep N. and Nguyen, Van-Dinh and Xiao, Yong and Dutkiewicz, Eryk},
title = {Encrypted Data Caching and Learning Framework for Robust Federated Learning-Based Mobile Edge Computing},
year = {2024},
issue_date = {June 2024},
publisher = {IEEE Press},
volume = {32},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3365815},
doi = {10.1109/TNET.2024.3365815},
abstract = {Federated Learning (FL) plays a pivotal role in enabling artificial intelligence (AI)-based mobile applications in mobile edge computing (MEC). However, due to the resource heterogeneity among participating mobile users (MUs), delayed updates from slow MUs may deteriorate the learning speed of the MEC-based FL system, commonly referred to as the straggling problem. To tackle the problem, this work proposes a novel privacy-preserving FL framework that utilizes homomorphic encryption (HE) based solutions to enable MUs, particularly resource-constrained MUs, to securely offload part of their training tasks to the cloud server (CS) and mobile edge nodes (MENs). Our framework first develops an efficient method for packing batches of training data into HE ciphertexts to reduce the complexity of HE-encrypted training at the MENs/CS. On that basis, the mobile service provider (MSP) can incentivize straggling MUs to encrypt part of their local datasets that are uploaded to certain MENs or the CS for caching and remote training. However, caching a large amount of encrypted data at the MENs and CS for FL may not only overburden those nodes but also incur a prohibitive cost of remote training, which ultimately reduces the MSP’s overall profit. To optimize the portion of MUs’ data to be encrypted, cached, and trained at the MENs/CS, we formulate an MSP’s profit maximization problem, considering all MUs’ and MENs’ resource capabilities and data handling costs (including encryption, caching, and training) as well as the MSP’s incentive budget. We then show that the problem is convex and can be efficiently solved using an interior point method. Extensive simulations on a real-world human activity recognition dataset show that our proposed framework can achieve much higher model accuracy (improving up to 24.29%) and faster convergence rate (by 2.86 times) than those of the conventional FedAvg approach when the straggling probability varies between 20% and 80%. Moreover, the proposed framework can improve the MSP’s profit up to 2.84 times compared with other baseline FL approaches without MEN-assisted training.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {2705–2720},
numpages = {16}
}

@article{10.1109/TNET.2024.3400953,
author = {Wang, Ziliang and Zhu, Shiyi and Li, Jianguo and Jiang, Wei and Ramakrishnan, K. K. and Yan, Meng and Zhang, Xiaohong and Liu, Alex X.},
title = {DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3400953},
doi = {10.1109/TNET.2024.3400953},
abstract = {Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.},
journal = {IEEE/ACM Trans. Netw.},
month = may,
pages = {3961–3976},
numpages = {16}
}

@article{10.1109/TNET.2024.3411021,
author = {Liu, Xiaoqing and Fan, Jianxi and Cheng, Baolei and Wang, Yan and Yin, Bai and Jia, Xiaohua},
title = {A Family of General Architectures Toward Interconnection Networks and Data Center Networks},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3411021},
doi = {10.1109/TNET.2024.3411021},
abstract = {Networks of large scales are an essential component in supercomputing systems as well as in data centers. As the network scale increases, the probability of processor/server failures also inevitably increases. It is therefore a worthwhile undertaking to make efforts reducing, as much as possible, the effect of faulty processors/servers to the entire network. This paper introduces a new class of network architectures, called circulant-based recursive networks (CRNs), and investigates CRN’s diameter, connectivity, and in particular, the fault diagnosability under the two diagnostic models−the PMC and the comparison diagnostic models. CRNs are a generalization of some well-known interconnection networks−hypercube, k-ary n-cube network and the data center network BCube, as well as some other less-known networks. In addition to obtaining its diagnosability properties, the paper also presents a one-to-one (unicast) path construction algorithm named SPath. Based on SPath, we further propose an algorithm FTPath for CRNs finding a fault-tolerant path between any two vertices, provided that the number of faulty vertices is no more than its connectivity minus one. Three parameters−average distance, message density, and cost−are used to assess CRNs’ performance. Experimental comparisons are conducted, and the results indicate that the average path length obtained by the algorithm SPath (resp., FTPath) is shorter than that of the Depth-First Search algorithm (DFS) and is on a par with the Breath-First Search algorithm (BFS).},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {4099–4113},
numpages = {15}
}

@article{10.1145/2043652.2043655,
author = {Kim, Changkyu and Chhugani, Jatin and Satish, Nadathur and Sedlar, Eric and Nguyen, Anthony D. and Kaldewey, Tim and Lee, Victor W. and Brandt, Scott A. and Dubey, Pradeep},
title = {Designing fast architecture-sensitive tree search on modern multicore/many-core processors},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/2043652.2043655},
doi = {10.1145/2043652.2043655},
abstract = {In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join, and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this article, we present FAST, an extremely fast architecture-sensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and Single Instruction Multiple Data (SIMD) width of the underlying hardware. FAST eliminates the impact of memory latency, and exploits thread-level and data-level parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second for large trees of 64M elements, with even better results on smaller trees. These are 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. We also evaluated FAST on the Intel$^tinytextregistered$ Many Integrated Core architecture (Intel$^tinytextregistered$ MIC), showing a speedup of 2.4X--3X over CPU and 1.8X--4.4X over GPU. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64M keys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {22},
numpages = {34},
keywords = {CPU, GPU, Tree search, compression, many-core, multicore, single instruction multiple data (SIMD)}
}

@article{10.1145/2566666,
author = {Biswas, Sounil and Wang, Hongfei and Blanton, R. D. (Shawn)},
title = {Reducing test cost of integrated, heterogeneous systems using pass-fail test data analysis},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/2566666},
doi = {10.1145/2566666},
abstract = {Stringent quality requirements for integrated, heterogeneous systems have led designers and test engineers to mandate large sets of tests to be applied to these systems, which, in turn, have resulted in increased test cost. However, many of these tests are unnecessary (i.e., redundant), since their outcomes can be reliably predicted using results from other applied tests. A methodology for identifying the redundant tests of an integrated, heterogeneous system that has only binary pass-fail test data is described. This methodology uses decision trees, Boolean minimization, and satisfiability as core components. Feasibility is empirically demonstrated using test data from two commercially fabricated systems, namely, a high-speed serializer/deserializer (HSS) and a phase-locked loop (PLL). Our analysis of test data from &gt; 38,000 HSS and &gt; 22,000 PLL circuits show that 14 out of 40 HSS tests and 11 out of 36 PLL tests are redundant.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {20},
numpages = {23},
keywords = {Integrated system test, minimum consistent subset covering, positive unate product test, statistical learning, test compaction}
}

@article{10.1145/2897824.2925961,
author = {Chai, Menglei and Shao, Tianjia and Wu, Hongzhi and Weng, Yanlin and Zhou, Kun},
title = {AutoHair: fully automatic hair modeling from a single image},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925961},
doi = {10.1145/2897824.2925961},
abstract = {We introduce AutoHair, the first fully automatic method for 3D hair modeling from a single portrait image, with no user interaction or parameter tuning. Our method efficiently generates complete and high-quality hair geometries, which are comparable to those generated by the state-of-the-art methods, where user interaction is required. The core components of our method are: a novel hierarchical deep neural network for automatic hair segmentation and hair growth direction estimation, trained over an annotated hair image database; and an efficient and automatic data-driven hair matching and modeling algorithm, based on a large set of 3D hair exemplars. We demonstrate the efficacy and robustness of our method on Internet photos, resulting in a database of around 50K 3D hair models and a corresponding hairstyle space that covers a wide variety of real-world hairstyles. We also show novel applications enabled by our method, including 3D hairstyle space navigation and hair-aware image retrieval.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {116},
numpages = {12},
keywords = {data-driven modeling, deep neural network, hair modeling, image segmentation}
}

@article{10.1145/2994148,
author = {Zhang, Yunquan and Li, Shigang and Yan, Shengen and Zhou, Huiyang},
title = {A Cross-Platform SpMV Framework on Many-Core Architectures},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2994148},
doi = {10.1145/2994148},
abstract = {Sparse Matrix-Vector multiplication (SpMV) is a key operation in engineering and scientific computing. Although the previous work has shown impressive progress in optimizing SpMV on many-core architectures, load imbalance and high memory bandwidth remain the critical performance bottlenecks. We present our novel solutions to these problems, for both GPUs and Intel MIC many-core architectures. First, we devise a new SpMV format, called Blocked Compressed Common Coordinate (BCCOO). BCCOO extends the blocked Common Coordinate (COO) by using bit flags to store the row indices to alleviate the bandwidth problem. We further improve this format by partitioning the matrix into vertical slices for better data locality. Then, to address the load imbalance problem, we propose a highly efficient matrix-based segmented sum/scan algorithm for SpMV, which eliminates global synchronization. At last, we introduce an autotuning framework to choose optimization parameters. Experimental results show that our proposed framework has a significant advantage over the existing SpMV libraries. In single precision, our proposed scheme outperforms clSpMV COCKTAIL format by 255\% on average on AMD FirePro W8000, and outperforms CUSPARSE V7.0 by 73.7\% on average and outperforms CSR5 by 53.6\% on average on GeForce Titan X; in double precision, our proposed scheme outperforms CUSPARSE V7.0 by 34.0\% on average and outperforms CSR5 by 16.2\% on average on Tesla K20, and has equivalent performance compared with CSR5 on Intel MIC.},
journal = {ACM Trans. Archit. Code Optim.},
month = oct,
articleno = {33},
numpages = {25},
keywords = {BCCOO, CUDA, GPU, Intel MIC, OpenCL, SpMV, parallel algorithms, segmented scan}
}

@article{10.1145/2994550,
author = {Mittal, Sparsh},
title = {A Survey of Techniques for Architecting Processor Components Using Domain-Wall Memory},
year = {2016},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1550-4832},
url = {https://doi.org/10.1145/2994550},
doi = {10.1145/2994550},
abstract = {Recent trends of increasing core-count and bandwidth/memory wall have motivated researchers to explore novel memory technologies for designing processor components such as cache, register file, shared memory, and so on. Domain-wall memory (DWM), also known as racetrack memory, is a promising emerging technology due to its non-volatility and very high density. However, use of DWM presents challenges due to characteristics of both DWM itself (e.g., requirement of shift operations, variable latency) and processor components. Recently, several techniques have been proposed to address these challenges. This article presents a survey of architectural techniques for using DWM for designing components in both CPU and GPU. We discuss techniques related to performance, energy, and reliability and also discuss works that compare DWM with other memory technologies. We also highlight the opportunities and obstacles in using DWM for designing processor components. This survey is expected to spark further research in this area and be useful for researchers, chip designers, and computer architects.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = nov,
articleno = {29},
numpages = {25},
keywords = {CPU, GPU, Review, cache, domain-wall memory, energy, performance, reliability, shift operations}
}

@article{10.1145/3012289,
author = {Dragoni, Mauro and Tonelli, Sara and Moretti, Giovanni},
title = {A Knowledge Management Architecture for Digital Cultural Heritage},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3012289},
doi = {10.1145/3012289},
abstract = {The increasing demand of technological facilities for galleries, museums, and archives has led to the need for designing practical and effective solutions for managing the digital life cycle of cultural heritage collections. These facilities have to support users in addressing several challenges directly related to the creation, management, preservation, and visualization of digital collections. Such challenges include, for example, the support for a collaborative management of the produced information, their curation from a multilingual perspective to break the language barriers and make collections available to different stakeholders, and the development of services for exposing structured version of data both to users and machines. Platforms satisfying all of these requirements have to support curators activities and, at the same time, provide facilities for engaging the virtual consumers of the produced data. In this article, we propose a description of an abstract architecture for managing digital collections built on a set of components, services, and APIs able to address the challenges mentioned previously. An instantiation of this architecture is discussed, and we present a use case concerning the management of a digital archive of verbo-visual art. Lessons learned from this experience are reported to outline future activities.},
journal = {J. Comput. Cult. Herit.},
month = jul,
articleno = {15},
numpages = {18},
keywords = {Knowledge management, curation, digital heritage, services, visualization}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@article{10.1145/3050437,
author = {Pathania, Anuj and Venkataramani, Vanchinathan and Shafique, Muhammad and Mitra, Tulika and Henkel, J\"{o}rg},
title = {Defragmentation of Tasks in Many-Core Architecture},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3050437},
doi = {10.1145/3050437},
abstract = {Many-cores can execute multiple multithreaded tasks in parallel. A task performs most efficiently when it is executed over a spatially connected and compact subset of cores so that performance loss due to communication overhead imposed by the task’s threads spread across the allocated cores is minimal. Over a span of time, unallocated cores can get scattered all over the many-core, creating fragments in the task mapping. These fragments can prevent efficient contiguous mapping of incoming new tasks leading to loss of performance. This problem can be alleviated by using a task defragmenter, which consolidates smaller fragments into larger fragments wherein the incoming tasks can be efficiently executed. Optimal defragmentation of a many-core is an NP-hard problem in the general case. Therefore, we simplify the original problem to a problem that can be solved optimally in polynomial time. In this work, we introduce a concept of exponentially separable mapping (ESM), which defines a set of task mapping constraints on a many-core. We prove that an ESM enforcing many-core can be defragmented optimally in polynomial time.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {2},
numpages = {21},
keywords = {Many-core, multiagent systems, task defragmentation}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Software product lines, software engineering teaching, software product line teaching, variability modeling}
}

@article{10.1145/3126496,
author = {Rouxel, Benjamin and Derrien, Steven and Puaut, Isabelle},
title = {Tightening Contention Delays While Scheduling Parallel Applications on Multi-core Architectures},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3126496},
doi = {10.1145/3126496},
abstract = {Multi-core systems are increasingly interesting candidates for executing parallel real-time applications, in avionic, space or automotive industries, as they provide both computing capabilities and power efficiency. However, ensuring that timing constraints are met on such platforms is challenging, because some hardware resources are shared between cores.Assuming worst-case contentions when analyzing the schedulability of applications may result in systems mistakenly declared unschedulable, although the worst-case level of contentions can never occur in practice. In this paper, we present two contention-aware scheduling strategies that produce a time-triggered schedule of the application’s tasks. Based on knowledge of the application’s structure, our scheduling strategies precisely estimate the effective contentions, in order to minimize the overall makespan of the schedule. An Integer Linear Programming (ILP) solution of the scheduling problem is presented, as well as a heuristic solution that generates schedules very close to ones of the ILP (5\% longer on average), with a much lower time complexity. Our heuristic improves by 19\% the overall makespan of the resulting schedules compared to a worst-case contention baseline.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {164},
numpages = {20},
keywords = {Real-time system, contention-aware scheduling}
}

@article{10.1145/3126501,
author = {Azimi, Iman and Anzanpour, Arman and Rahmani, Amir M. and Pahikkala, Tapio and Levorato, Marco and Liljeberg, Pasi and Dutt, Nikil},
title = {HiCH: Hierarchical Fog-Assisted Computing Architecture for Healthcare IoT},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3126501},
doi = {10.1145/3126501},
abstract = {The Internet of Things (IoT) paradigm holds significant promises for remote health monitoring systems. Due to their life- or mission-critical nature, these systems need to provide a high level of availability and accuracy. On the one hand, centralized cloud-based IoT systems lack reliability, punctuality and availability (e.g., in case of slow or unreliable Internet connection), and on the other hand, fully outsourcing data analytics to the edge of the network can result in diminished level of accuracy and adaptability due to the limited computational capacity in edge nodes. In this paper, we tackle these issues by proposing a hierarchical computing architecture, HiCH, for IoT-based health monitoring systems. The core components of the proposed system are 1) a novel computing architecture suitable for hierarchical partitioning and execution of machine learning based data analytics, 2) a closed-loop management technique capable of autonomous system adjustments with respect to patient’s condition. HiCH benefits from the features offered by both fog and cloud computing and introduces a tailored management methodology for healthcare IoT systems. We demonstrate the efficacy of HiCH via a comprehensive performance assessment and evaluation on a continuous remote health monitoring case study focusing on arrhythmia detection for patients suffering from CardioVascular Diseases (CVDs).},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {174},
numpages = {20},
keywords = {Fog Computing, Hierarchical Computing, Internet of Things, MAPE-K, Machine Learning, Remote Patient Monitoring}
}

@article{10.1145/3127069,
author = {Vermij, Erik and Fiorin, Leandro and Jongerius, Rik and Hagleitner, Christoph and Lunteren, Jan Van and Bertels, Koen},
title = {An Architecture for Integrated Near-Data Processors},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3127069},
doi = {10.1145/3127069},
abstract = {To increase the performance of data-intensive applications, we present an extension to a CPU architecture that enables arbitrary near-data processing capabilities close to the main memory. This is realized by introducing a component attached to the CPU system-bus and a component at the memory side. Together they support hardware-managed coherence and virtual memory support to integrate the near-data processors in a shared-memory environment. We present an implementation of the components, as well as a system-simulator, providing detailed performance estimations. With a variety of synthetic workloads we demonstrate the performance of the memory accesses, the mixed fine- and coarse-grained coherence mechanisms, and the near-data processor communication mechanism. Furthermore, we quantify the inevitable start-up penalty regarding coherence and data writeback, and argue that near-data processing workloads should access data several times to offset this penalty. A case study based on the Graph500 benchmark confirms the small overhead for the proposed coherence mechanisms and shows the ability to outperform a real CPU by a factor of two.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {30},
numpages = {25},
keywords = {Computer architecture, coherence, data locality, graph500, near-data processing, virtual memory}
}

@article{10.1145/3155335,
author = {Marin, Andrea and Rossi, Sabina and Burato, Dario and Sina, Andrea and Sottana, Matteo},
title = {A Product-Form Model for the Performance Evaluation of a Bandwidth Allocation Strategy in WSNs},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1049-3301},
url = {https://doi.org/10.1145/3155335},
doi = {10.1145/3155335},
abstract = {Wireless Sensor Networks (WSNs) are important examples of Collective Adaptive System, which consist of a set of motes that are spatially distributed in an indoor or outdoor space. Each mote monitors its surrounding conditions, such as humidity, intensity of light, temperature, and vibrations, but also collects complex information, such as images or small videos, and cooperates with the whole set of motes forming the WSN to allow the routing process. The traffic in the WSN consists of packets that contain the data harvested by the motes and can be classified according to the type of information that they carry. One pivotal problem in WSNs is the bandwidth allocation among the motes. The problem is known to be challenging due to the reduced computational capacity of the motes, their energy consumption constraints, and the fully decentralised network architecture. In this article, we study a novel algorithm to allocate the WSN bandwidth among the motes by taking into account the type of traffic they aim to send. Under the assumption of a mesh network and Poisson distributed harvested packets, we propose an analytical model for its performance evaluation that allows a designer to study the optimal configuration parameters. Although the Markov chain underlying the model is not reversible, we show it to be ρ-reversible under a certain renaming of states. By an extensive set of simulations, we show that the analytical model accurately approximates the performance of networks that do not satisfy the assumptions. The algorithm is studied with respect to the achieved throughput and fairness. We show that it provides a good approximation of the max-min fairness requirements.},
journal = {ACM Trans. Model. Comput. Simul.},
month = feb,
articleno = {13},
numpages = {23},
keywords = {Markov models, Wireless sensor networks, bandwidth allocation, product-forms}
}

@article{10.1145/3209882,
author = {Fan, Mingming and Truong, Khai N.},
title = {Guidelines for Creating Senior-Friendly Product Instructions},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3209882},
doi = {10.1145/3209882},
abstract = {Although older adults feel generally positive about technologies, many face difficulties when using them and need support during the process. One common form of support is the product instructions that come with devices. Unfortunately, when using them, older adults often feel confused, overwhelmed, or frustrated. In this work, we sought to address the issues that affect older adults’ ability to successfully complete tasks using product instructions. By observing how older adults used the product instructions of various devices and how they made modifications to simplify the use of the instructions, we identified 11 guidelines for creating senior-friendly product instructions. We validated the usability and effectiveness of the guidelines by evaluating how older adults used instruction manuals that were modified to adhere to these guidelines against the originals and those that were modified by interaction design researchers. Results show that, overall, participants had the highest task success rate and lowest task completion time when using guideline-modified user instructions. Participants also perceived these instructions to be the most helpful, the easiest to follow, the most complete, and the most concise among the three. We also compared the guidelines derived from this research to existing documentation guidelines and discussed potential challenges of applying them.},
journal = {ACM Trans. Access. Comput.},
month = jun,
articleno = {9},
numpages = {35},
keywords = {Guidelines, instruction design, older adults, product instructions, senior-friendly, seniors, technology support, user manuals, user-centered design, user-friendly}
}

@article{10.1145/3218823,
author = {Tan, Guangming and Liu, Junhong and Li, Jiajia},
title = {Design and Implementation of Adaptive SpMV Library for Multicore and Many-Core Architecture},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3218823},
doi = {10.1145/3218823},
abstract = {Sparse matrix vector multiplication (SpMV) is an important computational kernel in traditional high-performance computing and emerging data-intensive applications. Previous SpMV libraries are optimized by either application-specific or architecture-specific approaches but present difficulties for use in real applications. In this work, we develop an auto-tuning system (SMATER) to bridge the gap between specific optimizations and general-purpose use. SMATER provides programmers a unified interface based on the compressed sparse row (CSR) sparse matrix format by implicitly choosing the best format and fastest implementation for any input sparse matrix during runtime. SMATER leverages a machine-learning model and retargetable back-end library to quickly predict the optimal combination. Performance parameters are extracted from 2,386 matrices in the SuiteSparse matrix collection. The experiments show that SMATER achieves good performance (up to 10 times that of the Intel Math Kernel Library (MKL) on Intel E5-2680 v3) while being portable on state-of-the-art x86 multicore processors, NVIDIA GPUs, and Intel Xeon Phi accelerators. Compared with the Intel MKL library, SMATER runs faster by more than 2.5 times on average. We further demonstrate its adaptivity in an algebraic multigrid solver from the Hypre library and report greater than 20\% performance improvement.},
journal = {ACM Trans. Math. Softw.},
month = aug,
articleno = {46},
numpages = {25},
keywords = {Sparse matrix vector multiplication, auto-tuning, machine learning, multicore}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance}
}

@article{10.1145/3229049,
author = {Malik, Maria and Rafatirad, Setareh and Homayoun, Houman},
title = {System and Architecture Level Characterization of Big Data Applications on Big and Little Core Server Architectures},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2376-3639},
url = {https://doi.org/10.1145/3229049},
doi = {10.1145/3229049},
abstract = {The rapid growth in data yields challenges to process data efficiently using current high-performance server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers.&nbsp;Low-power embedded cores in servers such as little Atom&nbsp;have emerged as a promising solution to enhance energy-efficiency&nbsp;to address these challenges.&nbsp;Therefore, the question of whether to&nbsp;process the big data&nbsp;applications on big Xeon- or Little Atom-based servers becomes important.&nbsp;In this work, through methodical investigation of power and performance measurements, and comprehensive application-level, system-level, and micro-architectural level analysis, we characterize dominant big data applications on big Xeon- and little Atom-based server architectures. The characterization results across a wide range of real-world big data applications, and various software stacks demonstrate how the choice of big- versus little-core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator.&nbsp;In addition, we analyze processor resource utilization of this important class of applications,&nbsp;such as&nbsp;memory footprints, CPU&nbsp;utilization, and disk bandwidth,&nbsp;to understand their run-time behavior.&nbsp;Furthermore, we perform micro-architecture-level analysis to highlight where improvement is needed in big- and little-core microarchitectures to address their performance bottlenecks.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = jul,
articleno = {14},
numpages = {32},
keywords = {Performance, accelerator, big data, characterization, high-performance server, low-power server, power}
}

@article{10.1145/3231742,
author = {Chen, Zhineng and Ai, Shanshan and Jia, Caiyan},
title = {Structure-Aware Deep Learning for Product Image Classification},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3231742},
doi = {10.1145/3231742},
abstract = {Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {4},
numpages = {20},
keywords = {Image classification, category hierarchy, convolutional neural network, multi-class regression, multi-task learning}
}

@article{10.1145/3233770,
author = {Qu, Yanru and Fang, Bohui and Zhang, Weinan and Tang, Ruiming and Niu, Minzhe and Guo, Huifeng and Yu, Yong and He, Xiuqiang},
title = {Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3233770},
doi = {10.1145/3233770},
abstract = {User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this article, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then, we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network, which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network in Network (PIN), which can generalize previous models. Extensive experiments on four industrial datasets and one contest dataset demonstrate that our models consistently outperform eight baselines on both area under curve and log loss. Besides, PIN makes great click-through rate improvement (relatively 34.67\%) in online A/B test.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {5},
numpages = {35},
keywords = {Deep learning, product-based neural network, recommender system}
}

@article{10.1145/3237187,
author = {Brod\'{e}n, Bj\"{o}rn and Hammar, Mikael and Nilsson, Bengt J. and Paraschakis, Dimitris},
title = {A Bandit-Based Ensemble Framework for Exploration/Exploitation of Diverse Recommendation Components: An Experimental Study within E-Commerce},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3237187},
doi = {10.1145/3237187},
abstract = {This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. In addition, we detail the construction of a personalized predictor based on k-Nearest Neighbors (kNN), with temporal decay capabilities and event weighting. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {4},
numpages = {32},
keywords = {E-commerce recommender systems, Thompson Sampling, multi-arm bandit ensembles, reinforcement learning, session-based recommendations, streaming recommendations}
}

@article{10.1145/3243904,
author = {Jin, Hai and Liu, Bo and Jiang, Wenbin and Ma, Yang and Shi, Xuanhua and He, Bingsheng and Zhao, Shaofeng},
title = {Layer-Centric Memory Reuse and Data Migration for Extreme-Scale Deep Learning on Many-Core Architectures},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3243904},
doi = {10.1145/3243904},
abstract = {Due to the popularity of Deep Neural Network (DNN) models, we have witnessed extreme-scale DNN models with the continued increase of the scale in terms of depth and width. However, the extremely high memory requirements for them make it difficult to run the training processes on single many-core architectures such as a Graphic Processing Unit (GPU), which compels researchers to use model parallelism over multiple GPUs to make it work. However, model parallelism always brings very heavy additional overhead. Therefore, running an extreme-scale model in a single GPU is urgently required. There still exist several challenges to reduce the memory footprint for extreme-scale deep learning. To address this tough problem, we first identify the memory usage characteristics for deep and wide convolutional networks, and demonstrate the opportunities for memory reuse at both the intra-layer and inter-layer levels. We then present Layrub, a runtime data placement strategy that orchestrates the execution of the training process. It achieves layer-centric reuse to reduce memory consumption for extreme-scale deep learning that could not previously be run on a single GPU. Experiments show that, compared to the original Caffe, Layrub can cut down the memory usage rate by an average of 58.2\% and by up to 98.9\%, at the moderate cost of 24.1\% higher training execution time on average. Results also show that Layrub outperforms some popular deep learning systems such as GeePS, vDNN, MXNet, and Tensorflow. More importantly, Layrub can tackle extreme-scale deep learning tasks. For example, it makes an extra-deep ResNet with 1,517 layers that can be trained successfully in one GPU with 12GB memory, while other existing deep learning systems cannot.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {37},
numpages = {26},
keywords = {DNN, Data placement, GPU, memory efficiency}
}

@article{10.1145/3272127.3275009,
author = {Wang, Xiaogang and Zhou, Bin and Fang, Haiyue and Chen, Xiaowu and Zhao, Qinping and Xu, Kai},
title = {Learning to group and label fine-grained shape components},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3272127.3275009},
doi = {10.1145/3272127.3275009},
abstract = {A majority of stock 3D models in modern shape repositories are assembled with many fine-grained components. The main cause of such data form is the component-wise modeling process widely practiced by human modelers. These modeling components thus inherently reflect some function-based shape decomposition the artist had in mind during modeling. On the other hand, modeling components represent an over-segmentation since a functional part is usually modeled as a multi-component assembly. Based on these observations, we advocate that labeled segmentation of stock 3D models should not overlook the modeling components and propose a learning solution to grouping and labeling of the fine-grained components. However, directly characterizing the shape of individual components for the purpose of labeling is unreliable, since they can be arbitrarily tiny and semantically meaningless. We propose to generate part hypotheses from the components based on a hierarchical grouping strategy, and perform labeling on those part groups instead of directly on the components. Part hypotheses are mid-level elements which are more probable to carry semantic information. A multi-scale 3D convolutional neural network is trained to extract context-aware features for the hypotheses. To accomplish a labeled segmentation of the whole shape, we formulate higher-order conditional random fields (CRFs) to infer an optimal label assignment for all components. Extensive experiments demonstrate that our method achieves significantly robust labeling results on raw 3D models from public shape repositories. Our work also contributes the first benchmark for component-wise labeling.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {210},
numpages = {14},
keywords = {data-driven shape analysis, fine-grained components, part hypotheses, semantic labeling, shape segmentation}
}

@article{10.1145/3272127.3275024,
author = {Hu, Ruizhen and Wen, Cheng and Van Kaick, Oliver and Chen, Luanmin and Lin, Di and Cohen-Or, Daniel and Huang, Hui},
title = {Semantic object reconstruction via casual handheld scanning},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3272127.3275024},
doi = {10.1145/3272127.3275024},
abstract = {We introduce a learning-based method to reconstruct objects acquired in a casual handheld scanning setting with a depth camera. Our method is based on two core components. First, a deep network that provides a semantic segmentation and labeling of the frames of an input RGBD sequence. Second, an alignment and reconstruction method that employs the semantic labeling to reconstruct the acquired object from the frames. We demonstrate that the use of a semantic labeling improves the reconstructions of the objects, when compared to methods that use only the depth information of the frames. Moreover, since training a deep network requires a large amount of labeled data, a key contribution of our work is an active self-learning framework to simplify the creation of the training data. Specifically, we iteratively predict the labeling of frames with the neural network, reconstruct the object from the labeled frames, and evaluate the confidence of the labeling, to incrementally train the neural network while requiring only a small amount of user-provided annotations. We show that this method enables the creation of data for training a neural network with high accuracy, while requiring only little manual effort.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {219},
numpages = {12},
keywords = {3D scanning, active learning, object registration, semantic reconstruction}
}

@article{10.1145/3287306,
author = {Patil, Rajendra and Modi, Chirag},
title = {An Exhaustive Survey on Security Concerns and Solutions at Different Components of Virtualization},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3287306},
doi = {10.1145/3287306},
abstract = {Virtualization is a key enabler of various modern computing technologies. However, it brings additional vulnerabilities that can be exploited to affect the availability, integrity, and confidentiality of the underlying resources and services. The dynamic and shared nature of the virtualization poses additional challenges to the traditional security solutions. This article explores the vulnerabilities, threats, and attacks relevant to virtualization. We analyze the existing security solutions and identify the research gaps that can help the research community to develop a secured virtualization platform for current and future computing technologies.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {12},
numpages = {38},
keywords = {Virtualization, hypervisor, security, virtual machine, vulnerability}
}

@article{10.1145/3295822,
author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
title = {Attentive Long Short-Term Preference Modeling for Personalized Product Search},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295822},
doi = {10.1145/3295822},
abstract = {E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users’ inherent purchasing bias and evolves slowly. By contrast, the latter reflects users’ purchasing inclination in a relatively short period. They both affect users’ current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users’ current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {27},
keywords = {Personalized product search, attention mechanism, long short-term preference}
}

@article{10.1145/3300208,
author = {Dogan, Halit and Ahmad, Masab and Kahne, Brian and Khan, Omer},
title = {Accelerating Synchronization Using Moving Compute to Data Model at 1,000-core Multicore Scale},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3300208},
doi = {10.1145/3300208},
abstract = {Thread synchronization using shared memory hardware cache coherence paradigm is prevalent in multicore processors. However, as the number of cores increase on a chip, cache line ping-pong prevents performance scaling for algorithms that deploy fine-grain synchronization. This article proposes an in-hardware moving computation to data model (MC) that pins shared data at dedicated cores. The critical code sections are serialized and executed at these cores in a spatial setting to enable data locality optimizations. In-hardware messages enable non-blocking and blocking communication between cores, without involving the cache coherence protocol. The in-hardware MC model is implemented on Tilera Tile-Gx72 multicore platform to evaluate 8- to 64-core count scale. A simulated RISC-V multicore environment is built to further evaluate the performance scaling advantages of the MC model at 1,024-cores scale. The evaluation using graph and machine-learning benchmarks illustrates that atomic instructions based synchronization scales up to 512 cores, and the MC model at the same core count outperforms by 27\% in completion time and 39\% in dynamic energy consumption.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {4},
numpages = {27},
keywords = {Multicore, locality, synchronization}
}

@article{10.1145/3301279,
author = {Guha, Krishnendu and Saha, Debasri and Chakrabarti, Amlan},
title = {Stigmergy-Based Security for SoC Operations From Runtime Performance Degradation of SoC Components},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3301279},
doi = {10.1145/3301279},
abstract = {The semiconductor design industry of the embedded era has embraced the globalization strategy for system on chip (SoC) design. This involves incorporation of various SoC components or intellectual properties (IPs), procured from various third-party IP (3PIP) vendors. However, trust of an SoC is challenged when a supplied IP is counterfeit or implanted with a Hardware Trojan Horse. Both roots of untrust may result in sudden performance degradation at runtime. None of the existing hardware security approaches organize the behavior of the IPs at the low level, to ensure timely completion of SoC operations. However, real-time SoC operations are always associated with a deadline, and a deadline miss due to sudden performance degradation of any of the IPs may jeopardize mission-critical applications. We seek refuge to the stigmergic behavior exhibited in insect colonies to propose a decentralized self-aware security approach. The self-aware security modules attached with each IP works based on the Observe-Decide-Act paradigm and not only detects vulnerability but also organizes behavior of the IPs dynamically at runtime so that the high-level objective of task completion before a deadline is ensured. Experimental validation and low overhead of our proposed security modules over various benchmark IPs and crypto SoCs depict the prospects of our proposed mechanism.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = mar,
articleno = {14},
numpages = {26},
keywords = {Hardware Trojan Horses (HTH), Runtime SoC security, stigmergy}
}

@article{10.1145/3301308,
author = {Venkataramani, Vanchinathan and Chan, Mun Choon and Mitra, Tulika},
title = {Scratchpad-Memory Management for Multi-Threaded Applications on Many-Core Architectures},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3301308},
doi = {10.1145/3301308},
abstract = {Contemporary many-core architectures, such as Adapteva Epiphany and Sunway TaihuLight, employ per-core software-controlled Scratchpad Memory (SPM) rather than caches for better performance-per-watt and predictability. In these architectures, a core is allowed to access its own SPM as well as remote SPMs through the Network-On-Chip (NoC). However, the compiler/programmer is required to explicitly manage the movement of data between SPMs and off-chip memory. Utilizing SPMs for multi-threaded applications is even more challenging, as the shared variables across the threads need to be placed appropriately. Accessing variables from remote SPMs with higher access latency further complicates this problem as certain links in the NoC may be heavily contended by multiple threads. Therefore, certain variables may need to be replicated in multiple SPMs to reduce the contention delay and/or the overall access time. We present Coordinated Data Management (CDM), a compile-time framework that automatically identifies shared/private variables and places them with replication (if necessary) to suitable on-chip or off-chip memory, taking NoC contention into consideration. We develop both an exact Integer Linear Programming (ILP) formulation as well as an iterative, scalable algorithm for placing the data variables in multi-threaded applications on many-core SPMs. Experimental evaluation on the Parallella hardware platform confirms that our allocation strategy reduces the overall execution time and energy consumption by 1.84\texttimes{} and 1.83\texttimes{}, respectively, when compared to the existing approaches.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = feb,
articleno = {10},
numpages = {28},
keywords = {Scratchpad memory management, many-core architectures}
}

@article{10.1145/3301426,
author = {Schn\"{a}delbach, Holger and J\"{a}ger, Nils and Urquhart, Lachlan},
title = {Adaptive Architecture and Personal Data},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3301426},
doi = {10.1145/3301426},
abstract = {Through sensors carried by people and sensors embedded in the environment, personal data is being processed to try to understand activity patterns and people's internal states in the context of human-building interaction. This data is used to actuate adaptive buildings to make them more comfortable, convenient, and accessible or information rich. In a series of envisioning workshops, we queried the future relationships between people, personal data and the built environment, when there are no technical limits to the availability of personal data to buildings. Our analysis of created designs and user experience fictions allows us to contribute a systematic exposition of the emerging design space for adaptive architecture that draws on personal data. This is being situated within the context of the new European information privacy legislation, the EU General Data Protection Regulation 2016. Drawing on the tension space analysis method, we conclude with the illustration of the tensions in the temporal, spatial, and inhabitation-related relationships of personal data and adaptive buildings, re-usable for the navigation of the emerging, complex issues by future designers.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = mar,
articleno = {12},
numpages = {31},
keywords = {Adaptive Architecture, GDPR, Personal data, Tension Space}
}

@article{10.1145/3305268,
author = {Islam, Chadni and Babar, Muhammad Ali and Nepal, Surya},
title = {A Multi-Vocal Review of Security Orchestration},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3305268},
doi = {10.1145/3305268},
abstract = {Organizations use diverse types of security solutions to prevent cyber-attacks. Multiple vendors provide security solutions developed using heterogeneous technologies and paradigms. Hence, it is a challenging rather impossible to easily make security solutions to work an integrated fashion. Security orchestration aims at smoothly integrating multivendor security tools that can effectively and efficiently interoperate to support security staff of a Security Operation Centre (SOC). Given the increasing role and importance of security orchestration, there has been an increasing amount of literature on different aspects of security orchestration solutions. However, there has been no effort to systematically review and analyze the reported solutions. We report a Multivocal Literature Review that has systematically selected and reviewed both academic and grey (blogs, web pages, white papers) literature on different aspects of security orchestration published from January 2007 until July 2017. The review has enabled us to provide a working definition of security orchestration and classify the main functionalities of security orchestration into three main areas—unification, orchestration, and automation. We have also identified the core components of a security orchestration platform and categorized the drivers of security orchestration based on technical and socio-technical aspects. We also provide a taxonomy of security orchestration based on the execution environment, automation strategy, deployment type, mode of task and resource type. This review has helped us to reveal several areas of further research and development in security orchestration.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {37},
numpages = {45},
keywords = {Security orchestration, intelligent security assistant, multivocal literature review, security automation}
}

@article{10.1145/3310332,
author = {Ham, Tae Jun and Arag\'{o}n, Juan L. and Martonosi, Margaret},
title = {Efficient Data Supply for Parallel Heterogeneous Architectures},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3310332},
doi = {10.1145/3310332},
abstract = {Decoupling techniques have been proposed to reduce the amount of memory latency exposed to high-performance accelerators as they fetch data. Although decoupled access-execute (DAE) and more recent decoupled data supply approaches offer promising single-threaded performance improvements, little work has considered how to extend them into parallel scenarios. This article explores the opportunities and challenges of designing parallel, high-performance, resource-efficient decoupled data supply systems. We propose Mercury, a parallel decoupled data supply system that utilizes thread-level parallelism for high-throughput data supply with good portability attributes. Additionally, we introduce some microarchitectural improvements for data supply units to efficiently handle long-latency indirect loads.},
journal = {ACM Trans. Archit. Code Optim.},
month = apr,
articleno = {9},
numpages = {23},
keywords = {Heterogeneous architecture, data access optimization, decoupled architecture}
}

@article{10.1145/3320277,
author = {Zhang, Mingyue and Wei, Xuan and Guo, Xunhua and Chen, Guoqing and Wei, Qiang},
title = {Identifying Complements and Substitutes of Products: A Neural Network Framework Based on Product Embedding},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320277},
doi = {10.1145/3320277},
abstract = {Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {29},
keywords = {Complements, online reviews, product embedding, product recommendation, product relationship, substitutes}
}

@article{10.1145/3323212,
author = {Maiza, Claire and Rihani, Hamza and Rivas, Juan M. and Goossens, Jo\"{e}l and Altmeyer, Sebastian and Davis, Robert I.},
title = {A Survey of Timing Verification Techniques for Multi-Core Real-Time Systems},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323212},
doi = {10.1145/3323212},
abstract = {This survey provides an overview of the scientific literature on timing verification techniques for multi-core real-time systems. It reviews the key results in the field from its origins around 2006 to the latest research published up to the end of 2018. The survey highlights the key issues involved in providing guarantees of timing correctness for multi-core systems. A detailed review is provided covering four main categories: full integration, temporal isolation, integrating interference effects into schedulability analysis, and mapping and allocation. The survey concludes with a discussion of the advantages and disadvantages of these different approaches, identifying open issues, key challenges, and possible directions for future research.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {56},
numpages = {38},
keywords = {Real-time systems, WCET, architecture, co-runner interference, multi-core, schedulability analysis, timing analysis}
}

@article{10.1145/3328755,
author = {Ruaro, Marcelo and Jantsch, Axel and Moraes, Fernando Gehm},
title = {Self-Adaptive QoS Management of Computation and Communication Resources in Many-Core SoCs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3328755},
doi = {10.1145/3328755},
abstract = {Providing quality of service (QoS) for many-core systems with dynamic application admission is challenging due to the high amount of resources to manage and the unpredictability of computation and communication events. Related works propose a self-adaptive QoS mechanism concerned either in communication or computation resources, lacking, however, a comprehensive QoS management of both. Assuming a many-core system with QoS monitoring, runtime circuit-switching establishment, task migration, and a soft real-time task scheduler, this work fills this gap by proposing a novel self-adaptive QoS management. The contribution of this proposal comes with the following features in the QoS management: (i) comprehensiveness, by covering communication and computation resources; (ii) online, adopting the ODA (Observe, Decide, Act) runtime closed-loop adaptation; and (iii) reactive and proactive decisions, by using a dynamic application profile extraction technique, which enables the QoS management to be aware of the profile of running applications, allowing it to take proactive decisions based on a prediction analysis. The proposed QoS management adopts a decentralized organization by partitioning the system in clusters, each one managed by a dedicated processor, making the proposal scalable. Results show that the proactive feature accurately extracts the applications’ profile, and can prevent future QoS violations. The synergy of reactive and proactive decisions was able to sustain QoS, reducing the deadline miss rate by 99.5\% with a severe disturbance in communication and computation levels, and avoiding deadline misses up to 70\% of system utilization.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jun,
articleno = {37},
numpages = {21},
keywords = {Quality of service, many-core, prediction, self-adaptation}
}

@article{10.1145/3331469,
author = {Chen, Yunji and Lan, Huiying and Du, Zidong and Liu, Shaoli and Tao, Jinhua and Han, Dong and Luo, Tao and Guo, Qi and Li, Ling and Xie, Yuan and Chen, Tianshi},
title = {An Instruction Set Architecture for Machine Learning},
year = {2019},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/3331469},
doi = {10.1145/3331469},
abstract = {Machine Learning (ML) are a family of models for learning from the data to improve performance on a certain task. ML techniques, especially recent renewed neural networks (deep neural networks), have proven to be efficient for a broad range of applications. ML techniques are conventionally executed on general-purpose processors (such as CPU and GPGPU), which usually are not energy efficient, since they invest excessive hardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators have been proposed recently to improve energy efficiency. However, such accelerators were designed for a small set of ML techniques sharing similar computational patterns, and they adopt complex and informative instructions (control signals) directly corresponding to high-level functional blocks of an ML technique (such as layers in neural networks) or even an ML as a whole. Although straightforward and easy to implement for a limited set of similar ML techniques, the lack of agility in the instruction set prevents such accelerator designs from supporting a variety of different ML techniques with sufficient flexibility and efficiency.In this article, we first propose a novel domain-specific Instruction Set Architecture (ISA) for NN accelerators, called Cambricon, which is a load-store architecture that integrates scalar, vector, matrix, logical, data transfer, and control instructions, based on a comprehensive analysis of existing NN techniques. We then extend the application scope of Cambricon from NN to ML techniques. We also propose an assembly language, an assembler, and runtime to support programming with Cambricon, especially targeting large-scale ML problems. Our evaluation over a total of 16 representative yet distinct ML techniques have demonstrated that Cambricon exhibits strong descriptive capacity over a broad range of ML techniques and provides higher code density than general-purpose ISAs such as x86, MIPS, and GPGPU. Compared to the latest state-of-the-art NN accelerator design DaDianNao&nbsp;[7] (which can only accommodate three types of NN techniques), our Cambricon-based accelerator prototype implemented in TSMC 65nm technology incurs only negligible latency/power/area overheads, with a versatile coverage of 10 different NN benchmarks and 7 other ML benchmarks. Compared to the recent prevalent ML accelerator PuDianNao, our Cambricon-based accelerator is able to support all the ML techniques as well as the 10 NNs but with only approximate 5.1\% performance loss.},
journal = {ACM Trans. Comput. Syst.},
month = aug,
articleno = {9},
numpages = {35},
keywords = {Instruction set architecture, machine learning, machine-learning accelerator}
}

@article{10.1145/3361738,
author = {Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Croft, W. Bruce},
title = {Explainable Product Search with a Dynamic Relation Embedding Model},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361738},
doi = {10.1145/3361738},
abstract = {Product search is one of the most popular methods for customers to discover products online. Most existing studies on product search focus on developing effective retrieval models that rank items by their likelihood to be purchased. However, they ignore the problem that there is a gap between how systems and customers perceive the relevance of items. Without explanations, users may not understand why product search engines retrieve certain items for them, which consequentially leads to imperfect user experience and suboptimal system performance in practice. In this work, we tackle this problem by constructing explainable retrieval models for product search. Specifically, we propose to model the “search and purchase” behavior as a dynamic relation between users and items, and create a dynamic knowledge graph based on both the multi-relational product data and the context of the search session. Ranking is conducted based on the relationship between users and items in the latent space, and explanations are generated with logic inferences and entity soft matching on the knowledge graph. Empirical experiments show that our model, which we refer to as the Dynamic Relation Embedding Model (DREM), significantly outperforms the state-of-the-art baselines and has the ability to produce reasonable explanations for search results.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {Product search, explainable model, knowledge graph, relation embedding}
}

@article{10.1145/3366370,
author = {Kolb, John and AbdelBaky, Moustafa and Katz, Randy H. and Culler, David E.},
title = {Core Concepts, Challenges, and Future Directions in Blockchain: A Centralized Tutorial},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3366370},
doi = {10.1145/3366370},
abstract = {Blockchains are a topic of immense interest in academia and industry, but their true nature is often obscured by marketing and hype. In this tutorial, we explain the fundamental elements of blockchains. We discuss their ability to achieve availability, consistency, and data integrity as well as their inherent limitations. Using Ethereum as a case study, we describe the inner workings of blockchains in detail before comparing blockchains to traditional distributed systems. In the second part of our tutorial, we discuss the major challenges facing blockchains and summarize ongoing research and commercial offerings that seek to address these challenges.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {9},
numpages = {39},
keywords = {Blockchain, cryptocurrency, distributed ledger, smart contracts}
}

@article{10.1145/3376925,
author = {Ferreira-Lopes, Prof. Patricia},
title = {A Data-driven Approach for Architectural History Knowledge. Capturing Buildings’ Construction Events for Historical Research Collaboration},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3376925},
doi = {10.1145/3376925},
abstract = {The increase of multidisciplinary research in the field of architectural history has led to the need to set up new experiences and solutions for the handling and integration of the information extracted from historical documents. These solutions seek to support diverse users of the research community with the aim of solving challenges directly related with the digitalisation, structuring, standardisation, and management of historical information. These challenges include, for example, the creation of a digital support that enables a collaborative growth and management of information, the normalisation of terms and vocabularies to make its analysis efficient, the elaborating of a conceptual model, and the development of a metadata support that allows its more expanded dissemination and reuse.This article describes a case study project in which the documents of archives, of research, and of projects previously carried out by the Late Gothic Network (Red Tardog\'{o}tica) are the raw material for the proposal of an event-oriented historical database (e-database). This e-database means to record and systematise the information about the artistic transfers related with the architectural production in the transition of the Modern Age, a period also known as the Late Gothic. The e-database's design has considered the possibility of its use for the analysis of social networks (abstract-relational model, a Graph model) and the spatiotemporal analysis of the events (geo-temporal model, GIS). The main section of this article describes the architecture of the database, with a view to addressing the questions of the relations between the datasets and the matter of implementing the thesauri and controlled vocabularies that must be respected for the standardisation, recording, and later analysis of the data. Next, we contribute quantitative and qualitative analyses to evaluate the database's important gaps of information. This proposal initially covers the geographical framework of the western Andalusian territory, but it can be expanded to other areas and adapted to other case studies. Finally, the article summarises the learning achieved in this first phase of the case study project and describes the perspectives to broaden its use in the community of architectural history researchers.},
journal = {J. Comput. Cult. Herit.},
month = may,
articleno = {15},
numpages = {22},
keywords = {History of architecture, Modern Age, artistic transfer, cultural heritage, digital humanities, event database}
}

@article{10.1145/3378176,
author = {Jiang, Lijuan and Yang, Chao and Ma, Wenjing},
title = {Enabling Highly Efficient Batched Matrix Multiplications on SW26010 Many-core Processor},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3378176},
doi = {10.1145/3378176},
abstract = {We present a systematic methodology for optimizing batched matrix multiplications on SW26010 many-core processor of the Sunway TaihuLight supercomputer. Five surrogate algorithms and a machine learning–based algorithm selector are proposed to fully exploit the computing capability of SW26010 and cope with the sophisticated algorithm characteristics of batched matrix multiplications. Experiment results show that the algorithm selector is able to adaptively choose the appropriate algorithm for various matrix shapes and batch sizes with low overhead and high accuracy. In particular, the optimized batched matrix multiplications can substantially outperform the non-batched version and reach around 84.8\% of the performance upper bound.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {3},
numpages = {23},
keywords = {Batched matrix multiplication, SW26010 processor, Sunway TaihuLight, batched GEMM, many-core architecture}
}

@article{10.1145/3380930,
author = {Anzt, Hartwig and Cojean, Terry and Yen-Chen, Chen and Dongarra, Jack and Flegar, Goran and Nayak, Pratik and Tomov, Stanimire and Tsai, Yuhsiang M. and Wang, Weichung},
title = {Load-balancing Sparse Matrix Vector Product Kernels on GPUs},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2329-4949},
url = {https://doi.org/10.1145/3380930},
doi = {10.1145/3380930},
abstract = {Efficient processing of Irregular Matrices on Single Instruction, Multiple Data (SIMD)-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for Irregular Matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, in this article, we address the challenge of designing high performance sparse matrix-vector product (SpMV) kernels designed for Nvidia Graphics Processing Units (GPUs). We present a compressed sparse row (CSR) format suitable for unbalanced matrices. We also provide a load-balancing kernel for the coordinate (COO) matrix format and extend it to a hybrid algorithm that stores part of the matrix in SIMD-friendly Ellpack format (ELL) format. The ratio between the ELL- and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against SpMV kernels provided by NVIDIA’s cuSPARSE library and a heavily-tuned sliced ELL (SELL-P) kernel that prevents unnecessary padding by considering the irregular matrices as a combination of matrix blocks stored in ELL format.},
journal = {ACM Trans. Parallel Comput.},
month = mar,
articleno = {2},
numpages = {26},
keywords = {GPUs, Sparse Matrix Vector Product (SpMV), irregular matrices}
}

@article{10.1145/3382189,
author = {Tan, Liling and Li, Maggie Yundi and Kok, Stanley},
title = {E-Commerce Product Categorization via Machine Translation},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3382189},
doi = {10.1145/3382189},
abstract = {E-commerce platforms categorize their products into a multi-level taxonomy tree with thousands of leaf categories. Conventional methods for product categorization are typically based on machine learning classification algorithms. These algorithms take product information as input (e.g., titles and descriptions) to classify a product into a leaf category. In this article, we propose a new paradigm based on machine translation. In our approach, we translate a product’s natural language description into a sequence of tokens representing a root-to-leaf path in a product taxonomy. In our experiments on two large real-world datasets, we show that our approach achieves better predictive accuracy than a state-of-the-art classification system for product categorization. In addition, we demonstrate that our machine translation models can propose meaningful new paths between previously unconnected nodes in a taxonomy tree, thereby transforming the taxonomy into a directed acyclic graph. We discuss how the resultant taxonomy directed acyclic graph promotes user-friendly navigation, and how it is more adaptable to new products.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {11},
numpages = {14},
keywords = {E-commerce, classification, machine translation}
}

@article{10.1145/3385413,
author = {Gelli, Francesco and Uricchio, Tiberio and He, Xiangnan and Bimbo, Alberto Del and Chua, Tat-Seng},
title = {Learning Visual Elements of Images for Discovery of Brand Posts},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3385413},
doi = {10.1145/3385413},
abstract = {Online Social Network Sites have become a primary platform for brands and organizations to engage their audience by sharing image and video posts on their timelines. Different from traditional advertising, these posts are not restricted to the products or logo but include visual elements that express more in general the values and attributes of the brand, called brand associations. Since marketers are increasingly spending time in discovering and re-posting user generated posts that reflect the brand attributes, there is an increasing demand for such discovery systems. The goal of these systems is to assist brand experts in filtering through online collections of new user media to discover actionable posts, which match the brand value and have the potential to engage the consumers. Driven by this real-life application, we define and formulate a new task of content discovery for brands and propose a framework that learns to rank posts for brands from their historical timeline. We design a Personalized Content Discovery (PCD) framework to address the three challenges of high inter-brand similarity, sparsity of brand--post interactions, and diversification of timeline. To learn fine-grained brand representation and to generate explanations for the ranking, we automatically learn visual elements of posts from the timeline of brands and from a set of brand attributes in the domain of marketing. To test our framework we use two large-scale Instagram datasets that contain a total of more than 1.5 million image and video posts from the historical timeline of hundreds of brands from multiple verticals such as food and fashion. Extensive experiments indicate that our model can effectively learn fine-grained brand representations and outperform the closest state-of-the-art solutions.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {56},
numpages = {21},
keywords = {Content discovery, computational marketing, image ranking}
}

@article{10.1145/3389397,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Pattern-based Interactive Configuration Derivation for Cyber-physical System Product Lines},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3389397},
doi = {10.1145/3389397},
abstract = {Deriving a Cyber-Physical System (CPS) product from a product line requires configuring hundreds to thousands of configurable parameters of components and devices from multiple domains, e.g., computing, control, and communication. A fully automated configuration process for a CPS product line is seldom possible in practice, and a dynamic and interactive process is expected. Therefore, some configurable parameters are to be configured manually, and the rest can be configured either automatically or manually, depending on pre-defined constraints, the order of configuration steps, and previous configuration data in such a dynamic and interactive configuration process. In this article, we propose a pattern-based, interactive configuration derivation methodology (named as Pi-CD) to maximize opportunities of automatically deriving correct configurations of CPSs by benefiting from pre-defined constraints and configuration data of previous configuration steps. Pi-CD requires architectures of CPS product lines modeled with Unified Modeling Language extended with four types of variabilities, along with constraints specified in Object Constraint Language (OCL). Pi-CD is equipped with 324 configuration derivation patterns that we defined by systematically analyzing the OCL constructs and semantics. We evaluated Pi-CD by configuring 20 CPS products of varying complexity from two real-world CPS product lines. Results show that Pi-CD can achieve up to 72\% automation degree with a negligible time cost. Moreover, its time performance remains stable with the increase in the number of configuration parameters as well as constraints.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jun,
articleno = {44},
numpages = {24},
keywords = {Product line engineering, configuration derivation, object constraint language, product configuration}
}

@article{10.1145/3391906,
author = {Hsiao, Luke and Wu, Sen and Chiang, Nicholas and R\'{e}, Christopher and Levis, Philip},
title = {Creating Hardware Component Knowledge Bases with Training Data Generation and Multi-task Learning},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3391906},
doi = {10.1145/3391906},
abstract = {Hardware component databases are vital resources in designing embedded systems. Since creating these databases requires hundreds of thousands of hours of manual data entry, they are proprietary, limited in the data they provide, and have random data entry errors.We present a machine learning based approach for creating hardware component databases directly from datasheets. Extracting data directly from datasheets is challenging because: (1) the data is relational in nature and relies on non-local context, (2) the documents are filled with technical jargon, and (3) the datasheets are PDFs, a format that decouples visual locality from locality in the document. Addressing this complexity has traditionally relied on human input, making it costly to scale. Our approach uses a rich data model, weak supervision, data augmentation, and multi-task learning to create these knowledge bases in a matter of days.We evaluate the approach on datasheets of three types of components and achieve an average quality of 77&nbsp;F1 points—quality comparable to existing human-curated knowledge bases. We perform application studies that demonstrate the extraction of multiple data modalities including numerical properties and images. We show how different sources of supervision such as heuristics and human labels have distinct advantages that can be utilized together to improve knowledge base quality. Finally, we present a case study to show how this approach changes the way practitioners create hardware component knowledge bases.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {42},
numpages = {26},
keywords = {Knowledge base construction, design tools, machine learning}
}

@article{10.1145/3396850,
author = {Luckner, Marcin and Grzenda, Maciej and Kunicki, Robert and Legierski, Jaroslaw},
title = {IoT Architecture for Urban Data-Centric Services and Applications},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3396850},
doi = {10.1145/3396850},
abstract = {In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.},
journal = {ACM Trans. Internet Technol.},
month = jul,
articleno = {29},
numpages = {30},
keywords = {Data stream, big data, data processing, public transport}
}

@article{10.1145/3398665,
author = {Cerrolaza, Jon Perez and Obermaisser, Roman and Abella, Jaume and Cazorla, Francisco J. and Gr\"{u}ttner, Kim and Agirre, Irune and Ahmadian, Hamidreza and Allende, Imanol},
title = {Multi-core Devices for Safety-critical Systems: A Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398665},
doi = {10.1145/3398665},
abstract = {Multi-core devices are envisioned to support the development of next-generation safety-critical systems, enabling the on-chip integration of functions of different criticality. This integration provides multiple system-level potential benefits such as cost, size, power, and weight reduction. However, safety certification becomes a challenge and several fundamental safety technical requirements must be addressed, such as temporal and spatial independence, reliability, and diagnostic coverage. This survey provides a categorization and overview at different device abstraction levels (nanoscale, component, and device) of selected key research contributions that support the compliance with these fundamental safety requirements.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {79},
numpages = {38},
keywords = {Fault tolerance, diagnostic coverage, spatial independence, time independence}
}

@article{10.1145/3399714,
author = {Jiang, Peng and Xia, Yang and Agrawal, Gagan},
title = {Combining SIMD and Many/Multi-core Parallelism for Finite-state Machines with Enumerative Speculation},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2329-4949},
url = {https://doi.org/10.1145/3399714},
doi = {10.1145/3399714},
abstract = {Finite-state Machine (FSM) is the key kernel behind many popular applications, including regular expression matching, text tokenization, and Huffman decoding. Parallelizing FSMs is extremely difficult because of the strong dependencies and unpredictable memory accesses. Previous efforts have largely focused on multi-core parallelization and used different approaches, including speculative and enumerative execution, both of which have been effective but also have limitations. With increasing width and improving flexibility in SIMD instruction sets, this article focuses on combining SIMD and many/multi-core parallelism for FSMs. We have developed a novel strategy, called enumerative speculation. Instead of speculating on a single state as in speculative execution or enumerating all possible states as in enumerative execution, our strategy speculates transitions from several possible states, reducing the prediction overheads of speculation approach and the large amount of redundant work in the enumerative approach. A simple lookback approach produces a set of guessed states to achieve high speculation success rates in our enumerative speculation. In addition, to enable continued scalability of enumerative speculation with a large number of threads, we have developed a parallel merge method. We evaluate our method with four popular FSM applications: Huffman decoding, regular expression matching, HTML tokenization, and Div7. We obtain up to 2.5\texttimes{} speedup using SIMD on 1 core and up to 95\texttimes{} combining SIMD with 60 cores of an Intel Xeon Phi. On a single core, we outperform the best single-state speculative execution version by an average of 1.6\texttimes{}, and in combining SIMD and many-core parallelism, outperform enumerative execution by an average of 2\texttimes{}. Finally, when evaluate on a GPU, we show that our parallel merge implementations are 2.02--6.74\texttimes{} more efficient than corresponding sequential merge implementations and achieve better scalability on an Nvidia V100 GPU.},
journal = {ACM Trans. Parallel Comput.},
month = jun,
articleno = {15},
numpages = {26},
keywords = {Finite-state machine, SIMD, break dependence}
}

@article{10.1145/3400032,
author = {Venkataramani, Vanchinathan and Kulkarni, Aditi and Mitra, Tulika and Peh, Li-Shiuan},
title = {SPECTRUM: A Software-defined Predictable Many-core Architecture for LTE/5G Baseband Processing},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3400032},
doi = {10.1145/3400032},
abstract = {Wireless communication standards such as Long-term Evolution (LTE) are rapidly changing to support the high data-rate of wireless devices. The physical layer baseband processing has strict real-time deadlines, especially in the next-generation applications enabled by the 5G standard. Existing basestation transceivers utilize customized DSP cores or fixed-function hardware accelerators for physical layer baseband processing. However, these approaches incur significant non-recurring engineering costs and are inflexible to newer standards or updates. Software-programmable processors offer more adaptability. However, it is challenging to sustain guaranteed worst-case latency and throughput at reasonably low-power on shared-memory many-core architectures featuring inherently unpredictable design choices, such as caches and Network-on-chip (NoC).We propose SPECTRUM, a predictable, software-defined many-core architecture that exploits the massive parallelism of the LTE/5G baseband processing workload. The focus is on designing scalable lightweight hardware that can be programmed and defined by sophisticated software mechanisms. SPECTRUM employs hundreds of lightweight in-order cores augmented with custom instructions that provide predictable timing, a purely software-scheduled NoC that orchestrates the communication to avoid any contention, and per-core software-controlled scratchpad memory with deterministic access latency. Compared to many-core architecture like Skylake-SP (average power 215 W) that drops 14\% packets at high-traffic load, 256-core SPECTRUM by definition has zero packet drop rate at significantly lower average power of 24 W. SPECTRUM consumes 2.11\texttimes{} lower power than C66x DSP cores+accelerator platform in baseband processing. We also enable SPECTRUM to handle dynamic workloads with multiple service categories present in 5G mobile network (Enhanced Mobile Broadband (eMBB), Ultra-reliable and Low-latency Communications (URLLC), and Massive Machine Type Communications (mMTC)), using a run-time scheduling and mapping algorithm. Experimental evaluations show that our algorithm performs task/NoC mapping at run-time on fewer cores compared to the static mapping (that reserves cores exclusively for each service category) while still meeting the differentiated latency and reliability requirements.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {32},
numpages = {28},
keywords = {5G, LTE, Time-predictable architecture, baseband processing, low-power, many-cores}
}

@article{10.1145/3418075,
author = {Aggarwal, Karan and Bondhugula, Uday},
title = {Optimizing the Linear Fascicle Evaluation Algorithm for Multi-core and Many-core Systems},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3418075},
doi = {10.1145/3418075},
abstract = {Sparse matrix-vector multiplication (SpMV) operations are commonly used in various scientific and engineering applications. The performance of the SpMV operation often depends on exploiting regularity patterns in the matrix. Various representations and optimization techniques have been proposed to minimize the memory bandwidth bottleneck arising from the irregular memory access pattern involved. Among recent representation techniques, tensor decomposition is a popular one used for very large but sparse matrices. Post sparse-tensor decomposition, the new representation involves indirect accesses, making it challenging to optimize for multi-cores and even more demanding for the massively parallel architectures, such as on GPUs.Computational neuroscience algorithms often involve sparse datasets while still performing long-running computations on them. The Linear Fascicle Evaluation (LiFE) application is a popular neuroscience algorithm used for pruning brain connectivity graphs. The datasets employed herein involve the Sparse Tucker Decomposition (STD)—a widely used tensor decomposition method. Using this decomposition leads to multiple indirect array references, making it very difficult to optimize on both multi-core and many-core systems. Recent implementations of the LiFE algorithm show that its SpMV operations are the key bottleneck for performance and scaling.In this work, we first propose target-independent optimizations to optimize the SpMV operations of LiFE decomposed using the STD technique, followed by target-dependent optimizations for CPU and GPU systems. The target-independent techniques include: (1) standard compiler optimizations to prevent unnecessary and redundant computations, (2)&nbsp;data restructuring techniques to minimize the effects of indirect array accesses, and (3)&nbsp;methods to partition computations among threads to obtain coarse-grained parallelism with low synchronization overhead. Then, we present the target-dependent optimizations for CPUs such as: (1)&nbsp;efficient synchronization-free thread mapping and (2) utilizing BLAS calls to exploit hardware-specific speed. Following that, we present various GPU-specific optimizations to optimally map threads at the granularity of warps, thread blocks, and grid. Furthermore, to automate the CPU-based optimizations developed for this algorithm, we also extend the PolyMage domain-specific language, embedded in Python. Our highly optimized and parallelized CPU implementation obtains a speedup of 6.3\texttimes{} over the naive parallel CPU implementation running on 16-core Intel Xeon Silver (Skylake-based) system. In addition to that, our optimized GPU implementation achieves a speedup of 5.2\texttimes{} over a reference-optimized GPU code version on NVIDIA’s GeForce RTX 2080 Ti GPU and a speedup of 9.7\texttimes{} over our highly optimized and parallelized CPU implementation.},
journal = {ACM Trans. Parallel Comput.},
month = nov,
articleno = {22},
numpages = {45},
keywords = {GPU, LiFE algorithm, SpMV, connectome, indirect array accesses, multi-core, sparse tucker decomposition, tensor decomposition, tractography}
}

@article{10.1145/3418498,
author = {Wu, Nan and Deng, Lei and Li, Guoqi and Xie, Yuan},
title = {Core Placement Optimization for Multi-chip Many-core Neural Network Systems with Reinforcement Learning},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3418498},
doi = {10.1145/3418498},
abstract = {Multi-chip many-core neural network systems are capable of providing high parallelism benefited from decentralized execution, and they can be scaled to very large systems with reasonable fabrication costs. As multi-chip many-core systems scale up, communication latency related effects will take a more important portion in the system performance. While previous work mainly focuses on the core placement within a single chip, there are two principal issues still unresolved: the communication-related problems caused by the non-uniform, hierarchical on/off-chip communication capability in multi-chip systems, and the scalability of these heuristic-based approaches in a factorially growing search space. To this end, we propose a reinforcement-learning-based method to automatically optimize core placement through deep deterministic policy gradient, taking into account information of the environment by performing a series of trials (i.e., placements) and using convolutional neural networks to extract spatial features of different placements. Experimental results indicate that compared with a naive sequential placement, the proposed method achieves 1.99\texttimes{} increase in throughput and 50.5\% reduction in latency; compared with the simulated annealing, an effective technique to approximate the global optima in an extremely large search space, our method improves the throughput by 1.22\texttimes{} and reduces the latency by 18.6\%. We further demonstrate that our proposed method is capable to find optimal placements taking advantages of different communication properties caused by different system configurations, and work in a topology-agnostic manner.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = oct,
articleno = {11},
numpages = {27},
keywords = {Multi-chip many-core architecture, core placement optimization, machine learning for system, neural network accelerator}
}

@article{10.1145/3419103,
author = {Marques, Rafael Salema and Epiphaniou, Gregory and Al-Khateeb, Haider and Maple, Carsten and Hammoudeh, Mohammad and De Castro, Paulo Andr\'{e} Lima and Dehghantanha, Ali and Choo, Kim Kwang Raymond},
title = {A Flow-based Multi-agent Data Exfiltration Detection Architecture for Ultra-low Latency Networks},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3419103},
doi = {10.1145/3419103},
abstract = {Modern network infrastructures host converged applications that demand rapid elasticity of services, increased security, and ultra-fast reaction times. The Tactile Internet promises to facilitate the delivery of these services while enabling new economies of scale for high fidelity of machine-to-machine and human-to-machine interactions. Unavoidably, critical mission systems served by the Tactile Internet manifest high demands not only for high speed and reliable communications but equally, the ability to rapidly identify and mitigate threats and vulnerabilities. This article proposes a novel Multi-Agent Data Exfiltration Detector Architecture (MADEX), inspired by the mechanisms and features present in the human immune system. MADEX seeks to identify data exfiltration activities performed by evasive and stealthy malware that hides malicious traffic from an infected host in low-latency networks. Our approach uses cross-network traffic information collected by agents to effectively identify unknown illicit connections by an operating system subverted. MADEX does not require prior knowledge of the characteristics or behavior of the malicious code or a dedicated access to a knowledge repository. We tested the performance of MADEX in terms of its capacity to handle real-time data and the sensitivity of our algorithm’s classification when exposed to malicious traffic. Experimental evaluation results show that MADEX achieved 99.97\% sensitivity, 98.78\% accuracy, and an error rate of 1.21\% when compared to its best rivals. We created a second version of MADEX, called MADEX level 2, that further improves its overall performance with a slight increase in computational complexity. We argue for the suitability of MADEX level 1 in non-critical environments, while MADEX level 2 can be used to avoid data exfiltration in critical mission systems. To the best of our knowledge, this is the first article in the literature that addresses the detection of rootkits real-time in an agnostic way using an artificial immune system approach while it satisfies strict latency requirements.},
journal = {ACM Trans. Internet Technol.},
month = jul,
articleno = {103},
numpages = {30},
keywords = {Artificial immune systems, multi-agent systems, flow-based analysis, rootkits, Tactile Internet}
}

@article{10.1145/3428079,
author = {Xu, En and Yu, Zhiwen and Guo, Bin and Cui, Helei},
title = {Core Interest Network for Click-Through Rate Prediction},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3428079},
doi = {10.1145/3428079},
abstract = {In modern online advertising systems, the click-through rate (CTR) is an important index to measure the popularity of an item. It refers to the ratio of users who click on a specific advertisement to the number of total users who view it. Predicting the CTR of an item in advance can improve the accuracy of the advertisement recommendation. And it is commonly calculated based on users’ interests. Thus, extracting users’ interests is of great importance in CTR prediction tasks. In the literature, a lot of studies treat the interaction between users and items as sequential data and apply the recurrent neural network (RNN) model to extract users’ interests. However, these solutions cannot handle the case when the sequence length is relatively long, e.g., over 100. This is because of the vanishing gradient problem of RNN, i.e., the model cannot learn a users’ previous behaviors that are too far away from the current moment. To address this problem, we propose a new Core Interest Network (CIN) model to mitigate the problem of a long sequence in the CTR prediction task with sequential data. In brief, we first extract the core interests of users and then use the refined data as the input of subsequent learning tasks. Extensive evaluations on real dataset show that our CIN model can outperform the state-of-the-art solutions in terms of prediction accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {23},
numpages = {16},
keywords = {CTR prediction, computational advertising, sequential recommendation, time series prediction, user portrait}
}

@article{10.1145/3428199,
author = {Leobas, Guilherme Vieira and Pereira, Fernando Magno Quint\~{a}o},
title = {Semiring optimizations: dynamic elision of expressions with identity and absorbing elements},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428199},
doi = {10.1145/3428199},
abstract = {This paper describes a compiler optimization to eliminates dynamic occurrences of expressions in the format a ← a ⊕ b ⊗ c. The operation ⊕ must admit an identity element z, such that a ⊕ z = a. Also, z must be the absorbing element of ⊗, such that b ⊗ z = z ⊗ c = z. Semirings where ⊕ is the additive operator and ⊗ is the multiplicative operator meet this contract. This pattern is common in high-performance benchmarks—its canonical representative being the multiply-add operation a ← a + b \texttimes{} c. However, several other expressions involving arithmetic and logic operations satisfy the required algebra. We show that the runtime elimination of such assignments can be implemented in a performance-safe way via online profiling. The elimination of dynamic redundancies involving identity and absorbing elements in 35 programs of the LLVM test suite that present semiring patterns brings an average speedup of 1.19x (total optimized time over total unoptimized time) on top of clang -O3. When projected onto the entire test suite (259 programs) the optimization leads to a speedup of 1.025x. Once added onto clang, semiring optimizations approximates it to TACO, a specialized tensor compiler.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {131},
numpages = {28},
keywords = {Compiler, Optimization, Profiling, Semiring}
}

@article{10.1145/3428492,
author = {Kumari, Moothedath Chandran and Sagar, Biradar},
title = {Global Pandemic and Rapid New Product Development of Medical Products},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3428492},
doi = {10.1145/3428492},
abstract = {New product development is a strenuous and long process for companies. When the COVID-19 disease turned into a global pandemic, governments announced the need for various medical products that were scarce and at the same time imposed lockdowns and trade restrictions. During this period, rapid new product development of medical products was attempted by many companies worldwide using their existing supply chains. Data of 240 companies from non-medical product-segment background launching new medical products were collected and analyzed. Two case studies of Indian companies responding to shortages of ventilators and PPE are presented. A critical reflection of a new way of new product development and cooperation across firms is discussed.},
journal = {Digit. Gov.: Res. Pract.},
month = dec,
articleno = {17},
numpages = {38},
keywords = {COVID -19, global governance, industrial resilience, innovation, medical product design, new product development}
}

@article{10.1145/3433543,
author = {Tian, Hui and Peng, Fang and Quan, Hanyu and Chang, Chin-Chen},
title = {Identity-Based Public Auditing for Cloud Storage of Internet-of-Vehicles Data},
year = {2023},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3433543},
doi = {10.1145/3433543},
abstract = {The Internet of Vehicles (IoV), with the help of cloud computing, can provide rich and powerful application services for vehicles and drivers by sharing and analysing various IoV data. However, how to ensure the integrity of IoV data with multiple sources and diversity outsourced in the cloud is still an open challenge. To address this concern, this paper first presents an identity-based public auditing scheme for cloud storage of IoV data, which can fully achieve the essential function and security requirements, such as classified auditing, multi-source auditing and privacy protection. Particularly, we design a new authenticated data structure, called data mapping table, to track the distribution of each type of IoV data to ensure fine and rapid audits. Moreover, our scheme can reduce the overheads for both the key management and the generation of block tags. We formally prove the security of the presented scheme and evaluate its performance by comprehensive comparisons with the state-of-the-art schemes designed for traditional scenarios. The theoretical analyses and experimental results demonstrate that our scheme can securely and efficiently realize public auditing for IoV data, and outperforms the previous ones in both the computation and communication overheads in most cases.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {88},
numpages = {24},
keywords = {Public auditing, data integrity, internet of vehicles, cloud storage, data mapping table}
}

@article{10.1145/3440016,
author = {Thakker, Urmish and Fedorov, Igor and Zhou, Chu and Gope, Dibakar and Mattina, Matthew and Dasika, Ganesh and Beu, Jesse},
title = {Compressing RNNs to Kilobyte Budget for IoT Devices Using Kronecker Products},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3440016},
doi = {10.1145/3440016},
abstract = {Micro-controllers (MCUs) make up most of the processors in the world with widespread applicability from automobile to medical devices. The Internet of Things promises to enable these resource-constrained MCUs with machine learning algorithms to provide always-on intelligence. Many Internet of Things applications consume time-series data that are naturally suitable for recurrent neural networks (RNNs) like LSTMs and GRUs. However, RNNs can be large and difficult to deploy on these devices, as they have few kilobytes of memory. As a result, there is a need for compression techniques that can significantly compress RNNs without negatively impacting task accuracy. This article introduces a method to compress RNNs for resource-constrained environments using the Kronecker product (KP). KPs can compress RNN layers by 16\texttimes{} to 38\texttimes{} with minimal accuracy loss. By quantizing the resulting models to 8 bits, we further push the compression factor to 50\texttimes{}. We compare KP with other state-of-the-art compression techniques across seven benchmarks spanning five different applications and show that KP can beat the task accuracy achieved by other techniques by a large margin while simultaneously improving the inference runtime. Sometimes the KP compression mechanism can introduce an accuracy loss. We develop a hybrid KP approach to mitigate this. Our hybrid KP algorithm provides fine-grained control over the compression ratio, enabling us to regain accuracy lost during compression by adding a small number of model parameters.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jul,
articleno = {46},
numpages = {18},
keywords = {Neural networks, micro-controllers, matrix decomposition, Kronecker products, model compression, IoT}
}

@article{10.1145/3440033,
author = {Sun, Xiaoyu and Li, Li and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Octeau, Damien and Grundy, John},
title = {Taming Reflection: An Essential Step Toward Whole-program Analysis of Android Apps},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3440033},
doi = {10.1145/3440033},
abstract = {Android developers heavily use reflection in their apps for legitimate reasons. However, reflection is also significantly used for hiding malicious actions. Unfortunately, current state-of-the-art static analysis tools for Android are challenged by the presence of reflective calls, which they usually ignore. Thus, the results of their security analysis, e.g., for private data leaks, are incomplete, given the measures taken by malware writers to elude static detection. We propose a new instrumentation-based approach to address this issue in a non-invasive way. Specifically, we introduce to the community a prototype tool called DroidRA, which reduces the resolution of reflective calls to a composite constant propagation problem and then leverages the COAL solver to infer the values of reflection targets. After that, it automatically instruments the app to replace reflective calls with their corresponding Java calls in a traditional paradigm. Our approach augments an app so that it can be more effectively statically analyzable, including by such static analyzers that are not reflection-aware. We evaluate DroidRA on benchmark apps as well as on real-world apps, and we demonstrate that it can indeed infer the target values of reflective calls and subsequently allow state-of-the-art tools to provide more sound and complete analysis results.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {32},
numpages = {36},
keywords = {Android, DroidRA, reflection, static analysis}
}

@article{10.1145/3446372,
author = {Alharbi, Ahmed and Dong, Hai and Yi, Xun and Tari, Zahir and Khalil, Ibrahim},
title = {Social Media Identity Deception Detection: A Survey},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446372},
doi = {10.1145/3446372},
abstract = {Social media have been growing rapidly and become essential elements of many people’s lives. Meanwhile, social media have also come to be a popular source for identity deception. Many social media identity deception cases have arisen over the past few years. Recent studies have been conducted to prevent and detect identity deception. This survey analyzes various identity deception attacks, which can be categorized into fake profile, identity theft, and identity cloning. This survey provides a detailed review of social media identity deception detection techniques. It also identifies primary research challenges and issues in the existing detection techniques. This article is expected to benefit both researchers and social media providers.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {69},
numpages = {35},
keywords = {Identity deception, Sybil, detection techniques, fake profile, identity cloning, identity theft, social botnet, sockpuppet}
}

@article{10.1145/3446982,
author = {Hao, Shaoyang and Guo, Bin and Wang, Hao and Liang, Yunji and Yao, Lina and Wang, Qianru and Yu, Zhiwen},
title = {DeepDepict: Enabling Information Rich, Personalized Product Description Generation With the Deep Multiple Pointer Generator Network},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3446982},
doi = {10.1145/3446982},
abstract = {In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users’ interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {83},
numpages = {16},
keywords = {Personalized text generation, product description generation}
}

@article{10.1145/3451883,
author = {Doering, Malcolm and Br\v{s}\v{c}i\'{c}, Dra\v{z}en and Kanda, Takayuki},
title = {Data-Driven Imitation Learning for a Shopkeeper Robot with Periodically Changing Product Information},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
url = {https://doi.org/10.1145/3451883},
doi = {10.1145/3451883},
abstract = {Data-driven imitation learning enables service robots to learn social interaction behaviors, but these systems cannot adapt after training to changes in the environment, such as changing products in a store. To solve this, a novel learning system that uses neural attention and approximate string matching to copy information from a product information database to its output is proposed. A camera shop interaction dataset was simulated for training/testing. The proposed system was found to outperform a baseline and a previous state of the art in an offline, human-judged evaluation.},
journal = {J. Hum.-Robot Interact.},
month = jul,
articleno = {31},
numpages = {20},
keywords = {Human-robot interaction, database question answering, imitation learning, knowledge base question answering, retail robot, service robot, social robot}
}

@article{10.1145/3453164,
author = {Gong, Shijun and Li, Jiajun and Lu, Wenyan and Yan, Guihai and Li, Xiaowei},
title = {ShuntFlowPlus: An Efficient and Scalable Dataflow Accelerator Architecture for Stream Applications},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3453164},
doi = {10.1145/3453164},
abstract = {Streaming processing is an important and growing class of applications for analyzing continuous streams in real time. In such applications, sliding-window aggregation (SWAG) is a widely used approach, and general-purpose processors cannot efficiently handle SWAG because of the specific computation patterns. This article proposes an efficient dataflow accelerator architecture for ubiquitous SWAGs, called ShuntFlowPlus. ShuntFlowPlus supports two main categories of SWAGs that are widely used in streaming processing. Meanwhile, we propose a shunt rule to enable ShuntFlowPlus to efficiently handle SWAGs with arbitrary parameters. Furthermore, we propose a novel realization scheme of SWAG kernels based on buffer sharing to maximize buffer utilization. As a case study, we implemented ShuntFlowPlus on an Altera Arria 10 AX115N FPGA board at 150 MHz and compared it to previous approaches. The experimental results show that ShuntFlowPlus provides a tremendous throughput and latency advantage over CPU and GPU implementations on both reduce-like and index-like SWAGs. Compare to ShuntFlow, 41\% of buffer resources are saved.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jun,
articleno = {59},
numpages = {24},
keywords = {Streaming processing, sliding-window aggregations, dataflow, buffer sharing}
}

@article{10.1145/3458511,
author = {Ruaro, Marcelo and Sant’ana, Anderson and Jantsch, Axel and Moraes, Fernando Gehm},
title = {Modular and Distributed Management of Many-Core SoCs},
year = {2021},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1–2},
issn = {0734-2071},
url = {https://doi.org/10.1145/3458511},
doi = {10.1145/3458511},
abstract = {Many-Core Systems-on-Chip increasingly require Dynamic Multi-objective Management (DMOM) of resources. DMOM uses different management components for objectives and resources to implement comprehensive and self-adaptive system resource management. DMOMs are challenging because they require a scalable and well-organized framework to make each component modular, allowing it to be instantiated or redesigned with a limited impact on other components.This work evaluates two state-of-the-art distributed management paradigms and, motivated by their drawbacks, proposes a new one called Management Application (MA), along with a DMOM framework based on MA. MA is a distributed application, specific for management, where each task implements a management role. This paradigm favors scalability and modularity because the management design assumes different and parallel modules, decoupled from the OS.An experiment with a task mapping case study shows that MA reduces the overhead of management resources (-61.5\%), latency (-66\%), and communication volume (-96\%) compared to state-of-the-art per-application management. Compared to cluster-based management (CBM) implemented directly as part of the OS, MA is similar in resources and communication volume, increasing only the mapping latency (+16\%). Results targeting a complete DMOM control loop addressing up to three different objectives show the scalability regarding system size and adaptation frequency compared to CBM, presenting an overall management latency reduction of 17.2\% and an overall monitoring messages’ latency reduction of 90.2\%.},
journal = {ACM Trans. Comput. Syst.},
month = jul,
articleno = {1},
numpages = {16},
keywords = {Many-core, System-on-Chip (SoC), distributed resource management}
}

@article{10.1145/3461478,
author = {Reggiani, Enrico and Del Sozzo, Emanuele and Conficconi, Davide and Natale, Giuseppe and Moroni, Carlo and Santambrogio, Marco D.},
title = {Enhancing the Scalability of Multi-FPGA Stencil Computations via Highly Optimized HDL Components},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3461478},
doi = {10.1145/3461478},
abstract = {Stencil-based algorithms are a relevant class of computational kernels in high-performance systems, as they appear in a plethora of fields, from image processing to seismic simulations, from numerical methods to physical modeling. Among the various incarnations of stencil-based computations, Iterative Stencil Loops (ISLs) and Convolutional Neural Networks (CNNs) represent two well-known examples of kernels belonging to the stencil class. Indeed, ISLs apply the same stencil several times until convergence, while CNN layers leverage stencils to extract features from an image. The computationally intensive essence of ISLs, CNNs, and in general stencil-based workloads, requires solutions able to produce efficient implementations in terms of throughput and power efficiency. In this context, FPGAs are ideal candidates for such workloads, as they allow design architectures tailored to the stencil regular computational pattern. Moreover, the ever-growing need for performance enhancement leads FPGA-based architectures to scale to multiple devices to benefit from a distributed acceleration. For this reason, we propose a library of HDL components to effectively compute ISLs and CNNs inference on FPGA, along with a scalable multi-FPGA architecture, based on custom PCB interconnects. Our solution eases the design flow and guarantees both scalability and performance competitive with state-of-the-art works.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = aug,
articleno = {15},
numpages = {33},
keywords = {Multi-FPGA, stencil computation, HDL, CNN}
}

@article{10.1145/3461662,
author = {Qureshi, Yasir Mahmood and Simon, William Andrew and Zapater, Marina and Olcoz, Katzalin and Atienza, David},
title = {Gem5-X: A Many-core Heterogeneous Simulation Platform for Architectural Exploration and Optimization},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3461662},
doi = {10.1145/3461662},
abstract = {The increasing adoption of smart systems in our daily life has led to the development of new applications with varying performance and energy constraints, and suitable computing architectures need to be developed for these new applications. In this article, we present gem5-X, a system-level simulation framework, based on gem-5, for architectural exploration of heterogeneous many-core systems. To demonstrate the capabilities of gem5-X, real-time video analytics is used as a case-study. It is composed of two kernels, namely, video encoding and image classification using convolutional neural networks (CNNs). First, we explore through gem5-X the benefits of latest 3D high bandwidth memory (HBM2) in different architectural configurations. Then, using a two-step exploration methodology, we develop a new optimized clustered-heterogeneous architecture with HBM2 in gem5-X for video analytics application. In this proposed clustered-heterogeneous architecture, ARMv8 in-order cluster with in-cache computing engine executes the video encoding kernel, giving 20\% performance and 54\% energy benefits compared to baseline ARM in-order and Out-of-Order systems, respectively. Furthermore, thanks to gem5-X, we conclude that ARM Out-of-Order clusters with HBM2 are the best choice to run visual recognition using CNNs, as they outperform DDR4-based system by up to 30\% both in terms of performance and energy savings.},
journal = {ACM Trans. Archit. Code Optim.},
month = jul,
articleno = {44},
numpages = {27},
keywords = {HBM, Many-core, architectural exploration, cluster, gem5, heterogeneous architectures, in-cache}
}

@article{10.1145/3462775,
author = {Gade, Sri Harsha and Deb, Sujay},
title = {A Novel Hybrid Cache Coherence with Global Snooping for Many-core Architectures},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3462775},
doi = {10.1145/3462775},
abstract = {Cache coherence ensures correctness of cached data in multi-core processors. Traditional implementations of existing protocols make them unscalable for many core architectures. While snoopy coherence requires unscalable ordered networks, directory coherence is weighed down by high area and energy overheads. In this work, we propose Wireless-enabled Share-aware Hybrid (WiSH) to provide scalable coherence in many core processors. WiSH implements a novel Snoopy over Directory protocol using on-chip wireless links and hierarchical, clustered Network-on-Chip to achieve low-overhead and highly efficient coherence. A local directory protocol maintains coherence within a cluster of cores, while coherence among such clusters is achieved through global snoopy protocol. The ordered network for global snooping is provided through low-latency and low-energy broadcast wireless links. The overheads are further reduced through share-aware cache segmentation to eliminate coherence for private blocks. Evaluations show that WiSH reduces traffic by  and runtime by , while requiring  smaller storage and  lower energy as compared to existing hierarchical and hybrid coherence protocols. Owing to its modularity, WiSH provides highly efficient and scalable coherence for many core processors.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {2},
numpages = {31},
keywords = {Cache coherence, hybrid protocol, many core processors, mm-wave wireless links}
}

@article{10.1145/3464302,
author = {Zhang, Wei and Chen, Zeyuan and Zha, Hongyuan and Wang, Jianyong},
title = {Learning from Substitutable and Complementary Relations for Graph-based Sequential Product Recommendation},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3464302},
doi = {10.1145/3464302},
abstract = {Sequential product recommendation, aiming at predicting the products that a target user will interact with soon, has become a hotspot topic. Most of the sequential recommendation models focus on learning from users’ interacted product sequences in a purely data-driven manner. However, they largely overlook the knowledgeable substitutable and complementary relations between products. To address this issue, we propose a novel Substitutable and Complementary Graph-based Sequential Product Recommendation model, namely, SCG-SPRe. The innovations of SCG-SPRe lie in its two main modules: (1) The module of interactive graph neural networks jointly encodes the high-order product correlations in the substitutable graph and the complementary graph into two types of relation-specific product representations. (2) The module of kernel-enhanced transformer networks adaptively fuses multiple temporal kernels to characterize the unique temporal patterns between a candidate product to be recommended and any interacted product in a target behavior sequence. Thanks to the seamless integration of the two modules, SCG-SPRe obtains candidate-dependent user representations for different candidate products to compute the corresponding ranking scores. We conduct extensive experiments on three public datasets, demonstrating SCG-SPRe is superior to competitive sequential recommendation baselines and validating the benefits of explicitly modeling the product-product relations.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {26},
numpages = {28},
keywords = {Sequential recommendation, graph neural networks, attention mechanism, substitutable and complementary relations}
}

@article{10.1145/3465455,
author = {Wang, Yabin and Ma, Zhiheng and Wei, Xing and Zheng, Shuai and Wang, Yaowei and Hong, Xiaopeng},
title = {ECCNAS: Efficient Crowd Counting Neural Architecture Search},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3465455},
doi = {10.1145/3465455},
abstract = {Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96\% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {36},
numpages = {19},
keywords = {Neural architecture search, AutoDL, continuous search space, crowd counting, object counting}
}

@article{10.1145/3467022,
author = {Geisler, Sandra and Vidal, Maria-Esther and Cappiello, Cinzia and L\'{o}scio, Bernadette Farias and Gal, Avigdor and Jarke, Matthias and Lenzerini, Maurizio and Missier, Paolo and Otto, Boris and Paja, Elda and Pernici, Barbara and Rehof, Jakob},
title = {Knowledge-Driven Data Ecosystems Toward Data Transparency},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3467022},
doi = {10.1145/3467022},
abstract = {A data ecosystem (DE) offers a keystone-player or alliance-driven infrastructure that enables the interaction of different stakeholders and the resolution of interoperability issues among shared data. However, despite years of research in data governance and management, trustability is still affected by the absence of transparent and traceable data-driven pipelines. In this work, we focus on requirements and challenges that DEs face when ensuring data transparency. Requirements are derived from the data and organizational management, as well as from broader legal and ethical considerations. We propose a novel knowledge-driven DE architecture, providing the pillars for satisfying the analyzed requirements. We illustrate the potential of our proposal in a real-world scenario. Last, we discuss and rate the potential of the proposed architecture in the fulfillmentof these requirements.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {3},
numpages = {12},
keywords = {Data transparency, data ecosystems, data quality, trustability}
}

@article{10.1145/3468144,
author = {Song, Won Wook and Yang, Youngseok and Eo, Jeongyoon and Seo, Jangho and Kim, Joo Yeon and Lee, Sanha and Lee, Gyewon and Um, Taegeon and Cho, Haeyoon and Chun, Byung-Gon},
title = {Apache Nemo: A Framework for Optimizing Distributed Data Processing},
year = {2021},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3468144},
doi = {10.1145/3468144},
abstract = {Optimizing scheduling and communication of distributed data processing for resource and data characteristics is crucial for achieving high performance. Existing approaches to such optimizations largely fall into two categories. First, distributed runtimes provide low-level policy interfaces to apply the optimizations, but do not ensure the maintenance of correct application semantics and thus often require significant effort to use. Second, policy interfaces that extend a high-level application programming model ensure correctness, but do not provide sufficient fine control.We describe Apache Nemo, an optimization framework for distributed dataflow processing that provides fine control for high performance and also ensures correctness for ease of use. We combine several techniques to achieve this, including an intermediate representation of dataflow, compiler optimization passes, and runtime extensions. Our evaluation results show that Nemo enables composable and reusable optimizations that bring performance improvements on par with existing specialized runtimes tailored for a specific deployment scenario. Apache Nemo is open-sourced at  as an Apache incubator project.},
journal = {ACM Trans. Comput. Syst.},
month = oct,
articleno = {5},
numpages = {31},
keywords = {Data processing, distributed systems}
}

@article{10.1145/3469028,
author = {Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama},
title = {Data Analytics for Air Travel Data: A Survey and New Perspectives},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469028},
doi = {10.1145/3469028},
abstract = {From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {167},
numpages = {35},
keywords = {Airline, revenue management, big data}
}

@article{10.1145/3469661,
author = {Rybalkin, Vladimir and Ney, Jonas and Tekleyohannes, Menbere Kina and Wehn, Norbert},
title = {When Massive GPU Parallelism Ain’t Enough: A Novel Hardware Architecture of 2D-LSTM Neural Network},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3469661},
doi = {10.1145/3469661},
abstract = {Multidimensional Long Short-Term Memory (MD-LSTM) neural network is an extension of one-dimensional LSTM for data with more than one dimension. MD-LSTM achieves state-of-the-art results in various applications, including handwritten text recognition, medical imaging, and many more. However, its implementation suffers from the inherently sequential execution that tremendously slows down both training and inference compared to other neural networks.The main goal of the current research is to provide acceleration for inference of MD-LSTM. We advocate that Field-Programmable Gate Array (FPGA) is an alternative platform for deep learning that can offer a solution when the massive parallelism of GPUs does not provide the necessary performance required by the application.In this article, we present the first hardware architecture for MD-LSTM. We conduct a systematic exploration to analyze a tradeoff between precision and accuracy. We use a challenging dataset for semantic segmentation, namely historical document image binarization from the DIBCO 2017 contest and a well-known MNIST dataset for handwritten digit recognition. Based on our new architecture, we implement FPGA-based accelerators that outperform Nvidia Geforce RTX 2080 Ti with respect to throughput by up to 9.9 and Nvidia Jetson AGX Xavier with respect to energy efficiency by up to 48. Our accelerators achieve higher throughput, energy efficiency, and resource efficiency than FPGA-based implementations of convolutional neural networks (CNNs) for semantic segmentation tasks. For the handwritten digit recognition task, our FPGA implementations provide higher accuracy and can be considered as a solution when accuracy is a priority. Furthermore, they outperform earlier FPGA implementations of one-dimensional LSTMs with respect to throughput, energy efficiency, and resource efficiency.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = nov,
articleno = {2},
numpages = {35},
keywords = {Long short-term memory, LSTM, MD-LSTM, 2D-LSTM, FPGA, Zynq, MNIST, DIBCO, semantic segmentation, image segmentation, image binarization, hardware architecture, deep learning}
}

@article{10.1145/3473337,
author = {Pan, Yaoxin and Liang, Shangsong and Ren, Jiaxin and Meng, Zaiqiao and Zhang, Qiang},
title = {Personalized, Sequential, Attentive, Metric-Aware Product Search},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3473337},
doi = {10.1145/3473337},
abstract = {The task of personalized product search aims at retrieving a ranked list of products given a user’s input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9\% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {36},
numpages = {29},
keywords = {Product search, personalized web search, neural networks, LSTM, metric learning}
}

@article{10.1145/3476982,
author = {Parravicini, Daniele and Conficconi, Davide and Sozzo, Emanuele Del and Pilato, Christian and Santambrogio, Marco D.},
title = {CICERO: A Domain-Specific Architecture for Efficient Regular Expression Matching},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3476982},
doi = {10.1145/3476982},
abstract = {Regular Expression (RE) matching is a computational kernel used in several applications. Since RE complexity and data volumes are steadily increasing, hardware acceleration is gaining attention also for this problem. Existing approaches have limited flexibility as they require a different implementation for each RE. On the other hand, it is complex to map efficient RE representations like non-deterministic finite-state automata onto software-programmable engines or parallel architectures. In this work, we present CICERO&nbsp;, an end-to-end framework composed of a domain-specific architecture and a companion compilation framework for RE matching. Our solution is suitable for many applications, such as genomics/proteomics and natural language processing. CICERO aims at exploiting the intrinsic parallelism of non-deterministic representations of the REs. CICERO can trade-off accelerators’ efficiency and processors’ flexibility thanks to its programmable architecture and the compilation framework. We implemented CICERO prototypes on embedded FPGA achieving up to 28.6\texttimes{} and 20.8\texttimes{} more energy efficiency than embedded and mainstream processors, respectively. Since it is a programmable architecture, it can be implemented as a custom ASIC that is orders of magnitude more energy-efficient than mainstream processors.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {51},
numpages = {24},
keywords = {Domain-specific architecture, regular expressions, non-deterministic automata, energy efficiency}
}

@article{10.1145/3476995,
author = {Mendis, Hashan Roshantha and Kang, Chih-Kai and Hsiu, Pi-cheng},
title = {Intermittent-Aware Neural Architecture Search},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3476995},
doi = {10.1145/3476995},
abstract = {The increasing paradigm shift towards intermittent computing has made it possible to intermittently execute deep neural network (DNN) inference on edge devices powered by ambient energy. Recently, neural architecture search (NAS) techniques have achieved great success in automatically finding DNNs with high accuracy and low inference latency on the deployed hardware. We make a key observation, where NAS attempts to improve inference latency by primarily maximizing data reuse, but the derived solutions when deployed on intermittently-powered systems may be inefficient, such that the inference may not satisfy an end-to-end latency requirement and, more seriously, they may be unsafe given an insufficient energy budget. This work proposes iNAS, which introduces intermittent execution behavior into NAS to find accurate network architectures with corresponding execution designs, which can safely and efficiently execute under intermittent power. An intermittent-aware execution design explorer is presented, which finds the right balance between data reuse and the costs related to intermittent inference, and incorporates a preservation design search space into NAS, while ensuring the power-cycle energy budget is not exceeded. To assess an intermittent execution design, an intermittent-aware abstract performance model is presented, which formulates the key costs related to progress preservation and recovery during intermittent inference. We implement iNAS on top of an existing NAS framework and evaluate their respective solutions found for various datasets, energy budgets and latency requirements, on a Texas Instruments device. Compared to those NAS solutions that can safely complete the inference, the iNAS solutions reduce the intermittent inference latency by 60\% on average while achieving comparable accuracy, with an average 7\% increase in search overhead.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {64},
numpages = {27},
keywords = {Deep neural networks, neural architecture search, design space exploration, intermittent systems, energy harvesting, edge computing}
}

@article{10.1145/3484505,
author = {Fu, Yaosheng and Bolotin, Evgeny and Chatterjee, Niladrish and Nellans, David and Keckler, Stephen W.},
title = {GPU Domain Specialization via Composable On-Package Architecture},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3484505},
doi = {10.1145/3484505},
abstract = {As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a Composable On-PAckage GPU (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4\texttimes{} higher off-die bandwidth, 32\texttimes{} larger on-package cache, and 2.3\texttimes{} higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16\texttimes{} larger cache capacity and 1.6\texttimes{} higher DRAM bandwidth scales per-GPU training and inference performance by 31\% and 35\%, respectively, and reduces the number of GPU instances by 50\% in scale-out training scenarios.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {4},
numpages = {23},
keywords = {GPU computing, multi-chip module}
}

@article{10.1145/3485129,
author = {Meneguette, Rodolfo and De Grande, Robson and Ueyama, Jo and Filho, Geraldo P. Rocha and Madeira, Edmundo},
title = {Vehicular Edge Computing: Architecture, Resource Management, Security, and Challenges},
year = {2021},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485129},
doi = {10.1145/3485129},
abstract = {Vehicular Edge Computing (VEC), based on the Edge Computing motivation and fundamentals, is a promising technology supporting Intelligent Transport Systems services, smart city applications, and urban computing. VEC can provide and manage computational resources closer to vehicles and end-users, providing access to services at lower latency and meeting the minimum execution requirements for each service type. This survey describes VEC’s concepts and technologies; we also present an overview of existing VEC architectures, discussing them and exemplifying them through layered designs. Besides, we describe the underlying vehicular communication in supporting resource allocation mechanisms. With the intent to overview the risks, breaches, and measures in VEC, we review related security approaches and methods. Finally, we conclude this survey work with an overview and study of VEC’s main challenges. Unlike other surveys in which they are focused on content caching and data offloading, this work proposes a taxonomy based on the architectures in which VEC serves as the central element. VEC supports such architectures in capturing and disseminating data and resources to offer services aimed at a smart city through their aggregation and the allocation in a secure manner.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {4},
numpages = {46},
keywords = {Vehicular edge computer, resource management, security, architecture}
}

@article{10.1145/3485244,
author = {Zhang, Hu and Pan, Bangze and Li, Ru},
title = {Legal Judgment Elements Extraction Approach with Law Article-aware Mechanism},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3485244},
doi = {10.1145/3485244},
abstract = {Legal judgment elements extraction (LJEE) aims to identify the different judgment features from the fact description in legal documents automatically, which helps to improve the accuracy and interpretability of the judgment results. In real court rulings, judges usually need to scan both the fact descriptions and the law articles repeatedly to find out the relevant information, and it is hard to acquire the key judgment features quickly, so legal judgment elements extraction is a crucial and challenging task for legal judgment prediction. However, most existing methods follow the text classification framework, which fails to model the attentive relations of the law articles and the legal judgment elements. To address this issue, we simulate the working process of human judges, and propose a legal judgment elements extraction method with a law article-aware mechanism, which captures the complex semantic correlations of the law article and the legal judgment elements. Experimental results show that our proposed method achieves significant improvements than other state-of-the-art baselines on the element recognition task dataset. Compared with the BERT-CNN model, the proposed “All labels Law Articles Embedding Model (ALEM)” improves the accuracy, recall, and F1 value by 0.5, 1.4 and 1.0, respectively.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {55},
numpages = {15},
keywords = {Legal judgment elements extraction, fact description, law article-aware mechanism, wisdom justice, intelligent justice}
}

@article{10.1145/3485824,
author = {Zahedi, Mahdi and Lebdeh, Muah Abu and Bengel, Christopher and Wouters, Dirk and Menzel, Stephan and Le Gallo, Manuel and Sebastian, Abu and Wong, Stephan and Hamdioui, Said},
title = {MNEMOSENE: Tile Architecture and Simulator for Memristor-based Computation-in-memory},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1550-4832},
url = {https://doi.org/10.1145/3485824},
doi = {10.1145/3485824},
abstract = {In recent years, we are witnessing a trend toward in-memory computing for future generations of computers that differs from traditional von-Neumann architecture in which there is a clear distinction between computing and memory units. Considering that data movements between the central processing unit (CPU) and memory consume several orders of magnitude more energy compared to simple arithmetic operations in the CPU, in-memory computing will lead to huge energy savings as data no longer needs to be moved around between these units. In an initial step toward this goal, new non-volatile memory technologies, e.g., resistive RAM (ReRAM) and phase-change memory (PCM), are being explored. This has led to a large body of research that mainly focuses on the design of the memory array and its peripheral circuitry. In this article, we mainly focus on the tile architecture (comprising a memory array and peripheral circuitry) in which storage and compute operations are performed in the (analog) memory array and the results are produced in the (digital) periphery. Such an architecture is termed compute-in-memory-periphery (CIM-P). More precisely, we derive an abstract CIM-tile architecture and define its main building blocks. To bridge the gap between higher-level programming languages and the underlying (analog) circuit designs, an instruction-set architecture is defined that is intended to control and, in turn, sequence the operations within this CIM tile to perform higher-level more complex operations. Moreover, we define a procedure to pipeline the CIM-tile operations to further improve the performance. To simulate the tile and perform design space exploration considering different technologies and parameters, we introduce the fully parameterized first-of-its-kind CIM tile simulator and compiler. Furthermore, the compiler is technology-aware when scheduling the CIM-tile instructions. Finally, using the simulator, we perform several preliminary design space explorations regarding the three competing technologies, ReRAM, PCM, and STT-MRAM concerning CIM-tile parameters, e.g., the number of ADCs. Additionally, we investigate the effect of pipelining in relation to the clock speeds of the digital periphery assuming the three technologies. In the end, we demonstrate that our simulator is also capable of reporting energy consumption for each building block within the CIM tile after the execution of in-memory kernels considering the data-dependency on the energy consumption of the memory array. All the source codes are publicly available.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jan,
articleno = {44},
numpages = {24},
keywords = {Computation in-memory, memristor, architecture, simulator, ISA}
}

@article{10.1145/3487058,
author = {Morel, Lionel and Courouss\'{e}, Damien and Hiscock, Thomas},
title = {Code Polymorphism Meets Code Encryption: Confidentiality and Side-channel Protection of Software Components},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3487058},
doi = {10.1145/3487058},
abstract = {In this article, we consider that, in practice, attack scenarios involving side-channel analysis combine two successive phases: an analysis phase, targeting the extraction of information about the target and the identification of possible vulnerabilities, and an exploitation phase, applying attack techniques on candidate vulnerabilities. We advocate that protections need to cover these two phases to be effective against real-life attacks. We present PolEn, a toolchain and a processor architecture that combine countermeasures to provide an effective mitigation of side-channel attacks: As a countermeasure against the analysis phase, our approach considers the use of code encryption; as a countermeasure against the exploitation phase, our approach considers the use of code polymorphism, because it relies on runtime code generation, and its combination with code encryption is particularly challenging. Code encryption is supported by a processor extension such that machine instructions are only decrypted inside the CPU, which effectively prevents reverse engineering or any extraction of useful information from memory dumps. Code polymorphism is implemented by software means. It regularly changes the observable behaviour of the program, making it unpredictable for an attacker, hence reducing the possibility to exploit side-channel leakages. We present a prototype implementation, based on the RISC-V Spike simulator and a modified LLVM toolchain. In our experimental evaluation, we illustrate that PolEn effectively reduces side-channel leakages. For the protected functions evaluated, static memory use increases by a factor of 5 to 22, corresponding to the joint application of code encryption and code polymorphism. The overhead, in terms of execution time, ranges between a factor of 1.8 and 4.6.},
journal = {Digital Threats},
month = mar,
articleno = {18},
numpages = {27},
keywords = {Side-channel, code encryption, code polymorphism}
}

@article{10.1145/3490396,
author = {Miksa, Tomasz and Oblasser, Simon and Rauber, Andreas},
title = {Automating Research Data Management Using Machine-Actionable Data Management Plans},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3490396},
doi = {10.1145/3490396},
abstract = {Many research funders mandate researchers to create and maintain data management plans (DMPs) for research projects that describe how research data is managed to ensure its reusability. A DMP, being a static textual document, is difficult to act upon and can quickly become obsolete and impractical to maintain. A new generation of machine-actionable DMPs (maDMPs) was therefore proposed by the Research Data Alliance to enable automated integration of information and updates. maDMPs open up a variety of use cases enabling interoperability of research systems and automation of data management tasks.In this article, we describe a system for machine-actionable data management planning in an institutional context. We identify common use cases within research that can be automated to benefit from machine-actionability of DMPs. We propose a reference architecture of an maDMP support system that can be embedded into an institutional research data management infrastructure. The system semi-automates creation and maintenance of DMPs, and thus eases the burden for the stakeholders responsible for various DMP elements. We evaluate the proposed system in a case study conducted at the largest technical university in Austria and quantify to what extent the DMP templates provided by the European Commission and a national funding body can be pre-filled. The proof-of-concept implementation shows that maDMP workflows can be semi-automated, thus workload on involved parties can be reduced and quality of information increased. The results are especially relevant to decision makers and infrastructure operators who want to design information systems in a systematic way that can utilize the full potential of maDMPs.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {22},
keywords = {Data management plan, machine-actionable, business processes, enterprise architecture, funder template, requirements engineering, automation, RDM, RDA, FAIR}
}

@article{10.1145/3491199,
author = {Qin, Le and Peng, Fei and Long, Min and Ramachandra, Raghavendra and Busch, Christoph},
title = {Vulnerabilities of Unattended Face Verification Systems to Facial Components-based Presentation Attacks: An Empirical Study},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3491199},
doi = {10.1145/3491199},
abstract = {As face presentation attacks (PAs) are realistic threats for unattended face verification systems, face presentation attack detection (PAD) has been intensively investigated in past years, and the recent advances in face PAD have significantly reduced the success rate of such attacks. In this article, an empirical study on a novel and effective face impostor PA is made. In the proposed PA, a facial artifact is created by using the most vulnerable facial components, which are optimally selected based on the vulnerability analysis of different facial components to impostor PAs. An attacker can launch a face PA by presenting a facial artifact on his or her own real face. With a collected PA database containing various types of artifacts and presentation attack instruments (PAIs), the experimental results and analysis show that the proposed PA poses a more serious threat to face verification and PAD systems compared with the print, replay, and mask PAs. Moreover, the generalization ability of the proposed PA and the vulnerability analysis with regard to commercial systems are also investigated by evaluating unknown face verification and real-world PAD systems. It provides a new paradigm for the study of face PAs.},
journal = {ACM Trans. Priv. Secur.},
month = nov,
articleno = {4},
numpages = {28},
keywords = {Face verification, presentation attack, presentation attack detection, facial components}
}

@article{10.1145/3492762,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Kazman, Rick},
title = {Continuous and Proactive Software Architecture Evaluation: An IoT Case},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3492762},
doi = {10.1145/3492762},
abstract = {Design-time evaluation is essential to build the initial software architecture to be deployed. However, experts’ assumptions made at design-time are unlikely to remain true indefinitely in systems that are characterized by scale, hyperconnectivity, dynamism, and uncertainty in operations (e.g. IoT). Therefore, experts’ design-time decisions can be challenged at run-time. A continuous architecture evaluation that systematically assesses and intertwines design-time and run-time decisions is thus necessary. This paper proposes the first proactive approach to continuous architecture evaluation of the system leveraging the support of simulation. The approach evaluates software architectures by not only tracking their performance over time, but also forecasting their likely future performance through machine learning of simulated instances of the architecture. This enables architects to make cost-effective informed decisions on potential changes to the architecture. We perform an IoT case study to show how machine learning on simulated instances of architecture can fundamentally guide the continuous evaluation process and influence the outcome of architecture decisions. A series of experiments is conducted to demonstrate the applicability and effectiveness of the approach. We also provide the architect with recommendations on how to best benefit from the approach through choice of learners and input parameters, grounded on experimentation and evidence.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {46},
numpages = {54},
keywords = {Continuous evaluation, software architecture evaluation, time series forecasting, IoT}
}

@article{10.1145/3494523,
author = {Wu, Nan and Xie, Yuan},
title = {A Survey of Machine Learning for Computer Architecture and Systems},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494523},
doi = {10.1145/3494523},
abstract = {It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {54},
numpages = {39},
keywords = {Machine learning for computer architecture, machine learning for systems}
}

@article{10.1145/3495263,
author = {Rest, Christopher and Fisseler, Denis and Weichert, Frank and Somel, Turna and M\"{u}ller, Gerfrid G. W.},
title = {Illumination-based Augmentation for Cuneiform Deep Neural Sign Classification},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3495263},
doi = {10.1145/3495263},
abstract = {Automated content-based search for arbitrary cuneiform signs in photographic reproductions is a challenging task in the analysis of ancient documents, a central component of which is a reliable cuneiform sign classification. We present an illumination-based approach to generate synthetic training data for cuneiform sign classification via deep neural networks to overcome common issues with the transferability of machine learning training results. Starting from an analysis of the negative impact of illumination variations in the processed cuneiform data, we employ an illumination augmentation to two-dimensional (2D) training data generated from annotated 3D datasets. We demonstrate that our method is able to overcome the high visual variance of most digitized 2D cuneiform reproductions and achieve an illumination invariant generalization. The effectiveness of our approach is evaluated by its successful application to several subsets of a cuneiform script dataset with an originally poor transferability of mutual training results. Furthermore, we show that a sufficient sampling of the illumination space mostly removes the necessity to match the training data to specific target illumination conditions. The practical applicability of our approach is validated by applying it to a larger dataset, raising the overall classification accuracy by 4 percentage points to 90\%, resulting in a classification error reduction of 28.5\% when compared to results without the proposed data augmentation.},
journal = {J. Comput. Cult. Herit.},
month = sep,
articleno = {50},
numpages = {20},
keywords = {Cuneiform, sign classification, datasets, neural networks, data augmentation, illumination, 2D-3D cross domain}
}

@article{10.1145/3499424,
author = {Lakshminarasimhan, Kartik and Naithani, Ajeya and Feliu, Josu\'{e} and Eeckhout, Lieven},
title = {The Forward Slice Core: A High-Performance, Yet Low-Complexity Microarchitecture},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3499424},
doi = {10.1145/3499424},
abstract = {Superscalar out-of-order cores deliver high performance at the cost of increased complexity and power budget. In-order cores, in contrast, are less complex and have a smaller power budget, but offer low performance. A processor architecture should ideally provide high performance in a power- and cost-efficient manner. Recently proposed slice-out-of-order (sOoO) cores identify backward slices of memory operations which they execute out-of-order with respect to the rest of the dynamic instruction stream for increased instruction-level and memory-hierarchy parallelism. Unfortunately, constructing backward slices is imprecise and hardware-inefficient, leaving performance on the table.In this article, we propose Forward Slice Core (FSC), a novel core microarchitecture that builds on a stall-on-use in-order core and extracts more instruction-level and memory-hierarchy parallelism than slice-out-of-order cores. FSC does so by identifying and steering forward slices (rather than backward slices) to dedicated in-order FIFO queues. Moreover, FSC puts load-consumers that depend on L1 D-cache misses on the side to enable younger independent load-consumers to execute faster. Finally, FSC eliminates the need for dynamic memory disambiguation by replicating store-address instructions across queues. Considering 3-wide pipeline configurations, we find that FSC improves performance by 27.1\%, 21.1\%, and 14.6\% on average compared to Freeway, the state-of-the-art sOoO core, across SPEC CPU2017, GAP, and DaCapo, respectively, while at the same time incurring reduced hardware complexity. Compared to an OoO core, FSC reduces power consumption by 61.3\% and chip area by 47\%, providing a microarchitecture with high performance at low complexity.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {17},
numpages = {25},
keywords = {Superscalar microarchitecture, slice-out-of-order, dynamic instruction scheduling}
}

@article{10.1145/3501813,
author = {Antunes, Rodolfo Stoffel and Andr\'{e} da Costa, Cristiano and K\"{u}derle, Arne and Yari, Imrana Abdullahi and Eskofier, Bj\"{o}rn},
title = {Federated Learning for Healthcare: Systematic Review and Architecture Proposal},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3501813},
doi = {10.1145/3501813},
abstract = {The use of machine learning (ML) with electronic health records (EHR) is growing in popularity as a means to extract knowledge that can improve the decision-making process in healthcare. Such methods require training of high-quality learning models based on diverse and comprehensive datasets, which are hard to obtain due to the sensitive nature of medical data from patients. In this context, federated learning (FL) is a methodology that enables the distributed training of machine learning models with remotely hosted datasets without the need to accumulate data and, therefore, compromise it. FL is a promising solution to improve ML-based systems, better aligning them to regulatory requirements, improving trustworthiness and data sovereignty. However, many open questions must be addressed before the use of FL becomes widespread. This article aims at presenting a systematic literature review on current research about FL in the context of EHR data for healthcare applications. Our analysis highlights the main research topics, proposed solutions, case studies, and respective ML methods. Furthermore, the article discusses a general architecture for FL applied to healthcare data based on the main insights obtained from the literature review. The collected literature corpus indicates that there is extensive research on the privacy and confidentiality aspects of training data and model sharing, which is expected given the sensitive nature of medical data. Studies also explore improvements to the aggregation mechanisms required to generate the learning model from distributed contributions and case studies with different types of medical data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {54},
numpages = {23},
keywords = {Electronic health records, federated learning, systematic review}
}

@article{10.1145/3502740,
author = {Ahmadjee, Sabreen and Mera-G\'{o}mez, Carlos and Bahsoon, Rami and Kazman, Rick},
title = {A Study on Blockchain Architecture Design Decisions and Their Security Attacks and Threats},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3502740},
doi = {10.1145/3502740},
abstract = {Blockchain is a disruptive technology intended to implement secure decentralised distributed systems, in which transactional data can be shared, stored, and verified by participants of the system without needing a central authentication/verification authority. Blockchain-based systems have several architectural components and variants, which architects can leverage to build secure software systems. However, there is a lack of studies to assist architects in making architecture design and configuration decisions for blockchain-based systems. This knowledge gap may increase the chance of making unsuitable design decisions and producing configurations prone to potential security risks. To address this limitation, we report our comprehensive systematic literature review to derive a taxonomy of commonly used architecture design decisions in blockchain-based systems. We map each of these decisions to potential security attacks and their posed threats. MITRE’s attack tactic categories and Microsoft STRIDE threat modeling are used to systematically classify threats and their associated attacks to identify potential attacks and threats in blockchain-based systems. Our mapping approach aims to guide architects to make justifiable design decisions that will result in more secure implementations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {36e},
numpages = {45},
keywords = {Blockchain, security threat classification, architecture decision, design decisions}
}

@article{10.1145/3503465,
author = {Roorda, Esther and Rasoulinezhad, Seyedramin and Leong, Philip H. W. and Wilton, Steven J. E.},
title = {FPGA Architecture Exploration for DNN Acceleration},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3503465},
doi = {10.1145/3503465},
abstract = {Recent years have seen an explosion of machine learning applications implemented on Field-Programmable Gate Arrays (FPGAs). FPGA vendors and researchers have responded by updating their fabrics to more efficiently implement machine learning accelerators, including innovations such as enhanced Digital Signal Processing (DSP) blocks and hardened systolic arrays. Evaluating architectural proposals is difficult, however, due to the lack of publicly available benchmark circuits.This paper addresses this problem by presenting an open-source benchmark circuit generator that creates realistic DNN-oriented circuits for use in FPGA architecture studies. Unlike previous generators, which create circuits that are agnostic of the underlying FPGA, our circuits explicitly instantiate embedded blocks, allowing for meaningful comparison of recent architectural proposals without the need for a complete inference computer-aided design (CAD) flow. Our circuits are compatible with the VTR CAD suite, allowing for architecture studies that investigate routing congestion and other low-level architectural implications.In addition to addressing the lack of machine learning benchmark circuits, the architecture exploration flow that we propose allows for a more comprehensive evaluation of FPGA architectures than traditional static benchmark suites. We demonstrate this through three case studies which illustrate how realistic benchmark circuits can be generated to target different heterogeneous FPGAs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = may,
articleno = {33},
numpages = {37},
keywords = {FPGA architecture, neural networks, benchmarking, hardware acceleration}
}

@article{10.1145/3503925,
author = {Trotter, James D. and Cai, Xing and Funke, Simon W.},
title = {On Memory Traffic and Optimisations for Low-order Finite Element Assembly Algorithms on Multi-core CPUs},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3503925},
doi = {10.1145/3503925},
abstract = {Motivated by the wish to understand the achievable performance of finite element assembly on unstructured computational meshes, we dissect the standard cellwise assembly algorithm into four kernels, two of which are dominated by irregular memory traffic. Several optimisation schemes are studied together with associated lower and upper bounds on the estimated memory traffic volume. Apart from properly reordering the mesh entities, the two most significant optimisations include adopting a lookup table in adding element matrices or vectors to their global counterparts, and using a row-wise assembly algorithm for multi-threaded parallelisation. Rigorous benchmarking shows that, due to the various optimisations, the actual volumes of memory traffic are in many cases very close to the estimated lower bounds. These results confirm the effectiveness of the optimisations, while also providing a recipe for developing efficient software for finite element assembly.},
journal = {ACM Trans. Math. Softw.},
month = may,
articleno = {19},
numpages = {31},
keywords = {Finite element methods, assembly, multi-core, Intel Xeon, AMD Epyc, Cavium TX2}
}

@article{10.1145/3506704,
author = {Kumar, Rakesh and Alipour, Mehdi and Black-Schaffer, David},
title = {Dependence-aware Slice Execution to Boost MLP in Slice-out-of-order Cores},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3506704},
doi = {10.1145/3506704},
abstract = {Exploiting memory-level parallelism (MLP) is crucial to hide long memory and last-level cache access latencies. While out-of-order (OoO) cores, and techniques building on them, are effective at exploiting MLP, they deliver poor energy efficiency due to their complex and energy-hungry hardware. This work revisits slice-out-of-order (sOoO) cores as an energy-efficient alternative for MLP exploitation. sOoO cores achieve energy efficiency by constructing and executing slices of MLP-generating instructions out-of-order only with respect to the rest of instructions; the slices and the remaining instructions, by themselves, execute in-order. However, we observe that existing sOoO cores miss significant MLP opportunities due to their dependence-oblivious in-order slice execution, which causes dependent slices to frequently block MLP generation. To boost MLP generation, we introduce Freeway, a sOoO core based on a new dependence-aware slice execution policy that tracks dependent slices and keeps them from blocking subsequent independent slices and MLP extraction. The proposed core incurs minimal area and power overheads, yet approaches the MLP benefits of fully OoO cores. Our evaluation shows that Freeway delivers 12\% better performance than the state-of-the-art sOoO core and is within 7\% of the MLP limits of full OoO execution.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {25},
numpages = {28},
keywords = {Microarchitecture, memory level parallelism, instruction scheduling}
}

@article{10.1145/3506719,
author = {Gupta, Mehak and Phan, Thao-Ly T. and Bunnell, H. Timothy and Beheshti, Rahmatollah},
title = {Obesity Prediction with EHR Data: A Deep Learning Approach with Interpretable Elements},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3506719},
doi = {10.1145/3506719},
abstract = {Childhood obesity is a major public health challenge. Early prediction and identification of the children at an elevated risk of developing childhood obesity may help in engaging earlier and more effective interventions to prevent and manage obesity. Most existing predictive tools for childhood obesity primarily rely on traditional regression-type methods using only a few hand-picked features and without exploiting longitudinal patterns of children’s data. Deep learning methods allow the use of high-dimensional longitudinal datasets. In this article, we present a deep learning model designed for predicting future obesity patterns from generally available items on children’s medical history. To do this, we use a large unaugmented electronic health records dataset from a large pediatric health system in the United States. We adopt a general LSTM network architecture and train our proposed model using both static and dynamic EHR data. To add interpretability, we have additionally included an attention layer to calculate the attention scores for the timestamps and rank features of each timestamp. Our model is used to predict obesity for ages between 3 and 20 years using the data from 1 to 3 years in advance. We compare the performance of our LSTM model with a series of existing studies in the literature and show it outperforms their performance in most age ranges.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {32},
numpages = {19},
keywords = {Childhood obesity, electronic health records, temporal data, deep learning, long short-term memory, transfer learning}
}

@article{10.1145/3507700,
author = {Wen, Chenyi and Dong, Xiao and Chen, Baixin and Tida, Umamaheswara Rao and Shi, Yiyu and Zhuo, Cheng},
title = {Magnetic Core TSV-Inductor Design and Optimization for On-chip DC-DC Converter},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3507700},
doi = {10.1145/3507700},
abstract = {The conventional on-chip spiral inductor consumes a significant top-metal routing area, thereby preventing its popularity in many on-chip applications. Recently through-silicon-via– (TSV) based inductor (also known as a TSV-inductor) with a magnetic core has been proved to be a viable option for the on-chip DC-DC converter. The operating conditions of these inductors play a major role in maximizing the performance and efficiency of the DC-DC converter. However, there is a critical need to study the design and optimization details of magnetic core TSV-inductors with the unique three-dimensional structure embedding magnetic core. This article aims to provide a clear understanding of the modeling details of a magnetic core TSV-inductor and a design and optimization methodology to assist efficient inductor design. Moreover, a machine learning–assisted model combining physical details and artificial neural network is also proposed to extract the equivalent circuit to further facilitate DC-DC converter design. Experimental results show that the optimized TSV-inductor with the magnetic core and air-gap can achieve inductance density improvement of up to 7.7 ( times )  and quality factor improvements of up to 1.6 ( times )  for the same footprint compared with the TSV-inductor without a magnetic core. For on-chip DC-DC converter applications, the converter efficiency can be improved by up to 15.9\% and 6.8\% compared with the conventional spiral and TSV-inductor without magnetic core, respectively.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {52},
numpages = {23},
keywords = {Magnetic core TSV-inductor, equivalent circuit, air gap, DC-DC converter}
}

@article{10.1145/3507902,
author = {Du, Hang and Shi, Hailin and Zeng, Dan and Zhang, Xiao-Ping and Mei, Tao},
title = {The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3507902},
doi = {10.1145/3507902},
abstract = {Face recognition (FR) is one of the most popular and long-standing topics in computer vision. With the recent development of deep learning techniques and large-scale datasets, deep face recognition has made remarkable progress and has been widely used in many real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, a typical end-to-end system is built with three key elements: face detection, face alignment, and face representation. Face detection locates faces in the image or frame. Then, the face alignment is proceeded to calibrate the faces to the canonical view and crop them with a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the aligned face for recognition. Nowadays, all of the three elements are fulfilled by the technique of deep convolutional neural network. In this survey article, we present a comprehensive review about the recent advance of each element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved their capability of them. To start with, we present an overview of the end-to-end deep face recognition. Then, we review the advance of each element, respectively, covering many aspects such as the to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. Also, we provide a detailed discussion about the effect of each element on its subsequent elements and the holistic system. Through this survey, we wish to bring contributions in two aspects: first, readers can conveniently identify the methods which are quite strong-baseline style in the subcategory for further exploration; second, one can also employ suitable methods for establishing a state-of-the-art end-to-end face recognition system from scratch.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {212},
numpages = {42},
keywords = {Deep learning, convolutional neural network, face recognition, face detection, face alignment, face representation}
}

@article{10.1145/3508360,
author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, Bingsheng and Guo, Minyi},
title = {The Serverless Computing Survey: A Technical Primer for Design Architecture},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3508360},
doi = {10.1145/3508360},
abstract = {The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {220},
numpages = {34},
keywords = {Serverless computing, architecture design, FaaS, Lambda paradigm}
}

@article{10.1145/3508395,
author = {Ren, Bin and Chen, Yuquiang and Wang, Fujie},
title = {Application Massive Data Processing Platform for Smart Manufacturing Based on Optimization of Data Storage},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3508395},
doi = {10.1145/3508395},
abstract = {The aim of smart manufacturing is to reduce manpower requirements of the production line by applying technology of huge amounts of data to the manufacturing industry. Smart manufacturing is also called Industry 4.0, and the platform for processing huge amounts of data has an indispensable role. The massive data processing platform is like the brain of the entire factory, receiving all data from production line sensors via edge computing, processing, and analyzing, and finally making feedback decisions. With the innovation of production technology, the data that the platform needs to process has become diverse and complex, and the amount has become increasingly large. As well, many precision manufacturing industries have begun to enter the field of Industry 4.0. In addition to the accuracy and availability of data processing, there is emphasis on the real-time nature of data processing. After the sensor receives the data, the platform must provide feedback within a short period of time. This article proposes a massive data processing platform based on the Lambda architecture, which has the coexistence of stream processing and batch processing to meet real-time feedback needs of high-precision manufacturing. To verify the effectiveness of the optimization, it is based on real data from the manufacturing industry. To generate a large amount of test data to confirm the optimization of the storage of pictures. The results show that it optimizes the storage and optimization of the image data generated by the Automated Optical Inspection technology used in manufacturing today and optimizes the query for data storage. It also reduces the consumption of a large amount of memory as expected, and the query for Hive reduced the time spent.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {37},
numpages = {20},
keywords = {Smart manufacturing, Industry 4.0, massive data processing platform, Hadoop}
}

@article{10.1145/3510854,
author = {Lee, Jeong-Jun and Zhang, Wenrui and Xie, Yuan and Li, Peng},
title = {SaARSP: An Architecture for Systolic-Array Acceleration of Recurrent Spiking Neural Networks},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3510854},
doi = {10.1145/3510854},
abstract = {Spiking neural networks (SNNs) are brain-inspired event-driven models of computation with promising ultra-low energy dissipation. Rich network dynamics emergent in recurrent spiking neural networks (R-SNNs) can form temporally based memory, offering great potential in processing complex spatiotemporal data. However, recurrence in network connectivity produces tightly coupled data dependency in both space and time, rendering hardware acceleration of R-SNNs challenging. We present the first work to exploit spatiotemporal parallelisms to accelerate the R-SNN-based inference on systolic arrays using an architecture called SaARSP. We decouple the processing of feedforward synaptic connections from that of recurrent connections to allow for the exploitation of parallelisms across multiple time points. We propose a novel time window size optimization (TWSO) technique, to further explore the temporal granularity of the proposed decoupling in terms of optimal time window size and reconfiguration of the systolic array considering layer-dependent connectivity to boost performance. Stationary dataflow and time window size are jointly optimized to trade off between weight data reuse and movements of partial sums, the two bottlenecks in latency and energy dissipation of the accelerator. The proposed systolic-array architecture offers a unifying solution to an acceleration of both feedforward and recurrent SNNs, and delivers 4,000X EDP improvement on average for different R-SNN benchmarks over a conventional baseline.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = oct,
articleno = {68},
numpages = {23},
keywords = {Spiking neural networks, accelerators, computer architecture}
}

@article{10.1145/3511102,
author = {Gao, Yingying and Janssen, Marijn},
title = {The Open Data Canvas–Analyzing Value Creation from Open Data},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3511102},
doi = {10.1145/3511102},
abstract = {Expectations to derive value from open data are high. However, how value is created from open data is still largely unknown. Open data value is usually generated in constellation of actors in which each player has different capabilities and roles. To understand the open data value creation process, the business model canvas is introduced in this article. The typical components of the business model canvas and open data value creation are derived from the literature. By combining these two research streams, the open data value model canvas is created. The case of Coronavirus disease 2019 (COVID-19) worldwide dashboard developed by the Johns Hopkins University is used to evaluate the model's utility. Key components of the open data value model are creating an overview of various data sources from public and private organizations, having capabilities to combine heterogeneous data, and connecting data and needs. In this way, the open data canvas helps to grasp the value creation logic.},
journal = {Digit. Gov.: Res. Pract.},
month = mar,
articleno = {5},
numpages = {15},
keywords = {Open data, value creation, business model canvas, design science research, data infomediaries, open data canvas}
}

@article{10.1145/3511211,
author = {Chen, Lei and Zhao, Jiacheng and Wang, Chenxi and Cao, Ting and Zigman, John and Volos, Haris and Mutlu, Onur and Lv, Fang and Feng, Xiaobing and Xu, Guoqing Harry and Cui, Huimin},
title = {Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3511211},
doi = {10.1145/3511211},
abstract = {To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed runtime that already performs various dimensions of memory management. Supporting hybrid physical memories adds a new dimension, creating unique challenges in data replacement. This article proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division information is accurate enough to guide the GC for data layout, which hardly incurs overhead in data monitoring and moving. We implemented Panthera in OpenJDK and Apache Spark. Based on Big Data applications’ memory access pattern, we also implemented a new profiling-guided optimization strategy, which is transparent to applications. With this optimization, our extensive evaluation demonstrates that Panthera reduces energy by 32–53\% at less than 1\% time overhead on average. To show Panthera’s applicability, we extend it to QuickCached, a pure Java implementation of Memcached. Our evaluation results show that Panthera reduces energy by 28.7\% at 5.2\% time overhead on average.},
journal = {ACM Trans. Comput. Syst.},
month = jul,
articleno = {2},
numpages = {38},
keywords = {Hybrid memories, Big Data systems, memory management, garbage collection}
}

@article{10.1145/3513085,
author = {Lee, Jooyeon and Park, Junsang and Lee, Seunghyun and Kung, Jaeha},
title = {Implication of Optimizing NPU Dataflows on Neural Architecture Search for Mobile Devices},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3513085},
doi = {10.1145/3513085},
abstract = {Recent advances in deep learning have made it possible to implement artificial intelligence in mobile devices. Many studies have put a lot of effort into developing lightweight deep learning models optimized for mobile devices. To overcome the performance limitations of manually designed deep learning models, an automated search algorithm, called neural architecture search (NAS), has been proposed. However, studies on the effect of hardware architecture of the mobile device on the performance of NAS have been less explored. In this article, we show the importance of optimizing a hardware architecture, namely, NPU dataflow, when searching for a more accurate yet fast deep learning model. To do so, we first implement an optimization framework, named FlowOptimizer, for generating a best possible NPU dataflow for a given deep learning operator. Then, we utilize this framework during the latency-aware NAS to find the model with the highest accuracy satisfying the latency constraint. As a result, we show that the searched model with FlowOptimizer outperforms the performance by 87.1\% and 92.3\% on average compared to the searched model with NVDLA and Eyeriss, respectively, with better accuracy on a proxy dataset. We also show that the searched model can be transferred to a larger model to classify a more complex image dataset, i.e., ImageNet, achieving 0.2\%/5.4\% higher Top-1/Top-5 accuracy compared to MobileNetV2-1.0 with 3.6 ( times )  lower latency.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {48},
numpages = {24},
keywords = {Dataflow optimization, neural networks, neural architecture search, neural processing unit}
}

@article{10.1145/3513136,
author = {Jang, Sun-Young and Kim, Sung-Ah},
title = {Content Curation for Spatial Experience of Architectural Heritage},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3513136},
doi = {10.1145/3513136},
abstract = {Augmented space can be usefully applied to cultural heritage experience, because it is possible to experience physical space and acquire information at the same time. Experiencers can expand their experiences through virtual content while feeling the presence of relics or historic sites. The cultural heritage experiences currently implemented show simple forms in terms of their content and delivery method. Many cases repeatedly provide the same content regardless of the experience in a specific location. The present research, titled “the spatial-experience model of architectural heritage,” derived a methodology for construction of spatial experiences augmented by digital technology. Specifically, it deals with the problem of content curation and the limitations of the information system, which can be seen in the spatial-experience composition methods in the current digital heritage field. The information structure of the model was improved to enable spatial-oriented explanation of the expression of architectural heritage, which was insufficient in the existing metadata and ontology structures. The model has inference rules for creating an experience sequence and composes various combinations of experience content based on historical knowledge relationships. The model and reasoning rules were tested by applying them to real architectural heritage cases. Based on the test results, the role of the information structure in the spatial expression of its combination with content, as well as the possibility of diversification of sequence derivation, were considered. This research presents a syntax for construction of a semantic-rich environment while extending historical knowledge and spatial expression to the existing cultural heritage information system. In addition, by means of the content generator, which is a model for producing content using the extended descriptive language system, it is possible to create experience content in response to the increasing amount and demands of data. By applying the proposed model, it will be possible to quickly and easily derive various experience contents from the cultural heritage space, thereby reducing cost and time for content producers and providing an evolved spatial experience for consumers.},
journal = {J. Comput. Cult. Herit.},
month = mar,
articleno = {69},
numpages = {41},
keywords = {Architectural heritage, digital heritage, augmented reality, experience design, content curation}
}

@article{10.1145/3517130,
author = {Jiang, Yiyang and Yang, Fan and Yu, Bei and Zhou, Dian and Zeng, Xuan},
title = {Efficient Layout Hotspot Detection via Neural Architecture Search},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {6},
issn = {1084-4309},
url = {https://doi.org/10.1145/3517130},
doi = {10.1145/3517130},
abstract = {Layout hotspot detection is of great importance in the physical verification flow. Deep neural network models have been applied to hotspot detection and achieved great success. Despite their success, high-performance neural networks are still quite difficult to design. In this article, we propose a bayesian optimization-based neural architecture search scheme to automatically do this time-consuming and fiddly job. Experimental results on ICCAD 2012 and ICCAD 2019 Contest benchmarks show that the architectures designed by our proposed scheme achieve higher performance on hotspot detection task compared with state-of-the-art manually designed neural networks.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {62},
numpages = {16},
keywords = {Hotspot detection, neural architecture search}
}

@article{10.1145/3517805,
author = {Yinying, Cai and Li, Juan and Wang, Bo},
title = {Data Mining Techniques and Machine Learning Algorithms in the Multimedia System to Enhance Engineering Education},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517805},
doi = {10.1145/3517805},
abstract = {In the current digital era, engineering education worldwide faces a massive challenge in education and career development. By authorizing educators and administrators to migrate to the actions, cloud services technology has transformed into the educational environment. A Multimedia assisted smart learning system (MSLS) has been suggested in this paper where universities/colleges will advocate future development and begin skill-set enhancement courses by e-learning. To classify their employment prospects at the early stage of graduation, this proposed system measures learners' academic/skill data. Machine learning and Data mining are advanced research fields whose accelerated advancement is attributable to developments in data processing research, database industry growth, and business requirements for methods capable of extracting useful information from massive data stores. In addition, for skill set evaluation, a practical algorithm is suggested to find different groups of students that lack the appropriate skill set. The anticipated student groups can be provided with opportunities by e-learning to enhance their required skill set. The findings suggest that more critical choices can boost employment prospects and overall educational development by implementing the new engineering education system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {112},
numpages = {21},
keywords = {Machine learning, data mining, multimedia system, engineering education}
}

@article{10.1145/3519419,
author = {Mazilu, Lacramioara and Paton, Norman W. and Konstantinou, Nikolaos and Fernandes, Alvaro A. A.},
title = {Fairness-aware Data Integration},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3519419},
doi = {10.1145/3519419},
abstract = {Machine learning can be applied in applications that take decisions that impact people’s lives. Such techniques have the potential to make decision making more objective, but there also is a risk that the decisions can discriminate against certain groups as a result of bias in the underlying data. Reducing bias, or promoting fairness, has been a focus of significant investigation in machine learning, for example, based on pre-processing the training data, changing the learning algorithm, or post-processing the results of the learning. However, prior to these activities, data integration discovers and integrates the data that is used for training, and data integration processes have the potential to produce data that leads to biased conclusions. In this article, we propose an approach that generates schema mappings in ways that take into account: (i) properties that are intrinsic to mapping results that may give rise to bias in analyses; and (ii) bias observed in classifiers trained on the results of different sets of mappings. The approach explores a space of different ways of integrating the data, using a Tabu search algorithm, guided by bias-aware objective functions that represent different types of bias.The resulting approach is evaluated using Adult Census and German Credit datasets to explore the extent to which and the circumstances in which the approach can increase the fairness of the results of the data integration process.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {28},
numpages = {26},
keywords = {Data integration, data preparation, fairness, bias}
}

@article{10.1145/3519599,
author = {Shi, Kaichuang and Zhou, Xuegong and Zhou, Hao and Wang, Lingli},
title = {An Optimized GIB Routing Architecture with Bent Wires for FPGA},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3519599},
doi = {10.1145/3519599},
abstract = {Field-programmable gate arrays (FGPAs) are widely used because of the superiority in flexibility and lower non-recurring engineering cost. How to optimize the routing architecture is a key problem for FPGA architects because it has a large impact on FPGA area, delay, and routability. In academia, the routing architecture is mainly based on the connection blocks (CBs) and switch blocks (SBs), whereas most research has focused on SB architectures, such as Wilton, Universal, and Disjoint SB patterns. In this article, we propose a novel unidirectional routing architecture—general interconnection block (GIB)—to improve FPGA performance. With the GIB architecture, logic block (LB) pins can directly connect with the adjacent GIBs without programmable switches. Inside a GIB, LB pins can connect to the routing channel tracks on the four sides of a GIB. In particular, the logic pins from different neighboring LBs that connect to the same GIB can connect with each other with only one programmable switch. In addition, we enhance VTR to support the GIB with bent wires and develop a searching framework based on the simulated annealing algorithm to search for a near-optimal distribution of wire types. We evaluate the GIB architecture on VTR 8 with the provided benchmark circuits. The experimental results show that the GIB architecture with length-4 wires can achieve 9.5\% improvement on the critical path delay and 11.1\% improvement on the area-delay product compared to the VTR CB-SB architecture with length-4 wires. After exploring mixed wire types, the optimized GIB architecture can further improve the delay by 16.4\% and area-delay product by 17.1\% compared to the CB-SB architecture with length-4 wires.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {2},
numpages = {28},
keywords = {Routing architecture, connection block, switch block}
}

@article{10.1145/3520197,
author = {Langhammer, Martin and Nurvitadhi, Eriko and Gribok, Sergey and Pasca, Bogdan},
title = {Stratix 10 NX Architecture},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3520197},
doi = {10.1145/3520197},
abstract = {The advent of AI has driven the exploration of high-density low-precision arithmetic on FPGAs. This has resulted in new methods in mapping both arithmetic functions as well as dataflows onto the fabric, as well as some changes to the embedded DSP Blocks. Technologies outside of the FPGA realm have also evolved, such as the addition of tensor structures for GPUs, as well as the introduction of numerous AI ASSPs, all of which have a higher claimed performance and efficiency than current FPGAs. In this article, we will introduce the Stratix 10 NX device, which is a variant of FPGA specifically optimized for the AI application space. In addition to the computational capabilities of the standard programmable soft-logic fabric, a new type of DSP Block provides the dense arrays of low-precision multipliers typically used in AI implementations. The architecture of the block is tuned for the common matrix-matrix or vector-matrix multiplications in AI, with capabilities designed to work efficiently for both small and large matrix sizes. The base precisions are INT8 and INT4, along with shared exponent support to support block FP16 and block FP12 numerics. All additions/accumulations can be done in INT32 or IEEE-754 single precision floating point (FP32), and multiple blocks can be cascaded together to support larger matrices. We will also describe methods by which the smaller precision multipliers can be aggregated to create larger multipliers that are more applicable to standard signal processing requirements.In the AI market, the FPGA must compete directly with other types of devices, rather than occupy a unique niche. Deterministic system performance is as important as the performance of individual FPGA elements, such as logic, memory, and DSP. We will show that the feed forward datapath structures that are needed to support the typical AI matrix-vector and matrix-matrix multiplication operations can consistently close timing at over 500 MHz on a mid-speed grade device, even if all of the Tensor Blocks on the device are used. We will also show a full-chip NPU processor implementation that out performs GPUs at the same process node for a variety of AI inferencing workloads, even though it has a lower operating frequency of 365 MHz.In terms of overall compute throughput, Stratix 10 NX is specified at 143 INT8/FP16 TOPs/FLOPs or 286 INT4/FP12 TOPS/FLOPs. Depending on the configuration, power efficiency is in the range of 1–4 TOPs or TFLOPs/W.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = aug,
articleno = {45},
numpages = {32},
keywords = {FPGA architecture, AI tensor block, FPGA accelerator, place and route}
}

@article{10.1145/3520241,
author = {Trajkovic, Jelena and Karimi, Sara and Hangsan, Samantha and Zhang, Wenlu},
title = {Prediction Modeling for Application-Specific Communication Architecture Design of Optical NoC},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3520241},
doi = {10.1145/3520241},
abstract = {Multi-core systems-on-chip are becoming state-of-the-art. Therefore, there is a need for a fast and energy-efficient interconnect to take full advantage of the computational capabilities. Integration of silicon photonics with a traditional electrical interconnect in a Network-on-Chip (NoC) proposes a promising solution for overcoming the scalability issues of electrical interconnect. In this article, we derive and evaluate prediction modeling techniques for the design space exploration (DSE) of application-specific communication architectures for an Optical Network-on-Chip (ONoC). Our proposed model accurately predicts network packet latency, contention delay, and the static and dynamic energy consumption of the network. This work specifically addresses the challenge of accurately estimating performance metrics of the entire design space without having to perform time-consuming and computationally intensive exhaustive simulations. The proposed technique, based on machine learning (ML), can build accurate prediction models using only 10\% to 50\% (best case and worst case) of the entire design space. The accuracy, expressed as R2 (Coefficient of Determination) is 0.99901, 0.99967, 0.99996, and 0.99999 for network packet latency, contention delay, static energy consumption, and dynamic energy consumption, respectively, in six different benchmarks from the Splash-2 benchmark suite, chosen among 6 different machine learning prediction models.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {35},
numpages = {29},
keywords = {Optical Network-on-Chip (NoC), application-specific communication architecture design, prediction modeling, design space exploration, simulation, machine learning modeling}
}

@article{10.1145/3522575,
author = {Cao, Jihua and Li, Jie and Yin, Miao and Wang, Yunfeng},
title = {Online Reviews Sentiment Analysis and Product Feature Improvement with Deep Learning},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3522575},
doi = {10.1145/3522575},
abstract = {The text mining of online reviews is currently a popular research direction of e-commerce and is considered the next blue ocean. Online reviews can dig out consumer preferences and provide theoretical guidance for the improvement of product features. However, current research mostly focuses on sentiment analysis methods and rarely involves feature extraction and large-scale data recognition. This article uses word segmentation technology to create a new feature extraction method. With the long short-term memory neural network and latent Dirichlet allocation topic model, we propose a product feature improvement model—CESC (Consumer online reviews–Extract short text–Sentiment analysis–Cluster feature). The model can derive the product features and attitudes that consumers prefer based on consumer online reviews and use it to improve product features. According to the experimental results of three electronic products sold on the e-commerce platform, the model can effectively dig out consumer preferences for online reviews. Enterprises can improve the quality of products and services, better meet the needs of consumers, promote consumers’ consumption, and achieve the enterprises’ goals and values.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {203},
numpages = {17},
keywords = {Online reviews, text mining, sentiment analysis, product feature improvement, deep learning, consumer prefer}
}

@article{10.1145/3524500,
author = {Chitty-Venkata, Krishna Teja and Somani, Arun K.},
title = {Neural Architecture Search Survey: A Hardware Perspective},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524500},
doi = {10.1145/3524500},
abstract = {We review the problem of automating hardware-aware architectural design process of Deep Neural Networks (DNNs). The field of Convolutional Neural Network (CNN) algorithm design has led to advancements in many fields, such as computer vision, virtual reality, and autonomous driving. The end-to-end design process of a CNN is a challenging and time-consuming task, as it requires expertise in multiple areas such as signal and image processing, neural networks, and optimization. At the same time, several hardware platforms, general- and special-purpose, have equally contributed to the training and deployment of these complex networks in a different setting. Hardware-Aware Neural Architecture Search (HW-NAS) automates the architectural design process of DNNs to alleviate human effort and generate efficient models accomplishing acceptable accuracy-performance tradeoffs. The goal of this article is to provide insights and understanding of HW-NAS techniques for various hardware platforms (MCU, CPU, GPU, ASIC, FPGA, ReRAM, DSP, and VPU), followed by the co-search methodologies of neural algorithm and hardware accelerator specifications.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {78},
numpages = {36},
keywords = {Deep Neural Networks, Convolutional Neural Networks, Neural Architecture Search, Hardware-Aware Neural Architecture Search, general purpose hardware, domain specific accelerators, quantization, accelerator network co-search, literature review, survey, CPU, GPU, ASIC, FPGA}
}

@article{10.1145/3527621,
author = {Damaskinos, Georgios and Guerraoui, Rachid and Kermarrec, Anne-Marie and Nitu, Vlad and Patra, Rhicheek and Taiani, Francois},
title = {FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3527621},
doi = {10.1145/3527621},
abstract = {Federated learning (FL) is very appealing for its privacy benefits: essentially, a global model is trained with updates computed on mobile devices while keeping the data of users local. Standard FL infrastructures are however designed to have no energy or performance impact on mobile devices, and are therefore not suitable for applications that require frequent (online) model updates, such as news recommenders.This article presents FLeet, the first Online FL system, acting as a middleware between the Android operating system and the machine learning application. FLeet combines the privacy of Standard FL with the precision of online learning thanks to two core components: (1) I-Prof, a new lightweight profiler that predicts and controls the impact of learning tasks on mobile devices, and (2) AdaSGD, a new adaptive learning algorithm that is resilient to delayed updates.Our extensive evaluation shows that Online FL, as implemented by FLeet, can deliver a 2.3\texttimes{} quality boost compared to Standard FL while only consuming 0.036\% of the battery per day. I-Prof can accurately control the impact of learning tasks by improving the prediction accuracy by up to 3.6\texttimes{} in terms of computation time, and by up to 19\texttimes{} in terms of energy. AdaSGD outperforms alternative FL approaches by 18.4\% in terms of convergence speed on heterogeneous data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {79},
numpages = {30},
keywords = {Federated learning, online learning, asynchronous gradient descent, profiling, mobile Android devices}
}

@article{10.1145/3529097,
author = {Janiesch, Christian and Fischer, Marcus and Imgrund, Florian and Hofmann, Adrian and Winkelmann, Axel},
title = {An Architecture Using Payment Channel Networks for Blockchain-based Wi-Fi Sharing},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3529097},
doi = {10.1145/3529097},
abstract = {Enabling Internet access while taking load of mobile networks, the concept of Wi-Fi sharing holds much potential. While trust-based concepts require a trusted intermediary and cannot prevent malicious behavior, for example, conducted through fake profiles, security-based approaches lack adequate accounting mechanisms and coverage. Against this backdrop, we develop a Wi-Fi sharing architecture based on blockchain technology and payment channel networks. Our contribution is twofold: First, we present a comprehensive collection of design principles for workable Wi-Fi sharing networks. Second, we propose and evaluate a reference architecture that augments current approaches with adequate accounting mechanisms and facilitates performance, scalability, security, and participant satisfaction.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {24},
keywords = {Wi-Fi sharing, blockchain, payment channel networks, architecture}
}

@article{10.1145/3529098,
author = {Chen, Rongli and Chen, Xiaozhong and Wang, Lei and Li, Jianxin},
title = {The Core Industry Manufacturing Process of Electronics Assembly Based on Smart Manufacturing},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3529098},
doi = {10.1145/3529098},
abstract = {This research takes a case study approach to show the development of a diverse adoption and product strategy distinct from the core manufacturing industry process. It explains the development status in all aspects of smart manufacturing, via the example of ceramic circuit board manufacturing and electronic assembly, and outlines future smart manufacturing plans and processes. The research proposed two experiments using artificial intelligence and deep learning to demonstrate the problems and solutions regarding methods in manufacturing and factory facilities, respectively. In the first experiment, a Bayesian network inference is used to find the cause of the problem of metal residues between electronic circuits through key process and quality correlations. In the second experiment, a convolutional neural network is used to identify false defects that were overinspected during automatic optical inspection. This improves the manufacturing process by enhancing the yield rate and reducing cost. The contributions of the study built in circuit board production. Smart manufacturing, with the application of a Bayesian network to an Internet of Things setup, has addressed the problem of residue and redundant conductors on the edge of the ceramic circuit board pattern, and has improved and prevented leakage and high-frequency interference. The convolutional neural network and deep learning were used to improve the accuracy of the automatic optical inspection system, reduce the current manual review ratio, save labor costs, and provide defect classification as a reference for preprocess improvement.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {40},
numpages = {19},
keywords = {Smart manufacturing, artificial intelligence, Bayesian network, neural network, industry manufacturing process}
}

@article{10.1145/3533251,
author = {Chen, Weiwei and Wang, Ying and Xu, Ying and Gao, Chengsi and Liu, Cheng and Zhang, Lei},
title = {A Framework for Neural Network Architecture and Compile Co-optimization},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3533251},
doi = {10.1145/3533251},
abstract = {The efficiency of deep neural network (DNN) solutions on real hardware devices are mainly decided by the DNN architecture and the compiler-level scheduling strategy on the hardware. When we try to fully exploit the underlying hardware and obtain the optimal tradeoff between DNN accuracy and runtime performance, we discovered that the two optimization goals of DNN architecture and scheduling policy are intimately related to each other. However, current hardware-aware Neural Architecture Search (NAS) methods primarily focus on the DNN architecture search process, ignoring the effects of various compiler-level scheduling strategies (e.g., graph-level optimization, loop transformations, parallelization, etc.) on network candidates being evaluated in the search process. As a result, they may overlook the true-optimal DNN implementations on hardware, which can only be discovered by trying-out different combinations of scheduling strategies and DNN architectures. This work proposes a NAS framework (CHaNAS) that searches for not only the network architecture but also the dedicated compiler-level scheduling policy, as the optimal co-design solution on the target hardware. We propose to use a block-based pre-scheduling methodology to reduce the co-design search space and enable the automatic generation of the optimal co-design, including the network architecture and the tensor programs that practice the scheduling policy. Further, we introduce a new search objective function based on the generalization gap to prevent the selection of architectures that are prone to overfitting. We evaluate CHaNAS on Imagenet on different hardware back-ends against the state-of-the-art hardware-aware search method based on the MobileNet-v3 search space. Experimental results show that the co-design solutions obtained by ChaNAS show up to 1.6\texttimes{}, 1.9\texttimes{}, and 1.7\texttimes{}, 24 performance boost on NVIDIA P100 GPU, Intel Xeon 8163 CPU, and Samsung Note 10 Mobile, respectively, over the baselines of the same-level accuracy.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {5},
numpages = {24},
keywords = {DNN-scheduling Co-design, hardware-aware neural architecture search, compiler optimization}
}

@article{10.1145/3533381,
author = {Adhikari, Deepak and Jiang, Wei and Zhan, Jinyu and He, Zhiyuan and Rawat, Danda B. and Aickelin, Uwe and Khorshidi, Hadi A.},
title = {A Comprehensive Survey on Imputation of Missing Data in Internet of Things},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533381},
doi = {10.1145/3533381},
abstract = {The Internet of Things (IoT) is enabled by the latest developments in smart sensors, communication technologies, and Internet protocols with broad applications. Collecting data from IoT and generating information from these data become tedious tasks in real-life applications when missing data are encountered in datasets. It is of critical importance to deal with the missing data timely for intelligent decision-making. Hence, this survey attempts to provide a structured and comprehensive overview of the research on the imputation of incomplete data in IoT. The article starts by providing an overview of incomplete data based on the architecture of IoT. Then, it discusses the various strategies to handle the missing data, the assumptions used, the computing platform, and the issues related to them. The article also explores the application of imputation in the area of IoT. We encourage researchers and data analysts to use known imputation techniques and discuss various issues and challenges. Finally, potential future directions regarding the method are suggested. We believe this survey will provide a better understanding of the research of incomplete data and serve as a guide for future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {133},
numpages = {38},
keywords = {Imputation of missing data, multiple imputations, machine learning, deep learning, computing platform for incomplete data, Internet of Things}
}

@article{10.1145/3533704,
author = {Jero, Samuel and Burow, Nathan and Ward, Bryan and Skowyra, Richard and Khazan, Roger and Shrobe, Howard and Okhravi, Hamed},
title = {TAG: Tagged Architecture Guide},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533704},
doi = {10.1145/3533704},
abstract = {Software security defenses are routinely broken by the persistence of both security researchers and attackers. Hardware solutions based on tagging are emerging as a promising technique that provides strong security guarantees (e.g., memory safety) while incurring minimal runtime overheads and maintaining compatibility with existing codebases. Such schemes extend every word in memory with a tag and enforce security&nbsp;policies across them. This paper provides a survey of existing work on tagged architectures and describe the types of attacks such architectures aim to prevent as well as the guarantees they provide. It highlights the main distinguishing factors among tagged architectures and presents the diversity of designs and implementations that have been proposed. The survey reveals several real-world challenges have been neglected relating to both security and practical deployment. The challenges relate to the provisioning and enforcement phases of tagged architectures, and various overheads they incur. This work identifies these challenges as open research problems and provides suggestions for improving their security and practicality.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {124},
numpages = {34},
keywords = {Tagged Architecture Guide}
}

@article{10.1145/3534969,
author = {Que, Zhiqiang and Nakahara, Hiroki and Fan, Hongxiang and Li, He and Meng, Jiuxi and Tsoi, Kuen Hung and Niu, Xinyu and Nurvitadhi, Eriko and Luk, Wayne},
title = {Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3534969},
doi = {10.1145/3534969},
abstract = {This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0\% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {4},
numpages = {26},
keywords = {Accelerator architecture, recurrent neural networks, multi-tenant execution}
}

@article{10.1145/3538531,
author = {Kleyko, Denis and Rachkovskij, Dmitri A. and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3538531},
doi = {10.1145/3538531},
abstract = {This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and distributed vector representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent years, the necessity for a comprehensive survey of the field has become extremely important. Therefore, amongst other aspects of the field, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part&nbsp;II of this survey&nbsp;[84] is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {130},
numpages = {40},
keywords = {Artificial intelligence, machine learning, distributed representations, data structures, hyperdimensional computing, vector symbolic architectures, holographic reduced representations, tensor product representations, matrix binding of additive terms, binary spatter codes, multiply-add-permute, sparse binary distributed representations, sparse block codes, modular composite representations, geometric analogue of holographic reduced representations}
}

@article{10.1145/3543069,
author = {Ioannou, Lenos and Fahmy, Suhaib A.},
title = {Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3543069},
doi = {10.1145/3543069},
abstract = {Long-Short Term Memory (LSTM) networks, and Recurrent Neural Networks (RNNs) in general, have demonstrated their suitability in many time series data applications, especially in Natural Language Processing (NLP). Computationally, LSTMs introduce dependencies on previous outputs in each layer that complicate their computation and the design of custom computing architectures, compared to traditional feed-forward networks. Most neural network acceleration work has focused on optimising the core matrix-vector operations on highly capable FPGAs in server environments. Research that considers the embedded domain has often been unsuitable for streaming inference, relying heavily on batch processing to achieve high throughput. Moreover, many existing accelerator architectures have not focused on fully exploiting the underlying FPGA architecture, resulting in designs that achieve lower operating frequencies than the theoretical maximum. This paper presents a flexible overlay architecture for LSTMs on FPGA SoCs that is built around a streaming dataflow arrangement, uses DSP block capabilities directly, and is tailored to keep parameters within the architecture while moving input data serially to mitigate external memory access overheads. The architecture is designed as an overlay that can be configured to implement alternative models or update model parameters at runtime. It achieves higher operating frequency and demonstrates higher performance than other lightweight LSTM accelerators, as demonstrated in an FPGA SoC implementation.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {8},
numpages = {26},
keywords = {LSTM, neural networks, overlay, machine learning}
}

@article{10.1145/3543542,
author = {Ji, Yuede and Liu, Hang and Hu, Yang and Huang, H. Howie},
title = {iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2329-4949},
url = {https://doi.org/10.1145/3543542},
doi = {10.1145/3543542},
abstract = {Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices as well as many small SCCs whose sizes are reversely proportional to the frequency of their occurrences. For both types of SCCs, current approaches that rely on depth first search (DFS) or breadth first search (BFS) face the challenges of both strict synchronization requirements and high computation cost. In this article, we advocate a new paradigm of identifying SCCs with simple spanning trees since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan, which consists of parallel, relaxed synchronization construction of spanning trees for detecting large and small SCCs combined with fast trims for small SCCs. We further scale iSpan to the distributed memory system by applying different distribution strategies to the data and task parallel jobs. Not limited, we also extend iSpan to the GPU architecture. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS- and BFS-based methods by an average 18\texttimes{} and 4\texttimes{}, respectively.},
journal = {ACM Trans. Parallel Comput.},
month = aug,
articleno = {13},
numpages = {27},
keywords = {Strongly connected component, spanning tree, graph, parallel computation, GPU}
}

@article{10.1145/3546182,
author = {V\'{e}stias, M\'{a}rio and Duarte, Rui P. and de Sousa, Jos\'{e} T. and Neto, Hor\'{a}cio},
title = {Efficient Design of Low Bitwidth Convolutional Neural Networks on FPGA with Optimized Dot Product Units},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3546182},
doi = {10.1145/3546182},
abstract = {Designing hardware accelerators to run the inference of convolutional neural networks (CNN) is under intensive research. Several different architectures have been proposed along with hardware-oriented optimizations of the neural network models. One of the most used optimizations is quantization since it reduces the memory requirements to store weights and layer maps, the memory bandwidth requirements and the hardware complexity. As a consequence, the inference throughput has improved and the computing cost has been reduced, allowing inference to be executed on embedded devices. In this work, we propose highly efficient dot-product arithmetic units for ternary and non-ternary convolutional neural networks on FPGA. The non-ternary dot-product unit uses a fused multiply-add that avoids expensive adder trees, while the ternary dot-product unit uses a dual product unit followed by an optimized conditional adder tree structure. In both cases, designs with and without embedded DSP are considered. The solution is configurable and can be adapted to the available number of resources of the FPGA to achieve the best efficiency. A CNN architecture was developed and characterized using the proposed dot product units. The results show a performance improvement of 1.8 \texttimes{} with a 2\texttimes{} more area efficiency for low bit-width quantizations when compared to previous works running large CNNs in FPGA.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {13},
numpages = {36},
keywords = {Fused multiply-add, dot-product, convolutional neural network, hardware acceleration, FPGA}
}

@article{10.1145/3547140,
author = {Hirsch, Sharon and Novgorodov, Slava and Guy, Ido and Nus, Alexander},
title = {The Tip of the Buyer: Extracting Product Tips from Reviews},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3547140},
doi = {10.1145/3547140},
abstract = {Product reviews play a key role in e-commerce platforms. Studies show that many users read product reviews before a purchase and trust them to the same extent as personal recommendations. However, in many cases, the number of reviews per product is large and extracting useful information becomes a challenging task. Several websites have recently added an option to post tips—short, concise, practical, and self-contained pieces of advice about the products. These tips are complementary to the reviews and usually add a new non-trivial insight about the product, beyond its title, attributes, and description. Yet, most if not all major e-commerce platforms lack the notion of a tip as a first-class citizen and customers typically express their advice through other means, such as reviews. In this work, we propose an extractive method for tip generation from product reviews. We focus on five popular e-commerce domains whose reviews tend to contain useful non-trivial tips that are beneficial for potential customers. We formally define the task of tip extraction in e-commerce by providing the list of tip types, tip timing (before and/or after the purchase), and connection to the surrounding context sentences. To extract the tips, we propose a supervised approach and leverage a publicly available dataset, annotated by human editors, containing 14,000 product reviews. To demonstrate the potential of our approach, we compare different tip generation methods and evaluate them both manually and over the labeled set. Our approach demonstrates particularly high performance for popular products in the Baby, Home Improvement, and Sports \&amp; Outdoors domains, with precision of over 95\% for the top 3 tips per product. In addition, we evaluate the performance of our methods on previously unseen domains. Finally, we discuss the practical usage of our approach in real-world applications. Concretely, we explain how tips generated from user reviews can be integrated in various use cases within e-commerce platforms and benefit both buyers and sellers.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {4},
numpages = {30},
keywords = {E-commerce, tips generation, product reviews, machine learning, deep learning}
}

@article{10.1145/3547657,
author = {Abdelhamid, Riadh Ben and Yamaguchi, Yoshiki and Boku, Taisuke},
title = {A Scalable Many-core Overlay Architecture on an HBM2-enabled Multi-Die FPGA},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3547657},
doi = {10.1145/3547657},
abstract = {The overlay architecture enables to raise the abstraction level of hardware design and enhances hardware-accelerated applications’ portability. In FPGAs, there is a growing awareness of the overlay structure as typified by many-core architecture. It works in theory; however, it is difficult in practice, because it is beset with serious design issues. For example, the size of FPGAs is bigger than before. It is exacerbating the issue of the place-and-route. Besides, a single FPGA is actually the sum of small-to-middle FPGAs by advancing packaging technology like silicon interposers. Thus, the tightly coupled many-core designs will face this covert issue that the wires among the regions are extremely restricted. This article proposes efficient essential processing elements, micro-architecture design, and the interconnect architecture toward a scalable many-core overlay design. In particular, our work proposes a novel compact buffering technique to reduce memory resource utilization in tightly connected overlays while preserving computational efficiency. This technique reduces the utilization of BlockRAM to nearly 50\% while achieving a best-case computational efficiency of 91.93\% in a three-dimensional Jacobi benchmark. Besides, the proposed enhancements led to around 2\texttimes{} and 3\texttimes{} improvement in performance and power efficiency, respectively. Moreover, the improved scalability allowed increasing compute resources and delivering around 4\texttimes{} better performance and power efficiency, as compared to the baseline Dynamically Re-programmable Architecture of Gather-scatter Overlay Nodes overlay.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = jan,
articleno = {15},
numpages = {33},
keywords = {EPR, compact buffering, HBM2, multi-die FPGA, network interconnect, overlay architecture, stencil computation}
}

@article{10.1145/3548679,
author = {Serrano, Manuel A. and Cruz-Lemus, Jos\'{e} A. and Perez-Castillo, Ricardo and Piattini, Mario},
title = {Quantum Software Components and Platforms: Overview and Quality Assessment},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3548679},
doi = {10.1145/3548679},
abstract = {Quantum computing is the latest revolution in computing and will probably come to be seen as an advance as important as the steam engine or the information society. In the last few decades, our understanding of quantum computers has expanded and multiple efforts have been made to create languages, libraries, tools, and environments to facilitate their programming. Nonetheless, quantum computers are complex systems at the bottom of a stack of layers that programmers need to understand. Hence, efforts towards creating quantum programming languages and computing environments that can abstract low-level technology details have become crucial steps to achieve a useful quantum computing technology. However, most of these environments still lack many of the features that would be desirable, such as those outlined in The Talavera Manifesto for Quantum Software Engineering and Programming. For advancing quantum computing, we will need to develop quantum software engineering techniques and tools to ensure the feasibility of this new type of quantum software. To contribute to this goal, this paper provides a review of the main quantum software components and platforms. We also propose a set of quality requirements for the development of quantum software platforms and the conduct of their quality assessment.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {164},
numpages = {31},
keywords = {Quantum computing, quantum software, quantum platforms, Quantum Software Engineering}
}

@article{10.1145/3551638,
author = {Fang, Weiwei and Xu, Wenyuan and Yu, Chongchong and Xiong, Neal. N.},
title = {Joint Architecture Design and Workload Partitioning for DNN Inference on Industrial IoT Clusters},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3551638},
doi = {10.1145/3551638},
abstract = {The advent of Deep Neural Networks (DNNs) has empowered numerous computer-vision applications. Due to the high computational intensity of DNN models, as well as the resource constrained nature of Industrial Internet-of-Things (IIoT) devices, it is generally very challenging to deploy and execute DNNs efficiently in the industrial scenarios. Substantial research has focused on model compression or edge-cloud offloading, which trades off accuracy for efficiency or depends on high-quality infrastructure support, respectively. In this article, we present EdgeDI, a framework for executing DNN inference in a partitioned, distributed manner on a cluster of IIoT devices. To improve the inference performance, EdgeDI exploits two key optimization knobs, including: (1) Model compression based on deep architecture design, which transforms the target DNN model into a compact one that reduces the resource requirements for IIoT devices without sacrificing accuracy; (2) Distributed inference based on adaptive workload partitioning, which achieves high parallelism by adaptively balancing the workload distribution among IIoT devices under heterogeneous resource conditions. We have implemented EdgeDI based on PyTorch, and evaluated its performance with the NEU-CLS defect classification task and two typical DNN models (i.e., VGG and ResNet) on a cluster of heterogeneous Raspberry Pi devices. The results indicate that the proposed two optimization approaches significantly outperform the existing solutions in their specific domains. When they are well combined, EdgeDI can provide scalable DNN inference speedups that are very close to or even much higher than the theoretical speedup bounds, while still maintaining the desired accuracy.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {7},
numpages = {21},
keywords = {Industrial Internet-of-Things (IIoT), deep learning, edge computing, DNN architecture, distributed inference}
}

@article{10.1145/3555091,
author = {Hoffmann, Sven and Pinatti de Carvalho, Aparecido Fabiano and Schweitzer, Marcus and Abele, Nils Darwin and Wulf, Volker},
title = {Producing and Consuming Instructional Material in Manufacturing Contexts: Evaluation of an AR-based Cyber-Physical Production System for Supporting Knowledge and Expertise Sharing},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555091},
doi = {10.1145/3555091},
abstract = {Fast-paced knowledge and expertise sharing (KES) is a typical demand in contemporary workplaces due to dynamic markets and ever-changing work practices. Past and current computer supported cooperative work (CSCW) research has long been investigating how computer technologies can support people with KES. Recent claims have asserted that augmented reality- (AR-)based cyber-physical production systems (CPPS) are poised to bring significant changes in the ways that KES unfolds in manufacturing contexts. This paper scrutinises such claims by implementing a short-term evaluation of an AR-based CPPS and assessing how it can potentially support (1) the generation of AR content by experienced production workers and (2) the visualisation and processing of such content by novice workers. We, therefore, contribute a user study to the CSCW community that sheds light on the use of a particular type of AR-based CPPS for KES in industrial contexts.?},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {366},
numpages = {36},
keywords = {augmented reality, cyber-physical production systems, evaluation, knowledge and expertise sharing, machine set-up, manufacturing contexts}
}

@article{10.1145/3555179,
author = {Zhang, Yang and Zong, Ruohan and Kou, Ziyi and Shang, Lanyu and Wang, Dong},
title = {CrowdNAS: A Crowd-guided Neural Architecture Searching Approach to Disaster Damage Assessment},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555179},
doi = {10.1145/3555179},
abstract = {Disaster damage assessment (DDA) has emerged as an important application in disaster response and management, which aims to assess the damage severity of an affected area by leveraging AI (e.g., deep learning) techniques to examine the imagery data posted on social media during a disaster event. In this paper, we focus on a crowd-guided neural architecture searching (NAS) problem in DDA applications. Our goal is to leverage human intelligence from crowdsourcing systems to guide the discovery of the optimal neural network architecture in the design space to achieve the desirable damage assessment performance. Our work is motivated by the limitation that the deep neural network architectures in current DDA solutions are mainly designed by AI experts, which is known to be both time-consuming and error-prone. Two critical technical challenges exist in solving our problem: i) it is challenging to design a manageable NAS space for crowd-based solutions; ii) it is non-trivial to transfer the imperfect crowd knowledge to effective decisions in identifying the optimal neural network architecture of a DDA application. To address the above challenges, we develop CrowdNAS, a crowd-guided NAS framework that develops novel techniques inspired by AI, crowdsourcing, and estimation theory to address the NAS problem. The evaluation results from two real-world DDA applications show that CrowdNAS consistently outperforms the state-of-the-art AI-only, crowd-AI, and NAS baselines by achieving the highest classification accuracy in the damage assessment while maintaining a low computational cost under various evaluation scenarios.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {288},
numpages = {29},
keywords = {crowdsourcing, disaster damage assessment, neural architecture searching}
}

@article{10.1145/3555312,
author = {Asprino, Luigi and Daga, Enrico and Gangemi, Aldo and Mulholland, Paul},
title = {Knowledge Graph Construction with a Fa\c{c}ade: A Unified Method to Access Heterogeneous Data Sources on the Web},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3555312},
doi = {10.1145/3555312},
abstract = {Data integration is the dominant use case for RDF Knowledge Graphs. However, Web resources come in formats with weak semantics (for example, CSV and JSON), or formats specific to a given application (for example, BibTex, HTML, and Markdown). To solve this problem, Knowledge Graph Construction (KGC) is gaining momentum due to its focus on supporting users in transforming data into RDF. However, using existing KGC frameworks result in complex data processing pipelines, which mix structural and semantic mappings, whose development and maintenance constitute a significant bottleneck for KG engineers. Such frameworks force users to rely on different tools, sometimes based on heterogeneous languages, for inspecting sources, designing mappings, and generating triples, thus making the process unnecessarily complicated. We argue that it is possible and desirable to equip KG engineers with the ability of interacting with Web data formats by relying on their expertise in RDF and the well-established SPARQL query language&nbsp;[2]. In this article, we study a unified method for data access to heterogeneous data sources with Facade-X, a meta-model implemented in a new data integration system called SPARQL Anything. We demonstrate that our approach is theoretically sound, since it allows a single meta-model, based on RDF, to represent data from (a) any file format expressible in BNF syntax, as well as (b) any relational database. We compare our method to state-of-the-art approaches in terms of usability (cognitive complexity of the mappings) and general performance. Finally, we discuss the benefits and challenges of this novel approach by engaging with the reference user community.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {6},
numpages = {31},
keywords = {SPARQL, RDF, meta-model, re-engineering}
}

@article{10.1145/3555370,
author = {Sobczyk, Aleksandros and Gallopoulos, Efstratios},
title = {pylspack: Parallel Algorithms and Data Structures for Sketching, Column Subset Selection, Regression, and Leverage Scores},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3555370},
doi = {10.1145/3555370},
abstract = {We present parallel algorithms and data structures for three fundamental operations in Numerical Linear Algebra: (i) Gaussian and CountSketch random projections and their combination, (ii) computation of the Gram matrix, and (iii) computation of the squared row norms of the product of two matrices, with a special focus on “tall-and-skinny” matrices, which arise in many applications. We provide a detailed analysis of the ubiquitous CountSketch transform and its combination with Gaussian random projections, accounting for memory requirements, computational complexity and workload balancing. We also demonstrate how these results can be applied to column subset selection, least squares regression and leverage scores computation. These tools have been implemented in pylspack, a publicly available Python package1 whose core is written in C++ and parallelized with OpenMP and that is compatible with standard matrix data structures of SciPy and NumPy. Extensive numerical experiments indicate that the proposed algorithms scale well and significantly outperform existing libraries for tall-and-skinny matrices.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {27},
keywords = {Parallel algorithms, sparse data structures, sketching, column subset selection, regression, preconditioning, statistical leverage scores}
}

@article{10.1145/3555371,
author = {Zou, Jie and Huang, Jimmy and Ren, Zhaochun and Kanoulas, Evangelos},
title = {Learning to Ask: Conversational Product Search via Representation Learning},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3555371},
doi = {10.1145/3555371},
abstract = {Online shopping platforms, such as Amazon and AliExpress, are increasingly prevalent in society, helping customers purchase products conveniently. With recent progress in natural language processing, researchers and practitioners shift their focus from traditional product search to conversational product search. Conversational product search enables user-machine conversations and through them collects explicit user feedback that allows to actively clarify the users’ product preferences. Therefore, prospective research on an intelligent shopping assistant via conversations is indispensable. Existing publications on conversational product search either model conversations independently from users, queries, and products or lead to a vocabulary mismatch. In this work, we propose a new conversational product search model, ConvPS, to assist users in locating desirable items. The model is first trained to jointly learn the semantic representations of user, query, item, and conversation via a unified generative framework. After learning these representations, they are integrated to retrieve the target items in the latent semantic space. Meanwhile, we propose a set of greedy and explore-exploit strategies to learn to ask the user a sequence of high-performance questions for conversations. Our proposed ConvPS model can naturally integrate the representation learning of the user, query, item, and conversation into a unified generative framework, which provides a promising avenue for constructing accurate and robust conversational product search systems that are flexible and adaptive. Experimental results demonstrate that our ConvPS model significantly outperforms state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {45},
numpages = {27},
keywords = {Conversational product search, learning to ask, representation learning}
}

@article{10.1145/3555561,
author = {Miceli, Milagros and Posada, Julian},
title = {The Data-Production Dispositif},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555561},
doi = {10.1145/3555561},
abstract = {Machine learning (ML) depends on data to train and verify models. Very often, organizations outsource processes related to data work (i.e., generating and annotating data and evaluating outputs) through business process outsourcing (BPO) companies and crowdsourcing platforms. This paper investigates outsourced ML data work in Latin America by studying three platforms in Venezuela and a BPO in Argentina. We lean on the Foucauldian notion of dispositif to define the data-production dispositif as an ensemble of discourses, actions, and objects strategically disposed to (re)produce power/knowledge relations in data and labor. Our dispositif analysis comprises the examination of 210 data work instruction documents, 55 interviews with data workers, managers, and requesters, and participant observation. Our findings show that discourses encoded in instructions reproduce and normalize the worldviews of requesters. Precarious working conditions and economic dependency alienate workers, making them obedient to instructions. Furthermore, discourses and social contexts materialize in artifacts, such as interfaces and performance metrics, limiting workers' agency and normalizing specific ways of interpreting data. We conclude by stressing the importance of counteracting the data-production dispositif by fighting alienation and precarization, and empowering data workers to become assets in the quest for high-quality data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {460},
numpages = {37},
keywords = {crowdsourcing, data labeling, data production, data work, machine learning, platform labor}
}

@article{10.1145/3555568,
author = {Nguyen, Binh Vinh Duc and Han, Jihae and Vande Moere, Andrew},
title = {Towards Responsive Architecture that Mediates Place: Recommendations on How and When an Autonomously Moving Robotic Wall Should Adapt a Spatial Layout},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555568},
doi = {10.1145/3555568},
abstract = {Responsive architecture envisions the built environment to adapt to the changing needs of its occupants dynamically. Although it is increasingly feasible to move space-defining objects like room dividers by mobile robots, little is known about how or when such spatial adaptations should occur. We therefore measured the experience of 26 occupants while they performed six different activities inside an office breakout room that was being adapted by a robotically moving wall in either a reactive or proactive way. Based on these empirical findings, we propose how autonomous spatial adaptation should primarily aim to balance the spatial, situational and subjective qualities of the resulting sense of place. We also define eight distinct design recommendations that exploit the unique affordances of spatial adaptation. By asserting that future advances in human-building interaction (HBI) should be based on creating appropriate places rather than controlling functional spaces, we foresee how responsive architecture might become as compelling as its static counterpart.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {467},
numpages = {27},
keywords = {adaptive architecture, human-building interaction, human-robot interaction, interactive architecture, kinetic architecture, place-making, responsive architecture, robotic architecture, robotic furniture, robotic wall, sense of place, smart building, smart office, smart space, spatial layout}
}

@article{10.1145/3557886,
author = {Guo, Xunhua and Wang, Lingli and Zhang, Mingyue and Chen, Guoqing},
title = {First Things First? Order Effects in Online Product Recommender Systems},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3557886},
doi = {10.1145/3557886},
abstract = {Research on recommender systems has noted that the ranking of recommended items may play an important role in the performance of recommendation algorithms. To advance recommender systems research beyond the traditional approach that ranks recommended products in descending, it is crucial to understand the cognitive processes that online consumers experience when they evaluate products in a sequence. Drawing on evaluability theory and the order effects perspective, we formulate a scenario in which two products are presented sequentially and each product has two attributes, one of which can be evaluated independently while the other is difficult to evaluate without comparison. Analyses show that in two out of the three cases examined, presenting the most recommended product in the second place will result in stronger consumer purchase intentions and willingness to pay. Research hypotheses are proposed based on the results of the scenario analyses and are empirically tested through three laboratory experiments. In Study 1, evidence for the hypothesized order effects is found for the settings with randomly assigned product recommendations. In Study 2, the same effects are observed for the settings with personalized recommendations generated by a collaborative filtering algorithm. In Study 3, it is shown that such order effects also exist in terms of the recommendation strength of recommender systems. These findings provide novel insights into the behavioral implications of using recommender systems in e-commerce, shedding light on additional means of improving the design of such systems.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = mar,
articleno = {15},
numpages = {35},
keywords = {Order effects, recommender systems, product rating, evaluability theory, laboratory experiments}
}

@article{10.1145/3558000,
author = {Kleyko, Denis and Rachkovskij, Dmitri and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3558000},
doi = {10.1145/3558000},
abstract = {This is Part&nbsp;II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations&nbsp;[321, 326] is an influential HDC/VSA model that is well known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the field.Part&nbsp;I of this survey&nbsp;[222] covered foundational aspects of the field, such as the historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and the transformation of input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the Machine Learning/Artificial Intelligence domain; however, we also cover other applications to provide a complete picture. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {175},
numpages = {52},
keywords = {Artificial intelligence, machine learning, distributed representations, cognitive architectures, cognitive computing, applications, analogical reasoning, hyperdimensional computing, vector symbolic architectures, holographic reduced representations, tensor product representations, matrix binding of additive terms, binary spatter codes, multiply-add-permute, sparse binary distributed representations, sparse block codes, modular composite representations, geometric analogue of holographic reduced representations}
}

@article{10.1145/3559762,
author = {Shah, Parth and Shenoy, Ranjal Gautham and Srinivasan, Vaidyanathan and Bose, Pradip and Buyuktosunoglu, Alper},
title = {TokenSmart: Distributed, Scalable Power Management in the Many-core Era},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3559762},
doi = {10.1145/3559762},
abstract = {Centralized power management control systems are hitting a scalability limit. In particular, enforcing a power cap in a many-core system in a performance-friendly manner is quite challenging. Today’s on-chip controller reduces the clock speed of compute domains in response to local or global power limit alerts. However, this is opaque to the operating system (OS), which continues to request higher clock frequency based on the workload characteristics acting against the centralized on-chip controller. To address these issues, we introduce TokenSmart, which implements a set of scalable distributed frequency control heuristics within the OS, using a novel token-based mechanism. The number of system-allocated power tokens represents the maximum allowable power consumption; and the OS governor orchestrates a token-passing (or sharing) algorithm between the compute engines. Token allocation count increase (decrease) corresponds to a increase (decrease) of clock frequency. The compute units are connected in a ring-topology allowing minimal meta-data to be passed along with the token value for regulating power budget. We explore different heuristics to assign tokens smartly across the units. This results in efficient power regulation and sustenance of turbo frequencies over a longer duration. Our proposed methodology can be implemented in hardware with multiple on-chip controllers, or in software where each set of cores acts as a compute unit. The methodology is currently implemented within the Linux kernel of a real IBM POWER9 many-core system and experimentally verified on different real world workloads such as Redis, Cassandra, PostgreSQL along with a micro-benchmark such as rt-app. Our experiments indicate the increase in throughput for all the workloads along with the benefit of power savings. For instance, results show a considerable boost of about 4\% in throughput of both the PostgreSQL and Redis benchmark with a substantial savings in power consumption (18\% and 37\%, respectively). If the approach is implemented in hardware, then our experimental analysis speculates the throughput to increase up to 14\% in PostgreSQL benchmark.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {4},
numpages = {26},
keywords = {Power management, decentralized, scalable}
}

@article{10.1145/3561048,
author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
title = {Explainable AI (XAI): Core Ideas, Techniques, and Solutions},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3561048},
doi = {10.1145/3561048},
abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence&nbsp;(XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {194},
numpages = {33},
keywords = {Explainable artificial intelligence, interpretable AI, programming framework, software toolkits}
}

@article{10.1145/3563297,
author = {Yuan, Charles and Carbin, Michael},
title = {Tower: data structures in Quantum superposition},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563297},
doi = {10.1145/3563297},
abstract = {Emerging quantum algorithms for problems such as element distinctness, subset sum, and closest pair demonstrate computational advantages by relying on abstract data structures. Practically realizing such an algorithm as a program for a quantum computer requires an efficient implementation of the data structure whose operations correspond to unitary operators that manipulate quantum superpositions of data. 

To correctly operate in superposition, an implementation must satisfy three properties --- reversibility, history independence, and bounded-time execution. Standard implementations, such as the representation of an abstract set as a hash table, fail these properties, calling for tools to develop specialized implementations. 

In this work, we present Core Tower, the first language for quantum programming with random-access memory. Core Tower enables the developer to implement data structures as pointer-based, linked data. It features a reversible semantics enabling every valid program to be translated to a unitary quantum circuit. 

We present Boson, the first memory allocator that supports reversible, history-independent, and constant-time dynamic memory allocation in quantum superposition. 
We also present Tower, a language for quantum programming with recursively defined data structures. Tower features a type system that bounds all recursion using classical parameters as is necessary for a program to execute on a quantum computer. 

Using Tower, we implement Ground, the first quantum library of data structures, including lists, stacks, queues, strings, and sets. We provide the first executable implementation of sets that satisfies all three mandated properties of reversibility, history independence, and bounded-time execution.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {134},
numpages = {30},
keywords = {data structures, history independence, quantum programming, quantum random-access memory, reversible programming}
}

@article{10.1145/3563329,
author = {Jin, Charles and Phothilimthana, Phitchaya Mangpo and Roy, Sudip},
title = {Neural architecture search using property guided synthesis},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563329},
doi = {10.1145/3563329},
abstract = {Neural architecture search (NAS) has become an increasingly important tool within the deep learning community in recent years, yielding many practical advancements in the design of deep neural network architectures. However, most existing approaches operate within highly structured design spaces, and hence (1) explore only a small fraction of the full search space of neural architectures while also (2) requiring significant manual effort from domain experts. In this work, we develop techniques that enable efficient NAS in a significantly larger design space. In particular, we propose to perform NAS in an abstract search space of program properties. Our key insights are as follows: (1) an abstract search space can be significantly smaller than the original search space, and (2) architectures with similar program properties should also have similar performance; thus, we can search more efficiently in the abstract search space. To enable this approach, we also introduce a novel efficient synthesis procedure, which performs the role of concretizing a set of promising program properties into a satisfying neural architecture. We implement our approach, αNAS, within an evolutionary framework, where the mutations are guided by the program properties. Starting with a ResNet-34 model, αNAS produces a model with slightly improved accuracy on CIFAR-10 but 96\% fewer parameters. On ImageNet, αNAS is able to improve over Vision Transformer (30\% fewer FLOPS and parameters), ResNet-50 (23\% fewer FLOPS, 14\% fewer parameters), and EfficientNet (7\% fewer FLOPS and parameters) without any degradation in accuracy.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {166},
numpages = {30},
keywords = {Abstract Interpretation, Neural Architecture Search, Program Synthesis}
}

@article{10.1145/3563456,
author = {Shu, Jiwu and Fang, Kedong and Chen, Youmin and Wang, Shuo},
title = {TH-iSSD: Design and Implementation of a Generic and Reconfigurable Near-Data Processing Framework},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3563456},
doi = {10.1145/3563456},
abstract = {We present the design and implementation of TH-iSSD, a near-data processing framework to address the data movement problem. TH-iSSD does not pose any restriction to the hardware selection and is highly reconfigurable—its core components, such as the on-device compute unit (e.g., FPGA, embedded CPUs) and data collectors (e.g., camera, sensors), can be easily replaced to adapt to different use cases. TH-iSSD achieves this goal by incorporating highly flexible computation and data paths. In the data path, TH-iSSD adopts an efficient device-level data switch that exchanges data with both host CPUs and peripheral sensors; it also enables direct accesses between the sensing, computation, and storage hardware components, which completely eliminates the redundant data movement overhead, and thus delivers both high performance and energy efficiency. In the computation path, TH-iSSD provides an abstraction of filestream for developers, which abstracts a collection of data along with the related computation task as a file. Since existing applications are familiar with POSIX-like interfaces, they can be ported on top of our platform with minimal code modification. Moreover, TH-iSSD also introduces mechanisms including pipelined near-data processing and priority-aware I/O scheduling to make TH-iSSD perform more effectively. We deploy TH-iSSD to accelerate two types of applications: the content-based information retrieval system and the edge zero-streaming system. Our experimental results show that TH-iSSD achieves up to 1.6\texttimes{} higher throughput and 36\% lower latency than compute-centric designs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
articleno = {96},
numpages = {23},
keywords = {Near data processing, information retrieval, deep learning, storage architecture}
}

@article{10.1145/3564604,
author = {Bibi, Nazia and Rana, Tauseef and Maqbool, Ayesha and Alkhalifah, Tamim and Khan, Wazir Zada and Bashir, Ali Kashif and Zikria, Yousaf Bin},
title = {Reusable Component Retrieval: A Semantic Search Approach for Low-Resource Languages},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564604},
doi = {10.1145/3564604},
abstract = {A common practice among programmers is to reuse existing code, accomplished by performing natural language queries through search engines. The main aim of code retrieval is to search for the most relevant snippet from a corpus of code snippets. However, code retrieval frameworks for low-resource languages are insufficient. Retrieving the most relevant code snippet efficiently can be accomplished only by eliminating the semantic gap between the code snippets residing in the repository and the user’s query (natural language description). The primary objective of the research is to contribute to this field by providing a code search framework that can be extended for low-resource languages. The secondary objective is to provide a code retrieval mechanism that is semantically relevant to the user query and provide programmers with the ability to locate source code that they want to use when developing new applications. The proposed approach is implemented using a web platform to search for source code. As code retrieval is a sophisticated task, the proposed approach incorporates a semantic search mechanism. This research uses a semantic model for code retrieval, which generates meanings or synonyms of words. The proposed model integrates ontologies and Natural Language Processing. System performance measures and classification accuracy are computed using precision, recall, and F1-score. We also compare the proposed approach with state-of-the-art baseline models. The retrieved results are ranked, showing that our approach significantly outperforms robust code matching. Our evaluation shows that semantic matching leads to improved source code retrieval. This study marks a substantial advancement in integrating programming expertise with code retrieval techniques. Moreover, our system lets users know when and how it is used for successful semantic searching.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {141},
numpages = {31},
keywords = {Ontologies, web semantics, source code retrieval, source code search, information retrieval}
}

@article{10.1145/3564929,
author = {Lozano, Erika Susana Alcorta and Gerstlauer, Andreas},
title = {Learning-based Phase-aware Multi-core CPU Workload Forecasting},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1084-4309},
url = {https://doi.org/10.1145/3564929},
doi = {10.1145/3564929},
abstract = {Predicting workload behavior during workload execution is essential for dynamic resource optimization in multi-processor systems. Recent studies have proposed advanced machine learning techniques for dynamic workload prediction. Workload prediction can be cast as a time series forecasting problem. However, traditional forecasting models struggle to predict abrupt workload changes. These changes occur because workloads are known to go through phases. Prior work has investigated machine-learning-based approaches for phase detection and prediction, but such approaches have not been studied in the context of dynamic workload forecasting. In this article, we propose phase-aware CPU workload forecasting as a novel approach that applies long-term phase prediction to improve the accuracy of short-term workload forecasting. Phase-aware forecasting requires machine learning models for phase classification, phase prediction, and phase-based forecasting that have not been explored in this combination before. Furthermore, existing prediction approaches have only been studied in single-core settings. This work explores phase-aware workload forecasting with multi-threaded workloads running on multi-core systems. We propose different multi-core settings differentiated by the number of cores they access and whether they produce specialized or global outputs per core. We study various advanced machine learning models for phase classification, phase prediction, and phase-based forecasting in isolation and different combinations for each setting. We apply our approach to forecasting of multi-threaded Parsec and SPEC workloads running on an eight-core Intel Core-i9 platform. Our results show that combining GMM clustering with LSTMs for phase prediction and phase-based forecasting yields the best phase-aware forecasting results. An approach that uses specialized models per core achieves an average error of 23\% with up to 22\% improvement in prediction accuracy compared to a phase-unaware setup.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {23},
numpages = {27},
keywords = {Phase classification, multi-core workload forecasting, phase prediction, hardware counters}
}

@article{10.1145/3565269,
author = {Dantas, Yuri Gil and Nigam, Vivek},
title = {Automating Safety and Security Co-design through Semantically Rich Architecture Patterns},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3565269},
doi = {10.1145/3565269},
abstract = {During the design of safety-critical systems, safety and security engineers make use of architecture patterns, such as Watchdog and Firewall, to address identified failures and threats. Often, however, the deployment of safety architecture patterns has consequences on security; e.g., the deployment of a safety architecture pattern may lead to new threats. The other way around may also be possible; i.e., the deployment of a security architecture pattern may lead to new failures. Safety and security co-design is, therefore, required to understand such consequences and tradeoffs in order to reach appropriate system designs. Currently, architecture pattern descriptions, including their consequences, are described using natural language. Therefore, their deployment in system design is carried out manually by experts and thus is time-consuming and prone to human error, especially given the high system complexity. We propose the use of semantically rich architecture patterns to enable automated support for safety and security co-design by using Knowledge Representation and Reasoning (KRR) methods. Based on our domain-specific language, we specify reasoning principles as logic specifications written as answer-set programs. KRR engines enable the automation of safety and security co-engineering activities, including the automated recommendation of which architecture patterns can address failures or threats, and consequences of deploying such patterns. We demonstrate our approach on an example taken from the ISO 21434 standard.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = feb,
articleno = {5},
numpages = {28},
keywords = {Safety architecture patterns, security architecture patterns, automation, safety and security co-design, automotive vehicle systems}
}

@article{10.1145/3567444,
author = {Yuan, Gongsheng and Lu, Jiaheng and Yan, Zhengtong and Wu, Sai},
title = {A Survey on Mapping Semi-Structured Data and Graph Data to Relational Data},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567444},
doi = {10.1145/3567444},
abstract = {The data produced by various services should be stored and managed in an appropriate format for gaining valuable knowledge conveniently. This leads to the emergence of various data models, including relational, semi-structured, and graph models, and so on. Considering the fact that the mature relational databases established on relational data models are still predominant in today’s market, it has fueled interest in storing and processing semi-structured data and graph data in relational databases so that mature and powerful relational databases’ capabilities can all be applied to these various data. In this survey, we review existing methods on mapping semi-structured data and graph data into relational tables, analyze their major features, and give a detailed classification of those methods. We also summarize the merits and demerits of each method, introduce open research challenges, and present future research directions. With this comprehensive investigation of existing methods and open problems, we hope this survey can motivate new mapping approaches through drawing lessons from each model’s mapping strategies, as well as a new research topic - mapping multi-model data into relational tables.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {218},
numpages = {38},
keywords = {Relational schema, relational storage, semi-structured data, JSON, XML, graph data, RDF, property graph, model mapping}
}

@article{10.1145/3567594,
author = {Mahlaza, Zola and Keet, C. Maria},
title = {Surface Realization Architecture for Low-resourced African Languages},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3567594},
doi = {10.1145/3567594},
abstract = {There has been growing interest in building surface realization systems to support the automatic generation of text in African languages. Such tools focus on converting abstract representations of meaning to a text. Since African languages are low-resourced, economical use of resources and general maintainability are key considerations. However, there is no existing surface realizer architecture that possesses most of the maintainability characteristics (e.g., modularity, reusability, and analyzability) that will lead to maintainable software that can be used for the languages. Moreover, there is no consensus surface realization architecture created for other languages that can be adapted for the languages in question. In this work, we solve this by creating a novel surface realizer architecture suitable for low-resourced African languages that abides by the features of maintainable software. Its design comes after a granular analysis, classification, and comparison of the architectures used by 77 existing NLG systems. We compare our architecture to existing architectures and show that it supports the most features of a maintainable software product.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {84},
numpages = {26},
keywords = {Natural language generation, software architecture, low-resourced languages, surface realisation}
}

@article{10.1145/3567596,
author = {Yan, Han and Zhang, Haijun and Shi, Jianyang and Ma, Jianghong and Xu, Xiaofei},
title = {Toward Intelligent Fashion Design: A Texture and Shape Disentangled Generative Adversarial Network},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3567596},
doi = {10.1145/3567596},
abstract = {Texture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process, as they largely determine the success of a new design for fashion items. In this research, we propose a texture and shape disentangled generative adversarial network (TSD-GAN) to perform “intelligent” design with the transformation of texture and shape in fashion items. Our TSD-GAN aims to learn how to disentangle the features of texture and shape of different fashion items in an unsupervised manner. Specifically, a fashion attribute encoder is developed to decompose the input fashion items into independent representations of texture and shape. Then, to learn the coarse or fine styles hidden in the features of texture and shape, a texture mapping network and a shape mapping network are proposed to disentangle the features into different hierarchical representations. The different hierarchical representations of texture and shape are then fed into a multi-factor-based generator to generate mixed-style fashion items. In addition, a multi-discriminator framework is developed to distinguish the authenticity and texture similarity between the generated images and the real images. Experimental results on different fashion categories demonstrate that our proposed TSD-GAN may be useful for assisting designers to accomplish the design process by transforming the texture and shape of fashion items.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {107},
numpages = {23},
keywords = {Fashion design, generative adversarial network, disentanglement, texture, shape}
}

@article{10.1145/3569422,
author = {Shen, Ziyu and Liu, Binghui and Zhou, Qing and Liu, Zheng and Xia, Bin and Li, Yun},
title = {Cost-sensitive Tensor-based Dual-stage Attention LSTM with Feature Selection for Data Center Server Power Forecasting},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3569422},
doi = {10.1145/3569422},
abstract = {Power forecasting has a guiding effect on power-aware scheduling strategies to reduce unnecessary power consumption in data centers. Many metrics related to power consumption can be collected in physical servers, such as the status of CPU, memory, and other components. However, most existing methods empirically exploit a small number of metrics to forecast power consumption. To this end, this article uses feature selection based on causality to explore the metrics that strongly influence the power consumption of different tasks. Moreover, we propose a tensor-based dual-stage attention LSTM to forecast the non-linear and non-periodic power consumption. In the proposed model, a multi-way delay embedding transform is utilized to convert the time series into tensors along the temporal direction. The LSTM combines with the tensor technique and the attention mechanism to capture the temporal pattern effectively. In addition, we adopt the cost-sensitive loss function to optimize the specific power forecasting problem in data centers. The experimental results demonstrate that our method can achieve up to 1.4\% to 4.3\% forecasting accuracy improvement compared with the state-of-the-art models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {20},
keywords = {Power forecasting, data center, time series, tensor, LSTM, feature selection}
}

@article{10.1145/3569941,
author = {Zhang, Qing and Huang, Huajie and Li, Jizuo and Zhang, Yuhang and Li, Yongfu},
title = {CmpCNN: CMP Modeling with Transfer Learning CNN Architecture},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/3569941},
doi = {10.1145/3569941},
abstract = {Performing chemical mechanical polishing (CMP) modeling for physical verification on an integrated circuit (IC) chip is vital to minimize its manufacturing yield loss. Traditional CMP models calculate post-CMP topography height of the IC’s layout based on physical principles and empirical experiments, which is computationally costly and time-consuming. In this work, we propose a CmpCNN framework based on convolutional neural networks (CNNs) with a transfer learning method to accelerate the CMP modeling process. It utilizes a multi-input strategy by feeding the binary image of layout and its density into our CNN-based model to extract features more efficiently. The transfer learning method is adopted to different CMP process parameters and different categories of circuits to further improve its prediction accuracy and convergence speed. Experimental results show that our CmpCNN framework achieves a competitive root mean square error (RMSE) of 2.7733\r{A} with 1.89\texttimes{} reduction compared to the prior work, and a 57\texttimes{} speedup compared to the commercial CMP simulation tool.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
articleno = {58},
numpages = {18},
keywords = {Physical verification, chemical mechanical polishing, tranfer learning, convolutional neural network}
}

@article{10.1145/3570918,
author = {Dong, Yi and Huang, Wei and Bharti, Vibhav and Cox, Victoria and Banks, Alec and Wang, Sen and Zhao, Xingyu and Schewe, Sven and Huang, Xiaowei},
title = {Reliability Assessment and Safety Arguments for Machine Learning Components in System Assurance},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3570918},
doi = {10.1145/3570918},
abstract = {The increasing use of Machine Learning (ML) components embedded in autonomous systems—so-called Learning-Enabled Systems (LESs)—has resulted in the pressing need to assure their functional safety. As for traditional functional safety, the emerging consensus within both, industry and academia, is to use assurance cases for this purpose. Typically assurance cases support claims of reliability in support of safety, and can be viewed as a structured way of organising arguments and evidence generated from safety analysis and reliability modelling activities. While such assurance activities are traditionally guided by consensus-based standards developed from vast engineering experience, LESs pose new challenges in safety-critical application due to the characteristics and design of ML models. In this article, we first present an overall assurance framework for LESs with an emphasis on quantitative aspects, e.g., breaking down system-level safety targets to component-level requirements and supporting claims stated in reliability metrics. We then introduce a novel model-agnostic Reliability Assessment Model (RAM) for ML classifiers that utilises the operational profile and robustness verification evidence. We discuss the model assumptions and the inherent challenges of assessing ML reliability uncovered by our RAM and propose solutions to practical use. Probabilistic safety argument templates at the lower ML component-level are also developed based on the RAM. Finally, to evaluate and demonstrate our methods, we not only conduct experiments on synthetic/benchmark datasets but also scope our methods with case studies on simulated Autonomous Underwater Vehicles and physical Unmanned Ground Vehicles.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {48},
numpages = {48},
keywords = {Software reliability, safety arguments, assurance cases, safe AI, robustness verification, safety-critical systems, statistical testing, operational profile, probabilistic claims, Learning-Enabled Systems, Robotics and Autonomous Systems, safety regulation}
}

@article{10.1145/3572403,
author = {Yang, Yingguang and Yang, Renyu and Li, Yangyang and Cui, Kai and Yang, Zhiqin and Wang, Yue and Xu, Jie and Xie, Haiyong},
title = {RoSGAS: Adaptive Social Bot Detection with Reinforced Self-supervised GNN Architecture Search},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3572403},
doi = {10.1145/3572403},
abstract = {Social bots are referred to as the automated accounts on social networks that make attempts to behave like humans. While Graph Neural Networks (GNNs) have been massively applied to the field of social bot detection, a huge amount of domain expertise and prior knowledge is heavily engaged in the state-of-the-art approaches to design a dedicated neural network architecture for a specific classification task. Involving oversized nodes and network layers in the model design, however, usually causes the over-smoothing problem and the lack of embedding discrimination. In this article, we propose RoSGAS, a novel Reinforced and Self-supervised GNN Architecture Search framework to adaptively pinpoint the most suitable multi-hop neighborhood and the number of layers in the GNN architecture. More specifically, we consider the social bot detection problem as a user-centric subgraph embedding and classification task. We exploit the heterogeneous information network to present the user connectivity by leveraging account metadata, relationships, behavioral features, and content features. RoSGAS uses a multi-agent deep reinforcement learning (RL), 31 pages. mechanism for navigating the search of optimal neighborhood and network layers to learn individually the subgraph embedding for each target user. A nearest neighbor mechanism is developed for accelerating the RL training process, and RoSGAS can learn more discriminative subgraph embedding with the aid of self-supervised learning. Experiments on five Twitter datasets show that RoSGAS outperforms the state-of-the-art approaches in terms of accuracy, training efficiency, and stability and has better generalization when handling unseen samples.},
journal = {ACM Trans. Web},
month = may,
articleno = {15},
numpages = {31},
keywords = {Graph neural network, architecture search, reinforcement learning}
}

@article{10.1145/3573206,
author = {Picone, Marco and Mamei, Marco and Zambonelli, Franco},
title = {A Flexible and Modular Architecture for Edge Digital Twin: Implementation and Evaluation},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3573206},
doi = {10.1145/3573206},
abstract = {IoT systems based on Digital Twins (DTs) — virtual copies of physical objects and systems — can be very effective to enable data-driven services and promote better control and decisions, in particular by exploiting distributed approaches where cloud and edge computing cooperate effectively. In this context, digital twins deployed on the edge represents a new strategic element to design a new wave of distributed cyber-physical applications. Existing approaches are generally focused on fragmented and domain-specific monolithic solutions and are mainly associated to model-driven, simulative or descriptive visions. The idea of extending the DTs role to support last-mile digitalization and interoperability through a set of general purpose and well-defined properties and capabilities is still underinvestigated. In this paper, we present the novel Edge Digital Twins (EDT) architectural model and its implementation, enabling the lightweight replication of physical devices providing an efficient digital abstraction layer to support the autonomous and standard collaboration of things and services. We model the core capabilities with respect to the recent definition of the state of the art, present the software architecture and a prototype implementation. Extensive experimental analysis shows the obtained performance in multiple IoT application contexts and compares them with that of state-of-the-art approaches.},
journal = {ACM Trans. Internet Things},
month = feb,
articleno = {8},
numpages = {32},
keywords = {Digital Twin, Internet of Things, edge computing}
}

@article{10.1145/3573892,
author = {Levine, Alan and Tucker, Brett Alan},
title = {Zero Trust Architecture: Risk Discussion},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3573892},
doi = {10.1145/3573892},
journal = {Digital Threats},
month = mar,
articleno = {15},
numpages = {6},
keywords = {Zero Trust, cybersecurity, cyber controls, risk, risk management}
}

@article{10.1145/3574323,
author = {Yang, Suli and Liu, Jing and Arpaci-Dusseau, Andrea and Arpaci-Dusseau, Remzi},
title = {Principled Schedulability Analysis for Distributed Storage Systems Using Thread Architecture Models},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3574323},
doi = {10.1145/3574323},
abstract = {In this article, we present an approach to systematically examine the schedulability of distributed storage systems, identify their scheduling problems, and enable effective scheduling in these systems. We use Thread Architecture Models (TAMs) to describe the behavior and interactions of different threads in a system, and show both how to construct TAMs for existing systems and utilize TAMs to identify critical scheduling problems. We specify three schedulability conditions that a schedulable TAM should satisfy: completeness, local enforceability, and independence; meeting these conditions enables a system to easily support different scheduling policies. We identify five common problems that prevent a system from satisfying the schedulability conditions, and show that these problems arise in existing systems such as HBase, Cassandra, MongoDB, and Riak, making it difficult or impossible to realize various scheduling disciplines. We demonstrate how to address these schedulability problems using both direct and indirect solutions, with different trade-offs. To show how to apply our approach to enable scheduling in realistic systems, we develop Tamed-HBase and Muzzled-HBase, sets of modifications to HBase that can realize the desired scheduling disciplines, including fairness and priority scheduling, even when presented with challenging workloads.},
journal = {ACM Trans. Storage},
month = mar,
articleno = {17},
numpages = {47},
keywords = {Request scheduling, thread architecture, performance isolation}
}

@article{10.1145/3575798,
author = {Tuli, Shikhar and Li, Chia-Hao and Sharma, Ritvik and Jha, Niraj K.},
title = {CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3575798},
doi = {10.1145/3575798},
abstract = {Recently, automated co-design of machine learning (ML) models and accelerator architectures has attracted significant attention from both the industry and academia. However, most co-design frameworks either explore a limited search space or employ suboptimal exploration techniques for simultaneous design decision investigations of the ML model and the accelerator. Furthermore, training the ML model and simulating the accelerator performance is computationally expensive. To address these limitations, this work proposes a novel neural architecture and hardware accelerator co-design framework, called CODEBench. It comprises two new benchmarking sub-frameworks, CNNBench and AccelBench, which explore expanded design spaces of convolutional neural networks (CNNs) and CNN accelerators. CNNBench leverages an advanced search technique, Bayesian Optimization using Second-order Gradients and Heteroscedastic Surrogate Model for Neural Architecture Search, to efficiently train a neural heteroscedastic surrogate model to converge to an optimal CNN architecture by employing second-order gradients. AccelBench performs cycle-accurate simulations for diverse accelerator architectures in a vast design space. With the proposed co-design method, called Bayesian Optimization using Second-order Gradients and Heteroscedastic Surrogate Model for Co-Design of CNNs and Accelerators, our best CNN–accelerator pair achieves 1.4\% higher accuracy on the CIFAR-10 dataset compared to the state-of-the-art pair while enabling 59.1\% lower latency and 60.8\% lower energy consumption. On the ImageNet dataset, it achieves 3.7\% higher Top1 accuracy at 43.8\% lower latency and 11.2\% lower energy consumption. CODEBench outperforms the state-of-the-art framework, i.e., Auto-NBA, by achieving 1.5\% higher accuracy and 34.7\texttimes{} higher throughput while enabling 11.0\texttimes{} lower energy-delay product and 4.0\texttimes{} lower chip area on CIFAR-10.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {51},
numpages = {30},
keywords = {Active learning, application-specific integrated circuits, hardware-software co-design, machine learning, Neural Architecture Search, neural network accelerators}
}

@article{10.1145/3576856,
author = {Suto, Kai and Noma, Yuta and Tanimichi, Kotaro and Narumi, Koya and Tachi, Tomohiro},
title = {Crane: An Integrated Computational Design Platform for Functional, Foldable, and Fabricable Origami Products},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3576856},
doi = {10.1145/3576856},
abstract = {Despite the recent trend of computational origami for human-computer interaction (HCI) and digital fabrication, it is still difficult for designers to complete a series of design, simulation, and fabrication of objects leveraging computational origami theory. In this paper, we propose Crane, an integrated origami design platform implemented with Grasshopper. With this platform, users can seamlessly (1) design the 2D and 3D crease pattern, (2) simulate 3D folding transformation from the given crease pattern, (3) inversely find a new pattern under design constraints, (4) thicken the 2D pattern into a 3D volume along with the appropriate hinge structures for different fabrication methods, and (5) optionally connect the resulting design to other Rhinoceros or Grasshopper plugins for post-processes. To help understand how to use our system and demonstrate its feasibility, we showed three examples of origami products designed using our system. We also reported user feedback from the workshop as an evaluation.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {52},
numpages = {29},
keywords = {Computational origami, computational fabrication, computational design, digital fabrication, graphical user interfaces, seamless design and fabrication, industrial design}
}

@article{10.1145/3577032,
author = {Zhu, Guixiang and Cao, Jie and Chen, Lei and Wang, Youquan and Bu, Zhan and Yang, Shuxin and Wu, Jianqing and Wang, Zhiping},
title = {A Multi-Task Graph Neural Network with Variational Graph Auto-Encoders for Session-Based Travel Packages Recommendation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3577032},
doi = {10.1145/3577032},
abstract = {Session-based travel packages recommendation aims to predict users’ next click based on their current and historical sessions recorded by Online Travel Agencies (OTAs). Recently, an increasing number of studies attempted to apply Graph Neural Networks (GNNs) to the session-based recommendation and obtained promising results. However, most of them do not take full advantage of the explicit latent structure from attributes of items, making learned representations of items less effective and difficult to interpret. Moreover, they only combine historical sessions (long-term preferences) with a current session (short-term preference) to learn a unified representation of users, ignoring the effects of historical sessions for the current session. To this end, this article proposes a novel session-based model named STR-VGAE, which fills subtasks of the travel packages recommendation and variational graph auto-encoders simultaneously. STR-VGAE mainly consists of three components: travel packages encoder, users behaviors encoder, and interaction modeling. Specifically, the travel packages encoder module is used to learn a unified travel package representation from co-occurrence attribute graphs by using multi-view variational graph auto-encoders and a multi-view attention network. The users behaviors encoder module is used to encode user’ historical and current sessions with a personalized GNN, which considers the effects of historical sessions on the current session, and coalesce these two kinds of session representations to learn the high-quality users’ representations by exploiting a gated fusion approach. The interaction modeling module is used to calculate recommendation scores over all candidate travel packages. Extensive experiments on a real-life tourism e-commerce dataset from China show that STR-VGAE yields significant performance advantages over several competitive methods, meanwhile provides an interpretation for the generated recommendation list.},
journal = {ACM Trans. Web},
month = may,
articleno = {18},
numpages = {30},
keywords = {Travel recommendation, graph mining, session-based recommendation, auto-encoder, multi-task learning}
}

@article{10.1145/3578265,
author = {Berrimi, Mohamed and Oussalah, Mourad and Moussaoui, Abdelouahab and Saidi, Mohamed},
title = {Attention Mechanism Architecture for Arabic Sentiment Analysis},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3578265},
doi = {10.1145/3578265},
abstract = {This article tackles the problem of sentiment analysis in the Arabic language where a new deep learning model has been put forward. The proposed model uses a hybrid bidirectional gated recurrent unit (BiGRU) and bidirectional long short-term memory (BiLSTM) additive-attention model where the Bidirectional GRU/LSTM reads the individual sentence input from left to right and vice versa, enabling the capture of the contextual information. However, the model is trained on two types of embeddings: FastText and local learnable embeddings. The BiLSTM and BiGRU architectures are put into competition to identify the best hyperparameter set for the model. The developed model has been tested on three large-scale commonly employed Arabic sentiment dataset: large-scale Arabic Book Reviews Dataset (ABRD), Hotel Arabic-Reviews Dataset (HARD), and Books Reviews in the Arabic Dataset (BRAD). The testing results demonstrate that our model outperforms both the baseline models and the state-of-the-art models reported in the original references of these datasets, achieving accuracy scores of 98.6\%, 96.19\%, 95.65\% for LARB, HARD, and BRAD, respectively. Furthermore, to demonstrate the generalization capabilities of our model, the performances of the model have been evaluated on three other natural language processing tasks: news categorization, offensive speech detection, and Russian sentiment analysis. The results demonstrated the developed model is language- and task-independent, which offers new perspectives for the application of the developed models in several other natural language processing challenges.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {107},
numpages = {26},
keywords = {Arabic sentiment analysis, attention mechanism, pretrained word embeddings, language understanding}
}

@article{10.1145/3578365,
author = {Wang, Yuxiang and Liu, Jun and Xu, Xiaoliang and Ke, Xiangyu and Wu, Tianxing and Gou, Xiaoxuan},
title = {Efficient and Effective Academic Expert Finding on Heterogeneous Graphs through (k, 𝒫)-Core based Embedding},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3578365},
doi = {10.1145/3578365},
abstract = {Expert finding is crucial for a wealth of applications in both academia and industry. Given a user query and trove of academic papers, expert finding aims at retrieving the most relevant experts for the query, from the academic papers. Existing studies focus on embedding-based solutions that consider academic papers’ textual semantic similarities to a query via document representation and extract the top-n experts from the most similar papers. Beyond implicit textual semantics, however, papers’ explicit relationships (e.g., co-authorship) in a heterogeneous graph (e.g., DBLP) are critical for expert finding, because they help improve the representation quality. Despite their importance, the explicit relationships of papers generally have been ignored in the literature. In this article, we study expert finding on heterogeneous graphs by considering both the explicit relationships and implicit textual semantics of papers in one model. Specifically, we define the cohesive (k, 𝒫)-core community of papers w.r.t. a meta-path 𝒫 (i.e., relationship) and propose a (k, 𝒫)-core based document embedding model to enhance the representation quality. Based on this, we design a proximity graph-based index (PG-Index) of papers and present a threshold algorithm (TA)-based method to efficiently extract top-n experts from papers returned by PG-Index. We further optimize our approach in two ways: (1) we boost effectiveness by considering the (k, 𝒫)-core community of experts and the diversity of experts’ research interests, to achieve high-quality expert representation from paper representation; and (2) we streamline expert finding, going from “extract top-n experts from top-m (m&gt; n) semantically similar papers” to “directly return top-n experts”. The process of returning a large number of top-m papers as intermediate data is avoided, thereby improving the efficiency. Extensive experiments using real-world datasets demonstrate our approach’s superiority.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {85},
numpages = {35},
keywords = {Expert finding, (k 𝒫)-core community, document/expert embedding, heterogeneous graph}
}

@article{10.1145/3578709,
author = {Li, Yinqiao and Cao, Runzhe and He, Qiaozhi and Xiao, Tong and Zhu, Jingbo},
title = {Learning Reliable Neural Networks with Distributed Architecture Representations},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3578709},
doi = {10.1145/3578709},
abstract = {Neural architecture search (NAS) has shown the strong performance of learning neural models automatically in recent years. But most NAS systems are unreliable due to the architecture gap brought by discrete representations of atomic architectures. In this article, we improve the performance and robustness of NAS via narrowing the gap between architecture representations. More specifically, we apply a general contraction mapping to model neural networks with distributed representations (Neural Architecture Search with Distributed Architecture Representations (ArchDAR)). Moreover, for a better search result, we present a joint learning approach to integrating distributed representations with advanced architecture search methods. We implement our ArchDAR in a differentiable architecture search model and test learned architectures on the language modeling task. On the Penn Treebank data, it outperforms a strong baseline significantly by 1.8 perplexity scores. Also, the search process with distributed representations is more stable, which yields a faster structural convergence when it works with the differentiable architecture search model.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {109},
numpages = {20},
keywords = {Neural architecture search, neural networks, language modeling, natural language processing}
}

@article{10.1145/3579364,
author = {Ferdous, Javedul and Lee, Hae-Na and Jayarathna, Sampath and Ashok, Vikas},
title = {Enabling Efficient Web Data-Record Interaction for People with Visual Impairments via Proxy Interfaces},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3579364},
doi = {10.1145/3579364},
abstract = {Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for facilitating quick and easy access to desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by increased reduction in interaction time and user effort, compared to the state-of-the-art solutions.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = sep,
articleno = {13},
numpages = {27},
keywords = {Web accessibility, blind, low vision, visual impairment, screen reader, screen magnifier, data records}
}

@article{10.1145/3583139,
author = {Feng, Guanyu and Cao, Huanqi and Zhu, Xiaowei and Yu, Bowen and Wang, Yuanwei and Ma, Zixuan and Chen, Shengqi and Chen, Wenguang},
title = {TriCache: A User-Transparent Block Cache Enabling High-Performance Out-of-Core Processing with In-Memory Programs},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3583139},
doi = {10.1145/3583139},
abstract = {Out-of-core systems rely on high-performance cache sub-systems to reduce the number of I/O operations. Although the page cache in modern operating systems enables transparent access to memory and storage devices, it suffers from efficiency and scalability issues on cache misses, forcing out-of-core systems to design and implement their own cache components, which is a non-trivial task.This study proposes TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface.Our evaluation shows that in-memory systems operating on top of TriCache can outperform Linux OS page cache by more than one order of magnitude, and can deliver performance comparable to or even better than that of corresponding counterparts designed specifically for out-of-core scenarios.},
journal = {ACM Trans. Storage},
month = mar,
articleno = {15},
numpages = {30},
keywords = {Page cache, block cache, buffer management, solid-state drives}
}

@article{10.1145/3583560,
author = {Agullo, Emmanuel and Buttari, Alfredo and Guermouche, Abdou and Herrmann, Julien and Jego, Antoine},
title = {Task-based Parallel Programming for Scalable Matrix Product Algorithms},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3583560},
doi = {10.1145/3583560},
abstract = {Task-based programming models have succeeded in gaining the interest of the high-performance mathematical software community because they relieve part of the burden of developing and implementing distributed-memory parallel algorithms in an efficient and portable way.In increasingly larger, more heterogeneous clusters of computers, these models appear as a way to maintain and enhance more complex algorithms. However, task-based programming models lack the flexibility and the features that are necessary to express in an elegant and compact way scalable algorithms that rely on advanced communication patterns. We show that the Sequential Task Flow paradigm can be extended to write compact yet efficient and scalable routines for linear algebra computations. Although, this work focuses on dense General Matrix Multiplication, the proposed features enable the implementation of more complex algorithms. We describe the implementation of these features and of the resulting GEMM operation. Finally, we present an experimental analysis on two homogeneous supercomputers showing that our approach is competitive up to 32,768 CPU cores with state-of-the-art libraries and may outperform them for some problem dimensions. Although our code can use GPUs straightforwardly, we do not deal with this case because it implies other issues which are out of the scope of this work.},
journal = {ACM Trans. Math. Softw.},
month = jun,
articleno = {15},
numpages = {23},
keywords = {Parallel programming models, scalable linear algebra algorithms, runtime systems, distributed memory parallelism, sequential task flow}
}

@article{10.1145/3583741,
author = {Mavrogiannis, Christoforos and Baldini, Francesca and Wang, Allan and Zhao, Dapeng and Trautman, Pete and Steinfeld, Aaron and Oh, Jean},
title = {Core Challenges of Social Robot Navigation: A Survey},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
url = {https://doi.org/10.1145/3583741},
doi = {10.1145/3583741},
abstract = {Robot navigation in crowded public spaces is a complex task that requires addressing a variety of engineering and human factors challenges. These challenges have motivated a great amount of research resulting in important developments for the fields of robotics and human-robot interaction over the past three decades. Despite the significant progress and the massive recent interest, we observe a number of significant remaining challenges that prohibit the seamless deployment of autonomous robots in crowded environments. In this survey article, we organize existing challenges into a set of categories related to broader open problems in robot planning, behavior design, and evaluation methodologies. Within these categories, we review past work and offer directions for future research. Our work builds upon and extends earlier survey efforts by (a) taking a critical perspective and diagnosing fundamental limitations of adopted practices in the field and (b) offering constructive feedback and ideas that could inspire research in the field over the coming decade.},
journal = {J. Hum.-Robot Interact.},
month = apr,
articleno = {36},
numpages = {39},
keywords = {Social robot navigation, motion planning, motion prediction, multiagent systems, social robotics, benchmarking}
}

@article{10.1145/3584945,
author = {Wei, Lanning and Zhao, Huan and He, Zhiqiang and Yao, Quanming},
title = {Neural Architecture Search for GNN-Based Graph Classification},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3584945},
doi = {10.1145/3584945},
abstract = {Graph classification is an important problem with applications across many domains, for which graph neural networks (GNNs) have been state-of-the-art (SOTA) methods. In the literature, to adopt GNNs for the graph classification task, there are two groups of methods: global pooling and hierarchical pooling. The global pooling methods obtain the graph representation vectors by globally pooling all of the node embeddings together at the end of several GNN layers, whereas the hierarchical pooling methods provide one extra pooling operation between the GNN layers to extract hierarchical information and improve the graph representations. Both global and hierarchical pooling methods are effective in different scenarios. Due to highly diverse applications, it is challenging to design data-specific pooling methods with human expertise. To address this problem, we propose PAS (Pooling Architecture Search) to design adaptive pooling architectures by using the neural architecture search (NAS). To enable the search space design, we propose a unified pooling framework consisting of four modules: Aggregation, Pooling, Readout, and Merge. Two variants, PAS-G and PAS-NE, are provided to design the pooling operations in different scales. A set of candidate operations is designed in the search space using this framework. Then, existing human-designed pooling methods, including global and hierarchical ones, can be incorporated. To enable efficient search, a coarsening strategy is developed to continuously relax the search space, and then a differentiable search method can be adopted. We conduct extensive experiments on six real-world datasets, including the large-scale datasets MR and ogbg-molhiv. Experimental results in this article demonstrate the effectiveness and efficiency of the proposed PAS in designing the pooling architectures for graph classification. The Top-1 performance on two Open Graph Benchmark (OGB) datasets1 further indicates the utility of PAS when facing diverse realistic data. The implementation of PAS is available at: https://github.com/AutoML-Research/PAS.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {1},
numpages = {29},
keywords = {Graph classification, Graph Neural Networks, neural architecture search}
}

@article{10.1145/3585005,
author = {Qi, Hua and Wang, Zhijie and Guo, Qing and Chen, Jianlang and Juefei-Xu, Felix and Zhang, Fuyuan and Ma, Lei and Zhao, Jianjun},
title = {ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585005},
doi = {10.1145/3585005},
abstract = {Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons’ status and weights’ gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {129},
numpages = {31},
keywords = {Deep learning, DNN repair, neural architecture search}
}

@article{10.1145/3585537,
author = {Shaharudin, Ashraf and van Loenen, Bastiaan and Janssen, Marijn},
title = {Towards a Common Definition of Open Data Intermediaries},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3585537},
doi = {10.1145/3585537},
abstract = {The role of open data intermediaries is considered instrumental in the supply and use of open data. There are various definitions of open data intermediaries in the literature and some of them are quite different from each other. These definitions can benefit from harmonization so knowledge about open data intermediaries can be developed on top of a shared understanding of what open data intermediaries mean. The objective of this article is to propose a common definition of open data intermediaries. We first carried out a systematic literature review and compiled the definitions of open data intermediaries from the literature. We found that each definition can be broken down into four basic components: (i) Who are the actors of open data intermediaries? (ii) What do they do? (iii) Where are they located in the open data lifecycle? and (iv) Why are they needed? We then conducted another round of data gathering and analysis to substantiate the four basic components. We proposed the following common definition of open data intermediaries: Third-party actors who provide specialized resources and capabilities to (i) enhance the supply, flow, and/or use of open data and/or (ii) strengthen the relationships among various open data stakeholders.},
journal = {Digit. Gov.: Res. Pract.},
month = jun,
articleno = {6},
numpages = {21},
keywords = {Open data, intermediaries, infomediaries, definition}
}

@article{10.1145/3585539,
author = {Roberts, Joshua D. and Defranco, Joanna F. and Kuhn, D. Richard},
title = {Data Block Matrix and Hyperledger Implementation: Extending Distributed Ledger Technology for Privacy Requirements},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3585539},
doi = {10.1145/3585539},
abstract = {Distributed ledger technology (DLT), including blockchain, has a number of properties that make it useful for distributed systems. However, the immutability of blockchain and most forms of DLT make it impossible to delete data, as is required for compliance with many privacy rules regarding personally identifiable information. Thus, there is a need for DLT that can provide the integrity-preserving property of DLT while also allowing support for privacy rules. The data block matrix (DBM) is a variant of distributed ledger technology. It provides the integrity assurance of blockchain but allows for controlled revision or deletion of data. This property is essential for using DLT in applications that must guarantee privacy requirements by the deleting of a user's private data at their request. The DBM design solves the blockchain privacy conflict thus expanding the range of blockchain applications by also allowing exception management. It has been implemented and is available () as a configurable option for Hyperledger Fabric (HF), with a proof-of-concept application for data sharing in a health care environment. Other potential applications include logistics management and digital currency. This paper will cover the DBM properties and data structure, the DBM implementation in HF, and a use case and application design of the DBM implementation using the pharmaceutical industry supply chain.},
journal = {Distrib. Ledger Technol.},
month = jun,
articleno = {16},
numpages = {11},
keywords = {Blockchain, data structures, distributed ledger, security and privacy}
}

@article{10.1145/3586039,
author = {Kang, Chan Gu and Oh, Hakjoo},
title = {Modular Component-Based Quantum Circuit Synthesis},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586039},
doi = {10.1145/3586039},
abstract = {In this article, we present a novel method for synthesizing quantum circuits from user-supplied components. Given input-output state vectors and component quantum gates, our synthesizer aims to construct a quantum circuit that implements the provided functionality in terms of the supplied component gates.&nbsp;To achieve this, we basically use an enumerative search with pruning. To accelerate the procedure, however, we perform the search and pruning at the module level; instead of simply enumerating candidate circuits by appending component gates in sequence, we stack modules, which are groups of gate operations.  
With this modular approach, we can effectively reduce the search space by directing the search in a way that bridges the gap between the current circuit and the input-output specification.  
Evaluation on 17 benchmark problems shows that our technique is highly effective at synthesizing quantum circuits. Our method successfully synthesized 16 out of 17 benchmark circuits in 96.6 seconds on average. On the other hand, the conventional, gate-level synthesis algorithm succeeded in 10 problems with an average time of 639.1 seconds. Our algorithm increased the speed of the baseline by 20.3x for the 10 problems commonly solved by both approaches.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {87},
numpages = {28},
keywords = {Quantum circuit synthesis, Quantum programming}
}

@article{10.1145/3586040,
author = {Fox, Anthony C. J. and Stockwell, Gareth and Xiong, Shale and Becker, Hanno and Mulligan, Dominic P. and Petri, Gustavo and Chong, Nathan},
title = {A Verification Methodology for the Arm® Confidential Computing Architecture: From a Secure Specification to Safe Implementations},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586040},
doi = {10.1145/3586040},
abstract = {We present Arm's efforts in verifying the specification and prototype reference implementation of the Realm Management Monitor (RMM), an essential firmware component of Arm Confidential Computing Architecture (Arm CCA), the recently-announced Confidential Computing technologies incorporated in the Armv9-A architecture. Arm CCA introduced the Realm Management Extension (RME), an architectural extension for Armv9-A, and a technology that will eventually be deployed in hundreds of millions of devices. Given the security-critical nature of the RMM, and its taxing threat model, we use a combination of interactive theorem proving, model checking, and concurrency-aware testing to validate and verify security and safety properties of both the specification and a prototype implementation of the RMM. Crucially, our verification efforts were, and are still being, developed and refined contemporaneously with active development of both specification and implementation, and have been adopted by Arm's product teams.  

We describe our major achievements, realized through the application of formal techniques, as well as challenges that remain for future work. We believe that the work reported in this paper is the most thorough application of formal techniques to the design and implementation of any current commercially-viable Confidential Computing implementation, setting a new high-water mark for work in this area.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {88},
numpages = {30},
keywords = {Arm Confidential Computing Architecture (Arm CCA), Confidential Computing, formal methods, operating system verification, separation kernel}
}

@article{10.1145/3586158,
author = {Jang, Sun-Young and Kim, Sung-Ah},
title = {Automatic Extraction and Linkage between Textual and Spatial Data for Architectural Heritage},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3586158},
doi = {10.1145/3586158},
abstract = {Recent developments in experience technologies such as augmented reality (AR)/virtual reality (VR) have facilitated receiving content about the audience on site and experiencing architectural heritage in a virtual space. Despite the development of experience devices, if the quantity and quality of content are not sufficiently supported, then immersive user experiences are bound to be limited. Considerable amounts of money, manpower, and time are required to make a building into experience content. Tasks such as building a database create experiential content that occupies a large proportion of the overall process. Therefore, it is necessary to devise an automated method for building data, which is the basis for content creation. This study extracted data on architectural heritage automatically and structured it around spatial expression so it can function as base work for mass content creation. Specifically, this study devised a method to link and structure text and spatial data centering on the architectural spatial data model. Text and spatial data were extracted automatically using deep learning, and each derived result was mapped to Indoor Affordance Spaces—an indoor spatial data model—to test whether information inference is possible based on the interconnection relationship. The spatial experience route inferred using the data model expresses the detailed area where the viewing element exists, based on the description method of the model. It also shows the process of reconstructing an efficient movement line with topological relationships between spaces. The series of processes presented herein showed sufficient applicability to the extraction of data and the connection and utilization of data models. This is useful for extracting and classifying information used for content from massive raw data. This study also considered the specificity arising from architectural heritage and spatial information. Therefore, the research concept can be applied in exhibition and experience spaces, such as architectural heritage, museums, and art galleries, to create sources for content creation and refer to content composition.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {48},
numpages = {19},
keywords = {Architectural heritage, data linkage, experience design, Indoor Affordance Spaces, information extraction}
}

@article{10.1145/3588578,
author = {Shibata, Ryoichi and Matsumori, Shoya and Fukuchi, Yosuke and Maekawa, Tomoyuki and Kimoto, Mitsuhiko and Imai, Michita},
title = {Conversational Context-sensitive Ad Generation with a Few Core-Queries},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3588578},
doi = {10.1145/3588578},
abstract = {When people are talking together in front of digital signage, advertisements that are aware of the context of the dialogue will work the most effectively. However, it has been challenging for computer systems to retrieve the appropriate advertisement from among the many options presented in large databases. Our proposed system, the Conversational Context-sensitive Advertisement generator (CoCoA), is the first attempt to apply masked word prediction to web information retrieval that takes into account the dialogue context. The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA automatically generates a context-sensitive expression as a complete search query by utilizing a masked word prediction technique that adds a word related to the dialogue context to one of the prepared Core-Queries. This automatic generation frees the advertisers from having to come up with context-sensitive phrases to attract users’ attention. Another unique point is that the modified Core-Query offers users speaking in front of the CoCoA system a list of context-sensitive advertisements. CoCoA was evaluated by crowd workers regarding the context-sensitivity of the generated search queries against the dialogue text of multiple domains prepared in advance. The results indicated that CoCoA could present more contextual and practical advertisements than other web-retrieval systems. Moreover, CoCoA acquired a higher evaluation in a particular conversation that included many travel topics to which the Core-Queries were designated, implying that it succeeded in adapting the Core-Queries for the specific ongoing context better than the compared method without any effort on the part of the advertisers. In addition, case studies with users and advertisers revealed that the context-sensitive advertisements generated by CoCoA also had an effect on the content of the ongoing dialogue. Specifically, since pairs unfamiliar with each other more frequently referred to the advertisement CoCoA displayed, the advertisements had an effect on the topics about which the pairs spoke. Moreover, participants of an advertiser role recognized that some of the search queries generated by CoCoA fit the context of a conversation and that CoCoA improved the effect of the advertisement. In particular, they learned how to design of designing a good Core-Query at ease by observing the users’ response to the advertisements retrieved with the generated search queries.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = sep,
articleno = {15},
numpages = {37},
keywords = {Dialogue, context, advertisement, query generation, mask prediction}
}

@article{10.1145/3588772,
author = {Zhang, Xiaobin and Xu, Hongzhe and Liu, Jianwei and Han, Jinsong},
title = {TomFi: Small Object Tracking Using Commodity WiFi},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3588772},
doi = {10.1145/3588772},
abstract = {Rodent infestation has always been one of the most severe threats to humans, which to solve consumes massive manpower and resources. People usually use traps or poisons to treat rat infestation. Such passive countermeasures are inefficient. Damage often occurs when the rats are finally trapped or killed, not to mention the potential risk to injure humans or cause pollution. In this article, we propose a WiFi-based active small object tracking system named TomFi. The core components of TomFi include several deep learning techniques that bridge rat locations/motions and WiFi signal variations (represented by the channel state information). TomFi first employs a detection model to detect the appearance of the rat and then localize it based on a two-branch localization model. Through the design of the two-branch localization model, to our best knowledge, we are the first to solve the information loss and distortion problem. We conducted extensive experiments in both the laboratory environment and real-world kitchen scenarios. The results show that TomFi can achieve a detection success rate of 99\%+ and a centimeter-level localization accuracy in real time.},
journal = {ACM Trans. Sen. Netw.},
month = may,
articleno = {80},
numpages = {15},
keywords = {WiFi sensing, object detection, deep learning}
}

@article{10.1145/3588921,
author = {Zhu, Yiwen and Sen, Rathijit and Horton, Robert and Agosta, John Mark},
title = {Runtime Variation in Big Data Analytics},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588921},
doi = {10.1145/3588921},
abstract = {The dynamic nature of resource allocation and runtime conditions on Cloud can result in high variability in a job's runtime across multiple iterations, leading to a poor experience. Identifying the sources of such variation and being able to predict and adjust for them is crucial to cloud service providers to design reliable data processing pipelines, provision and allocate resources, adjust pricing services, meet SLOs and debug performance hazards.In this paper, we analyze the runtime variation of millions of production Scope jobs on Cosmos, an exabyte-scale internal analytics platform at Microsoft. We propose an innovative 2-step approach to predict job runtime distribution by characterizing typical distribution shapes combined with a classification model with an average accuracy of &gt;96\%, using an innovative interpretable machine-learning algorithm out-performing traditional regression models and better capturing long tails. We examine factors such as job plan characteristics and inputs, resource allocation, physical cluster heterogeneity and utilization, and scheduling policies.To the best of our knowledge, this is the first study on predicting categories of runtime distributions for enterprise analytics workloads at scale. Furthermore, we examine how our methods can be used to analyze what-if scenarios, focusing on the impact of resource allocation, scheduling, and physical cluster provisioning decisions on a job's runtime consistency and predictability.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {67},
numpages = {20},
keywords = {big data, clustering, interpretability, predictions, variation}
}

@article{10.1145/3588938,
author = {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song},
title = {Unicorn: A Unified Multi-tasking Model for Supporting Matching Tasks in Data Integration},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588938},
doi = {10.1145/3588938},
abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {84},
numpages = {26},
keywords = {data integration, data matching, multi-task learning}
}

@article{10.1145/3588951,
author = {Wu, Xinle and Zhang, Dalin and Zhang, Miao and Guo, Chenjuan and Yang, Bin and Jensen, Christian S.},
title = {AutoCTS+: Joint Neural Architecture and Hyperparameter Search for Correlated Time Series Forecasting},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588951},
doi = {10.1145/3588951},
abstract = {Sensors in cyber-physical systems often capture interconnected processes and thus emit correlated time series (CTS), the forecasting of which enables important applications. The key to successful CTS forecasting is to uncover the temporal dynamics of time series and the spatial correlations among time series. Deep learning-based solutions exhibit impressive performance at discerning these aspects. In particular, automated CTS forecasting, where the design of an optimal deep learning architecture is automated, enables forecasting accuracy that surpasses what has been achieved by manual approaches. However, automated CTS solutions remain in their infancy and are only able to find optimal architectures for predefined hyperparameters and scale poorly to large-scale CTS. To overcome these limitations, we propose AutoCTS+, a joint, scalable framework, to automatically devise effective CTS forecasting models. Specifically, we encode each candidate architecture and accompanying hyperparameters into a joint graph representation. We introduce an efficient Architecture-Hyperparameter Comparator (AHC) to rank all architecture-hyperparameter pairs, and we then further evaluate the top-ranked pairs to select an architecture-hyperparameter pair as the final model. Extensive experiments on six benchmark datasets demonstrate that AutoCTS+ not only eliminates manual efforts but also is capable of better performance than manually designed and existing automatically designed CTS models. In addition, it shows excellent scalability to large CTS.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {97},
numpages = {26},
keywords = {architecture-hyperparameter comparator, correlated time series, efficiency, joint search, scalability}
}

@article{10.1145/3589262,
author = {Li, Tianyu and Chandramouli, Badrish and Burckhardt, Sebastian and Madden, Samuel},
title = {DARQ Matter Binds Everything: Performant and Composable Cloud Programming via Resilient Steps},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589262},
doi = {10.1145/3589262},
abstract = {Providing strong fault-tolerant guarantees for the modern cloud is difficult, as application developers must coordinate between independent stateful services and ephemeral compute and handle various failure-induced anomalies. We propose Composable Resilient Steps (CReSt), a new abstraction for resilient cloud applications. CReSt uses fault-tolerant steps as its core building block, which allows participants to receive, process, and send messages as a single uninterruptible atomic unit. Composability and reliability are orthogonally achieved by reusable CReSt implementations, for example, leveraging reliable message queues. Thus, CReSt application builders focus solely on translating application logic into steps, and infrastructure builders focus on efficient CReSt implementations. We propose one such implementation called DARQ (for Deduplicated Asynchronously Recoverable Queues). At its core, DARQ is a storage service that encapsulates CReSt participant state and enforces CReSt semantics; developers attach ephemeral compute nodes to DARQ instances to implement stateful distributed components. Services built with DARQ are resilient by construction, and CReSt-compatible services naturally compose without loss of resilience. For performance, we propose a novel speculative execution scheme to execute CReSt steps without waiting for message persistence in DARQ, effectively eliding cloud persistence overheads; our scheme maintains CReSt's fault-tolerance guarantees and automatically restores to a consistent system state upon failure. We showcase the generality of CReSt and DARQ using two applications: cloud streaming and workflow processing. Experiments show that DARQ is able to achieve extremely low latency and high throughput across these use cases, often beating state-of-the-art customized solutions.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {117},
numpages = {27},
keywords = {cloud programming, distributed system, recoverability, service composition}
}

@article{10.1145/3589317,
author = {Castro Fernandez, Raul},
title = {Data-Sharing Markets: Model, Protocol, and Algorithms to Incentivize the Formation of Data-Sharing Consortia},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589317},
doi = {10.1145/3589317},
abstract = {Organizations that would mutually benefit from pooling their data are otherwise wary of sharing. This is because sharing data is costly-in time and effort-and, at the same time, the benefits of sharing are not clear. Without a clear cost-benefit analysis, participants default in not sharing. As a consequence, many opportunities to create valuable data-sharing consortia never materialize, and the value of data remains locked.We introduce a new sharing model, market protocol, and algorithms to incentivize the creation of data-sharing markets. The combined contributions of this paper, which we call DSC, incentivize the creation of data-sharing markets that unleash the value of data for its participants. The sharing model introduces two incentives; one that guarantees that participating is better than not doing so and another that compensates participants according to how valuable their data is. Because operating the consortia is costly, we are also concerned with ensuring its operation is sustainable: we design a protocol that ensures that a valuable data-sharing consortium forms when it is sustainable.We introduce algorithms to elicit the value of data from the participants, which is used first to cover the costs of operating the consortia and second to compensate for data contributions. For the latter, we challenge using the Shapley value to allocate revenue. We offer analytical and empirical evidence for this and introduce an alternative method that compensates participants better and leads to the formation of data-sharing consortia.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {172},
numpages = {25},
keywords = {data markets, data sharing, incentives, machine learning sharing}
}

@article{10.1145/3589328,
author = {Li, Peng and Chen, Zhiyi and Chu, Xu and Rong, Kexin},
title = {DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589328},
doi = {10.1145/3589328},
abstract = {Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuous and differentiable one, which allows us to perform the pipeline search using gradient descent with training the ML model only once. Our experiments show that DiffPrep achieves the best test accuracy on 15 out of the 18 real-world datasets evaluated and improves the model's test accuracy by up to 6.6 percentage points.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {183},
numpages = {26},
keywords = {automated machine learning, data cleaning, data preprocessing}
}

@article{10.1145/3589762,
author = {Hamidi Rad, Radin and Fani, Hossein and Bagheri, Ebrahim and Kargar, Mehdi and Srivastava, Divesh and Szlichta, Jaroslaw},
title = {A Variational Neural Architecture for Skill-based Team Formation},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3589762},
doi = {10.1145/3589762},
abstract = {Team formation is concerned with the identification of a group of experts who have a high likelihood of effectively collaborating with each other to satisfy a collection of input skills. Solutions to this task have mainly adopted graph operations and at least have the following limitations: (1) they are computationally demanding, as they require finding shortest paths on large collaboration networks; (2) they use various types of heuristics to reduce the exploration space over the collaboration network to become practically feasible; therefore, their results are not necessarily optimal; and (3) they are not well-suited for collaboration network structures given the sparsity of these networks. Our work proposes a variational Bayesian neural network architecture that learns representations for teams whose members have collaborated with each other in the past. The learned representations allow our proposed approach to mine teams that have a past collaborative history and collectively cover the requested desirable set of skills. Through our experiments, we demonstrate that our approach shows stronger performance compared to a range of strong team formation techniques from both quantitative and qualitative perspectives.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {7},
numpages = {28},
keywords = {Team formation, expert networks, task assignment, variational Bayesian neural network}
}

@article{10.1145/3589773,
author = {Zhao, Hanyu and Yang, Zhi and Cheng, Yu and Tian, Chao and Ren, Shiru and Xiao, Wencong and Yuan, Man and Chen, Langshi and Liu, Kaibo and Zhang, Yang and Li, Yong and Lin, Wei},
title = {GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines for Deep Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589773},
doi = {10.1145/3589773},
abstract = {Training data pre-processing pipelines are essential to deep learning (DL). As the performance of model training keeps increasing with both hardware advancements (e.g., faster GPUs) and various software optimizations, the data pre-processing on CPUs is becoming more resource-intensive and a severe bottleneck of the pipeline. This problem is even worse in the cloud, where training jobs exhibit diverse CPU-GPU demands that usually result in mismatches with fixed hardware configurations and resource fragmentation, degrading both training performance and cluster utilization.We introduce GoldMiner, an input data processing service for stateless operations used in pre-processing data for DL model training. GoldMiner decouples data pre-processing from model training into a new role called the data worker. Data workers facilitate scaling of data pre-processing to anywhere in a cluster, effectively pooling the resources across the cluster to satisfy the diverse requirements of training jobs. GoldMiner achieves this decoupling in a fully automatic and elastic manner. The key insight is that data pre-processing is inherently stateless, thus can be executed independently and elastically. This insight guides GoldMiner to automatically extract stateless computation out of a monolithic training program, efficiently disaggregate it across data workers, and elastically scale data workers to tune the resource allocations across jobs to optimize cluster efficiency. We have applied GoldMiner to industrial workloads, and our evaluation shows that GoldMiner can transform unmodified training programs to use data workers, accelerating individual training jobs by up to 12.1x. GoldMiner also improves average job completion time and aggregate GPU utilization by up to 2.5x and 2.1x in a 64-GPU cluster, respectively, by scheduling data workers with elasticity.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {193},
numpages = {25},
keywords = {data pre-processing, deep learning, disaggregation, scheduling}
}

@article{10.1145/3589780,
author = {Yu, Wenyuan and He, Tao and Wang, Lei and Meng, Ke and Cao, Ye and Zhu, Diwen and Li, Sanhong and Zhou, Jingren},
title = {Vineyard: Optimizing Data Sharing in Data-Intensive Analytics},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589780},
doi = {10.1145/3589780},
abstract = {Modern data analytics and AI jobs become increasingly complex and involve multiple tasks performed on specialized systems. Sharing of intermediate data between different systems is often a significant bottleneck in such jobs. When the intermediate data is large, it is mostly exchanged through files in standard formats (e.g., CSV and ORC), causing high I/O and (de)serialization overheads. To solve these problems, we develop Vineyard, a high-performance, extensible, and cloud-native object store, trying to provide an intuitive experience for users to share data across systems in complex real-life workflows. Since different systems usually work on data structures (e.g., dataframes, graphs, hashmaps) with similar interfaces, and their computation logic is often loosely-coupled with how such interfaces are implemented over specific memory layouts, it enables Vineyard to conduct data sharing efficiently at a high level via memory mapping and method sharing. Vineyard provides an IDL named VCDL to facilitate users to register their own intermediate data types into Vineyard such that objects of the registered types can then be efficiently shared across systems in a polyglot workflow. As a cloud-native system, Vineyard is designed to work closely with Kubernetes, as well as achieve fault-tolerance and high performance in production environments. Evaluations on real-life datasets and data analytics jobs show that the above optimizations of Vineyard can significantly improve the end-to-end performance of data analytics jobs, by reducing their data-sharing time up to 68.4x.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {200},
numpages = {27},
keywords = {data sharing, in-memory object store}
}

@article{10.1145/3589786,
author = {Langenecker, Sven and Sturm, Christoph and Schalles, Christian Schalles and Binnig, Carsten},
title = {Steered Training Data Generation for Learned Semantic Type Detection},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589786},
doi = {10.1145/3589786},
abstract = {In this paper, we introduce STEER to adapt learned semantic type extraction approaches to a new, unseen data lake. STEER provides a data programming framework for semantic labeling which is used to generate new labeled training data with minimal overhead. At its core, STEER comes with a novel training data generation procedure called Steered-Labeling that can generate high quality training data not only for non-numeric but also for numerical columns. With this generated training data STEER is able to fine-tune existing learned semantic type extraction models. We evaluate our approach on four different data lakes and show that we can significantly improve the performance of two different types of learned models across all data lakes.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {201},
numpages = {25},
keywords = {data discovery, data lakes, semantic type detection}
}

@article{10.1145/3589974,
author = {Kumar, Adithya and Sivasubramaniam, Anand and Zhu, Timothy},
title = {SplitRPC: A {Control + Data} Path Splitting RPC Stack for ML Inference Serving},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3589974},
doi = {10.1145/3589974},
abstract = {The growing adoption of hardware accelerators driven by their intelligent compiler and runtime system counterparts has democratized ML services and precipitously reduced their execution times. This motivates us to shift our attention to efficiently serve these ML services under distributed settings and characterize the overheads imposed by the RPC mechanism ('RPC tax') when serving them on accelerators. The RPC implementations designed over the years implicitly assume the host CPU services the requests, and we focus on expanding such works towards accelerator-based services. While recent proposals calling for SmartNICs to take on this task are reasonable for simple kernels, serving complex ML models requires a more nuanced view to optimize both the data-path and the control/orchestration of these accelerators. We program today's commodity network interface cards (NICs) to split the control and data paths for effective transfer of control while efficiently transferring the payload to the accelerator. As opposed to unified approaches that bundle these paths together, limiting the flexibility in each of these paths, we design and implement SplitRPC - a control + data path optimizing RPC mechanism for ML inference serving. SplitRPC allows us to optimize the datapath to the accelerator while simultaneously allowing the CPU to maintain full orchestration capabilities. We implement SplitRPC on both commodity NICs and SmartNICs and demonstrate how GPU-based ML services running different compiler/runtime systems can benefit. For a variety of ML models served using different inference runtimes, we demonstrate that SplitRPC is effective in minimizing the RPC tax while providing significant gains in throughput and latency over existing kernel by-pass approaches, without requiring expensive SmartNIC devices.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = may,
articleno = {30},
numpages = {26},
keywords = {data path, ml inference, orchestration, remote procedure call, smartnic}
}

@article{10.1145/3590955,
author = {Campos, Virg\'{\i}nia P. and Gon\c{c}alves, Luiz M. G. and Ribeiro, Wesnydy L. and Ara\'{u}jo, Tiago M. U. and Do Rego, Tha\'{\i}s G. and Figueiredo, Pedro H. V. and Vieira, Suanny F. S. and Costa, Thiago F. S. and Moraes, Caio C. and Cruz, Alexandre C. S. and Ara\'{u}jo, Felipe A. and Souza Filho, Guido L.},
title = {Machine Generation of Audio Description for Blind and Visually Impaired People},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3590955},
doi = {10.1145/3590955},
abstract = {Automating the generation of audio descriptions (AD) for blind and visually impaired (BVI) people is a difficult task, since it has several challenges involved, such as: identifying gaps in dialogues; describing the essential elements; summarizing and fitting the descriptions into the dialogue gaps; generating an AD narration track, and synchronizing it with the main soundtrack. In our previous work (Campos et&nbsp;al.&nbsp;[6]), we propose a solution for automatic AD script generation, named CineAD, which uses the movie’s script as a basis for the AD generation. This article proposes extending this solution to complement the information extracted from the script and reduce its dependency based on the classification of visual information from the video. To assess the viability of the proposed solution, we implemented a proof of concept of the solution and evaluated it with 11 blind users. The results showed that the solution could generate a more succinct and objective AD but with a similar users’ level of understanding compared to our previous work. Thus, the solution can provide relevant information to blind users using less video time for descriptions.},
journal = {ACM Trans. Access. Comput.},
month = jun,
articleno = {14},
numpages = {28},
keywords = {Audio description, automatic, accessibility, people with visual impairment}
}

@article{10.1145/3591238,
author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},
title = {Architecture-Preserving Provable Repair of Deep Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591238},
doi = {10.1145/3591238},
abstract = {Deep neural networks (DNNs) are becoming increasingly important components  
of software, and are considered the state-of-the-art solution for a number  
of problems, such as image recognition. However, DNNs are far from  
infallible, and incorrect behavior of DNNs can have disastrous real-world  
consequences. This paper addresses the problem of architecture-preserving  
V-polytope provable repair of DNNs.  
A V-polytope defines a convex bounded polytope using its vertex representation.  
V-polytope provable repair guarantees that the repaired DNN  
satisfies the given specification on the infinite set of points in the given V-polytope.  
An architecture-preserving repair only modifies the parameters of the DNN, without  
modifying its architecture. The repair has the flexibility to  
modify multiple layers of the DNN, and runs in polynomial time.  
It supports DNNs with activation functions that have some linear pieces,  
as well as fully-connected, convolutional, pooling and residual layers.  
To the best our knowledge, this is the first provable repair approach that  
has all of these features.  
We implement our approach in a tool called APRNN. Using  
MNIST, ImageNet, and ACAS Xu DNNs, we show that  
it has better efficiency, scalability, and generalization  
compared to PRDNN and REASSURE, prior provable repair methods that are  
not architecture preserving.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {124},
numpages = {25},
keywords = {Bug fixing, Deep Neural Networks, Repair, Synthesis}
}

@article{10.1145/3591470,
author = {Reza, Md Farhadur},
title = {Machine Learning Enabled Solutions for Design and Optimization Challenges in Networks-on-Chip based Multi/Many-Core Architectures},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1550-4832},
url = {https://doi.org/10.1145/3591470},
doi = {10.1145/3591470},
abstract = {Due to the advancement of transistor technology, a single chip processor can now have hundreds of cores. Network-on-Chip (NoC) has been the superior interconnect fabric for multi/many-core on-chip systems because of its scalability and parallelism. Due to the rise of dark silicon with the end of Dennard Scaling, it becomes essential to design energy efficient and high performance heterogeneous NoC-based multi/many-core architectures. Because of the large and complex design space, the solution space becomes difficult to explore within a reasonable time for optimal trade-offs of energy-performance-reliability. Furthermore, reactive resource management is not effective in preventing problems from happening in adaptive systems. Therefore, in this work, we explore machine learning techniques to design and configure the NoC resources based on the learning of the system and applications workloads. Machine learning can automatically learn from past experiences and guide the NoC intelligently to achieve its objective on performance, power, and reliability. We present the challenges of NoC design and resource management and propose a generalized machine learning framework to uncover near-optimal solutions quickly. We propose and implement a NoC design and optimization solution enabled by neural networks, using the generalized machine learning framework. Simulation results demonstrated that the proposed neural networks-based design and optimization solution improves performance by 15\% and reduces energy consumption by 6\% compared to an existing non-machine learning-based solution while the proposed solution improves NoC latency and throughput compared to two existing machine learning-based NoC optimization solutions. The challenges of machine learning technique adaptation in multi/many-core NoC have been presented to guide future research.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jun,
articleno = {23},
numpages = {26},
keywords = {Machine Learning (ML), multi/many-core systems, Network-on-Chip (NoC), training (learning), prediction (inference), online learning, classification, regression, design and optimization, design-time, run-time, energy-efficiency, power, thermal}
}

@article{10.1145/3591594,
author = {Yip, Eugene and Girault, Alain and Roop, Partha S. and Biglari-Abhari, Morteza},
title = {Synchronous Deterministic Parallel Programming for Multi-Cores with ForeC},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/3591594},
doi = {10.1145/3591594},
abstract = {Embedded real-time systems are tightly integrated with their physical environment. Their correctness depends both on the outputs and timeliness of their computations. The increasing use of multi-core processors in such systems is pushing embedded programmers to be parallel programming experts. However, parallel programming is challenging because of the skills, experiences, and knowledge needed to avoid common parallel programming traps and pitfalls. This article proposes the ForeC synchronous multi-threaded programming language for the deterministic, parallel, and reactive programming of embedded multi-cores. The synchronous semantics of ForeC is designed to greatly simplify the understanding and debugging of parallel programs. ForeC ensures that ForeC programs can be compiled efficiently for parallel execution and be amenable to static timing analysis. ForeC’s main innovation is its shared variable semantics that provides thread isolation and deterministic thread communication. All ForeC programs are correct by construction and deadlock free because no non-deterministic constructs are needed. We have benchmarked our ForeC compiler with several medium-sized programs (e.g., a 2.274-line ForeC program with up to 26 threads and distributed on up to 10 cores, which was based on a 2.155-line non-multi-threaded C program). These benchmark programs show that ForeC can achieve better parallel performance than Esterel, a widely used imperative synchronous language for concurrent safety-critical systems, and is competitive in performance to OpenMP, a popular desktop solution for parallel programming (which implements classical multi-threading, hence is intrinsically non-deterministic). We also demonstrate that the worst-case execution time of ForeC programs can be estimated to a high degree of precision.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jun,
articleno = {11},
numpages = {74},
keywords = {Programming language, semantics, parallelism, synchronous, determinism, reactive programming, multi-core, worst-case execution time, code generation}
}

@article{10.1145/3592427,
author = {Shacklett, Brennan and Rosenzweig, Luc Guy and Xie, Zhiqiang and Sarkar, Bidipta and Szot, Andrew and Wijmans, Erik and Koltun, Vladlen and Batra, Dhruv and Fatahalian, Kayvon},
title = {An Extensible, Data-Oriented Architecture for High-Performance, Many-World Simulation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592427},
doi = {10.1145/3592427},
abstract = {Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today's fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment's logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33\texttimes{} over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {90},
numpages = {13},
keywords = {game AI, reinforcement learning}
}

@article{10.1145/3592615,
author = {Iqbal, Farkhund and Abbasi, Ahmed and Javed, Abdul Rehman and Almadhor, Ahmad and Jalil, Zunera and Anwar, Sajid and Rida, Imad},
title = {Data Augmentation-based Novel Deep Learning Method for Deepfaked Images Detection},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {11},
issn = {1551-6857},
url = {https://doi.org/10.1145/3592615},
doi = {10.1145/3592615},
abstract = {Recent advances in artificial intelligence have led to deepfake images, enabling users to replace a real face with a genuine one. deepfake images have recently been used to malign public figures, politicians, and even average citizens. deepfake but realistic images have been used to stir political dissatisfaction, blackmail, propagate false news, and even carry out bogus terrorist attacks. Thus, identifying real images from fakes has got more challenging. To avoid these issues, this study employs transfer learning and data augmentation technique to classify deepfake images. For experimentation, 190,335 RGB-resolution deepfake and real images and image augmentation methods are used to prepare the dataset. The experiments use the deep learning models: convolutional neural network (CNN), Inception V3, visual geometry group (VGG19), and VGG16 with a transfer learning approach. Essential evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix, and AUC-ROC curve score) are used to test the efficacy of the proposed approach. Results revealed that the proposed approach achieves an accuracy, recall, F1-score and AUC-ROC score of 90\% and 91\% precision, with our fine-tuned VGG16 model outperforming other DL models in recognizing real and deepfakes.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {339},
numpages = {15},
keywords = {Deepfake detection, data augmentation, image processing, deep learning, artificial intelligence, transfer learning}
}

@article{10.1145/3592616,
author = {Priestley, Maria and O’donnell, Fionnt\'{a}n and Simperl, Elena},
title = {A Survey of Data Quality Requirements That Matter in ML Development Pipelines},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3592616},
doi = {10.1145/3592616},
abstract = {The fitness of the systems in which Machine Learning (ML) is used depends greatly on good-quality data. Specifications on what makes a good-quality dataset have traditionally been defined by the needs of the data users—typically analysts and engineers. Our article critically examines the extent to which established data quality frameworks are applicable to contemporary use cases in ML. Using a review of recent literature at the intersection of ML, data management, and human-computer interaction, we find that the classical “fitness-for-use” view of data quality can benefit from a more stage-specific approach that is sensitive to where in the ML lifecycle the data are encountered. This helps practitioners to plan their data quality tasks in a manner that meets the needs of the stakeholders who will encounter the dataset, whether it be data subjects, software developers or organisations. We therefore propose a new treatment of traditional data quality criteria by structuring them according to two dimensions: (1) the stage of the ML lifecycle where the use case occurs vs. (2) the main categories of data quality that can be pursued (intrinsic, contextual, representational and accessibility). To illustrate how this works in practice, we contribute a temporal mapping of the various data quality requirements that are important at different stages of the ML data pipeline. We also share some implications for data practitioners and organisations that wish to enhance their data management routines in preparation for ML.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {39},
keywords = {Data quality, machine learning, data ecosystems, data management, data innovation}
}

@article{10.1145/3593043,
author = {Goknil, Arda and Nguyen, Phu and Sen, Sagar and Politaki, Dimitra and Niavis, Harris and Pedersen, Karl John and Suyuthi, Abdillah and Anand, Abhilash and Ziegenbein, Amina},
title = {A Systematic Review of Data Quality in CPS and IoT for Industry 4.0},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3593043},
doi = {10.1145/3593043},
abstract = {The Internet of Things (IoT) and Cyber-Physical Systems (CPS) are the backbones of Industry 4.0, where data quality is crucial for decision support. Data quality in these systems can deteriorate due to sensor failures or uncertain operating environments. Our objective is to summarize and assess the research efforts that address data quality in data-centric CPS/IoT industrial applications. We systematically review the state-of-the-art data quality techniques for CPS and IoT in Industry 4.0 through a systematic literature review (SLR) study. We pose three research questions, define selection and exclusion criteria for primary studies, and extract and synthesize data from these studies to answer our research questions. Our most significant results are (i) the list of data quality issues, their sources, and application domains, (ii) the best practices and metrics for managing data quality, (iii) the software engineering solutions employed to manage data quality, and (iv) the state of the data quality techniques (data repair, cleaning, and monitoring) in the application domains. The results of our SLR can help researchers obtain an overview of existing data quality issues, techniques, metrics, and best practices. We suggest research directions that require attention from the research community for follow-up work.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {327},
numpages = {38},
keywords = {Data quality, IoT, CPS, Industry 4.0, systematic review}
}

@article{10.1145/3593294,
author = {Pan, Bofeng and Stakhanova, Natalia and Ray, Suprio},
title = {Data Provenance in Security and Privacy},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3593294},
doi = {10.1145/3593294},
abstract = {Provenance information corresponds to essential metadata that describes the entities, users, and processes involved in the history and evolution of a data object. The benefits of tracking provenance information have been widely understood in a variety of domains; however, only recently have provenance solutions gained interest in the security community. Indeed, on the one hand, provenance allows for a reliable historical analysis enabling security-related applications such as forensic analysis and attribution of malicious activity. On the other hand, the unprecedented changes in the threat landscape place demands for securing provenance information to facilitate its trustworthiness. With the recent growth of provenance studies in security, in this work we examine the role of data provenance in security and privacy. To set this work in context, we outline fundamental principles and models of data provenance and explore how the existing studies achieve security principles. We further review the existing schemes for securing data provenance collection and manipulation known as secure provenance and the role of data provenance for security and privacy, which we refer to as threat provenance.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {323},
numpages = {35},
keywords = {Data provenance, security, privacy, secure provenance, threat provenance}
}

@article{10.1145/3593579,
author = {Heidari, Alireza and Michalopoulos, George and Ilyas, Ihab F. and Rekatsinas, Theodoros},
title = {Record Fusion via Inference and Data Augmentation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3593579},
doi = {10.1145/3593579},
abstract = {We introduce a learning framework for the problem of unifying conflicting data in multiple records referring to the same entity—we call this problem “record fusion.” Record fusion generalizes two known problems: “data fusion” and “golden record.” Our approach expresses record fusion as a learning problem over probabilistic models. In contrast to prior approaches, our method achieves high performance with or without the records source information and outperforms state-of-the-art baselines. Furthermore, we show how our learned fusion model can solve the problem of scarcity of training data. On multiple datasets, we show that our framework fuses records with an average precision of ∼98\% when source information is available and ∼94\% without source information across a diverse array of datasets. We compare our approach to a comprehensive collection of data fusion and entity consolidation methods, ranging from source information–related methods to approaches that do not need any source information. We show that our approach can achieve an average improvement of ∼20/∼45 precision points with/without source information. Our data augmentation method improves previous approaches an average of ∼10 precision points.Problem statementRecord fusion involves merging duplicate records from different sources into a single, unified record. It helps to improve data quality, reduce redundancy, and enable more accurate analysis and decision-making. However, the process can be challenging due to inconsistencies in data, spelling variations, missing data, or intentional duplications. Record fusion is a critical step in data management that helps organizations make informed decisions, reduce costs, and improve efficiency. Ultimately, record fusion enables organizations to better leverage the value of their data and gain a competitive advantage.MethodsThe proposed learning framework for record fusion uses a weak supervision approach to automatically combine related data, such as integrity constraints, data rules, quantitative statistics, and source information. The approach addresses several technical challenges, such as the need for an expressive model to capture all data characteristics, the difficulty of gathering enough labeled examples, and the limited information available for designing a reliable ML approach. To overcome these challenges, the approach generates additional training data automatically, and the model learns from partial and noisy estimates of the correct values. The goal is to infer the correct values with an expressive model that captures all data context features, and the approach uses an iterative mechanism to improve predictions over time.ResultsThe proposed method for record fusion achieves an average precision of around 98\% when source information is available and around 94\% without source information. This is a significant improvement over previous approaches and demonstrates the effectiveness of the method in merging data records. The approach also improves the precision compared to other data fusion and entity consolidation methods by an average of around 20/45 precision points with/without source information. Additionally, the data augmentation method used in this approach improves previous approaches by an average of around 10 precision points.SignificanceThe proposed machine learning framework offers an effective solution to the problem of merging data records that combines two well-known problems: data fusion and golden record. By using rich data representation models, data augmentation techniques, and iterative model applications, the approach achieves a high level of accuracy and outperforms previous methods. These results demonstrate the effectiveness and significance of the proposed method, offering an efficient solution to the problem of merging data records and improving the quality of data analysis and decision-making.},
journal = {ACM / IMS J. Data Sci.},
month = jan,
articleno = {2},
numpages = {23},
keywords = {Data integration, data fusion, generative data augmentation}
}

@article{10.1145/3593587,
author = {Gao, Yizhao and Wang, Song and So, Hayden Kwok-Hay},
title = {A Reconfigurable Architecture for Real-time Event-based Multi-Object Tracking},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3593587},
doi = {10.1145/3593587},
abstract = {Although advances in event-based machine vision algorithms have demonstrated unparalleled capabilities in performing some of the most demanding tasks, their implementations under stringent real-time and power constraints in edge systems remain a major challenge. In this work, a reconfigurable hardware-software architecture called REMOT, which performs real-time event-based multi-object tracking on FPGAs, is presented. REMOT performs vision tasks by defining a set of actions over attention units (AUs). These actions allow AUs to track an object candidate autonomously by adjusting its region of attention and allow information gathered by each AU to be used for making algorithmic-level decisions. Taking advantage of this modular structure, algorithm-architecture codesign can be performed by implementing different parts of the algorithm in either hardware or software for different tradeoffs. Results show that REMOT can process 0.43–2.91 million events per second at 1.75–5.45 W. Compared with the software baseline, our implementation achieves up to 44 times higher throughput and 35.4 times higher power efficiency. Migrating the Merge operation to hardware further reduces the worst-case latency to be 95 times shorter than the software baseline. By varying the AU configuration and operation, a reduction of 0.59–0.77 mW per AU on the programmable logic has also been demonstrated.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = sep,
articleno = {58},
numpages = {26},
keywords = {REMOT, Dynamic Vision Sensors, multi-object tracking, event sensors, event camera, hardware/software co-design, attention unit, FPGA, HOTA}
}

@article{10.1145/3594540,
author = {Le\'{o}n-Vega, Luis G. and Salazar-Villalobos, Eduardo and Rodriguez-Figueroa, Alejandro and Castro-God\'{\i}nez, Jorge},
title = {Automatic Generation of Resource and Accuracy Configurable Processing Elements},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3594540},
doi = {10.1145/3594540},
abstract = {Low-power consumption and scarce computational resources limit the computation at the edge. Besides, the approximate computing paradigm reports promising techniques for designing accelerators to deal with inherent limitations of the edge, and high-level synthesis with C++ opens the opportunity to use meta-programming for specialisable generic design. This work proposes a framework for automatically generating synthesis-time configurable processing elements (PEs) for matrix multiplication-addition (GEMMA) and convolution. To evaluate our work, we perform a design exploration after varying data bit-width, operand sizes, and kernel sizes. Our analyses include resource consumption scaling, clocks-to-solution, design efficiency, and error distribution, presenting a comprehensive view of how the parameters affect the properties of our generic implementations. The GEMMA presented a trade-off between granularity vs efficiency, where large PEs with short data widths are favoured by the design efficiency, achieving, theoretically, up to 75 GMAC/s on a Xilinx XC7Z020 @ 100 MHz with an efficiency of 27\%. For design efficiency, we propose a figure of merit to evaluate operations per second and resource utilisation with respect to the maximum achievable by the FPGA. Regarding the convolution PEs, we implemented two algorithms: a window-based spatial convolution and Winograd. The former is the best in terms of performance with 150 GMAC/s, reaching up to 47\% of efficiency. Winograd also outperformed numerically using a 3\texttimes{} 3 kernel filter, presenting a mean error of 11.01\% in 4-bits operands with a PSNR=16.28 dB, compared to the spatial convolution with 38.2\% of mean error and PSNR=5.89 dB. Finally, we discuss how the error is mostly dependent on the PE’s parameters. In the GEMMA, the error depends on the matrix size, causing limitations in the PE scaling but still applicable to accelerators. The PEs developed during this research will lead to further granular approximate accelerator research.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jul,
articleno = {75},
numpages = {27},
keywords = {Approximate computing, edge computing, hardware acceleration, deep neural networks, neural network hardware, multi-layer neural network}
}

@article{10.1145/3594723,
author = {Koho, Mikko and Coladangelo, L. P. and Ransom, Lynn and Emery, Doug},
title = {Wikibase Model for Premodern Manuscript Metadata Harmonization, Linked Data Integration, and Discovery},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594723},
doi = {10.1145/3594723},
abstract = {To facilitate discovery of premodern manuscripts in U.S. memory institutions, Digital Scriptorium, a growing consortium of over 35 institutional members representing American libraries, museums, and other cultural heritage institutions, has developed a digital platform for an online national union catalog. The platform will allow low-barrier and efficient collection, aggregation, and enrichment of member metadata and sustainably publish it as Linked Open Data. This article describes the methods and principles behind the data model development and the decision to use Wikibase. The results of the prototype implementation and testing phase demonstrate the practicality and sustainability of Digital Scriptorium’s approach to building an online national union catalog based on Linked Open Data technologies and practices.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {56},
numpages = {25},
keywords = {Linked Open Data, data modeling, Wikibase, data interoperability, premodern manuscripts, digital humanities, cultural heritage, semantic web}
}

@article{10.1145/3594737,
author = {McMillan-Major, Angelina and Bender, Emily M. and Friedman, Batya},
title = {Data Statements: From Technical Concept to Community Practice},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3594737},
doi = {10.1145/3594737},
abstract = {Responsible computing ultimately requires that technical communities develop and adopt tools, processes, and practices that mitigate harms and support human flourishing. Prior efforts toward the responsible development and use of datasets, machine learning models, and other technical systems have led to the creation of documentation toolkits to facilitate transparency, diagnosis, and inclusion. This work takes the next step: to catalyze community uptake, alongside toolkit improvement. Specifically, starting from one such proposed toolkit specialized for language datasets, data statements for natural language processing, we explore how to improve the toolkit in three senses: (1) the content of the toolkit itself, (2) engagement with professional practice, and (3) moving from a conceptual proposal to a tested schema that the intended community of use may readily adopt. To achieve these goals, we first conducted a workshop with natural language processing practitioners to identify gaps and limitations of the toolkit as well as to develop best practices for writing data statements, yielding an interim improved toolkit. Then we conducted an analytic comparison between the interim toolkit and another documentation toolkit, datasheets for datasets. Based on these two integrated processes, we present our revised Version 2 schema and best practices in a guide for writing data statements. Our findings more generally provide integrated processes for co-evolving both technology and practice to address ethical concerns within situated technical communities.},
journal = {ACM J. Responsib. Comput.},
month = mar,
articleno = {1},
numpages = {17},
keywords = {Dataset documentation toolkits, data statements, professional practice, responsible innovation, value sensitive design}
}

@article{10.1145/3595376,
author = {Coullon, H\'{e}l\'{e}ne and Henrio, Ludovic and Loulergue, Fr\'{e}d\'{e}ric and Robillard, Simon},
title = {Component-based Distributed Software Reconfiguration:A Verification-oriented Survey},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3595376},
doi = {10.1145/3595376},
abstract = {Distributed software built from components has become a mainstay of service-oriented applications, which frequently undergo reconfigurations to adapt to changes in their operating environment or their functional requirements. Given the complexity of distributed software and the adverse effects of incorrect reconfigurations, a suitable methodology is needed to ensure the correctness of reconfigurations in component-based systems. This survey gives the reader a global perspective over existing formal techniques that pursue this goal. It distinguishes different ways in which formal methods can improve the reliability of reconfigurations, and lists techniques that contribute to solving each of these particular scientific challenges.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {2},
numpages = {37},
keywords = {Reconfiguration, software adaptation, component-based software engineering, formal methods, verification}
}

@article{10.1145/3595633,
author = {Rashid, Hasib-Al and Kallakuri, Utteja and Mohsenin, Tinoosh},
title = {TinyM2Net-V2: A Compact Low-power Software Hardware Architecture for Multimodal Deep Neural Networks},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3595633},
doi = {10.1145/3595633},
abstract = {With the evaluation of Artificial Intelligence (AI), there has been a resurgence of interest in how to use AI algorithms on low-power embedded systems to broaden potential use cases of the Internet of Things (IoT). To mimic multimodal human perception, multimodal deep neural networks (M-DNN) have recently become very popular with the classification task due to their impressive performance for computer vision and audio processing tasks. This article presents TinyM2Net-V2—a compact low-power software hardware architecture for multimodal deep neural networks for resource-constrained tiny devices. To compress the models to implement on tiny devices, cyclicly sparsification and hybrid quantization (4-bits weights and 8-bits activations) methods are used. Although model compression techniques are an active research area, we are the first to demonstrate their efficacy for multimodal deep neural networks, using cyclicly sparsification and hybrid quantization of weights/activations. TinyM2Net-V2 shows that even a tiny multimodal deep neural network model can improve the classification accuracy more than that of any unimodal counterparts. Parameterized M-DNN model architecture was designed to be evaluated in two different case-studies: vehicle detection from multimodal images and audios and COVID-19 detection from multimodal audio recordings. The most compressed TinyM2Net-V2 achieves 92.5\% COVID-19 detection accuracy (6.8\% improvement from the unimodal full precision model) and 90.6\% vehicle classification accuracy (7.7\% improvement from the unimodal full precision model). A parameterized and flexible FPGA hardware accelerator was designed as well for TinyM2Net-V2 models. To the best of our knowledge, this is the first work accelerating multimodal deep neural network models on low-power Artix-7 FPGA hardware. We achieved energy efficiency of 9.04 GOP/s/W and 15.38 GOP/s/W for case-study 1 and case-study 2, respectively, which is comparable to the state-of-the-art results. Finally, we compared our tiny FPGA hardware implementation results with off-the-shelf resource-constrained devices and showed our implementation is faster and consumed less power compared to the off-the-shelf resource-constrained devices.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = may,
articleno = {47},
numpages = {23},
keywords = {tinyML, multimodal deep neural networks, FPGA, model compression}
}

@article{10.1145/3596265,
author = {Lambrichts, Mannu and Ramakers, Raf and Hodges, Steve and Devine, James and Underwood, Lorraine and Finney, Joe},
title = {CircuitGlue: A Software Configurable Converter for Interconnecting Multiple Heterogeneous Electronic Components},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596265},
doi = {10.1145/3596265},
abstract = {We present CircuitGlue, an electronic converter board that allows heterogeneous electronic components to be readily interconnected. Electronic components are plugged into an eight-pin programmable header on the board, and the assignment of each pin in the header is configured in software. CircuitGlue supports a variety of connections, including power, ground, analog signals, and various digital protocols at different voltages. As such, off-the-shelf electronic components and modules are instantly compatible no matter what voltage levels, interface types, communication protocols, and pinouts they use. In this paper, we demonstrate the use of CircuitGlue to ease and expedite prototyping with electronics and we explore new opportunities enabled by CircuitGlue. Finally, we reflect on the results of a preliminary user study evaluating the usability of CircuitGlue for people new to electronics.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {63},
numpages = {30},
keywords = {Electronics, Prototyping, Software Programmable Converter}
}

@article{10.1145/3596267,
author = {Ekambaranathan, Anirudh and Zhao, Jun and Chalhoub, George},
title = {Navigating the Data Avalanche: Towards Supporting Developers in Developing Privacy-Friendly Children's Apps},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596267},
doi = {10.1145/3596267},
abstract = {This paper critically examines the intersection of privacy concerns in children's apps and the support required by developers to effectively address these concerns. Third-party libraries and software development kits (SDKs) are widely used in mobile app development, however, these libraries are commonly known for posing significant data privacy risks to users. Recent research has shown that app developers for children are particularly struggling with the lack of support in navigating the complex market of third-party SDKs. The support needed for developers to build privacy-friendly apps is largely understudied. Motivated by the needs of developers and an empirical analysis of 137 'expert-approved' children's apps, we designed DataAvalanche.io, a web-based tool to support app developers in navigating the privacy and legal implications associated with common third-party SDKs on the market. Through semi-structured interviews with 12 app developers for children, we demonstrate that app developers largely perceive the transparency supported by our tool positively. However, they raised several barriers, including the challenges of adopting privacy-friendly alternatives and the struggle to safeguard their own legal interests when facing the imbalance of power in the app market. We contribute to our understanding of the open challenges and barriers faced by app developers in creating privacy-friendly apps for children and provide critical future design and policy directions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {53},
numpages = {24},
keywords = {apps, children, developers, privacy, tracking}
}

@article{10.1145/3597305,
author = {Bono, Carlo A. and Cappiello, Cinzia and Pernici, Barbara and Ramalli, Edoardo and Vitali, Monica},
title = {Pipeline Design for Data Preparation for Social Media Analysis},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597305},
doi = {10.1145/3597305},
abstract = {In a data-driven culture, in which analytics applications are the main resources for supporting decision-making, the use of high-quality datasets is mandatory to minimize errors and risks. For this reason, data analysis tasks need to be preceded by a data preparation pipeline. The design of such a pipeline is not trivial: the data analyst must carefully choose the appropriate operations considering several aspects. This is often performed by adopting a trial-and-error approach that does not always lead to the most effective solution. In addition, extracting information from social media poses specific problems due to the need to consider only posts relevant for the analysis, for its dependence from the context being considered, for its multimedia contents, and for the risk of filtering out informative posts with automatic filters. In this article, we propose a systematic approach to support the design of pipelines that are able to effectively extract a relevant dataset for the goal of the analysis of data from social media. We provide a conceptual model for designing and annotating the data preparation pipeline with quality and performance information, thus providing the data analyst preliminary information on the expected quality of the resulting dataset in a context-aware manner. The generation of metadata related to the processing tasks has been recognized as essential for enabling data sharing and reusability. To this aim, the dataset resulting from the pipeline application is automatically annotated with provenance metadata to get a detailed description of all the activities performed by the pipeline on them. As a case study, we consider the design of a pipeline for creating datasets of images extracted from social media in order to analyze behavioural aspects during COVID-19.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {42},
numpages = {25},
keywords = {Data preparation pipeline, human-in-the-loop processes, data provenance}
}

@article{10.1145/3597435,
author = {Russo Russo, Gabriele and Cardellini, Valeria and Lo Presti, Francesco},
title = {Hierarchical Auto-scaling Policies for Data Stream Processing on Heterogeneous Resources},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3597435},
doi = {10.1145/3597435},
abstract = {Data Stream Processing (DSP) applications analyze data flows in near real-time by means of operators, which process and transform incoming data. Operators handle high data rates running parallel replicas across multiple processors and hosts. To guarantee consistent performance without wasting resources in the face of variable workloads, auto-scaling techniques have been studied to adapt operator parallelism at run-time. However, most of the effort has been spent under the assumption of homogeneous computing infrastructures, neglecting the complexity of modern environments.We consider the problem of deciding both how many operator replicas should be executed and which types of computing nodes should be acquired. We devise heterogeneity-aware policies by means of a two-layered hierarchy of controllers. While application-level components steer the adaptation process for whole applications, aiming to guarantee user-specified requirements, lower-layer components control auto-scaling of single operators. We tackle the fundamental challenge of performance and workload uncertainty, exploiting Bayesian optimization (BO) and reinforcement learning (RL) to devise policies. The evaluation shows that our approach is able to meet users’ requirements in terms of response time and adaptation overhead, while minimizing the cost due to resource usage, outperforming state-of-the-art baselines. We also demonstrate how partial model information is exploited to reduce training time for learning-based controllers.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = oct,
articleno = {14},
numpages = {44},
keywords = {Auto-scaling, Data Stream Processing, resource management, reinforcement learning}
}

@article{10.1145/3598421,
author = {Wang, Zilu and Shi, Xinming and Yao, Xin},
title = {A Brain-Inspired Hardware Architecture for Evolutionary Algorithms Based on Memristive Arrays},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3598421},
doi = {10.1145/3598421},
abstract = {Brain-inspired computing takes inspiration from the brain to create energy-efficient hardware systems for information processing, capable of performing highly sophisticated tasks. Systems built with emerging electronics, such as memristive devices, can achieve gains in speed and energy by mimicking the distributed topology of the brain. In this work, a brain-inspired hardware architecture for evolutionary algorithms is proposed based on memristive arrays, which can realize sparse and approximate computing as a result of the parallel analog computing characteristic of the memristive arrays. On this basis, an efficient evolvable brain-inspired hardware system is implemented. We experimentally show that the approach can offer at least a four orders of magnitude speed improvement. We also use experimentally grounded simulations to explore fault tolerance and different parameter settings in the implemented hardware system. The experimental results show that the evolvable hardware system, implemented based on the proposed hardware architecture, can continuously evolve toward a better system even if there are failures or parameter changes in the memristive arrays, demonstrating that the proposed hardware architecture has good adaptability and fault tolerance.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {82},
numpages = {32},
keywords = {Memristor, evolutionary algorithms, brain-inspired architecture, parallel analog computing, circuit implementation}
}

@article{10.1145/3603703,
author = {Zabihi, Zeinab and Eftekhari Moghadam, Amir Masoud and Rezvani, Mohammad Hossein},
title = {Reinforcement Learning Methods for Computation Offloading: A Systematic Review},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3603703},
doi = {10.1145/3603703},
abstract = {Today, cloud computation offloading may not be an appropriate solution for delay-sensitive applications due to the long distance between end-devices and remote datacenters. In addition, offloading to a remote cloud can consume bandwidth and dramatically increase costs. However, end-devices such as sensors, cameras, and smartphones have limited computing and storage capacity. Processing tasks on such battery-powered and energy-constrained devices becomes even more complex. To address these challenges, a new paradigm called Edge Computing (EC) emerged nearly a decade ago to bring computing resources closer to end-devices. Here, edge servers located between the end-device and the remote cloud perform user tasks. Recently, several new computing paradigms such as Mobile Edge Computing (MEC) and Fog Computing (FC) have emerged to complement Cloud Computing (CC) and EC. Although these paradigms are heterogeneous, they can further reduce energy consumption and task response time, especially for delay-sensitive applications. Computation offloading is a multi-objective, NP-hard optimization problem. A significant part of previous research in this field is devoted to Machine Learning (ML) methods. One of the essential types of ML is Reinforcement Learning (RL), in which an agent learns how to make the best decision using the experiences gained from the environment. This article provides a systematic review of the widely used RL approaches in computation offloading. It covers research in complementary paradigms such as mobile cloud computing, edge computing, fog computing, and the Internet of Things. We explain the reasons for using various RL methods in computation offloading from a technical point of view. This analysis includes both binary offloading and partial offloading techniques. For each method, the essential elements of RL and the characteristics of the environment are discussed regarding the most important criteria. Research challenges and Future trends are also mentioned.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {17},
numpages = {41},
keywords = {Fog Computing, Mobile Edge Computing, Mobile Cloud Computing}
}

@article{10.1145/3603706,
author = {Fadlallah, Hadi and Kilany, Rima and Dhayne, Houssein and El Haddad, Rami and Haque, Rafiqul and Taher, Yehia and Jaber, Ali},
title = {BIGQA: Declarative Big Data Quality Assessment},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603706},
doi = {10.1145/3603706},
abstract = {In the big data domain, data quality assessment operations are often complex and must be implementable in a distributed and timely manner. This article tries to generalize the quality assessment operations by providing a new ISO-based declarative data quality assessment framework (BIGQA). BIGQA is a flexible solution that supports data quality assessment in different domains and contexts. It facilitates the planning and execution of big data quality assessment operations for data domain experts and data management specialists at any phase in the data life cycle. This work implements BIGQA to demonstrate its ability to produce customized data quality reports while running efficiently on parallel or distributed computing frameworks. BIGQA generates data quality assessment plans using straightforward operators designed to handle big data and guarantee a high degree of parallelism when executed. Moreover, it allows incremental data quality assessment to avoid reading the whole dataset each time the quality assessment operation is required. The result was validated using radiation wireless sensor data and Stack Overflow users’ data to show that it can be implemented within different contexts. The experiments show a 71\% performance improvement over a 1 GB flat file on a single processing machine compared with a non-parallel application and a 75\% performance improvement over a 25 GB flat file within a distributed environment compared to a non-distributed application.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {27},
numpages = {30},
keywords = {Declarative framework, quality assessment, big data, data quality}
}

@article{10.1145/3603707,
author = {Fadlallah, Hadi and Kilany, Rima and Dhayne, Houssein and El Haddad, Rami and Haque, Rafiqul and Taher, Yehia and Jaber, Ali},
title = {Context-aware Big Data Quality Assessment: A Scoping Review},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603707},
doi = {10.1145/3603707},
abstract = {The term data quality refers to measuring the fitness of data regarding the intended usage. Poor data quality leads to inadequate, inconsistent, and erroneous decisions that could escalate the computational cost, cause a decline in profits, and cause customer churn. Thus, data quality is crucial for researchers and industry practitioners.Different factors drive the assessment of data quality. Data context is deemed one of the key factors due to the contextual diversity of real-world use cases of various entities such as people and organizations. Data used in a specific context (e.g., an organization policy) may need to be more efficacious for another context. Hence, implementing a data quality assessment solution in different contexts is challenging.Traditional technologies for data quality assessment reached the pinnacle of maturity. Existing solutions can solve most of the quality issues. The data context in these solutions is defined as validation rules applied within the ETL (extract, transform, load) process, i.e., the data warehousing process. In contrast to traditional data quality management, it is impossible to specify all the data semantics beforehand for big data. We need context-aware data quality rules to detect semantic errors in a massive amount of heterogeneous data generated at high speed. While many researchers tackle the quality issues of big data, they define the data context from a specific standpoint. Although data quality is a longstanding research issue in academia and industries, it remains an open issue, especially with the advent of big data, which has fostered the challenge of data quality assessment more than ever.This article provides a scoping review to study the existing context-aware data quality assessment solutions, starting with the existing big data quality solutions in general and then covering context-aware solutions. The strength and weaknesses of such solutions are outlined and discussed. The survey showed that none of the existing data quality assessment solutions could guarantee context awareness with the ability to handle big data. Notably, each solution dealt only with a partial view of the context. We compared the existing quality models and solutions to reach a comprehensive view covering the aspects of context awareness when assessing data quality. This led us to a set of recommendations framed in a methodological framework shaping the design and implementation of any context-aware data quality service for big data. Open challenges are then identified and discussed.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {25},
numpages = {33},
keywords = {Data quality, big data, context awareness, data quality assessment}
}

@article{10.1145/3603708,
author = {Krasikov, Pavel and Legner, Christine},
title = {A Method to Screen, Assess, and Prepare Open Data for Use},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603708},
doi = {10.1145/3603708},
abstract = {Open data's value-creating capabilities and innovation potential are widely recognized, resulting in a notable increase in the number of published open data sources. A crucial challenge for companies intending to leverage open data is to identify suitable open datasets that support specific business scenarios and prepare these datasets for use. Researchers have developed several open data assessment techniques, but those are restricted in scope, do not consider the use context, and are not embedded in the complete set of activities required for open data consumption in enterprises. Therefore, our research aims to develop prescriptive knowledge in the form of a meaningful method to screen, assess, and prepare open data for use in an enterprise setting. Our findings complement existing open data assessment techniques by providing methodological guidance to prepare open data of uncertain quality for use in a value-adding and demand-oriented manner, enabled by knowledge graphs and linked data concepts. From an academic perspective, our research conceptualizes open data preparation as a purposeful and value-creating process.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {43},
numpages = {25},
keywords = {Open data, Data preparation, Data quality, Action Design Research, Knowledge Graph, Data sourcing, Enterprise data integration}
}

@article{10.1145/3603710,
author = {Wenz, Viola and Kesper, Arno and Taentzer, Gabriele},
title = {Clustering Heterogeneous Data Values for Data Quality Analysis},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603710},
doi = {10.1145/3603710},
abstract = {Data is of high quality if it is fit for its intended purpose. Data heterogeneity can be a major quality problem, as quality aspects such as understandability and consistency can be compromised. Heterogeneity of data values is particularly common when data is manually entered by different people using inadequate control rules. In this case, syntactic and semantic heterogeneity often go hand in hand. Heterogeneity of data values may be a direct result of problems in the acquisition process, quality problems of the underlying data model, or possibly erroneous data transformations. For example, in the cultural heritage domain, it is common to analyze data fields by manually searching lists of data values sorted alphabetically or by number of occurrences. Additionally, search functions such as regular expression matching are used to detect specific patterns. However, this requires a priori knowledge and technical skills that domain experts often do not have. Since such datasets often contain thousands of values, the entire process is very time-consuming. Outliers or subtle differences between values that may be critical to data quality can be easily overlooked. To improve this process of analyzing the quality of data values, we propose a bottom-up human-in-the-loop approach that clusters values of a data field according to syntactic similarity. The clustering is intended to help domain experts explore the heterogeneity of values in a data field and can be configured by domain experts according to their domain knowledge. The overview of the syntactic diversity of the data values gives an impression of the rules and practices of data acquisition as well as their violations. From this, experts can infer potential quality issues with the data acquisition process and system, as well as the data model and data transformations. We outline a proof-of-concept implementation of the approach. Our evaluation found that clustering adds value to data quality analysis, especially for detecting quality problems in data models.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {28},
numpages = {33},
keywords = {Data quality, clustering, data heterogeneity, data analysis, value abstraction, semi-structured data}
}

@article{10.1145/3603715,
author = {Al–Qerem, Ahmad and Ali, Ali Mohd and Attar, Hani and Nashwan, Shadi and Qi, Lianyong and Moghimi, Mohammad Kazem and Solyman, Ahmed},
title = {Synthetic Generation of Multidimensional Data to Improve Classification Model Validity},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603715},
doi = {10.1145/3603715},
abstract = {This article aims to compare Generative Adversarial Network (GAN) models and feature selection methods for generating synthetic data in order to improve the validity of a classification model. The synthetic data generation technique involves generating new data samples from existing data to increase the diversity of the data and help the model generalize better. The multidimensional aspect of the data refers to the fact that it can have multiple features or variables that describe it. The GAN models have proven to be effective in preserving the statistical properties of the original data. However, the order of data augmentation and feature selection is crucial to build robust and accurate predictive models. By comparing the different GAN models with feature selection methods on multidimensional datasets, this article aims to determine the best combination to support the validity of a classification model in multidimensional data.},
journal = {J. Data and Information Quality},
month = sep,
articleno = {37},
numpages = {20},
keywords = {Multidimensional data, model validity, data augmentation, filter method, wrapper method}
}

@article{10.1145/3604236,
author = {Tan, Jingweijia and Wang, Weiren and Ma, Maodi and Wei, Xiaohui and Yan, Kaige},
title = {Improving the Performance of CNN Accelerator Architecture under the Impact of Process Variations},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3604236},
doi = {10.1145/3604236},
abstract = {Convolutional neural network (CNN) accelerators are popular specialized platforms for efficient CNN processing. As semiconductor manufacturing technology scales down to nano scale, process variation dramatically affects the chip’s quality. Process variation causes delay variation within the chip due to transistor parameter differences. CNN accelerators adopt a large number of processing elements (PEs) for parallel computing, which are highly susceptible to process variation effects. Fast CNN processing desires consistent performance among PEs; otherwise the processing speed is limited by the slowest PE within the chip. In this work, we first quantitatively model and analyze the impact of process variation on CNN accelerators’ operating frequency. We further analyze the utilization of CNN accelerators and the characteristics of CNN models. We then leverage the PE underutilization to propose a sub-matrix reformation mechanism and leverage the pixel similarity of images to propose a weight transfer technique. Both techniques are able to tolerate the low-frequency PEs and achieve performance improvement at chip level. Furthermore, a novel resilience-aware mapping technique that exploits the diversity in the importance of weights is also proposed to improve the performance. Evaluation results show that our techniques are able to achieve significant processing speed improvement with negligible accuracy loss.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {85},
numpages = {21},
keywords = {Process variation, CNN accelerator, systolic array}
}

@article{10.1145/3604283,
author = {Wang, Ruihang and Xia, Deneng and Cao, Zhiwei and Wen, Yonggang and Tan, Rui and Zhou, Xin},
title = {Toward Data Center Digital Twins via Knowledge-based Model Calibration and Reduction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3604283},
doi = {10.1145/3604283},
abstract = {Computational fluid dynamics (CFD) models have been widely used for prototyping data centers. Evolving them into high-fidelity and real-time digital twins is desirable for the online operations of data centers. However, CFD models often have unsatisfactory accuracy and high computation overhead. Manually calibrating the CFD model parameters is tedious and labor-intensive. Existing automatic calibration approaches apply heuristics to search the model configurations. However, each search step requires a long-lasting process of repeatedly solving the CFD model, rendering them impractical, especially for complex CFD models. This article presents Kalibre, a knowledge-based neural surrogate approach that calibrates a CFD model by iterating four steps of (i) training a neural surrogate model, (ii) finding the optimal parameters through neural surrogate retraining, (iii) configuring the found parameters back to the CFD model, and (iv) validating the CFD model using sensor-measured data. Thus, the parameter search is offloaded to the lightweight neural surrogate. To speed up Kalibre’s convergence, we incorporate prior knowledge in training data initialization and surrogate architecture design. With about ten hours of computation on a 64-core processor, Kalibre achieves mean absolute errors (MAEs) of 0.57°C and 0.88°C in calibrating the CFD models of two production data halls hosting thousands of servers. To accelerate CFD-based simulation, we further propose Kalibreduce that incorporates the energy balance principle to reduce the order of the calibrated CFD model. Evaluation shows the model reduction only introduces 0.1°C to 0.27°C extra errors while accelerating the CFD-based simulations by thousand times.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {11},
numpages = {24},
keywords = {Data center, computational fluid dynamics, surrogate model, knowledge-based neural network, proper orthogonal decomposition}
}

@article{10.1145/3604801,
author = {Margara, Alessandro and Cugola, Gianpaolo and Felicioni, Nicol\`{o} and Cilloni, Stefano},
title = {A Model and Survey of Distributed Data-Intensive Systems},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604801},
doi = {10.1145/3604801},
abstract = {Data is a precious resource in today’s society, and it is generated at an unprecedented and constantly growing pace. The need to store, analyze, and make data promptly available to a multitude of users introduces formidable challenges in modern software platforms. These challenges radically impacted the research fields that gravitate around data management and processing, with the introduction of distributed data-intensive systems that offer innovative programming models and implementation strategies to handle data characteristics such as its volume, the rate at which it is produced, its heterogeneity, and its distribution. Each data-intensive system brings its specific choices in terms of data model, usage assumptions, synchronization, processing strategy, deployment, guarantees in terms of consistency, fault tolerance, and ordering. Yet, the problems data-intensive systems face and the solutions they propose are frequently overlapping. This article proposes a unifying model that dissects the core functionalities of data-intensive systems, and discusses alternative design and implementation strategies, pointing out their assumptions and implications. The model offers a common ground to understand and compare highly heterogeneous solutions, with the potential of fostering cross-fertilization across research communities. We apply our model by classifying tens of systems: an exercise that brings to interesting observations on the current trends in the domain of data-intensive systems and suggests open research directions.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {16},
numpages = {69},
keywords = {Data-intensive systems, distributed systems, data management, data processing, model, taxonomy}
}

@article{10.1145/3604935,
author = {Giles, Michael and Sheridan-Methven, Oliver},
title = {Approximating Inverse Cumulative Distribution Functions to Produce Approximate Random Variables},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3604935},
doi = {10.1145/3604935},
abstract = {For random variables produced through the inverse transform method, approximate random variables are introduced, which are produced using approximations to a distribution’s inverse cumulative distribution function. These approximations are designed to be computationally inexpensive and much cheaper than library functions, which are exact to within machine precision and, thus, highly suitable for use in Monte Carlo simulations. The approximation errors they introduce can then be eliminated through use of the multilevel Monte Carlo method. Two approximations are presented for the Gaussian distribution: a piecewise constant on equally spaced intervals and a piecewise linear using geometrically decaying intervals. The errors of the approximations are bounded and the convergence demonstrated, and the computational savings are measured for C and C++ implementations. Implementations tailored for Intel and Arm hardware are inspected alongside hardware agnostic implementations built using OpenMP. The savings are incorporated into a nested multilevel Monte Carlo framework with the Euler-Maruyama scheme to exploit the speedups without losing accuracy, offering speed ups by a factor of 5–7. These ideas are empirically extended to the Milstein scheme and the non-central χ2 distribution for the Cox-Ingersoll-Ross process, offering speedups of a factor of 250 or more.},
journal = {ACM Trans. Math. Softw.},
month = sep,
articleno = {26},
numpages = {29},
keywords = {Approximations, random variables, inverse cumulative distribution functions, random number generation, the Gaussian distribution, geometric Brownian motion, the Cox-Ingersoll-Ross process, the non-central χ2 distribution, multilevel Monte Carlo, the Euler-Maruyama scheme, the Milstein scheme, and high-performance computing}
}

@article{10.1145/3605910,
author = {Koch, In\^{e}s and Teixeira Lopes, Carla and Ribeiro, Cristina},
title = {Moving from ISAD(G) to a CIDOC CRM-based Linked Data Model in the Portuguese Archives},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3605910},
doi = {10.1145/3605910},
abstract = {Archives are facing numerous challenges. On the one hand, archival assets are evolving to encompass digitized documents and increasing quantities of born-digital information in diverse formats. On the other hand, the audience is changing along with how it wishes to access archival material. Moreover, the interoperability requirements of cultural heritage repositories are growing. In this context, the Portuguese Archives started an ambitious program aiming to evolve its data model, migrate existing records, and build a new archival management system appropriate to both archival tasks and public access. The overall goal is to have a fine-grained and flexible description, more machine-actionable than the current one. This work describes ArchOnto, a linked open data model for archives, and rules for its automatic population from existing records. ArchOnto adopts a semantic web approach and encompasses the CIDOC Conceptual Reference Model and additional ontologies, envisioning interoperability with datasets curated by multiple communities of practice. Existing ISAD(G)-conforming descriptions are being migrated to the new model using the direct mappings provided here. We used a sample of 25 records associated with different description levels to validate the completeness and conformity of ArchOnto to existing data. This work is in progress and is original in several respects: (1) it is one of the first approaches to use CIDOC CRM in the context of archives, identifying problems and questions that emerged during the process and pinpointing possible solutions; (2) it addresses the balance in the model between the migration of existing records and the construction of new ones by archive professionals; and (3) it adopts an open world view on linking archival data to global information sources.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {71},
numpages = {21},
keywords = {Cultural heritage, archives, archival description, linked open data, semantic web, data migration}
}

@article{10.1145/3605944,
author = {Chen, Zhida and Cong, Gao and Aref, Walid G.},
title = {STAR: A Cache-based Stream Warehouse System for Spatial Data},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/3605944},
doi = {10.1145/3605944},
abstract = {The proliferation of mobile phones and location-based services has given rise to an explosive growth in spatial data. To enable spatial data analytics, spatial data needs to be streamed into a data stream warehouse system that can provide real-time analytical results over the most recent and historical spatial data in the warehouse. Existing data stream warehouse systems are not tailored for spatial data. In this article, we introduce the STAR system. STAR is a distributed in-memory data stream warehouse system that provides low-latency and up-to-date analytical results over a fast-arriving spatial data stream. STAR supports both snapshot and continuous queries that are composed of aggregate functions and ad hoc query constraints over spatial, textual, and temporal data attributes. STAR implements a cache-based mechanism to facilitate the processing of snapshot queries that collectively utilizes the techniques of query-based caching (i.e., view materialization) and object-based caching. Moreover, to speed up processing continuous queries, STAR proposes a novel index structure that achieves high efficiency in both object checking and result updating. Extensive experiments over real datasets demonstrate the superior performance of STAR over existing systems.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = nov,
articleno = {28},
numpages = {27},
keywords = {Spatial data, data stream, warehouse system, distributed system}
}

@article{10.1145/3607534,
author = {Sheff, Isaac and Wang, Xinwen and Babel, Kushal and Ni, Haobin and van Renesse, Robbert and Myers, Andrew C.},
title = {Charlotte: Reformulating Blockchains into a Web of Composable Attested Data Structures for Cross-Domain Applications},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3607534},
doi = {10.1145/3607534},
abstract = {Cross-domain applications are rapidly adopting blockchain techniques for immutability, availability, integrity, and interoperability. However, for most applications, global consensus is unnecessary and may not even provide sufficient guarantees. We propose a new distributed data structure: Attested Data Structures &nbsp;(ADS), which generalize not only blockchains but also many other structures used by distributed applications. As in blockchains, data in ADSs is immutable and self-authenticating. ADSs go further by supporting application-defined proofs (attestations). Attestations enable applications to plug in their own mechanisms to ensure availability and integrity. We present Charlotte, a framework for composable ADSs. Charlotte deconstructs conventional blockchains into more primitive mechanisms. Charlotte can be used to construct blockchains but does not impose the usual global-ordering overhead. Charlotte offers a flexible foundation for interacting applications that define their own policies for availability and integrity. Unlike traditional distributed systems, Charlotte supports heterogeneous trust: different observers have their own beliefs about who might fail, and how. Nevertheless, each observer has a consistent, available view of data.Charlotte’s data structures are interoperable and composable: applications and data structures can operate fully independently or can share data when desired. Charlotte defines a language-independent format for data blocks and a network API for servers.To demonstrate Charlotte’s flexibility, we implement several integrity mechanisms, including consensus and proof of work. We explore the power of disentangling availability and integrity mechanisms in prototype applications. The results suggest that Charlotte can be used to build flexible, fast, composable applications with strong guarantees.},
journal = {ACM Trans. Comput. Syst.},
month = dec,
articleno = {2},
numpages = {52},
keywords = {Blockchain, DAG, authenticated data structure, distributed systems}
}

@article{10.1145/3608040,
author = {Shen, Yixian and Schreuders, Leo and Pathania, Anuj and Pimentel, Andy D.},
title = {Thermal Management for 3D-Stacked Systems via Unified Core-Memory Power Regulation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3608040},
doi = {10.1145/3608040},
abstract = {3D-stacked processor-memory systems stack memory (DRAM banks) directly on top of logic (CPU cores) using chiplet-on-chiplet packaging technology to provide the next-level computing performance in embedded platforms. Stacking, however, severely increases the system’s power density without any accompanying increase in the heat dissipation capacity. Consequently, 3D-stacked processor-memory systems suffer more severe thermal issues than their non-stacked counterparts. Nevertheless, 3D-stacked processor-memory systems do inherit power&nbsp;(thermal) management knobs from their non-stacked predecessors - namely Dynamic Voltage and Frequency Scaling (DVFS) for cores and Low Power Mode (LPM) for memory banks. In the context of 3D-stacked processor-memory systems, DVFS and LPM are performance- and power-wise deeply intertwined. Their non-unified independent use on 3D-stacked processor-memory systems results in sub-optimal thermal management. The unified use of DVFS and LPM for thermal management for 3D-stacked processor-memory systems remains unexplored. The lack of implementation of LPM in thermal simulators for 3D-stacked processor-memory systems hinders real-world representative evaluation for a unified approach.We extend the state-of-the-art interval thermal simulator for 3D-stacked processor-memory systems CoMeT with an LPM power management knob for memory banks. We also propose a learning-based thermal management technique for 3D-stacked processor-memory systems that employ DVFS and LPM in a unified manner. Detailed interval thermal simulations with the extended CoMeT framework show a 10.15\% average response time improvement with the PARSEC and SPLASH-2 benchmark suites, along with widely-used Deep Neural Network (DNN) workloads against a state-of-the-art thermal management technique for 2.5D processor-memory systems&nbsp;(ported directly to 3D-stacked processor-memory systems) that also proposes unified use of DVFS and LPM.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {120},
numpages = {26},
keywords = {Low power design, resource-constrained edge computing, embedded systems, 3D-stacked processors, AI for systems, chiplet-on-chiplet stacking}
}

@article{10.1145/3608447,
author = {Metz, David and Kumar, Vineet and Sj\"{a}lander, Magnus},
title = {BISDU: A Bit-Serial Dot-Product Unit for Microcontrollers},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3608447},
doi = {10.1145/3608447},
abstract = {Low-precision quantized neural networks (QNNs) reduce the required memory space, bandwidth, and computational power, and hence are suitable for deployment in applications such as IoT edge devices. Mixed-precision QNNs, where weights commonly have lower precision than activations or different precision is used for different layers, can limit the accuracy loss caused by low-bit quantization, while still benefiting from reduced memory footprint and faster execution. Previous multiple-precision functional units supporting 8-bit, 4-bit, and 2-bit SIMD instructions have limitations, such as large area overhead, under-utilization of multipliers, and wasted memory space for low and mixed bit-width operations.This article introduces BISDU, a bit-serial dot-product unit to support and accelerate execution of mixed-precision low-bit QNNs on resource-constrained microcontrollers. BISDU is a multiplier-less dot-product unit, with frugal hardware requirements (a population count unit and 2:1 multiplexers). The proposed bit-serial dot-product unit leverages the conventional logical operations of a microcontroller to perform multiplications, which enables efficient software implementations of binary (Xnor), ternary (Xor), and mixed-precision [W\texttimes{}A] (And) dot-product operations.The experimental results show that BISDU achieves competitive performance compared to two state-of-the-art units, XpulpNN and Dustin, when executing low-bit-width CNNs. We demonstrate the advantage that bit-serial execution provides by enabling trading accuracy against weight footprint and execution time. BISDU increases the area of the ALU by 68\% and the ALU power consumption by 42\% compared to a baseline 32-bit RISC-V (RV32IC) microcontroller core. In comparison, XpulpNN and Dustin increase the area by 6.9\texttimes{} and 11.1\texttimes{} and the power consumption by 3.8\texttimes{} and 5.97\texttimes{}, respectively. The bit-serial state-of-the-art, based on a conventional popcount instruction, increases the area by 42\% and power by 32\%, with BISDU providing a 37\% speedup over it.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {79},
numpages = {22},
keywords = {Bit-serial, Dot-product, Microcontroller, Quantized neural networks, Low power, ISA extension}
}

@article{10.1145/3608948,
author = {Zhong, Zhengwu},
title = {Research on Product Advertising Design Combining Feature Extraction Technology and Web3D Technology},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3608948},
doi = {10.1145/3608948},
abstract = {This work, built on the Unity3D development platform, presents a way for merging feature extraction technology and Web3D technology into advertising design to effectively address the issues of poor efficiency and distortion in the field. Using the candidate text layout generating technique of visual salience, we first build the vector function set based on the three main colors, then we produce the visual communication partition model of advertising design. Next, the number of feature parameters of the shape advertising design is obtained via the establishment of coding coefficient constraint features and the use of an upgraded neural network technique to extract local feature parameter information about the product. Finally, the product design model is brought to life using Web3D technology to boost advertising design's productivity and accuracy. The experiments show that this method not only results in a high rate of correct product identification but also offers a fresh viewpoint on the visual communication of product advertising design by merging the two disciplines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {90},
numpages = {13},
keywords = {Unity3D development platform, feature extraction, Web3D technology, vector function, product design from nature}
}

@article{10.1145/3609128,
author = {Singh, Nikhilesh and Renganathan, Karthikeyan and Rebeiro, Chester and Jose, Jithin and Mader, Ralph},
title = {
Kryptonite: Worst-Case Program Interference Estimation on Multi-Core Embedded Systems},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609128},
doi = {10.1145/3609128},
abstract = {Due to the low costs and energy needed, cyber-physical systems are adopting multi-core processors for their embedded computing requirements. In order to guarantee safety when the application has real-time constraints, a critical requirement is to estimate the worst-case interference from other executing programs. However, the complexity of multi-core hardware inhibits precisely determining the Worst-Case Program Interference. Existing solutions are either prone to overestimate the interference or are not scalable to different hardware sizes and designs.In this paper we present&nbsp;Kryptonite, an automated framework to synthesize Worst-Case Program Interference (WCPI) environments for multi-core systems. Fundamental to&nbsp;Kryptoniteis a set of tiny hardware-specific code gadgets that are crafted to maximize interference locally. The gadgets are arranged using a greedy approach and then molded using a Reinforcement Learning algorithm to create the WCPI environment. We demonstrate&nbsp;Kryptoniteon the automotive grade Infineon AURIX TC399 processor with a wide range of programs that includes a commercial real-time automotive application. We show that, while being easily scalable and tunable,&nbsp;Kryptonitecreates WCPI environments increasing the runtime by up to 58\% for benchmark applications and 26\% for the automotive application.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {149},
numpages = {23},
keywords = {Program interference, worst-case analysis}
}

@article{10.1145/3609225,
author = {Wu, Bin and Meng, Zaiqiao and Liang, Shangsong},
title = {Dynamic Bayesian Contrastive Predictive Coding Model for Personalized Product Search},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3609225},
doi = {10.1145/3609225},
abstract = {In this article, we study the problem of dynamic personalized product search. Due to the data-sparsity problem in the real world, existing methods suffer from the challenge of data inefficiency. We address the challenge by proposing a Dynamic Bayesian Contrastive Predictive Coding model (DBCPC), which aims to capture the rich structured information behind search records to improve data efficiency. Our proposed DBCPC utilizes contrastive predictive learning to jointly learn dynamic embeddings with structure information of entities (i.e., users, products, and words). Specifically, our DBCPC employs structured prediction to tackle the intractability caused by non-linear output space and utilizes the time embedding technique to avoid designing different encoders each time in the Dynamic Bayesian models. In this way, our model jointly learns the underlying embeddings of entities (i.e., users, products, and words) via prediction tasks, which enables the embeddings to focus more on their general attributes and capture the general information during the preference evolution with time. For inferring the dynamic embeddings, we propose an inference algorithm combining the variational objective and the contrastive objectives. Experiments were conducted on an Amazon dataset and the experimental results show that our proposed DBCPC can learn the higher-quality embeddings and outperforms the state-of-the-art non-dynamic and dynamic models for product search.},
journal = {ACM Trans. Web},
month = oct,
articleno = {33},
numpages = {31},
keywords = {Contrastive learning, dynamic model, product search, contrastive predictive coding}
}

@article{10.1145/3609235,
author = {Oralbayeva, Nurziya and Aly, Amir and Sandygulova, Anara and Belpaeme, Tony},
title = {Data-driven Communicative Behaviour Generation: A Survey},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
url = {https://doi.org/10.1145/3609235},
doi = {10.1145/3609235},
abstract = {The development of data-driven behaviour generating systems has recently become the focus of considerable attention in the fields of human–agent interaction and human–robot interaction. Although rule-based approaches were dominant for years, these proved inflexible and expensive to develop. The difficulty of developing production rules, as well as the need for manual configuration to generate artificial behaviours, places a limit on how complex and diverse rule-based behaviours can be. In contrast, actual human–human interaction data collected using tracking and recording devices makes humanlike multimodal co-speech behaviour generation possible using machine learning and specifically, in recent years, deep learning. This survey provides an overview of the state of the art of deep learning-based co-speech behaviour generation models and offers an outlook for future research in this area.},
journal = {J. Hum.-Robot Interact.},
month = jan,
articleno = {2},
numpages = {39},
keywords = {Datasets, neural networks, data-driven behaviour generation}
}

@article{10.1145/3609324,
author = {Roy, Satyaki and Ghosh, Nirnay and Uplavikar, Nitish and Ghosh, Preetam},
title = {Towards a Unified Pandemic Management Architecture: Survey, Challenges, and Future Directions},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3609324},
doi = {10.1145/3609324},
abstract = {The pandemic caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has impacted the economy, health, and society. Emerging strains are making pandemic management challenging. There is an urge to collect epidemiological, clinical, and physiological data to make an informed decision on mitigation. Advances in the Internet of Things (IoT) and edge computing provide solutions for pandemic management through data collection and intelligent computation. While existing data-driven architectures operate on specific application domains and attempt to automate decision-making, they do not capture the multifaceted interaction among computational models, communication infrastructure, and data. In this article, we survey the existing approaches for pandemic management, including data repositories and contact-tracing applications. We envision a unified pandemic management architecture that leverages the IoT and edge computing paradigms to automate recommendations on vaccine distribution, dynamic lockdown, mobility scheduling, and pandemic trend prediction. We elucidate the data flow among the layers, namely, cloud, edge, and end device layers. Moreover, we address the privacy implications, threats, regulations, and solutions that may be adapted to optimize the utility of health data with security guarantees. The article ends with a discussion of the limitations of the architecture and research directions to enhance its practicality.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {38},
numpages = {32},
keywords = {Pandemic management, IoT, optimization, machine learning, privacy}
}

@article{10.1145/3609336,
author = {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and Rana, Omer and Perera, Charith},
title = {Query Interface for Smart City Internet of Things Data Marketplaces: A Case Study},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3609336},
doi = {10.1145/3609336},
abstract = {Cities are increasingly becoming augmented with sensors through public, private, and academic sector initiatives. Most of the time, these sensors are deployed with a primary purpose (objective) in mind (e.g., deploy sensors to understand noise pollution) by a sensor owner (i.e., the organization that invests in sensing hardware, e.g., a city council). Over the past few years, communities undertaking smart city development projects have understood the importance of making the sensor data available to a wider community—beyond their primary usage. Different business models have been proposed to achieve this, including creating data marketplaces. The vision is to encourage new startups and small and medium-scale businesses to create novel products and services using sensor data to generate additional economic value. Currently, data are sold as pre-defined independent datasets (e.g., noise level and parking status data may be sold separately). This approach creates several challenges, such as (i) difficulties in pricing, which leads to higher prices (per dataset); (ii) higher network communication and bandwidth requirements; and (iii) information overload for data consumers (i.e., those who purchase data). We investigate the benefit of semantic representation and its reasoning capabilities toward creating a business model that offers data on demand within smart city Internet of Things data marketplaces. The objective is to help data consumers (i.e., small and medium enterprises) acquire the most relevant data they need. We demonstrate the utility of our approach by integrating it into a real-world IoT data marketplace (developed by the synchronicity-iot.eu project). We discuss design decisions and their consequences (i.e., tradeoffs) on the choice and selection of datasets. Subsequently, we present a series of data modeling principles and recommendations for implementing IoT data marketplaces.},
journal = {ACM Trans. Internet Things},
month = sep,
articleno = {19},
numpages = {39},
keywords = {Internet of Things, semantic interoperability, data discovery, multi-dimensional querying, linked data, knowledge management}
}

@article{10.1145/3609386,
author = {Odema, Mohanad and Bouzidi, Halima and Ouarnoughi, Hamza and Niar, Smail and Al Faruque, Mohammad Abdullah},
title = {MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609386},
doi = {10.1145/3609386},
abstract = {Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNAS proposes a GNN architectural design space coupled with prospective mapping options on a heterogeneous SoC to identify model architectures that maximize on-device resource efficiency. To achieve this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs and mapping pairings that yield the best performance trade-offs. Through designing a supernet derived from the recent Vision GNN (ViG) architecture, we conducted experiments on four (04) state-of-the-art vision datasets using both (i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a performance/cost model simulator for DNN accelerators. Our experimental results demonstrate that MaGNAS is able to provide 1.57\texttimes{} latency speedup and is 3.38\texttimes{} more energy-efficient for several vision datasets executed on the Xavier MPSoC vs. the GPU-only deployment while sustaining an average 0.11\% accuracy reduction from the baseline.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {108},
numpages = {26},
keywords = {Graph neural networks, MPSoCs, HW-SW codesign, edge computing}
}

@article{10.1145/3609425,
author = {Hussein, Dina and Bhat, Ganapati},
title = {SensorGAN: A Novel Data Recovery Approach for Wearable Human Activity Recognition},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609425},
doi = {10.1145/3609425},
abstract = {Human activity recognition&nbsp;(HAR) and, more broadly, activities of daily life recognition using wearable devices have the potential to transform a number of applications, including mobile healthcare, smart homes, and fitness monitoring. Recent approaches for HAR use multiple sensors on various locations on the body to achieve higher accuracy for complex activities. While multiple sensors increase the accuracy, they are also susceptible to reliability issues when one or more sensors are unable to provide data to the application due to sensor malfunction, user error, or energy limitations. Training multiple activity classifiers that use a subset of sensors is not desirable, since it may lead to reduced accuracy for applications. To handle these limitations, we propose a novel generative approach that recovers the missing data of sensors using data available from other sensors. The recovered data are then used to seamlessly classify activities. Experiments using three publicly available activity datasets show that with data missing from one sensor, the proposed approach achieves accuracy that is within 10\% of the accuracy with no missing data. Moreover, implementation on a wearable device prototype shows that the proposed approach takes about 1.5&nbsp;ms for recovering data in the w-HAR dataset, which results in an energy consumption of 606&nbsp;μJ. The low-energy consumption ensures that SensorGAN is suitable for effectively recovering data in tinyML applications on energy-constrained devices.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = may,
articleno = {53},
numpages = {28},
keywords = {Human activity recognition, wearable electronics, missing data detection, data imputation, clustering, health monitoring}
}

@article{10.1145/3609797,
author = {Bruch, Sebastian and Nardini, Franco Maria and Ingber, Amir and Liberty, Edo},
title = {An Approximate Algorithm&nbsp;for Maximum Inner Product Search over Streaming Sparse Vectors},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3609797},
doi = {10.1145/3609797},
abstract = {Maximum Inner Product Search or top-k retrieval on sparse vectors is well understood in information retrieval, with a number of mature algorithms that solve it exactly. However, all existing algorithms are tailored to text and frequency-based similarity measures. To achieve optimal memory footprint and query latency, they rely on the near stationarity of documents and on laws governing natural languages. We consider, instead, a setup in which collections are streaming—necessitating dynamic indexing—and where indexing and retrieval must work with arbitrarily distributed real-valued vectors. As we show, existing algorithms are no longer competitive in this setup, even against na\"{\i}ve solutions. We investigate this gap and present a novel approximate solution, called Sinnamon, that can efficiently retrieve the top-k results for sparse real valued vectors drawn from arbitrary distributions. Notably, Sinnamon offers levers to trade off memory consumption, latency, and accuracy, making the algorithm suitable for constrained applications and systems. We give theoretical results on the error introduced by the approximate nature of the algorithm and present an empirical evaluation of its performance on two hardware platforms and synthetic and real-valued datasets. We conclude by laying out concrete directions for future research on this general top-k retrieval problem over sparse vectors.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {42},
numpages = {43},
keywords = {Approximate algorithms, maximum inner product search, sparse vectors}
}

@article{10.1145/3610299,
author = {Chen, Kuan-Chun and Li, Cheng-Te and Lee, Kuo-Jung},
title = {DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3610299},
doi = {10.1145/3610299},
abstract = {Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture nor encodes the latent hierarchical categorization behind text input. This article presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the candidates of NAS building blocks, its promising performance is noticeable and extensible to obtain further improvement by adding more different operations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {88},
numpages = {22},
keywords = {Neural architecture search, text classification, discretization, differentiable neural architecture search, mutual information maximization, representation learning}
}

@article{10.1145/3611093,
author = {Kang, Daniel and Guibas, John and Bailis, Peter and Hashimoto, Tatsunori and Sun, Yi and Zaharia, Matei},
title = {Data Management for ML-Based Analytics and Beyond},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3611093},
doi = {10.1145/3611093},
abstract = {The increasing capabilities of machine learning (ML) has enabled the deployment of ML methods in a variety of applications, ranging from unstructured data analytics to autonomous vehicles. Due to the volumes of data over which ML is deployed, it is infeasible for humans to monitor deployments: the Tesla fleet of vehicles produces exabytes of data and millions of hours of video per day. As a result, ML deployments can fail in unexpected and catastrophic ways.In this work, we highlight three important but underlooked aspects of ML deployment pipelines: (1) managing high-quality training data, (2) monitoring ML errors at deployment time, and (3) connecting end use to deployment algorithms. We first demonstrate that training labels are often erroneous, contrary to standard practice, even when labeled by leading vendors. We then demonstrate that standard methods of deploying ML methods can lead to downstream errors. As a first step toward addressing these issues, we review and contextualize two abstractions for finding errors in training data and deployments. We further describe how to improve algorithms for analytics queries as a case study for optimizing ML pipelines end to end.Problem statementThis paper considers the problem of end-to-end machine learning (ML) deployments, from the collection of training data all the way to answering queries using ML models. We focus on errors in ML models and the data used to train them, along with algorithms for end-to-end uses of these models.MethodsWe provide simple abstractions, model assertions and learned observation assertions (LOA) to find errors that are pervasive in ML model deployments and the data used to train these ML models. We further implemented our abstractions in open-source APIs. Our abstractions and APIs are easy for those who are not experts in ML to use and deploy.ResultsWe show that model assertions and LOA can be deployed in as few as 10 lines of code per assertion. They can find errors with up to 100\% precision across domains ranging from video analytics, tabular data analytics, and translation.SignificanceIt is standard in the literature to assume that training data is "gold" (i.e., 100\% accurate) and that bulk ML model metrics such as accuracy properly reflect model performance. We show that these are not the case, even in widely studied settings. Our simple abstractions point towards methods of checking end-to-end deployments. We hope that future work builds on our APIs.},
journal = {ACM / IMS J. Data Sci.},
month = jan,
articleno = {4},
numpages = {23},
keywords = {ML Analytics, Errors in ML data, Errors in ML model outputs}
}

@article{10.1145/3612918,
author = {Sun, Danfeng and Hu, Junjie and Wu, Huifeng and Wu, Jia and Yang, Jian and Sheng, Quan Z. and Dustdar, Schahram},
title = {A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3612918},
doi = {10.1145/3612918},
abstract = {The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {50},
numpages = {37}
}

@article{10.1145/3612919,
author = {Leventidis, Aristotelis and Di Rocco, Laura and Gatterbauer, Wolfgang and Miller, Ren\'{e}e J. and Riedewald, Mirek},
title = {DomainNet: Homograph Detection and Understanding in Data Lake Disambiguation},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3612919},
doi = {10.1145/3612919},
abstract = {Modern data lakes are heterogeneous in the vocabulary that is used to describe data. We study a problem of disambiguation in data lakes: How can we determine if a data value occurring more than once in the lake has different meanings and is therefore a homograph?
While word and entity disambiguation have been well studied in computational linguistics, data management, and data science, we show that data lakes provide a new opportunity for disambiguation of data values, because tables implicitly define a massive network of interconnected values. We introduce DomainNet, which efficiently represents this network, and investigate to what extent it can be used to disambiguate values without requiring any supervision.DomainNet leverages network-centrality measures on a bipartite graph whose nodes represent data values and attributes to determine if a value is a homograph. A thorough experimental evaluation demonstrates that state-of-the-art techniques in domain discovery cannot be re-purposed to compete with our method. Specifically, using a domain discovery method to identify homographs achieves an F1-score of 0.38 versus 0.69 for DomainNet, which separates homographs well from data values that have a unique meaning. On a real data lake, our top-100 precision is 93\%. Given a homograph, we also present a novel method for determining the number of meanings of the homograph and for assigning its data lake attributes to a meaning. We show the influence of homographs on two downstream tasks: entity-matching and domain discovery.},
journal = {ACM Trans. Database Syst.},
month = sep,
articleno = {9},
numpages = {40},
keywords = {Data Discovery, homograph detection, network-centrality measures}
}

@article{10.1145/3615356,
author = {Perti, Ashwin and Sinha, Amit and Vidyarthi, Ankit},
title = {Cognitive Hybrid Deep Learning-based Multi-modal Sentiment Analysis for Online Product Reviews},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3615356},
doi = {10.1145/3615356},
abstract = {Recently the field of sentiment analysis has gained a lot of attraction in literature. The idea that a machine can dynamically spot the text’s sentiments is fascinating. In this paper, we propose a method to classify the textual sentiments in Twitter feeds. In particular, we focus on analyzing the tweets of products as either positive or negative. The proposed technique utilizes a deep learning schema to learn and predict the sentiment by extracting features directly from the text. Specifically, we use Convolutional Neural Networks with different convolutional layers. Further, we experiment with LSTMs and try an ensemble of multiple models to get the best results. We employ an n-gram-based word embeddings approach to get the machine-level word representations. Testing of the method is conducted on real-world datasets. We have discovered that the ensemble technique yields the best results after conducting experiments on a huge corpus of more than one million tweets. To be specific, we get an accuracy of 84.95\%. The proposed method is also compared with several existing methods. An extensive numerical investigation has revealed the superiority of the proposed work in actual deployment scenarios.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {113},
numpages = {14},
keywords = {Sentiment analysis, convolutional neural network, ensemble voting approach, product review mining}
}

@article{10.1145/3617183,
author = {Zhang, Feng and Zhang, Leping and Zhao, Yongwang and Liu, Yang and Sun, Jun},
title = {Refinement-based Specification and Analysis of Multi-core ARINC 653 Using Event-B},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3617183},
doi = {10.1145/3617183},
abstract = {ARINC 653 as the de facto standard of partitioning operating systems has been applied in many safety-critical domains. The multi-core version of ARINC 653, ARINC 653 Part 1-4 (Version 4), provides support for services to be utilized with a module that contains multiple processor cores. Formal specification and analysis of this standard document could provide a rigorous specification and uncover concealed errors in the textual description of service requirements. This article proposes a specification method for concurrency on a multi-core platform using Event-B, and a refinement structure for the complicated ARINC 653 Part 1-4 provides a comprehensive, stepwise refinement-based Event-B specification with seven refinement layers and then performs formal proof and analysis in RODIN. We verify that the errors discovered in the single-core version standard (ARINC 653 Part 1-3) also exist in the ARINC 653 Part 1-4 during the formal specification and analysis.},
journal = {Form. Asp. Comput.},
month = nov,
articleno = {24},
numpages = {29},
keywords = {Multi-core ARINC 653, Event-B, refinement, formal specification and analysis}
}

@article{10.1145/3617308,
author = {Chockchowwat, Supawit and Liu, Wenjie and Park, Yongjoo},
title = {AirIndex: Versatile Index Tuning Through Data and Storage},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617308},
doi = {10.1145/3617308},
abstract = {The end-to-end lookup latency of a hierarchical index---such as a B-tree or a learned index---is determined by its structure such as the number of layers, the kinds of branching functions appearing in each layer, the amount of data we must fetch from layers, etc. Our primary observation is that by optimizing those structural parameters (or designs) specifically to a target system's I/O characteristics (e.g., latency, bandwidth), we can offer a faster lookup compared to the ones that are not optimized. Can we develop a systematic method for finding those optimal design parameters? Ideally, the method must have the potential to generate almost any existing index or a novel combination of them for the fastest possible lookup.In this work, we present new data and an I/O-aware index builder (called AirIndex) that can find high-speed hierarchical index designs in a principled way. Specifically, AirIndex minimizes an objective function expressing the end-to-end latency in terms of various designs---the number of layers, types of layers, and more---for given data and a storage profile, using a graph-based optimization method purpose-built to address the computational challenges rising from the inter-dependencies among index layers and the exponentially many candidate parameters in a large search space. Our empirical studies confirm that AirIndex can find optimal index designs, build optimal indexes within the times comparable to existing methods, and deliver up to 4.1x faster lookup than a lightweight B-tree library (LMDB), 3.3x--46.3x faster than state-of-the-art learned indexes (RMI/CDFShop, PGM-index, ALEX/APEX, PLEX), and 2.0 faster than Data Calculator's suggestion on various dataset and storage settings.},
journal = {Proc. ACM Manag. Data},
month = nov,
articleno = {204},
numpages = {26},
keywords = {constrained optimization, external memory, graph search, hierarchical indexes, index complexity, indexing, instance optimization, learned indexes, parallel tuning, storage, tuning}
}

@article{10.1145/3617366,
author = {Hornecker, Eva and Hogan, Trevor and Hinrichs, Uta and Van Koningsbruggen, Rosa},
title = {A Design Vocabulary for Data Physicalization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3617366},
doi = {10.1145/3617366},
abstract = {Although physical artifacts that represent data have been used for centuries, the research field—known as data physicalization—has only recently gained traction. Compared to data visualization, there is no established vocabulary for analyzing and discussing the properties of physicalizations. Through a grounded analysis of examples and literature, we propose a comprehensive design vocabulary, which consist of three separate, but connected parts: explicit variables, implicit properties, and consequential aspects. Explicit variables build on visual variables known from visualization and extend it to account for physicalization’s multi-modal nature. Implicit properties concern elements which are central to the design intention and user experience of physicalizations, yet are not a result of “explicit” encoding strategies. Finally, consequential aspects refer to unintentional effects of design decisions, that influence how a physicalization is experienced. Our work illustrates how physicalizations incorporate opportunities and challenges that are not afforded in other data representations, such as embodiment and imagined touch. With this, we contribute to generating theory on physicalization. Our design vocabulary can support (1) creators through informing their design processes and highlighting design strategies, (2) educators, and (3) academics and practitioners to analyse existing physicalizations, and reflect on the impact of design decisions on interpretation and experience.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {2},
numpages = {62},
keywords = {Visualization, InfoVis, user experience, data narrative, materiality, material turn, embodiment, design language}
}

@article{10.1145/3617824,
author = {Wu, Haiqin and D\"{u}dder, Boris and Wang, Liangmin and Cao, Zhenfu and Zhou, Jun and Feng, Xia},
title = {Survey on Secure Keyword Search over Outsourced Data: From Cloud to Blockchain-assisted Architecture},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3617824},
doi = {10.1145/3617824},
abstract = {Secure keyword search is a prevailing search service offered in outsourced environments. However, with the increasingly severe security vulnerabilities of conventional centralized outsourcing, the architecture of secure keyword search, with searchable encryption (SE) as the underlying technique, has recently shifted from cloud-centered models to blockchain-assisted models. Existing surveys commonly fail to capture such an evolution and the corresponding benefits. What on earth does blockchain bring about and what are the unexplored challenges? This survey provides a systematic review of secure keyword search over outsourced data from cloud to blockchain-assisted architectures. We propose a taxonomy assorting present studies, depending on whether cloud/blockchain and data sharing are included, in which blockchain-assisted architecture is further divided into blockchain-side and cloud-side keyword search, respectively. Technically, we conclude five types of representative SE techniques with fitting architectures, either cryptographic-based or hardware-dependent. Notably, we propose comprehensive methodologies to select relevant papers, discuss, and compare existing schemes regarding functionalities, security, efficiency, and fairness (up to 21 compared items). Finally, open issues and potential research directions are identified for future work. We aspire to help pave the way for addressing the theoretical and empirical aspects of secure keyword search and full-fledged real-world implementation of blockchain-based keyword search applications.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {63},
numpages = {40},
keywords = {Outsourced keyword search, searchable encryption, cloud security, blockchain, verifiability}
}

@article{10.1145/3622805,
author = {Chen, Qinlin and Zhang, Nairen and Wang, Jinpeng and Tan, Tian and Xu, Chang and Ma, Xiaoxing and Li, Yue},
title = {The Essence of Verilog: A Tractable and Tested Operational Semantics for Verilog},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622805},
doi = {10.1145/3622805},
abstract = {With the increasing need to apply modern software techniques to hardware design, Verilog, the most popular Hardware Description Language (HDL), plays an infrastructure role. However, Verilog has several semantic pitfalls that often confuse software and hardware developers. Although prior research on formal semantics for Verilog exists, it is not comprehensive and has not fully addressed these issues. In this work, we present a novel scheme inspired by previous work on defining core languages for software languages like JavaScript and Python. Specifically, we define the formal semantics of Verilog using a core language called λV, which captures the essence of Verilog using as few language structures as possible. λV not only covers the most complete set of language features to date, but also addresses the aforementioned pitfalls. We implemented λV with about 27,000 lines of Java code, and comprehensively tested its totality and conformance with Verilog. As a reliable reference semantics, λV can detect semantic bugs in real-world Verilog simulators and expose ambiguities in Verilog’s standard specification. Moreover, as a useful core language, λV has the potential to facilitate the development of tools such as a state-space explorer and a concolic execution tool for Verilog.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {230},
numpages = {30},
keywords = {Core Languages, Hardware Description Languages, Semantics, Verilog}
}

@article{10.1145/3622879,
author = {Damm, Werner and Hess, David and Schweda, Mark and Sztipanovits, Janos and Bengler, Klaus and Biebl, Bianca and Fr\"{a}nzle, Martin and Hagemann, Willem and Held, Moritz and Ihme, Klas and Kacianka, Severin and Kerscher, Alyssa J. and Lehnhoff, Sebastian and Luedtke, Andreas and Pretschner, Alexander and Rakow, Astrid and Rieger, Jochem and Sonntag, Daniel and Schwammberger, Maike and Austel, Benedikt and Unni, Anirudh and Veith, Eric},
title = {A Reference Architecture of Human Cyber-Physical Systems – Part I: Fundamental Concepts},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3622879},
doi = {10.1145/3622879},
abstract = {We propose a reference architecture of safety-critical or industry-critical human cyber-physical systems (CPSs) capable of expressing essential classes of system-level interactions between CPS and humans relevant for the societal acceptance of such systems. To reach this quality gate, the expressivity of the model must go beyond classical viewpoints such as operational, functional, and architectural views and views used for safety and security analysis. The model does so by incorporating elements of such systems for mutual introspections in situational awareness, capabilities, and intentions to enable a synergetic, trusted relation in the interaction of humans and CPSs, which we see as a prerequisite for their societal acceptance. The reference architecture is represented as a metamodel incorporating conceptual and behavioral semantic aspects. We illustrate the key concepts of the metamodel with examples from cooperative autonomous driving, the operating room of the future, cockpit-tower interaction, and crisis management.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jan,
articleno = {2},
numpages = {32},
keywords = {Real-time systems, cyber-physical systems, architecture, interaction design}
}

@article{10.1145/3622880,
author = {Bengler, Klaus and Damm, Werner and Luedtke, Andreas and Jochem, Reiger and Austel, Benedikt and Biebl, Bianca and Fr\"{a}nzle, Martin and Hagemann, Willem and Held, Moritz and Hess, David and Ihme, Klas and Kacianka, Severin and Kerscher, Alyssa J. and Forrest, Laine and Lehnhoff, Sebastian and Pretschner, Alexander and Rakow, Astrid and Sonntag, Daniel and Sztipanovits, Janos and Schwammberger, Maike and Schweda, Mark and Unni, Anirudh and Veith, Eric},
title = {A References Architecture for Human Cyber Physical Systems, Part II: Fundamental Design Principles for Human-CPS Interaction},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3622880},
doi = {10.1145/3622880},
abstract = {As automation increases qualitatively and quantitatively in safety-critical human cyber-physical systems, it is becoming more and more challenging to increase the probability or ensure that human operators still perceive key artifacts and comprehend their roles in the system. In the companion paper, we proposed an abstract reference architecture capable of expressing all classes of system-level interactions in human cyber-physical systems. Here we demonstrate how this reference architecture supports the analysis of levels of communication between agents and helps to identify the potential for misunderstandings and misconceptions. We then develop a metamodel for safe human machine interaction. Therefore, we ask what type of information exchange must be supported on what level so that humans and systems can cooperate as a team, what is the criticality of exchanged information, what are timing requirements for such interactions, and how can we communicate highly critical information in a limited time frame in spite of the many sources of a distorted perception. We highlight shared stumbling blocks and illustrate shared design principles, which rest on established ontologies specific to particular application classes. In order to overcome the partial opacity of internal states of agents, we anticipate a key role of virtual twins of both human and technical cooperation partners for designing a suitable communication.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jan,
articleno = {3},
numpages = {27},
keywords = {Real-time systems, Cyber-Physical Systems, architecture, interaction design, Human-CPS Interaction}
}

@article{10.1145/3623511,
author = {Del Vasto-Terrientes, Luis},
title = {Experience: Data Management for Delivering COVID-19 Relief in Panama},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3623511},
doi = {10.1145/3623511},
abstract = {A data-driven public sector recognizes data as a key element for implementing policies based on evidence. The open data movement has been a major catalyst for elevating data to a privileged position in many governments around the globe. In Panama, open data has enabled the improvement of data management in each institution. However, it is required to go further to create an integrated data-driven government with a common objective. Public institutions collect a huge amount of data that may never be used, and some others do not contain enough quality to provide trustworthy results. The state of emergency caused by the COVID-19 showed the necessity of establishing a common digital government vision for planning, delivering, and monitoring public services, as well as strengthening the technical foundation in the public sector to improve the data value cycle: acquisition, storage, and exploitation. This paper reports from a data custodian perspective how the state of emergency worked as a catalyst to boost government data management, specifically for the Vale Digital program, a social relief program linked to the identity card implemented by the Panamanian government during the COVID-19 pandemic, which may possibly be the greatest government data integration to date in terms of impact, data volume, rapid implementation, and institutions involved.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {45},
numpages = {12},
keywords = {Data management, data-driven government, data integration, public policies}
}

@article{10.1145/3625101,
author = {Ye, Xiaoqing and Sun, Yang and Liu, Dun and Li, Tianrui},
title = {A Multisource Data Fusion-based Heterogeneous Graph Attention Network for Competitor Prediction},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3625101},
doi = {10.1145/3625101},
abstract = {Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {39},
numpages = {20},
keywords = {Competitor identification, business intelligence, business data, graph neural network}
}

@article{10.1145/3625264,
author = {Ponton, Jose Luis and Yun, Haoran and Aristidou, Andreas and Andujar, Carlos and Pelechano, Nuria},
title = {SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3625264},
doi = {10.1145/3625264},
abstract = {Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions though, it is important to minimize the number of sensors attached to the subject’s body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {5},
numpages = {14},
keywords = {Motion tracking, character animation, wearable devices, sparse data}
}

@article{10.1145/3625389,
author = {Herodotou, Herodotos and Kakoulli, Elena},
title = {Cost-based Data Prefetching and Scheduling in Big Data Platforms over Tiered Storage Systems},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3625389},
doi = {10.1145/3625389},
abstract = {The use of storage tiering is becoming popular in data-intensive compute clusters due to the recent advancements in storage technologies. The Hadoop Distributed File System, for example, now supports storing data in memory, SSDs, and HDDs, while OctopusFS and hatS offer fine-grained storage tiering solutions. However, current big data platforms (such as Hadoop and Spark) are not exploiting the presence of storage tiers and the opportunities they present for performance optimizations. Specifically, schedulers and prefetchers will make decisions only based on data locality information and completely ignore the fact that local data are now stored on a variety of storage media with different performance characteristics. This article presents Trident, a scheduling and prefetching framework that is designed to make task assignment, resource scheduling, and prefetching decisions based on both locality and storage tier information. Trident formulates task scheduling as a minimum cost maximum matching problem in a bipartite graph and utilizes two novel pruning algorithms for bounding the size of the graph, while still guaranteeing optimality. In addition, Trident extends YARN’s resource request model and proposes a new storage-tier-aware resource scheduling algorithm. Finally, Trident includes a cost-based data prefetching approach that coordinates with the schedulers for optimizing prefetching operations. Trident is implemented in both Spark and Hadoop and evaluated extensively using a realistic workload derived from Facebook traces as well as an industry-validated benchmark, demonstrating significant benefits in terms of application performance and cluster efficiency.},
journal = {ACM Trans. Database Syst.},
month = nov,
articleno = {11},
numpages = {40},
keywords = {Distributed file systems, tiered storage, data prefetching, task scheduling}
}

@article{10.1145/3625548,
author = {Gao, Jing and Li, Peng and Laghari, Asif Ali and Srivastava, Gautam and Gadekallu, Thippa Reddy and Abbas, Sidra and Zhang, Jianing},
title = {Incomplete Multiview Clustering via Semidiscrete Optimal Transport for Multimedia Data Mining in IoT},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3625548},
doi = {10.1145/3625548},
abstract = {With the wide deployment of the Internet of Things (IoT), large volumes of incomplete multiview data that violates data integrity is generated by various applications, which inevitably produces negative impacts on the quality of service of IoT systems. Incomplete multiview clustering (IMC), as an essential technique of data processing, has the potential for mining patterns of incomplete IoT data. However, previous methods utilize notion-strong distances that can only measure differences between distributions at the overlap of data manifolds in fusing complementary information of data for pattern mining. They may suffer from biased estimation and information loss in capturing intrinsic structures of incomplete multiview data. To address these challenges, a semidiscrete multiview optimal transport (SD-MOT) is defined for IMC, which utilizes distances with weak notions to capture intrinsic structures of incomplete multiview data. Specifically, IMC is recast as an equivalent optimal transport between continuous incomplete multiview data and discrete clustering centroids, to avoid the strict assumption on overlap between manifolds in pattern mining. Then, SD-MOT is instantiated as a deep incomplete contrastive clustering network to remedy biased estimation and information loss on intrinsic structures of incomplete multiview data. Afterwards, a variational solution to SD-MOT is derived to effectively train the network parameters for pattern mining. Finally, extensive experiments on four representative incomplete multiview datasets verify the superiority of SD-MOT in comparison with nine baseline methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {158},
numpages = {20},
keywords = {Incomplete multiview data, data integrity, internet of things, optimal transport}
}

@article{10.1145/3626097,
author = {Sun, Hui and Lou, Bendong and Zhao, Chao and Kong, Deyan and Zhang, Chaowei and Huang, Jianzhong and Yue, Yinliang and Qin, Xiao},
title = {Asynchronous Compaction Acceleration Scheme for Near-data Processing-enabled LSM-tree-based KV Stores},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3626097},
doi = {10.1145/3626097},
abstract = {LSM-tree-based key-value stores (KV stores) convert random-write requests to sequence-write ones to achieve high I/O performance. Meanwhile, compaction operations in KV stores update SSTables&nbsp;in forms of reorganizing low-level data components to high-level ones, thereby guaranteeing an orderly data layout in each component. Repeated writes caused by compaction (a.k.a. write amplification) impacts I/O bandwidth and overall system performance. Near-data processing (NDP) is one of the effective approaches to addressing this write-amplification issue. Most NDP-based techniques adopt synchronous parallel schemes to perform a compaction task on both the host and its NDP-enabled device. In synchronous parallel compaction schemes, the execution time of compaction is determined by a subsystem that has lower compaction performance coupled by under-utilized computing resources in a NDP framework. To solve this problem, we propose an asynchronous parallel scheme named PStore to improve the compaction performance in KV stores. In PStore, we designed a multi-tasks queue and three priority-based scheduling methods. PStore elects proper compaction tasks to be offloaded in host- and device-side compaction modules. Our proposed cross-leveled compaction mechanism mitigates write amplification induced by asynchronous compaction. PStore featured with the asynchronous compaction mechanism fully utilizes computing resources in both host- and device-side subsystems. Compared with the two popular synchronous compaction modes based on KV stores (TStore and LevelDB), our PStore immensely improves the throughput by up to a factor of 14 and 10.52 with an average of a factor of 2.09 and 1.73, respectively.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {93},
numpages = {33},
keywords = {Near-data processing, key-value store, asynchronous compaction, write amplification}
}

@article{10.1145/3626200,
author = {Kromes, Roland and Verdier, Fran\c{c}ois},
title = {Accelerating Blockchain Applications on IoT Architecture Models—Solutions and Drawbacks},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3626200},
doi = {10.1145/3626200},
abstract = {More and more IoT use cases require trustworthy computing from cloud/back-end services, which cannot necessarily provide a fully trusted execution environment, data immutability, and traceability. The integration of IoT with the blockchain technology is one of the most promising solutions to achieve the previously mentioned features in the IoT networks. Researchers are also interested in integration solutions, and several solutions are already present in the scientific literature. However, there are still some uncertainties in establishing a direct and effective interaction between an IoT device and the given blockchain. In this work, we propose the first IoT hardware architecture model designed to accelerate time-consuming operations of IoT-Blockchain. The proposed IoT hardware architecture model is programmed in SystemC-TLM and can provide a significant reduction in execution time, 53\% and 18\% when running Hyperledger Sawtooth and Ethereum applications, respectively.},
journal = {Distrib. Ledger Technol.},
month = jun,
articleno = {14},
numpages = {24},
keywords = {IoT, blockchain, embedded systems, hardware modelling, SystemC}
}

@article{10.1145/3626468,
author = {Bhatnagar, Tigmanshu and Higgins, Albert and Marquardt, Nicolai and Miodownik, Mark and Holloway, Catherine},
title = {Analysis of Product Architectures of Pin Array Technologies for Tactile Displays},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ISS},
url = {https://doi.org/10.1145/3626468},
doi = {10.1145/3626468},
abstract = {Refreshable tactile displays based on pin array technologies have a significant impact on the education of children with visual impairments, but they are prohibitively expensive. To better understand their design and the reason for the high cost, we created a database and analyzed the product architectures of 67 unique pin array technologies from literature and patents. We qualitatively coded their functional elements and analyzed the physical parts that execute the functions. Our findings highlight that pin array surfaces aim to achieve three key functions, i.e., raise and lower pins, lock pins, and create a large array. We also contribute a concise morphological chart that organises the various mechanisms for these three functions. Based on this, we discuss the reasons for the high cost and complexity of these surface haptic technologies and infer why larger displays and more affordable devices are not available. Our findings can be used to design new mechanisms for more affordable and scalable pin array display systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {432},
numpages = {21},
keywords = {Literature Review, Pin Array, Product Architecture, Tactile}
}

@article{10.1145/3626757,
author = {Sha, Mo and Li, Jialin and Wang, Sheng and Li, Feifei and Tan, Kian-Lee},
title = {TEE-based General-purpose Computational Backend for Secure Delegated Data Processing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626757},
doi = {10.1145/3626757},
abstract = {The increasing prevalence of data breaches necessitates robust data protection measures in computational tasks. Secure computation outsourcing (SCO) presents a viable solution by safeguarding the confidentiality of inputs and outputs in data processing without disclosure. Nonetheless, this approach assumes the existence of a trustworthy coordinator to orchestrate and oversee the process, typically implying that data owners must fulfill this role themselves. In this paper, we consider secure delegated data processing (SDDP), an expanded data processing scenario wherein data owners simply delegate their data to SDDP providers for subsequent value mining or other downstream applications, eliminating the necessary involvement of data owners or trusted entities to dive into data processing deeply. However, general-purpose SDDP poses significant challenges in permitting the discretionary execution of computational tasks by SDDP providers on sensitive data while ensuring confidentiality. Existing approaches are insufficient to support SDDP in either efficiency or universality. To tackle this issue, we propose TGCB, a TEE-based General-purpose Computational Backend, designed to endow general-purpose computation with SDDP capabilities from an engineering perspective, powered by TEE-based code integrity and data confidentiality. Central to TGCB is the Encryption Programming Language (EPL) that defines computational tasks in SDDP. Specifically, SDDP providers can express arbitrary computable functions as EPL scripts, processed by TGCB's interfaces, securely interpreted and executed in TEE, ensuring data confidentiality throughout the process. As a universal computational backend, TGCB extensively bolsters data security in existing general-purpose computational tasks, allowing data owners to leverage SDDP without privacy concerns.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {263},
numpages = {28},
keywords = {data confidentiality, data processing, programming language and interpreter, secure delegated computing, trust execution environment}
}

@article{10.1145/3627166,
author = {Amrani, Racha and Kacher, Sabrina and Khouri, Selma and Oufaida, Houda and Ouahab, Safia and Cherrad, Mouna},
title = {Proposal of a Knowledge Capitalization Process to Construct Eco-Diars: A Knowledge-driven Platform Applied to Traditional Algerian Domestic Architecture},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3627166},
doi = {10.1145/3627166},
abstract = {This article is part of doctoral research that proposes to capitalize on the environmental knowledge drawn from traditional Algerian domestic architecture, supported by a knowledge-based platform. This research aims to (1) build a capital of knowledge related to traditional environmental devices (EDs) allowing to suggest them as “references” to propose conceptual solutions during the upstream phases of the architectural design, (2) support the Algerian government's policy of preservation and digitization of architectural heritage, and (3) support the government's policy of reducing energy consumption in the building sector. From this perspective, this article proposes an interactive knowledge capitalization process involving Information and Communication Technologies (ICTs) for the modeling, exploitation, and visualization of ED-related knowledge. This article will present the proposed process for knowledge capitalization leading to the development of the knowledge-driven platform Eco-Diars intended to enable designers to efficiently perform their queries related to traditional environmental devices.},
journal = {J. Comput. Cult. Herit.},
month = jan,
articleno = {4},
numpages = {28},
keywords = {Traditional domestic architecture, Algeria, environmental device, knowledge capitalization, knowledge-driven platform}
}

@article{10.1145/3627169,
author = {Gooch, Megan and Strange, Damon},
title = {Consolidating Research Data Management Infrastructure: Towards Sustainable Digital Scholarship},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3627169},
doi = {10.1145/3627169},
abstract = {The sustainability of digital research outputs, particularly in the humanities where these frequently comprise archives of digital cultural heritage material, has always offered a challenge to the researchers and institutions who have responsibility for them. The amount of upfront care, effort, and funding that goes into developing a research project during the active (and funded) research phase is rarely replicated within the post-project maintenance and curation of the delivered digital assets or archives. What often defines the sustainability of a research project and its archive is a combination of research method and expected life span for the digital collection. Innovation in research data design is often at the expense of its longevity. But this does not need to be so. The tradeoff between longevity and functionality is a false dichotomy. Yet what is clear is that care and consideration in planning the research data storage or archive for a project can make a big difference. A data management plan that meets grant funder requirements is asked for for many research projects, but it is more than simply a funding document. Good research data management ensures that outputs are available online for years to come and available for future research and innovation. This article offers a practical insight to the methods being employed at the University of Oxford to support Digital Humanities scholars (and beyond) safeguard their digital legacy for future generations.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {82},
numpages = {16},
keywords = {Research data management, digital sustainability, digital humanities}
}

@article{10.1145/3627817,
author = {Xiong, Peiyu and Tegegn, Michael and Sarin, Jaskeerat Singh and Pal, Shubhraneel and Rubin, Julia},
title = {It Is All about Data: A Survey on the Effects of Data on Adversarial Robustness},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3627817},
doi = {10.1145/3627817},
abstract = {Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews a particular subset of this literature that focuses on investigating properties of training data in the context of model robustness under evasion attacks. It first summarizes the main properties of data leading to adversarial vulnerability. It then discusses guidelines and techniques for improving adversarial robustness by enhancing the data representation and learning procedures, as well as techniques for estimating robustness guarantees given particular data. Finally, it discusses gaps of knowledge and promising future research directions in this area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {174},
numpages = {41},
keywords = {Machine learning, adversarial robustness, evasion attack, data properties}
}

@article{10.1145/3628430,
author = {Ullah, Obaid and Khan, Habib Ullah and Halim, Zahid and Anwar, Sajid and Waqas, Muhammad},
title = {On Neuroevolution of Multi-Input Compositional Pattern Producing Networks: A Case of Entertainment Computing, Edge Devices, and Smart Cities},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3628430},
doi = {10.1145/3628430},
abstract = {This work presents a novel approach by utilizing Heterogeneous Activation Neural Networks (HA-NNs) to evolve the weights of Artificial Neural Networks (ANNs) for reinforcement learning in console and arcade computer games like Atari's Breakout and Sonic the Hedgehog. It is the first study to explore the potential of HA-NNs as potent ANNs in solving gaming-related reinforcement learning problems. Additionally, the proposed solution optimizes data transmission over networks for edge devices, marking a novel application of HA-NNs. The study achieved outstanding results, outperforming recent works in benchmark environments like CartPole-v1, Lunar Lander Continuous, and MountainCar-Continuous, with HA-NNs and ANNs evolved using the Neuroevolution of Augmenting Topologies (NEAT) algorithm. Notably, the key advancements include exceptional scores of 500 in CartPole-v1 and 98.2 in Mountain Car Continuous, demonstrating the efficacy of HA-NNs in reinforcement learning tasks. Beyond gaming, the research addresses the challenge of efficient data communication between edge devices, which has the potential to enhance performance in smart cities while reducing the load on edge devices and supporting seamless entertainment experiences with minimal commuting. This work pioneers the application of HA-NNs in reinforcement learning for computer games and introduces a novel approach for optimizing edge device communication, promising significant advancements in the fields of AI, neural networks, and smart city technologies.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = oct
}

@article{10.1145/3629040,
author = {Heltweg, Philip and Riehle, Dirk},
title = {A Systematic Analysis of Problems in Open Collaborative Data Engineering},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3–4},
url = {https://doi.org/10.1145/3629040},
doi = {10.1145/3629040},
abstract = {Collaborative workflows are common in open-source software development. They reduce individual costs and improve the quality of work results. Open data shares many characteristics with open-source software, as it can be used, modified, and redistributed by anyone, for free. However, in contrast to open-source software engineering, collaborative data engineering on open data lacks a shared understanding of processes, methods, and tools. This article presents a systematic literature review of collaboration processes, methods, and tools in data engineering as performed by open data users. An additional interview study with practitioners confirms and enhances the findings and strengthens the resulting insights. We find an ecosystem with heterogeneous participants and no standardized processes, methods, and tools. Participants face a variety of technical and social challenges during their work. Our work provides a structured overview of collaboration systems in open collaborative data engineering, enabling further research. Additionally, we contribute preliminary guidelines for successful open collaborative data engineering projects and recommendations to increase its adoption for open data ecosystems.},
journal = {Trans. Soc. Comput.},
month = dec,
articleno = {8},
numpages = {30},
keywords = {Collaboration, data engineering, open data}
}

@article{10.1145/3629521,
author = {Zhang, Shiqing and Naderan-Tahan, Mahmood and Jahre, Magnus and Eeckhout, Lieven},
title = {Characterizing Multi-Chip GPU Data Sharing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3629521},
doi = {10.1145/3629521},
abstract = {Multi-chip Graphics Processing Unit (GPU) systems are critical to scale performance beyond a single GPU chip for a wide variety of important emerging applications. A key challenge for multi-chip GPUs, though, is how to overcome the bandwidth gap between inter-chip and intra-chip communication. Accesses to shared data, i.e., data accessed by multiple chips, pose a major performance challenge as they incur remote memory accesses possibly congesting the inter-chip links and degrading overall system performance. This article characterizes the shared dataset in multi-chip GPUs in terms of (1) truly versus falsely shared data, (2) how the shared dataset scales with input size, (3) along which dimensions the shared dataset scales, and (4) how sensitive the shared dataset is with respect to the input’s characteristics, i.e., node degree and connectivity in graph workloads. We observe significant variety in scaling behavior across workloads: some workloads feature a shared dataset that scales linearly with input size, whereas others feature sublinear scaling (following a  (sqrt {2})  or  (sqrt [3]{2})  relationship). We further demonstrate how the shared dataset affects the optimum last-level cache organization (memory-side versus SM-side) in multi-chip GPUs, as well as optimum memory page allocation and thread scheduling policy. Sensitivity analyses demonstrate the insights across the broad design space.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {56},
numpages = {24},
keywords = {Graphics processing unit (GPU), multi-GPU systems, data sharing}
}

@article{10.1145/3629522,
author = {Susskind, Zachary and Arora, Aman and Miranda, Igor D. S. and Bacellar, Alan T. L. and Villon, Luis A. Q. and Katopodis, Rafael F. and de Ara\'{u}jo, Leandro S. and Dutra, Diego L. C. and Lima, Priscila M. V. and Fran\c{c}a, Felipe M. G. and Breternitz Jr., Mauricio and John, Lizy K.},
title = {ULEEN: A Novel Architecture for Ultra-low-energy Edge Neural Networks},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3629522},
doi = {10.1145/3629522},
abstract = {‘‘Extreme edge”1 devices, such as smart sensors, are a uniquely challenging environment for the deployment of machine learning. The tiny energy budgets of these devices lie beyond what is feasible for conventional deep neural networks, particularly in high-throughput scenarios, requiring us to rethink how we approach edge inference. In this work, we propose ULEEN, a model and FPGA-based accelerator architecture based on weightless neural networks (WNNs). WNNs eliminate energy-intensive arithmetic operations, instead using table lookups to perform computation, which makes them theoretically well-suited for edge inference. However, WNNs have historically suffered from poor accuracy and excessive memory usage. ULEEN incorporates algorithmic improvements and a novel training strategy inspired by binary neural networks (BNNs) to make significant strides in addressing these issues. We compare ULEEN against BNNs in software and hardware using the four MLPerf Tiny datasets and MNIST. Our FPGA implementations of ULEEN accomplish classification at 4.0–14.3 million inferences per second, improving area-normalized throughput by an average of 3.6\texttimes{} and steady-state energy efficiency by an average of 7.1\texttimes{} compared to the FPGA-based Xilinx FINN BNN inference platform. While ULEEN is not a universally applicable machine learning model, we demonstrate that it can be an excellent choice for certain applications in energy- and latency-critical edge environments.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {61},
numpages = {24},
keywords = {Weightless neural networks, WiSARD, neural networks, inference, edge computing, MLPerf tiny, high throughput computing}
}

@article{10.1145/3630006,
author = {Zhao, Laiping and Cui, Yushuai and Yang, Yanan and Zhou, Xiaobo and Qiu, Tie and Li, Keqiu and Bao, Yungang},
title = {Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1–2},
issn = {0734-2071},
url = {https://doi.org/10.1145/3630006},
doi = {10.1145/3630006},
abstract = {Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3\%, CPU utilization by 38.6\%, and memory bandwidth utilization by 45.4\% while guaranteeing the tail latency requirement.},
journal = {ACM Trans. Comput. Syst.},
month = feb,
articleno = {2},
numpages = {37},
keywords = {Datacenters, resource utilization, tail latency, co-locating}
}

@article{10.1145/3630258,
author = {Chen, Hongzhou and Duan, Haihan and Abdallah, Maha and Zhu, Yufeng and Wen, Yonggang and Saddik, Abdulmotaleb El and Cai, Wei},
title = {Web3 Metaverse: State-of-the-Art and Vision},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3630258},
doi = {10.1145/3630258},
abstract = {The metaverse, as a rapidly evolving socio-technical phenomenon, exhibits significant potential across diverse domains by leveraging Web3 (a.k.a. Web 3.0) technologies such as blockchain, smart contracts, and non-fungible tokens (NFTs). This survey aims to provide a comprehensive overview of the Web3 metaverse from a human-centered perspective. We (i) systematically review the development of the metaverse over the past 30 years, highlighting the balanced contributions from its core components: Web3, immersive convergence, and crowd intelligence communities, (ii) define the metaverse that integrates the Web3 community as the Web3 metaverse and propose an analysis framework from the community, society, and human layers to describe the features, missions, and relationships for each community and their overlapping sections, (iii) survey the state-of-the-art of the Web3 metaverse from a human-centered perspective, namely, the identity, field, and behavior aspects, and (iv) provide supplementary technical reviews. To the best of our knowledge, this work represents the first systematic, interdisciplinary survey on the Web3 metaverse. Specifically, we commence by discussing the potential for establishing decentralized identities (DID) utilizing mechanisms such as profile picture (PFP) NFTs, domain name NFTs, and soulbound tokens (SBTs). Subsequently, we examine land, utility, and equipment NFTs within the Web3 metaverse, highlighting interoperable and full on-chain solutions for existing centralization challenges. Lastly, we spotlight current research and practices about individual, intra-group, and inter-group behaviors within the Web3 metaverse, such as Creative Commons Zero license (CC0) NFTs, decentralized education, decentralized science (DeSci), and decentralized autonomous organizations (DAO). Furthermore, we share our insights into several promising directions, encompassing three key socio-technical facets of Web3 metaverse development.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {101},
numpages = {42},
keywords = {Metaverse, Web3, human-centered, definition and framework, survey}
}

@article{10.1145/3631118,
author = {Wu, Dan and Chen*, Peng and Bandara, Thilini Kaushalya and Li, Zhaoying and Mitra, Tulika},
title = {Flip: Data-centric Edge CGRA Accelerator},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3631118},
doi = {10.1145/3631118},
abstract = {Coarse-Grained Reconfigurable Arrays (CGRA) are promising edge accelerators due to the outstanding balance in flexibility, performance, and energy efficiency. Classic CGRAs statically map compute operations onto the processing elements (PE) and route the data dependencies among the operations through the Network-on-Chip. However, CGRAs are designed for fine-grained static instruction-level parallelism and struggle to accelerate applications with dynamic and irregular data-level parallelism, such as graph processing. To address this limitation, we present Flip, a novel accelerator that enhances traditional CGRA architectures to boost the performance of graph applications. Flip retains the classic CGRA execution model while introducing a special data-centric mode for efficient graph processing. Specifically, it leverages the inherent data parallelism of graph algorithms by mapping graph vertices onto PEs rather than the operations and supporting dynamic routing of temporary data according to the runtime evolution of the graph frontier. Experimental results demonstrate that Flip achieves up to 36\texttimes{} speedup with merely 19\% more area compared to classic CGRAs. Compared to state-of-the-art large-scale graph processors, Flip has similar energy efficiency and 2.2\texttimes{} better area efficiency at a much-reduced power/area budget.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {22},
numpages = {25},
keywords = {Coarse-grained Reconfigurable Array, Graph Processing, Accelerator}
}

@article{10.1145/3631391,
author = {V, Jothi Prakash and S, Arul Antran Vijay},
title = {Cross-lingual Sentiment Analysis of Tamil Language Using a Multi-stage Deep Learning Architecture},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {12},
issn = {2375-4699},
url = {https://doi.org/10.1145/3631391},
doi = {10.1145/3631391},
abstract = {In recent years, sentiment analysis has become a focal point in natural language processing. Cross-lingual sentiment analysis is a particularly demanding yet essential task that seeks to construct models capable of effectively analyzing sentiments across a variety of languages. The primary motivation behind this research is to bridge the gap in current techniques that often struggle to perform well with low-resource languages, due to the scarcity of large, annotated datasets, and their unique linguistic characteristics. In light of these challenges, we propose a novel Multi-Stage Deep Learning Architecture (MSDLA) for cross-lingual sentiment analysis of the Tamil language, a low-resource language. Our approach utilizes transfer learning from a source language with abundant resources to overcome data limitations. Our proposed model significantly outperforms existing methods on the Tamil Movie Review dataset, achieving an accuracy, precision, recall, and F1-score of 0.8772, 0.8614, 0.8825, and 0.8718, respectively. ANOVA statistical comparison demonstrates that the MSDLA’s improvements over other models, including mT5, XLM, mBERT, ULMFiT, BiLSTM, LSTM with Attention, and ALBERT with Hugging Face English Embedding are significant, with p-values all less than 0.005. Ablation studies confirm the importance of both cross-lingual semantic attention and domain adaptation in our architecture. Without these components, the model’s performance drops to 0.8342 and 0.8043 in accuracy, respectively. Furthermore, MSDLA demonstrates robust cross-domain performance on the Tamil News Classification and Thirukkural datasets, achieving an accuracy of 0.8551 and 0.8624, respectively, significantly outperforming the baseline models. These findings illustrate the robustness and efficacy of our approach, making a significant contribution to cross-lingual sentiment analysis techniques, especially for low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {254},
numpages = {28},
keywords = {Artificial intelligence, attention mechanism, cross-lingual sentiment analysis, tamil sentiment analysis}
}

@article{10.1145/3631528,
author = {Zhang, Dunbo and Lang, Qingjie and Wang, Ruoxi and Shen, Li},
title = {Extension VM: Interleaved Data Layout in Vector Memory},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3631528},
doi = {10.1145/3631528},
abstract = {While vector architecture is widely employed in processors for neural networks, signal processing, and high-performance computing; however, its performance is limited by inefficient column-major memory access. The column-major access limitation originates from the unsuitable mapping of multidimensional data structures to two-dimensional vector memory spaces. In addition, the traditional data layout mapping method creates an irreconcilable conflict between row- and column-major accesses. Ideally, both row- and column-major accesses can take advantage of the bank parallelism of vector memory.To this end, we propose the Interleaved Data Layout (IDL) method in vector memory, which can distribute vector elements into different banks regardless of whether they are in the row- or column-major category, so that any vector memory access can benefit from bank parallelism. Additionally, we propose an Extension Vector Memory (EVM) architecture to achieve IDL in vector memory. EVM can support two data layout methods and vector memory access modes simultaneously. The key idea is to continuously distribute the data that needs to be accessed from the main memory to different banks during the loading period. Thus, EVM can provide a larger spatial locality level through careful programming and the extension ISA support.The experimental results showed a 1.43-fold improvement of state-of-the-art vector processors by the proposed architecture, with an area cost of only 1.73\%. Furthermore, the energy consumption was reduced by 50.1\%.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {18},
numpages = {23},
keywords = {Vector architecture, vector memory, processing-in-memory, data layout}
}

@article{10.1145/3632863,
author = {Hammond, Angus and Liu, Zongyuan and P\'{e}rami, Thibaut and Sewell, Peter and Birkedal, Lars and Pichon-Pharabod, Jean},
title = {An Axiomatic Basis for Computer Programming on the Relaxed Arm-A Architecture: The AxSL Logic},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632863},
doi = {10.1145/3632863},
abstract = {Very relaxed concurrency memory models, like those of the Arm-A, RISC-V, and IBM Power hardware architectures, underpin much of computing but break a fundamental intuition about programs, namely that syntactic program order and the reads-from relation always both induce order in the execution. Instead, out-of-order execution is allowed except where prevented by certain pairwise dependencies, barriers, or other synchronisation. This means that there is no notion of the 'current' state of the program, making it challenging to design (and prove sound) syntax-directed, modular reasoning methods like Hoare logics, as usable resources cannot implicitly flow from one program point to the next.  

We present AxSL, a separation logic for the relaxed memory model of Arm-A, that captures the fine-grained reasoning underpinning the low-overhead synchronisation mechanisms used by high-performance systems code. In particular, AxSL allows transferring arbitrary resources using relaxed reads and writes when they induce inter-thread ordering. We mechanise AxSL in the Iris separation logic framework, illustrate it on key examples, and prove it sound with respect to the axiomatic memory model of Arm-A. Our approach is largely generic in the axiomatic model and in the instruction-set semantics, offering a potential way forward for compositional reasoning for other similar models, and for the combination of production concurrency models and full-scale ISAs.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {21},
numpages = {34},
keywords = {Arm, Iris, program logic, relaxed memory models, separation logic}
}

@article{10.1145/3632902,
author = {Cohen, Joshua M. and Johnson-Freyd, Philip},
title = {A Formalization of Core Why3 in Coq},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632902},
doi = {10.1145/3632902},
abstract = {Intermediate verification languages like Why3 and Boogie have made it much easier to build program verifiers, transforming the process into a logic compilation problem rather than a proof automation one.  
Why3 in particular implements a rich logic for program specification with polymorphism, algebraic data types, recursive functions and predicates, and inductive predicates; it translates this logic to over a dozen solvers and proof assistants.  
Accordingly, it serves as a backend for many tools, including Frama-C, EasyCrypt, and GNATProve for Ada SPARK.  
But how can we be sure that these tools are correct?  
The alternate foundational approach, taken by tools like VST and CakeML, provides strong guarantees by implementing the entire toolchain in a proof assistant, but these tools are harder to build and cannot directly take advantage of SMT solver automation.  
As a first step toward enabling automated tools with similar foundational guarantees, we give a formal semantics in Coq for the logic fragment of Why3.  
We show that our semantics are useful by giving a correct-by-construction natural deduction proof system for this logic, using this proof system to verify parts of Why3's standard library, and proving sound two of Why3's transformations used to convert terms and formulas into the simpler logics supported by the backend solvers.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {60},
numpages = {30},
keywords = {Coq, First-Order Logic, Formal Semantics, Why3}
}

@article{10.1145/3632957,
author = {Zhao, Yunping and Ma, Sheng and Liu, Heng and Huang, Libo and Dai, Yi},
title = {SAC: An Ultra-Efficient Spin-based Architecture for Compressed DNNs},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3632957},
doi = {10.1145/3632957},
abstract = {Deep Neural Networks (DNNs) have achieved great progress in academia and industry. But they have become computational and memory intensive with the increase of network depth. Previous designs seek breakthroughs in software and hardware levels to mitigate these challenges. At the software level, neural network compression techniques have effectively reduced network scale and energy consumption. However, the conventional compression algorithm is complex and energy intensive. At the hardware level, the improvements in the semiconductor process have effectively reduced power and energy consumption. However, it is difficult for the traditional Von-Neumann architecture to further reduce the power consumption, due to the memory wall and the end of Moore’s law. To overcome these challenges, the spintronic device based DNN machines have emerged for their non-volatility, ultra low power, and high energy efficiency. However, there is no spin-based design that has achieved innovation at both the software and hardware level. Specifically, there is no systematic study of spin-based DNN architecture to deploy compressed networks.In our study, we present an ultra-efficient Spin-based Architecture for Compressed DNNs (SAC), to substantially reduce power consumption and energy consumption. Specifically, we propose a One-Step Compression algorithm (OSC) to reduce the computational complexity with minimum accuracy loss. We also propose a spin-based architecture to realize better performance for the compressed network. Furthermore, we introduce a novel computation flow that enables the reuse of activations and weights. Experimental results show that our study can reduce the computational complexity of compression algorithm from 𝒪(Tk3 to 𝒪(k2 log k), and achieve 14\texttimes{} ∼ 40\texttimes{} compression ratio. Furthermore, our design can attain a 2\texttimes{} enhancement in power efficiency and a 5\texttimes{} improvement in computational efficiency compared to the Eyeriss. Our models are available at an anonymous link .},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {7},
numpages = {26},
keywords = {Artificial intelligence chips, Neural network hardware, Neuromorphic computing}
}

@article{10.1145/3635161,
author = {Rogenmoser, Michael and Tortorella, Yvan and Rossi, Davide and Conti, Francesco and Benini, Luca},
title = {Hybrid Modular Redundancy: Exploring Modular Redundancy Approaches in RISC-V Multi-Core Computing Clusters for Reliable Processing in Space},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2378-962X},
url = {https://doi.org/10.1145/3635161},
doi = {10.1145/3635161},
abstract = {Space Cyber-Physical Systems (S-CPS) such as spacecraft and satellites strongly rely on the reliability of onboard computers to guarantee the success of their missions. Relying solely on radiation-hardened technologies is extremely expensive, and developing inflexible architectural and microarchitectural modifications to introduce modular redundancy within a system leads to significant area increase and performance degradation. To mitigate the overheads of traditional radiation hardening and modular redundancy approaches, we present a novel Hybrid Modular Redundancy (HMR) approach, a redundancy scheme that features a cluster of RISC-V processors with a flexible on-demand dual-core and triple-core lockstep grouping of computing cores with runtime split-lock capabilities. Further, we propose two recovery approaches, software-based and hardware-based, trading off performance and area overhead. Running at 430MHz, our fault-tolerant cluster achieves up to 1160MOPS on a matrix multiplication benchmark when configured in non-redundant mode and 617 and 414 MOPS in dual and triple mode, respectively. A software-based recovery in triple mode requires 363 clock cycles and occupies 0.612 mm2, representing a 1.3\% area overhead over a non-redundant 12-core RISC-V cluster. As a high-performance alternative, a new hardware-based method provides rapid fault recovery in just 24 clock cycles and occupies 0.660 mm2, namely ∼ 9.4\% area overhead over the baseline non-redundant RISC-V cluster. The cluster is also enhanced with split-lock capabilities to enter one of the available redundant modes with minimum performance loss, allowing execution of a mission-critical portion of code when in independent mode, or a performance section when in a reliability mode, with &lt;400 clock cycles overhead for entry and exit. The proposed system is the first to integrate these functionalities on an open-source RISC-V-based compute device, enabling finely tunable reliability vs. performance trade-offs.},
note = {Just Accepted},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = nov
}

@article{10.1145/3635309,
author = {Clement, Nathan and Schoen, Alan and Boedihardjo, Arnold and Jenkins, Andrew},
title = {Synthetic Data and Hierarchical Object Detection in Overhead&nbsp;Imagery},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3635309},
doi = {10.1145/3635309},
abstract = {The performance of neural network models is often limited by the availability of big datasets. To treat this problem, we survey and develop novel synthetic data generation and augmentation techniques for enhancing low/zero-sample learning in satellite imagery. In addition to extending synthetic data generation approaches, we propose a hierarchical detection architecture to improve the utility of synthetic training samples. We consider existing techniques for producing synthetic imagery–3D models and neural style transfer–as well as introducing our own adversarially trained reskinning network, the GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic data in a two-stage, hierarchical detection/classification model of our own construction. To test the effectiveness of synthetic imagery, we employ it in the training of detection models and our two stage model, and evaluate the resulting models on real satellite images. All modalities of synthetic data are tested extensively on practical, geospatial analysis problems. Our experiments show that synthetic data developed using our approach can often enhance detection performance, particularly when combined with some real training images. When the only source of data is synthetic, our GAN-Reskinner often boosts performance over conventionally rendered 3D models and in all cases, the hierarchical model outperforms the baseline end-to-end detection architecture.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {117},
numpages = {20},
keywords = {Deep learning, object detection, remote sensing, synthetic data, low-shot learning}
}

@article{10.1145/3635708,
author = {Belgacem, Hichem and Li, Xiaochen and Bianculli, Domenico and Briand, Lionel},
title = {Learning-based Relaxation of Completeness Requirements for Data Entry Forms},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635708},
doi = {10.1145/3635708},
abstract = {Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users. However, because of the evolving nature of software, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields. Since obsolete required fields usually have “not-null” validation checks before submitting the form, users have to enter meaningless values in such fields to complete the form submission. These meaningless values threaten the quality of the filled data and could negatively affect stakeholders or learning-based tools that use the data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly.In this article, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. During the data entry session, LACQUER predicts the completeness requirement of a target based on the already filled fields and their conditional dependencies in the trained model.Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20\% to 64\% of meaningless values, with negative predictive values (i.e., the ability to correctly predict a field as “optional”) between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {77},
numpages = {32},
keywords = {Form filling, data entry forms, completeness requirements relaxation, machine learning, software data quality, user interfaces}
}

@article{10.1145/3635712,
author = {Ferrari, Alessio and Huichapa, Thaide and Spoletini, Paola and Novielli, Nicole and Fucci, Davide and Girardi, Daniela},
title = {Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635712},
doi = {10.1145/3635712},
abstract = {Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 ∼ 70\% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {87},
numpages = {36},
keywords = {Software engineering, requirements engineering, emotion detection, voice analysis, speech analysis, biofeedback analysis, affective requirements engineering}
}

@article{10.1145/3637289,
author = {Richardson, Dan and Islam, Md Adnanul and Cumbo, Bronwyn J. and Shrestha, Pranita and Varghese, Delvin and Bartindale, Tom and Olivier, Patrick},
title = {A Design Vocabulary for Scaffolding Group Interaction Archetypes through Synchronous Telephony},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637289},
doi = {10.1145/3637289},
abstract = {Multiple HCI projects have demonstrated the potential of digitally-enhanced, synchronous telephony platforms for use with and by resource-limited communities. However, these platforms were each designed to only facilitate a single archetype of community engagement, limiting their capacity for adaptation when contextual or stakeholder requirements change. This paper builds upon these projects to introduce a design vocabulary, grounded in a formal ontology describing the core components necessary to run adaptable, structured engagements through synchronous group telephony. Through a series of scenarios, we present how this design vocabulary can be used to: help design and communicate different models of synchronous audio engagements, describe existing technologies, and highlight other novel ways in which such platforms could be used. We discuss how while under-explored to this point, synchronous telephony platforms can be designed to orchestrate stakeholder engagements with a degree of flexibility previously impossible in remote, offline contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {12},
numpages = {22},
keywords = {ICT4D, IVR, design, resource-limited settings}
}

@article{10.1145/3637315,
author = {Shelby, Renee and Srinivasan, Ramya and Burgdorf, Katharina and Lena, Jennifer C. and Rostamzadeh, Negar},
title = {Creative ML Assemblages: The Interactive Politics of People, Processes, and Products},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637315},
doi = {10.1145/3637315},
abstract = {Creative ML tools are collaborative systems that afford artistic creativity through their myriad interactive relationships. We propose using "assemblage thinking" to support analyses of creative ML by approaching it as a system in which the elements of people, organizations, culture, practices, and technology constantly influence each other. We model these interactions as "coordinating elements" that give rise to the social and political characteristics of a particular creative ML context, and call attention to three dynamic elements of creative ML whose interactions provide unique context for the social impact a particular system has: people, creative processes, and products. As creative assemblages are highly contextual, we present these as analytical concepts that computing researchers can adapt to better understand the functioning of a particular system or phenomena and identify intervention points to foster desired change. This paper contributes to theorizing interactions with AI in the context of art, and how these interactions shape the production of algorithmic art.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {38},
numpages = {30},
keywords = {art \&amp; technology, assemblage, creative ai, cultural studies, machine learning}
}

@article{10.1145/3637553,
author = {Kirkman, Stephen S. and Fulton, Steven and Hemmes, Jeffrey and Garcia, Christopher and Wilson, Justin C.},
title = {A Blockchain Architecture to Increase the Resilience of Industrial Control Systems from the Effects of a Ransomware Attack: A Proposal and Initial Results},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3637553},
doi = {10.1145/3637553},
abstract = {The motivation of this research (and also one of the nation’s cyber goals) is enhancing the resilience of Industrial Control Systems (ICS)/Supervisory Control and Data Acquisition (SCADA) systems against ransomware attacks. ICS and SCADA systems run some of the most important networks in the country: our critical infrastructure (i.e., water flow, power grids, etc.). Disruption of these systems causes confusion, panic, and in some cases loss of life. We propose a SCADA architecture that uses blockchain to help protect ICS data from ransomware. We focus on the historian. In a SCADA system, the historian collects events from devices in the control network for real-time and future analysis. We choose to use Ethereum and its Proof of Stake (PoS) consensus protocol. The other goal of this research focuses on the resilience of blockchain. There is very little research in protecting the blockchain itself. By performing encryption testing on an Ethereum private network, we explore how vulnerable blockchain is and discuss potential ways to make a blockchain client more resilient.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jan,
articleno = {9},
numpages = {13},
keywords = {SCADA, ransomware, blockchain, ethereum, industrial control systems}
}

@article{10.1145/3637871,
author = {Gao, Jingtong and Zhao, Xiangyu and Li, Muyang and Zhao, Minghao and Wu, Runze and Guo, Ruocheng and Liu, Yiding and Yin, Dawei},
title = {SMLP4Rec: An Efficient All-MLP Architecture for Sequential Recommendations},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3637871},
doi = {10.1145/3637871},
abstract = {Self-attention models have achieved the state-of-the-art performance in sequential recommender systems by capturing the sequential dependencies among user–item interactions. However, they rely on adding positional embeddings to the item sequence to retain the sequential information, which may break the semantics of item embeddings due to the heterogeneity between these two types of embeddings. In addition, most existing works assume that such dependencies exist solely in the item embeddings, but neglect their existence among the item features. In our previous study, we proposed a novel sequential recommendation model, i.e., MLP4Rec, based on the recent advances of MLP-Mixer architectures, which is naturally sensitive to the order of items in a sequence because matrix elements related to different positions of a sequence will be given different weights in training. We developed a tri-directional fusion scheme to coherently capture sequential, cross-channel, and cross-feature correlations with linear computational complexity as well as much fewer model parameters than existing self-attention methods. However, the cascading mixer structure, the large number of normalization layers between different mixer layers, and the noise generated by these operations limit the efficiency of information extraction and the effectiveness of MLP4Rec. In this extended version, we propose a novel framework – SMLP4Rec for sequential recommendation to address the aforementioned issues. The new framework changes the flawed cascading structure to a parallel mode, and integrates normalization layers to minimize their impact on the model’s efficiency while maximizing their effectiveness. As a result, the training speed and prediction accuracy of SMLP4Rec are vastly improved in comparison to MLP4Rec. Extensive experimental results demonstrate that the proposed method is significantly superior to the state-of-the-art approaches. The implementation code is available online to ease reproducibility.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {86},
numpages = {23},
keywords = {Recommender system, sequential recommendation, multi-layer perceptron}
}

@article{10.1145/3639037,
author = {Jiang, Xi and Liu, Shinan and Gember-Jacobson, Aaron and Bhagoji, Arjun Nitin and Schmitt, Paul and Bronzino, Francesco and Feamster, Nick},
title = {NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3639037},
doi = {10.1145/3639037},
abstract = {Datasets of labeled network traces are essential for a multitude of machine learning (ML) tasks in networking, yet their availability is hindered by privacy and maintenance concerns, such as data staleness. To overcome this limitation, synthetic network traces can often augment existing datasets. Unfortunately, current synthetic trace generation methods, which typically produce only aggregated flow statistics or a few selected packet attributes, do not always suffice, especially when model training relies on having features that are only available from packet traces. This shortfall manifests in both insufficient statistical resemblance to real traces and suboptimal performance on ML tasks when employed for data augmentation. In this paper, we apply diffusion models to generate high-resolution synthetic network traffic traces. We present NetDiffusion1, a tool that uses a finely-tuned, controlled variant of a Stable Diffusion model to generate synthetic network traffic that is high fidelity and conforms to protocol specifications. Our evaluation demonstrates that packet captures generated from NetDiffusion can achieve higher statistical similarity to real data and improved ML model performance than current state-of-the-art approaches (e.g., GAN-based approaches). Furthermore, our synthetic traces are compatible with common network analysis tools and support a myriad of network tasks, suggesting that NetDiffusion can serve a broader spectrum of network analysis and testing tasks, extending beyond ML-centric applications.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = feb,
articleno = {11},
numpages = {32},
keywords = {diffusion model}, keywords{network traffic, synthesis}
}

@article{10.1145/3639056,
author = {Gribok, Sergey and Pasca, Bogdan and Langhammer, Martin},
title = {CSAIL2019 Crypto-Puzzle Solver Architecture},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3639056},
doi = {10.1145/3639056},
abstract = {tThe CSAIL2019 time-lock puzzle is an unsolved cryptographic challenge introduced by Ron Rivest in 2019, replacing the solved LCS35 puzzle. Solving these types of puzzles requires large amounts of intrinsically sequential computations, with each iteration performing a very large (3,072-bit for CSAIL2019) modular multiplication operation. The complexity of each iteration is several times greater than known field-programmable gate array (FPGA) implementations, and the number of iterations has been increased by about 1,000x compared with LCS35. Because of the high complexity of this new puzzle, a number of intermediate, or milestone, versions of the puzzle have been specified. In this article, we present several FPGA architectures for the CSAIL2019 solver, which we implement on a medium-sized Intel Agilex device. We develop a new multi-cycle modular multiplication method, which is flexible and can fit on a wide variety of sizes of current FPGAs. We introduce a class of multi-cycle squarer-based architectures that allow for better resource and area trade-offs. We also demonstrate a new approach for improving the fitting and timing closure of large, chip-filling arithmetic designs. We used the solver to compute the first 23 out of 28 milestone solutions of the puzzle, which are the first reported results for this problem.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = sep,
articleno = {44},
numpages = {32},
keywords = {Crypto-puzzle, FPGA, modular exponentiation, modular multiplication, modular squaring, iterative algorithm, DSP-based modular reduction}
}

@article{10.1145/3639057,
author = {Sagor, Mohammad and Haroon, Amran and Stoleru, Radu and Bhunia, Suman and Altaweel, Ala and Chao, Mengyuan and Jin, Liuyi and Maurice, Maxwell and Blalock, Roger},
title = {DistressNet-NG: A Resilient Data Storage and Sharing Framework for Mobile Edge Computing in Cyber-Physical Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3639057},
doi = {10.1145/3639057},
abstract = {Mobile Edge Computing (MEC) has been gaining a major interest for use in Cyber-Physical Systems (CPS) for Disaster Response and Tactical applications. These CPS generate a very large amount of mission-critical and personal data that require resilient and secure storage and sharing. In this article, we present the design, implementation, and evaluation of a framework for resilient data storage and sharing for MEC in CPS targeting the aforementioned applications. Our framework is built on the resiliency of three main components: EdgeKeeper, which ensures resilient coordination of the framework’s components; RSock, which provides resilient communication among CPS’s nodes; and R-Drive/R-Share which, leveraging EdgeKeeper and RSock, provides resilient data storage and sharing. EdgeKeeper employs a set of replicas and a consensus protocol for storing critical meta-data and ensuring fast reorganization of the CPS; RSock decides an optimal degree for replicating data that is communicated over lossy links. R-Drive employs an adaptive erasure-coded and encrypted resilient data storage; R-Share, leveraging RSock provides resilient peer-to-peer data sharing. We implemented our proposed framework on rapidly deployable systems (e.g., manpacks, testMobile Edge Clouds) and on Android devices, and integrated it with existing MEC applications. Performance evaluation results from three real-world deployments show that our framework provides resilient data storage and sharing in MEC for CPS.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jul,
articleno = {37},
numpages = {31},
keywords = {Mobile Edge Cloud (MEC), disaster response, resilient storage, secure sharing, resilient coordination, adaptive erasure coding}
}

@article{10.1145/3639283,
author = {Liu, Tongyu and Fan, Ju and Tang, Nan and Li, Guoliang and Du, Xiaoyong},
title = {Controllable Tabular Data Synthesis Using Diffusion Models},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639283},
doi = {10.1145/3639283},
abstract = {Controllable tabular data synthesis plays a crucial role in numerous applications by allowing users to generate synthetic data with specific conditions. These conditions can include synthesizing tuples with predefined attribute values or creating tuples that exhibit a particular correlation with an external table. However, existing approaches lack the flexibility to support new conditions and can be time-consuming when dealing with multiple conditions. To overcome these limitations, we propose a novel approach that leverages diffusion models to first learn an unconditional generative model. Subsequently, we introduce lightweight controllers to guide the unconditional generative model in generating synthetic data that satisfies different conditions. The primary research challenge lies in effectively supporting controllability using lightweight solutions while ensuring the realism of the synthetic data. To address this challenge, we design an unconditional diffusion model tailored specifically for tabular data. Additionally, we propose a new sampling method that enables correlation-aware controls throughout the data generation process. We conducted extensive experiments across various applications for controllable tabular data synthesis, which show that our approach outperforms the state-of-the-art methods.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {28},
numpages = {29},
keywords = {controllable data synthesis, diffusion model, tabular data synthesis}
}

@article{10.1145/3639326,
author = {Perini, Massimo and Nikolic, Milos},
title = {In-Database Data Imputation},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639326},
doi = {10.1145/3639326},
abstract = {Missing data is a widespread problem in many domains, creating challenges in data analysis and decision making. Traditional techniques for dealing with missing data, such as excluding incomplete records or imputing simple estimates (e.g., mean), are computationally efficient but may introduce bias and disrupt variable relationships, leading to inaccurate analyses. Model-based imputation techniques offer a more robust solution that preserves the variability and relationships in the data, but they demand significantly more computation time, limiting their applicability to small datasets.This work enables efficient, high-quality, and scalable data imputation within a database system using the widely used MICE method. We adapt this method to exploit computation sharing and a ring abstraction for faster model training. To impute both continuous and categorical values, we develop techniques for in-database learning of stochastic linear regression and Gaussian discriminant analysis models. Our MICE implementations in PostgreSQL and DuckDB outperform alternative MICE implementations and model-based imputation techniques by up to two orders of magnitude in terms of computation time, while maintaining high imputation quality.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {70},
numpages = {27},
keywords = {MICE, factorized computation, incomplete data, missing data, ring}
}

@article{10.1145/3639411,
author = {Li, Xinjiao and Wu, Guowei and Yao, Lin and Zheng, Zhaolong and Geng, Shisong},
title = {Utility-aware Privacy Perturbation for Training Data},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3639411},
doi = {10.1145/3639411},
abstract = {Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget&nbsp;allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget&nbsp;allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget&nbsp;allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget&nbsp;allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget&nbsp;allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {103},
numpages = {21},
keywords = {Training data privacy, Data perturbation, Data utility}
}

@article{10.1145/3639703,
author = {Montesano, Federica and Marotta, Romolo and Quaglia, Francesco},
title = {Spatial/Temporal Locality-Based Load-sharing in Speculative Discrete Event Simulation on Multi-core Machines},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/3639703},
doi = {10.1145/3639703},
abstract = {Shared-memory multi-processor/multi-core machines have become a reference for many application contexts. In particular, the recent literature on speculative parallel discrete event simulation has reshuffled the architectural organization of simulation systems in order to deeply exploit the main features of this type of machine. A core aspect dealt with has been the full sharing of the workload at the level of individual simulation events, which enables keeping the rollback incidence minimal. However, making each worker thread continuously switch its execution between events destined to different simulation objects does not favor locality. This problem appears even more evident in the case of Non-Uniform-Memory-Access (NUMA) machines, in which memory accesses generating a cache miss to be served by a far NUMA node give rise to both higher latency and higher traffic at the level of the NUMA interconnection. In this article, we propose a workload-sharing algorithm in which the worker threads can have short-term binding with specific simulation objects to favor spatial locality. The new bindings—carried out when a thread decides to switch its execution to other simulation objects—are based on both (a) the timeline according to which the object states have passed through the caching hierarchy and (b) the (dynamic) placement of objects within the NUMA architecture. At the same time, our solution still enables the worker threads to focus their activities on the events to be processed whose timestamps are closer to the simulation commit horizon—hence, we exploit temporal locality along virtual time and keep the rollback incidence minimal. In our design, we exploit lock-free constructs to support scalable thread synchronization while accessing the shared event pool. Furthermore, we exploit a multi-view approach of the event pool content, which additionally favors local accesses to the parts of the event pool that are currently relevant for the thread activity. Our solution has been released as an integration within the USE (Ultimate-Share-Everything) open-source speculative simulation platform available to the community. Furthermore, in this article we report the results of an experimental study that shows the effectiveness of our proposal.},
journal = {ACM Trans. Model. Comput. Simul.},
month = nov,
articleno = {2},
numpages = {31},
keywords = {PDES, load sharing, shared memory, multi-core}
}

@article{10.1145/3640469,
author = {Li, Yonggen and Li, Xin and Shen, Haibin and Fan, Jicong and Xu, Yanfeng and Huang, Kejie},
title = {An All-digital Compute-in-memory FPGA Architecture for Deep Learning Acceleration},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3640469},
doi = {10.1145/3640469},
abstract = {Field Programmable Gate Array (FPGA) is a versatile and programmable hardware platform, which makes it a promising candidate for accelerating Deep Neural Networks (DNNs). However, FPGA’s computing energy efficiency is low due to the domination of energy consumption by interconnect data movement. In this article, we propose an all-digital Compute-in-memory FPGA architecture for deep learning acceleration. Furthermore, we present a bit-serial computing circuit of the Digital CIM core for accelerating vector-matrix multiplication (VMM) operations. A Network-CIM-deployer (NCIMD) is also developed to support automatic deployment and mapping of DNN networks. NCIMD provides a user-friendly API of DNN models in Caffe format. Meanwhile, we introduce a Weight-stationary dataflow and describe the method of mapping a single layer of the network to the CIM array in the architecture. We conduct experimental tests on the proposed FPGA architecture in the field of Deep Learning (DL), as well as in non-DL fields, using different architectural layouts and mapping strategies. We also compare the results with the conventional FPGA architecture. The experimental results show that compared to the conventional FPGA architecture, the energy efficiency can achieve a maximum speedup of 16.1\texttimes{}, while the latency can decrease up to 40\% in our proposed CIM FPGA architecture.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = feb,
articleno = {18},
numpages = {27},
keywords = {Deep neural networks, Verilog-to-Routing, FPGA architecture, Compute-In-Memory, Toolchain}
}

@article{10.1145/3643134,
author = {Zhao, Yunping and Ma, Sheng and Liu, Hengzhu and Huang, Libo},
title = {EPHA: An Energy-efficient Parallel Hybrid Architecture for ANNs and SNNs},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/3643134},
doi = {10.1145/3643134},
abstract = {Artificial neural networks (ANNs) and spiking neural networks (SNNs) are two general approaches to achieve artificial intelligence (AI). The former have been widely used in academia and industry fields; the latter, SNNs, are more similar to biological neural networks and can realize ultra-low power consumption, thus have received widespread research attention. However, due to their fundamental differences in computation formula and information coding, the two methods often require different and incompatible platforms. Alongside the development of AI, a general platform that can support both ANNs and SNNs is necessary. Moreover, there are some similarities between ANNs and SNNs, which leaves room to deploy different networks on the same architecture. However, there is little related research on this topic. Accordingly, this article presents an energy-efficient, scalable, and non-Von Neumann architecture (EPHA) for ANNs and SNNs. Our study combines device-, circuit-, architecture-, and algorithm-level innovations to achieve a parallel architecture with ultra-low power consumption. We use the compensated ferrimagnet to act as both synapses and neurons to store weights and perform dot-product operations, respectively. Moreover, we propose a novel computing flow to reduce the operations across multiple crossbar arrays, which enables our design to conduct large and complex tasks. On a suite of ANN and SNN workloads, the EPHA is 1.6\texttimes{} more power-efficient than a state-of-the-art design, NEBULA, in the ANN mode. In the SNN mode, our design is 4 orders of magnitude more than the Loihi in power efficiency.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {43},
numpages = {28},
keywords = {Artificial intelligence chips, neural network hardware, neuromorphic computing, AI chip}
}

@article{10.1145/3643677,
author = {Shang, Xiuwei and Zhang, Shuai and Zhang, Yitong and Guo, Shikai and Li, Yulong and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He},
title = {Analyzing and Detecting Information Types of Developer Live Chat Threads},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643677},
doi = {10.1145/3643677},
abstract = {Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads, automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat, respectively, achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60\% improvement over the state-of-the-art approaches. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {131},
numpages = {32},
keywords = {Developer chatroom, information type classification, data augmentation, deep learning}
}

@article{10.1145/3643686,
author = {Howison, Mark and Angell, Mintaka and Hastings, Justine S.},
title = {Protecting Sensitive Data with Secure Data Enclaves},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3643686},
doi = {10.1145/3643686},
abstract = {A Secure Data Enclave is a system that allows data owners, such as governments and private firms, to control data access and ensure data security while facilitating approved uses of data by other parties. This model of data use offers additional protections and technical controls for the data owner compared to the more commonly used approach of transferring data from the owner to another party through a data sharing agreement. Under the data use model, the data owner retains full transparency and auditing over the other party's access, which can be difficult to achieve in practice with even the best legal instrument for data sharing. We describe the key technical requirements for a Secure Data Enclave, provide a reference architecture for its implementation on Amazon Web Services using managed cloud services, and describe four use cases of this architecture in partnerships with state governments to control access to sensitive administrative data.},
journal = {Digit. Gov.: Res. Pract.},
month = jun,
articleno = {14},
numpages = {11},
keywords = {Data governance, data management, data sharing, security controls, managed services, remote workspaces, scientific collaboration, public policy}
}

@article{10.1145/3643762,
author = {Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram},
title = {CORE: Resolving Code Quality Issues using LLMs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643762},
doi = {10.1145/3643762},
abstract = {As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\% Java files (across 10 quality checks) comparable to 78.3\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {36},
numpages = {23},
keywords = {Code quality, LLMs, code revision, static analysis}
}

@article{10.1145/3643812,
author = {Li, Wen and Bao, Lingfeng and Chen, Jiachi and Grundy, John and Xia, Xin and Yang, Xiaohu},
title = {Market Manipulation of Cryptocurrencies: Evidence from Social Media and Transaction Data},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3643812},
doi = {10.1145/3643812},
abstract = {The cryptocurrency market cap has experienced a great increase in recent years. However, large price fluctuations demonstrate the need for governance structures and identify whether there are market manipulations. In this article, we conduct three analyses—social media data analysis, blockchain data analysis, and price bubble analysis—to investigate whether market manipulation exists on Bitcoin, Ethereum, and Dogecoin platforms. Social media data analysis aims to find the reasons for price fluctuations. Blockchain data analysis is used to find detailed behavior of the manipulators. Price bubble analysis is used to investigate the relation between price fluctuation and manipulators’ behavior. By using the three analyses, we show that market manipulation exists on Bitcoin, Ethereum, and Dogecoin. However, market manipulation of Bitcoin is limited, and for most of Bitcoin’s price fluctuations, we found other explanations. The price for Ethereum is the most sensitive to technical updates. Technical companies/teams usually hype some new concepts (e.g., ICO, DeFi), which causes a price spike. The price of Dogecoin has a high correlation with Elon Musk’s X (formerly known as Twitter) activity, showing that influential individuals have the ability to manipulate its prices. In addition, the poor monetary liquidity of Dogecoin allows some users to manipulate its price.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {8},
numpages = {26},
keywords = {Blockchain, cryptocurrencies, market manipulation, empirical study}
}

@article{10.1145/3644385,
author = {Chapman, Adriane and Lauro, Luca and Missier, Paolo and Torlone, Riccardo},
title = {Supporting Better Insights of Data Science Pipelines with Fine-grained Provenance},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3644385},
doi = {10.1145/3644385},
abstract = {Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semi-automatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange.},
journal = {ACM Trans. Database Syst.},
month = apr,
articleno = {6},
numpages = {42},
keywords = {Provenance, data science, data preparation, preprocessing}
}

@article{10.1145/3645099,
author = {He, Weidong and Li, Zhi and Wang, Hao and Xu, Tong and Wang, Zhefeng and Huai, Baoxing and Yuan, Nicholas Jing and Chen, Enhong},
title = {Multimodal Dialogue Systems via Capturing Context-aware Dependencies and Ordinal Information of Semantic Elements},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3645099},
doi = {10.1145/3645099},
abstract = {The topic of multimodal conversation systems has recently garnered significant attention across various industries, including travel and retail, among others. While pioneering works in this field have shown promising performance, they often focus solely on context information at the utterance level, overlooking the context-aware dependencies of multimodal semantic elements like words and images. Furthermore, the ordinal information of images, which indicates the relevance between visual context and users’ demands, remains underutilized during the integration of visual content. Additionally, the exploration of how to effectively utilize corresponding attributes provided by users when searching for desired products is still largely unexplored. To address these challenges, we propose PMATE, a Position-aware Multimodal diAlogue system with semanTic Elements. Specifically, to obtain semantic representations at the element level, we first unfold the multimodal historical utterances and devise a position-aware multimodal element-level encoder. This component considers all images that may be relevant to the current turn and introduces a novel position-aware image selector to choose related images before fusing the information from the two modalities. Finally, we present a knowledge-aware two-stage decoder and an attribute-enhanced image searcher for the tasks of generating textual responses and selecting image responses, respectively. We extensively evaluate our model on two large-scale multimodal dialogue datasets, and the results of our experiments demonstrate that our approach outperforms several baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {45},
numpages = {25},
keywords = {Multimodal dialogue system, natural language generation, conversational image search}
}

@article{10.1145/3648105,
author = {Yao, Zhenjie and Chen, Yixin and Wang, Jinwei and Li, Junjuan and Chen, Shuohua and Wu, Shouling and Tu, Yanhui and Zhao, Ming-Hui and Zhang, Luxia},
title = {Interpretable Trend Analysis Neural Networks for Longitudinal Data Analysis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3648105},
doi = {10.1145/3648105},
abstract = {Cohort study is one of the most commonly used study methods in medical and public health researches, which result in longitudinal data. Conventional statistical models and machine learning methods are not capable of modeling the evolution trend of the variables in longitudinal data. In this article, we propose a Trend Analysis Neural Networks (TANN), which models the evolution trend of the variables by adaptive feature learning. TANN was tested on dataset of Kaiuan research. The task was to predict occurrence of cardiovascular events within 2 and 5 years, with three repeated medical examinations during 2008 and 2013. For 2-year prediction, The AUC of the TANN is 0.7378, which is a significant improvement than that of conventional methods, while that of TRNS, RNN, DNN, GBDT, RF, and LR are 0.7222, 0.7034, 0.7054, 0.7136, 0.7160, and 0.7024, respectively. For 5-year prediction, TANN also shows improvement. The experimental results show that the proposed TANN achieves better prediction performance on cardiovascular events prediction than conventional models. Furthermore, by analyzing the weights of TANN, we could find out important trends of the indicators, which are ignored by conventional machine learning models. The trend discovery mechanism interprets the model well. TANN is an appropriate balance between high performance and interpretability.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {8},
numpages = {13},
keywords = {Neural networks, longitudinal data, trend analysis, interpretability}
}

@article{10.1145/3648362,
author = {Ahmed, Anil and Huang, Degen and Arafat, Syed Yasser and Hameed, Imran},
title = {Enriching Urdu NER with BERT Embedding, Data Augmentation, and Hybrid Encoder-CNN Architecture},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3648362},
doi = {10.1145/3648362},
abstract = {Named Entity Recognition (NER) is an indispensable component of Natural Language Processing (NLP), which aims to identify and classify entities within text data. While Deep Learning (DL) models have excelled in NER for well-resourced languages such as English, Spanish, and Chinese, they face significant hurdles when dealing with low-resource languages such as Urdu. These challenges stem from the intricate linguistic characteristics of Urdu, including morphological diversity, a context-dependent lexicon, and the scarcity of training data. This study addresses these issues by focusing on Urdu Named Entity Recognition (U-NER) and introducing three key contributions. First, various pre-trained embedding methods are employed, encompassing Word2vec (W2V), GloVe, FastText, Bidirectional Encoder Representations from Transformers (BERT), and Embeddings from language models (ELMo). In particular, fine-tuning is performed on BERTBASE and ELMo using Urdu Wikipedia and news articles. Second, a novel generative Data Augmentation (DA) technique replaces Named Entities (NEs) with mask tokens, employing pre-trained masked language models to predict masked tokens, effectively expanding the training dataset. Finally, the study introduces a novel hybrid model combining a Transformer Encoder with a Convolutional Neural Network (CNN) to capture the intricate morphology of Urdu. These modules enable the model to handle polysemy, extract short- and long-range dependencies, and enhance learning capacity. Empirical experiments demonstrate that the proposed model, incorporating BERT embeddings and an innovative DA approach, attains the highest F1-score of 93.99\%, highlighting its efficacy for the U-NER task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {52},
numpages = {38},
keywords = {Urdu, Named Entity Recognition, low-resource languages, Asian languages}
}

@article{10.1145/3648436,
author = {Chatty, Tejaswini and Moeller, Bryton L. and Pantelimon, Ioana A. and Parnell, Catherine D. and Khan, Tahsin M. and Laurin, Lise and Faludi, Jeremy and Murnane, Elizabeth L.},
title = {EcoSketch: Promoting Sustainable Design through Iterative Environmental Assessment during Early-Stage Product Development},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3648436},
doi = {10.1145/3648436},
abstract = {Sustainability has long been a topic of substantial interest to the design and human-centered computing communities. With industries increasingly prioritizing climate targets, there is a growing demand for sustainable product design. This article addresses this need through EcoSketch, a digital tool designed to democratize environmental impact assessments for product designers. Shifting typically retrospective evaluations to the early stages of product development, EcoSketch enables proactive consideration and adoption of sustainable alternatives. Unlike software tailored to environmental scientists, it minimizes the need for specialized training or extensive data inputs. We delve into the development and evaluation of EcoSketch, highlighting its unique features and usability strengths. This article concludes by discussing design implications and proposing future research avenues to strengthen the intersection of human-computer interaction and sustainable product design, with the aim of advancing progress on environmental challenges at the systems level.},
journal = {ACM J. Comput. Sustain. Soc.},
month = jun,
articleno = {22},
numpages = {29},
keywords = {Environmental sustainability, sustainable design, product design, life cycle assessment}
}

@article{10.1145/3648476,
author = {Pereira, Jo\~{a}o L. M. and Fonseca, Manuel J. and Lopes, Ant\'{o}nia and Galhardas, Helena},
title = {Cleenex: Support for User Involvement during an Iterative Data Cleaning Process},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3648476},
doi = {10.1145/3648476},
abstract = {The existence of large amounts of data increases the probability of occurring data quality problems. A data cleaning process that corrects these problems is usually an iterative process, because it may need to be re-executed and refined to produce high-quality data. Moreover, due to the specificity of some data quality problems and the limitation of data cleaning programs to cover all problems, often a user has to be involved during the program executions by manually repairing data. However, there is no data cleaning framework that appropriately supports this involvement in such an iterative process, a form of human-in-the-loop, to clean structured data. Moreover, data preparation tools that somehow involve the user in data cleaning processes have not been evaluated with real users to assess their effort.Therefore, we propose Cleenex, a data cleaning framework with support for user involvement during an iterative data cleaning process, and conduct two data cleaning experimental evaluations: an assessment of the Cleenex components that support the user when manually repairing data with a simulated user; and a comparison, in terms of user involvement, of data preparation tools with real users.Results show that Cleenex components reduce the user effort when manually cleaning data during a data cleaning process, for example, the number of tuples visualized is reduced in 99\%. Moreover, when performing data cleaning tasks with Cleenex, real users need less time/effort (e.g., half the clicks) and, based on questionnaires, prefer it to the other tools used for comparison, OpenRefine and Pentaho Data Integration.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {6},
numpages = {26},
keywords = {Data quality, data curation, user involvement, human-in-the-loop}
}

@article{10.1145/3649134,
author = {Bugedo, Sebasti\'{a}n and Riveros, Cristian and Salas, Jorge},
title = {A Family of Centrality Measures for Graph Data Based on Subgraphs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3649134},
doi = {10.1145/3649134},
abstract = {We present the theoretical foundations and first experimental study of a new approach in centrality measures for graph data. The main principle is straightforward: the more relevant subgraphs around a vertex, the more central it is in the network. We formalize the notion of “relevant subgraphs” by choosing a family of subgraphs that, given a graph G and a vertex v, assigns a subset of connected subgraphs of G that contains v. Any of such families defines a measure of centrality by counting the number of subgraphs assigned to the vertex, i.e., a vertex will be more important for the network if it belongs to more subgraphs in the family. We show several examples of this approach. In particular, we propose the All-Subgraphs (All-Trees) centrality, a centrality measure that considers every subgraph (tree). We study fundamental properties over families of subgraphs that guarantee desirable properties over the centrality measure. Interestingly, All-Subgraphs and All-Trees satisfy all these properties, showing their robustness as centrality notions. To conclude the theoretical analysis, we study the computational complexity of counting certain families of subgraphs and show a linear time algorithm to compute the All-Subgraphs and All-Trees centrality for graphs with bounded treewidth. Finally, we implemented these algorithms and computed these measures over more than one hundred real-world networks. With this data, we present an empirical comparison between well-known centrality measures and those proposed in this work.},
journal = {ACM Trans. Database Syst.},
month = may,
articleno = {10},
numpages = {45},
keywords = {Graph data, graph centrality, centrality measures}
}

@article{10.1145/3649447,
author = {Zhao, Fei and Zhang, Chengcui and Geng, Baocheng},
title = {Deep Multimodal Data Fusion},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3649447},
doi = {10.1145/3649447},
abstract = {Multimodal Artificial Intelligence (Multimodal AI), in general, involves various types of data (e.g., images, texts, or data collected from different sensors), feature engineering (e.g., extraction, combination/fusion), and decision-making (e.g., majority vote). As architectures become more and more sophisticated, multimodal neural networks can integrate feature extraction, feature fusion, and decision-making processes into one single model. The boundaries between those processes are increasingly blurred. The conventional multimodal data fusion taxonomy (e.g., early/late fusion), based on which the fusion occurs in, is no longer suitable for the modern deep learning era. Therefore, based on the main-stream techniques used, we propose a new fine-grained taxonomy grouping the state-of-the-art (SOTA) models into five classes: Encoder-Decoder methods, Attention Mechanism methods, Graph Neural Network methods, Generative Neural Network methods, and other Constraint-based methods. Most existing surveys on multimodal data fusion are only focused on one specific task with a combination of two specific modalities. Unlike those, this survey covers a broader combination of modalities, including Vision + Language (e.g., videos, texts), Vision + Sensors (e.g., images, LiDAR), and so on, and their corresponding tasks (e.g., video captioning, object detection). Moreover, a comparison among these methods is provided, as well as challenges and future directions in this area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {216},
numpages = {36},
keywords = {Data fusion, neural networks, multimodal deep learning}
}

@article{10.1145/3649458,
author = {Vukadin, Davor and Afri\'{c}, Petar and \v{S}ili\'{c}, Marin and Dela\v{c}, Goran},
title = {Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3649458},
doi = {10.1145/3649458},
abstract = {Recent advancement in deep-neural network performance led to the development of new state-of-the-art approaches in numerous areas. However, the black-box nature of neural networks often prohibits their use in areas where model explainability and model transparency are crucial. Over the years, researchers proposed many algorithms to aid neural network understanding and provide additional information to the human expert. One of the most popular methods being Layer-Wise Relevance Propagation (LRP). This method assigns local relevance based on the pixel-wise decomposition of nonlinear classifiers. With the rise of attribution method research, there has emerged a pressing need to assess and evaluate their performance. Numerous metrics have been proposed, each assessing an individual property of attribution methods such as faithfulness, robustness, or localization. Unfortunately, no single metric is deemed optimal for every case, and researchers often use several metrics to test the quality of the attribution maps. In this work, we address the shortcomings of the current LRP formulations and introduce a novel method for determining the relevance of input neurons through layer-wise relevance propagation. Furthermore, we apply this approach to the recently developed Vision Transformer architecture and evaluate its performance against existing methods on two image classification datasets, namely ImageNet and PascalVOC. Our results clearly demonstrate the advantage of our proposed method. Furthermore, we discuss the insufficiencies of current evaluation metrics for attribution-based explainability and propose a new evaluation metric that combines the notions of faithfulness, robustness, and contrastiveness. We utilize this new metric to evaluate the performance of various attribution-based methods. Our code is available at:},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {47},
numpages = {30},
keywords = {Explainable artificial intelligence, Vision Transformer, layer-wise relevance propagation, attribution-based evaluation}
}

@article{10.1145/3650729,
author = {Li, Yueting and Wang, Xueyan and Zhang, He and Pan, Biao and Qiu, Keni and Kang, Wang and Wang, Jun and Zhao, Weisheng},
title = {Toward Energy-efficient STT-MRAM-based Near Memory Computing Architecture for Embedded Systems},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3650729},
doi = {10.1145/3650729},
abstract = {Convolutional Neural Networks (CNNs) have significantly impacted embedded system applications across various domains. However, this exacerbates the real-time processing and hardware resource-constrained challenges of embedded systems. To tackle these issues, we propose spin-transfer torque magnetic random-access memory (STT-MRAM)-based near memory computing (NMC) design for embedded systems. We optimize this design from three aspects: Fast-pipelined STT-MRAM readout scheme provides higher memory bandwidth for NMC design, enhancing real-time processing capability with a non-trivial area overhead. Direct index compression format in conjunction with digital sparse matrix-vector multiplication (SpMV) accelerator supports various matrices of practical applications that alleviate computing resource requirements. Custom NMC instructions and stream converter for NMC systems dynamically adjust available hardware resources for better utilization. Experimental results demonstrate that the memory bandwidth of STT-MRAM achieves 26.7 GB/s. Energy consumption and latency improvement of digital SpMV accelerator are up to 64\texttimes{} and 1,120\texttimes{} across sparsity matrices spanning from 10\% to 99.8\%. Single-precision and double-precision elements transmission increased up to 8\texttimes{} and 9.6\texttimes{}, respectively. Furthermore, our design achieves a throughput of up to 15.9\texttimes{} over state-of-the-art designs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = apr,
articleno = {37},
numpages = {24},
keywords = {Real-time processing, compressed format, computing resources, digital accelerator, throughput}
}

@article{10.1145/3651858,
author = {Li, Ziyang and Li, Dongsheng and Chen, Yingwen and Chen, Kai and Zhang, Yiming},
title = {Decentralized Scheduling for Data-Parallel Tasks in the Cloud},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/3651858},
doi = {10.1145/3651858},
abstract = {For latency-sensitive data processing applications in the cloud, concurrent data-parallel tasks need to be scheduled and processed quickly. A data-parallel task usually consists of a set of sub-tasks, generating a set of flows that are collectively referred to as coflows. The state-of-the-art schedulers collect coflow information in the cloud to optimize coflow-level performance. However, most of the coflows, classified as small coflows because they consist of only short flows, have been largely overlooked. This article presents OptaX, a decentralized network scheduling service that collaboratively schedules data-parallel tasks’ small coflows. OptaX adopts a cross-layer, commercial off-the-shelf switch-compatible design that leverages the sendbuffer information in the kernel to adaptively optimize flow scheduling in the network. Specifically, OptaX (i) monitors the system calls (syscalls) in the hosts to obtain their sendbuffer footprints, and (ii) recognizes small coflows and assigns high priorities to their flows. OptaX transfers these flows in a FIFO manner by adjusting TCP’s two attributes: window size and round-trip time. We have implemented OptaX as a Linux kernel module. The evaluation shows that OptaX is at least 2.2\texttimes{} faster than fair sharing and 1.2\texttimes{} faster than only assigning small coflows with the highest priority. We further apply OptaX to improve the small I/O performance of Ursa, a distributed block storage system that provides virtual disks where small I/O is dominant. Ursa with OptaX achieves significant improvement compared to the original Ursa for small I/O latency.},
journal = {ACM Trans. Parallel Comput.},
month = jun,
articleno = {10},
numpages = {23},
keywords = {Decentralized scheduling, data-parallel tasks, coflows, cross-layer scheduling}
}

@article{10.1145/3652158,
author = {Mokbel, Mohamed and Sakr, Mahmoud and Xiong, Li and Z\"{u}fle, Andreas and Almeida, Jussara and Anderson, Taylor and Aref, Walid and Andrienko, Gennady and Andrienko, Natalia and Cao, Yang and Chawla, Sanjay and Cheng, Reynold and Chrysanthis, Panos and Fei, Xiqi and Ghinita, Gabriel and Graser, Anita and Gunopulos, Dimitrios and Jensen, Christian S. and Kim, Joon-Seok and Kim, Kyoung-Sook and Kr\"{o}ger, Peer and Krumm, John and Lauer, Johannes and Magdy, Amr and Nascimento, Mario and Ravada, Siva and Renz, Matthias and Sacharidis, Dimitris and Salim, Flora and Sarwat, Mohamed and Schoemans, Maxime and Shahabi, Cyrus and Speckmann, Bettina and Tanin, Egemen and Teng, Xu and Theodoridis, Yannis and Torp, Kristian and Trajcevski, Goce and van Kreveld, Marc and Wenk, Carola and Werner, Martin and Wong, Raymond and Wu, Song and Xu, Jianqiu and Youssef, Moustafa and Zeinalipour, Demetris and Zhang, Mengxuan and Zim\'{a}nyi, Esteban},
title = {Mobility Data Science: Perspectives and Challenges},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2374-0353},
url = {https://doi.org/10.1145/3652158},
doi = {10.1145/3652158},
abstract = {Mobility data captures the locations of moving objects such as humans, animals, and cars. With the availability of Global Positioning System (GPS)–equipped mobile devices and other inexpensive location-tracking technologies, mobility data is collected ubiquitously. In recent years, the use of mobility data has demonstrated a significant impact in various domains, including traffic management, urban planning, and health sciences. In this article, we present the domain of mobility data science. Towards a unified approach to mobility data science, we present a pipeline having the following components: mobility data collection, cleaning, analysis, management, and privacy. For each of these components, we explain how mobility data science differs from general data science, we survey the current state-of-the-art, and describe open challenges for the research community in the coming years.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jul,
articleno = {10},
numpages = {35},
keywords = {Spatiotemporal data, Geospatial intelligence, GPS data, Mobility Patterns, Environmental impacts, Urban Mobility}
}

@article{10.1145/3652604,
author = {Huan, Chengying and Liu, Yongchao and Zhang, Heng and Song, Shuaiwen and Pandey, Santosh and Chen, Shiyang and Fang, Xiangfei and Jin, Yue and Lepers, Baptiste and Wu, Yanjun and Liu, Hang},
title = {TEA+: A Novel Temporal Graph Random Walk Engine with Hybrid Storage Architecture},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3652604},
doi = {10.1145/3652604},
abstract = {Many real-world networks are characterized by being temporal and dynamic, wherein the temporal information signifies the changes in connections, such as the addition or removal of links between nodes. Employing random walks on these temporal networks is a crucial technique for understanding the structural evolution of such graphs over time. However, existing state-of-the-art sampling methods are designed for traditional static graphs, and as such, they struggle to efficiently handle the dynamic aspects of temporal networks. This deficiency can be attributed to several challenges, including increased sampling complexity, extensive index space, limited programmability, and a lack of scalability.In this article, we introduce TEA+, a robust, fast, and scalable engine for conducting random walks on temporal graphs. Central to TEA+ is an innovative hybrid sampling method that amalgamates two Monte Carlo sampling techniques. This fusion significantly diminishes space complexity while maintaining a fast sampling speed. Additionally, TEA+ integrates a range of optimizations that significantly enhance sampling efficiency. This is further supported by an effective graph updating strategy, skilled in managing dynamic graph modifications and adeptly handling the insertion and deletion of both edges and vertices. For ease of implementation, we propose a temporal-centric programming model, designed to simplify the development of various random walk algorithms on temporal graphs. To ensure optimal performance across storage constraints, TEA+ features a degree-aware hybrid storage architecture, capable of adeptly scaling in different memory environments. Experimental results showcase the prowess of TEA+, as it attains up to three orders of magnitude speedups compared to current random walk engines on extensive temporal graphs.},
journal = {ACM Trans. Archit. Code Optim.},
month = may,
articleno = {37},
numpages = {26},
keywords = {Random walk; Graph algorithm; Temporal graph; External Storage}
}

@article{10.1145/3652865,
author = {Sun, Zhu and Feng, Kaidong and Yang, Jie and Fang, Hui and Qu, Xinghua and Ong, Yew-Soon and Liu, Wenyuan},
title = {Revisiting Bundle Recommendation for Intent-aware Product Bundling},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3652865},
doi = {10.1145/3652865},
abstract = {Product bundling represents a prevalent marketing strategy in both offline stores and e-commerce systems. Despite its widespread use, previous studies on bundle recommendation face two significant limitations. Firstly, they rely on noisy datasets, where bundles are defined by heuristics, e.g., products co-purchased in the same session. Secondly, they target specific tasks by holding unrealistic assumptions, e.g., the availability of bundles for recommendation directly. This paper proposes to take a step back and considers the process of bundle recommendation from a holistic user experience perspective. We first construct high-quality bundle datasets with rich metadata, particularly bundle intents, through a carefully designed crowd-sourcing task. We then define a series of tasks that together, support all key steps in a typical bundle recommendation process, from bundle detection, completion and ranking, to explanation and auto-naming, whereby 19 research questions are raised correspondingly to guide the analysis. Finally, we conduct extensive experiments and analyses with representative recommendation models and large language models (LLMs), demonstrating the challenges and opportunities, especially with the emergence of LLMs. To summarize, our study contributes by introducing novel data sources, paving the way for new research avenues, and offering insights to guide product bundling in real e-commerce platforms.},
journal = {ACM Trans. Recomm. Syst.},
month = jun,
articleno = {24},
numpages = {34},
keywords = {Product bundling, crowd-sourcing task, bundle datasets, bundle recommendation}
}

@article{10.1145/3653317,
author = {Masmoudi, Maroua and Ben Abdallah Ben Lamine, Sana and Karray, Mohamed Hedi and Archimede, Bernard and Baazaoui Zghal, Hajer},
title = {Semantic Data Integration and Querying: A Survey and Challenges},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3653317},
doi = {10.1145/3653317},
abstract = {Digital revolution produces massive, heterogeneous and isolated data. These latter remain underutilized, unsuitable for integrated querying and knowledge discovering. Hence the importance of this survey on data integration which identifies challenging issues and trends. First, an overview of the different generations and basics of data integration is given. Then, semantic data integration is focused, since it semantically links data allowing wider insights and decision-making. More than thirty works are reviewed. The goal is to help analysts to identify relevant criteria to compare then choose among semantic data integration approaches, focusing on the category (materialized, virtual or hybrid) and querying techniques.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {209},
numpages = {35},
keywords = {Data integration, ontology, query processing, ETL, OBDA, semantic mapping}
}

@article{10.1145/3653459,
author = {Lopez-Valdivieso, Jonathan and Cumplido, Rene},
title = {Design and Implementation of Hardware-Software Architecture Based on Hashes for SPHINCS+},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3653459},
doi = {10.1145/3653459},
abstract = {Advances in quantum computing have posed a future threat to today’s cryptography. With the advent of these quantum computers, security could be compromised. Therefore, the National Institute of Standards and Technology (NIST) has issued a request for proposals to standardize algorithms for post-quantum cryptography (PQC), which is considered difficult to solve for both classical and quantum computers. Among the proposed technologies, the most popular choices are lattice-based (shortest vector problem) and hash-based approaches. Other important categories are public key cryptography (PKE) and digital signatures. Within the realm of digital signatures lies SPHINCS+. However, there are few implementations of this scheme in hardware architectures. In this article, we present a hardware-software architecture for the SPHINCS+ scheme. We utilized a free RISC-V (Reduced Instruction Set Computer) processor synthesized on a Field Programmable Gate Array (FPGA), primarily integrating two accelerator modules for Keccak-1600 and the Haraka hash function. Additionally, modifications were made to the processor to accommodate the execution of these added modules. Our implementation yielded a 15-fold increase in performance with the SHAKE-256 function and nearly 90-fold improvement when using Haraka, compared to the reference software. Moreover, it is more compact compared to related works. This implementation was realized on a Xilinx FPGA Arty S7: Spartan-7.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = oct,
articleno = {54},
numpages = {22},
keywords = {FPGA, RISC-V, SPHINCS+, hardware-software, processor, post-quantum cryptography}
}

@article{10.1145/3653673,
author = {Li, Hanzhe and Gu, Jingjing and Lu, Xinjiang and Shen, Dazhong and Liu, Yuting and Deng, YaNan and Shi, Guoliang and Xiong, Hui},
title = {Beyond Relevance: Factor-level Causal Explanation for User Travel Decisions with Counterfactual Data Augmentation},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {1046-8188},
url = {https://doi.org/10.1145/3653673},
doi = {10.1145/3653673},
abstract = {Point-of-Interest (POI) recommendation, an important research hotspot in the field of urban computing, plays a crucial role in urban construction. While understanding the process of users’ travel decisions and exploring the causality of POI choosing is not easy due to the complex and diverse influencing factors in urban travel scenarios. Moreover, the spurious explanations caused by severe data sparsity, i.e., misrepresenting universal relevance as causality, may also hinder us from understanding users’ travel decisions. To this end, in this article, we propose a factor-level causal explanation generation framework based on counterfactual data augmentation for user travel decisions, named Factor-level Causal Explanation for User Travel Decisions (FCE-UTD), which can distinguish between true and false causal factors and generate true causal explanations. Specifically, we first assume that a user decision is composed of a set of several different factors. Then, by preserving the user decision structure with a joint counterfactual contrastive learning paradigm, we learn the representation of factors and detect the relevant factors. Next, we further identify true causal factors by constructing counterfactual decisions with a counterfactual representation generator, in particular, it can not only augment the dataset and mitigate the sparsity but also contribute to clarifying the causal factors from other false causal factors that may cause spurious explanations. Besides, a causal dependency learner is proposed to identify causal factors for each decision by learning causal dependency scores. Extensive experiments conducted on three real-world datasets demonstrate the superiority of our approach in terms of check-in rate, fidelity, and downstream tasks under different behavior scenarios. The extra case studies also demonstrate the ability of FCE-UTD to generate causal explanations in POI choosing.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {128},
numpages = {31},
keywords = {Causal explanation generation, urban travel decisions, counterfactual data augmentation, contrastive learning}
}

@article{10.1145/3653982,
author = {DeSmet, Chance and Cook, Diane},
title = {HydraGAN: A Cooperative Agent Model for Multi-Objective Data Generation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3653982},
doi = {10.1145/3653982},
abstract = {Generative adversarial networks have become a de facto approach to generate synthetic data points that resemble their real counterparts. We tackle the situation where the realism of individual samples is not the sole criterion for synthetic data generation. Additional constraints such as privacy preservation, distribution realism, and diversity promotion may also be essential to optimize. To address this challenge, we introduce HydraGAN, a multi-agent network that performs multi-objective synthetic data generation. We theoretically verify that training the HydraGAN system, containing a single generator and an arbitrary number of discriminators, leads to a Nash equilibrium. Experimental results for six datasets indicate that HydraGAN consistently outperforms prior methods in maximizing the Area under the Radar Chart, balancing a combination of cooperative or competitive data generation goals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {60},
numpages = {21},
keywords = {Synthetic data generation, multi-agent GAN, contrasting objectives, privacy-preserving data mining}
}

@article{10.1145/3654439,
author = {Gavidia-Calderon, Carlos and Kordoni, Anastasia and Bennaceur, Amel and Levine, Mark and Nuseibeh, Bashar},
title = {The IDEA of Us: An Identity-Aware Architecture for Autonomous Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654439},
doi = {10.1145/3654439},
abstract = {Autonomous systems, such as drones and rescue robots, are increasingly used during emergencies. They deliver services and provide situational awareness that facilitate emergency management and response. To do so, they need to interact and cooperate with humans in their environment. Human behaviour is uncertain and complex, so it can be difficult to reason about it formally. In this article, we propose IDEA: an adaptive software architecture that enables cooperation between humans and autonomous systems, by leveraging the social identity approach. This approach establishes that group membership drives human behaviour. Identity and group membership are crucial during emergencies, as they influence cooperation among survivors. IDEA systems infer the social identity of surrounding humans, thereby establishing their group membership. By reasoning about groups, we limit the number of cooperation strategies the system needs to explore. IDEA systems select a strategy from the equilibrium analysis of game-theoretic models that represent interactions between group members and the IDEA system. We demonstrate our approach using a search-and-rescue scenario, in which an IDEA rescue robot optimises evacuation by collaborating with survivors. Using an empirically validated agent-based model, we show that the deployment of the IDEA system can reduce median evacuation time by 13.6\%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {164},
numpages = {38},
keywords = {Autonomous systems, game theory, social identity, agent-based modelling}
}

@article{10.1145/3654664,
author = {Sarafraz, Gita and Behnamnia, Armin and Hosseinzadeh, Mehran and Balapour, Ali and Meghrazi, Amin and Rabiee, Hamid R.},
title = {Domain Adaptation and Generalization of Functional Medical Data: A Systematic Survey of Brain Data},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3654664},
doi = {10.1145/3654664},
abstract = {Despite the excellent capabilities of machine learning algorithms, their performance deteriorates when the distribution of test data differs from the distribution of training data. In medical data research, this problem is exacerbated by its connection to human health, expensive equipment, and meticulous setups. Consequently, achieving domain generalizations and domain adaptations under distribution shifts is an essential step in the analysis of medical data. As the first systematic review of domain generalization and domain adaptation on functional brain signals, the article discusses and categorizes various methods, tasks, and datasets in this field. Moreover, it discusses relevant directions for future research.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {255},
numpages = {39},
keywords = {Domain adaptation, domain generalization, functional medical data}
}

@article{10.1145/3654806,
author = {Xu, Aobo and Jian, Ling and Yin, Yue and Zhang, Na},
title = {UISA: User Information Separating Architecture for Commodity Recommendation Policy with Deep Reinforcement Learning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654806},
doi = {10.1145/3654806},
abstract = {Commodity recommendation contributes an important part of individuals’ daily life. In this context, deep reinforcement learning methods have demonstrated substantial efficacy in enhancing recommender systems’ performance. Nevertheless, several recommender systems directly utilize original feature information as a foundational element for decision-making, which seems simplistic and low efficient. Furthermore, the incorporation of sequential decision-making adds complexity to the task of recommendation. In pursuit of maximizing the long-term sequential returns of recommender systems, our study introduces a novel architecture, named User Information Separating Architecture (UISA). This framework is tailored to align with classic reinforcement learning algorithms and aims to extract the user’s interest value through the discrete processing of both static and dynamic user information. Through integration with deep reinforcement learning, the architecture is oriented towards the maximization of long-term profit and is applicable in sequential recommendation scenarios. We conduct experimental assessments by combining the proposed architecture with proximal policy optimization (PPO) and deep deterministic policy gradient (DDPG) algorithms. The outcomes illustrate marked improvements in commodity recommendation, showcasing enhancements ranging from approximately 5\% to 40\% in both reward and click-through rate metrics across a self-constructed JDEnv environment and the Virtual Taobao environment. Through comparison experiments, the UISA models demonstrate comparable performance.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = apr,
keywords = {Recommender Systems, Reinforcement Learning, Sequential Decision-making}
}

@article{10.1145/3654952,
author = {Glaze, Nick and McNeely, Tria and Zhu, Yiwen and Gleeson, Matthew and Serr, Helen and Bhopi, Rajeev and Krishnan, Subru},
title = {Lorentz: Learned SKU Recommendation Using Profile Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654952},
doi = {10.1145/3654952},
abstract = {In response to diverse demands, cloud operators have significantly expanded the array of service offerings, often referred to as Stock Keeping Units (SKUs) available for computing resource configurations. Such diversity has led to increased complexity for customers to choose the appropriate SKU. In the analyzed system, only 43\% of the resource capacity was rightly chosen. Although various automated solutions have attempted to resolve this issue, they often rely on the availability of enriched data, such as workload traces, which are unavailable for newly established services. Since these services amass a substantial volume of telemetry from existing users, cloud operators can leverage this information to better understand customer needs and mitigate the risk of over- or under-provisioning. Furthermore, customer satisfaction feedback serves as a crucial resource for continuous learning and improving the recommendation mechanism. In this paper, we present Lorentz, an intelligent SKU recommender for provisioning new compute resources that circumvents the need for workload traces. Lorentz leverages customer profile data to forecast resource capacities for new users based on detailed profiling of existing users. Furthermore, using a continuous learned feedback loop, Lorentz tailors capacity recommendations according to customer performance vs. cost preferences captured through satisfaction signals. Validated using the production data from provisioned VMs supporting Database Platform X, we demonstrate that Lorentz outperforms user selections and existing defaults, reducing slack by &gt;60\% without increasing throttling. Evaluated using synthetic data, Lorentz's personalization stage iteratively learns the user preferences over time with high accuracy.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {149},
numpages = {25},
keywords = {machine learning, resource management, simulation}
}

@article{10.1145/3654957,
author = {Gong, Yue and Galhotra, Sainyam and Castro Fernandez, Raul},
title = {Nexus: Correlation Discovery over Collections of Spatio-Temporal Tabular Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654957},
doi = {10.1145/3654957},
abstract = {Causal analysis is essential for gaining insights into complex real-world processes and making informed decisions. However, performing accurate causal analysis on observational data is generally infeasible, and therefore, domain experts start exploration with the identification of correlations. The increased availability of data from open government websites, organizations, and scientific studies presents an opportunity to harness observational datasets in assisting domain experts during this exploratory phase.In this work, we introduce Nexus, a system designed to align large repositories of spatio-temporal datasets and identify correlations, facilitating the exploration of causal relationships. Nexus addresses the challenges of aligning tabular datasets across space and time, handling missing data, and identifying correlations deemed "interesting". Empirical evaluation on Chicago Open Data and United Nations datasets demonstrates the effectiveness of Nexus in exposing interesting correlations, many of which have undergone extensive scrutiny by social scientists.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {154},
numpages = {28},
keywords = {correlation analysis, data discovery, hypothesis generation, spatio-temporal data}
}

@article{10.1145/3654958,
author = {Sha, Mo and Cai, Yifan and Wang, Sheng and Phan, Linh Thi Xuan and Li, Feifei and Tan, Kian-Lee},
title = {Object-oriented Unified Encrypted Memory Management for Heterogeneous Memory Architectures},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654958},
doi = {10.1145/3654958},
abstract = {In contemporary database applications, the demand for memory resources is intensively high. To enhance adaptability to varying resource needs and improve cost efficiency, the integration of diverse storage technologies within heterogeneous memory architectures emerges as a promising solution. Despite the potential advantages, there exists a significant gap in research related to the security of data within these complex systems. This paper endeavors to fill this void by exploring the intricacies and challenges of ensuring data security in object-oriented heterogeneous memory systems. We introduce the concept of Unified Encrypted Memory (UEM) management, a novel approach that provides unified object references essential for data management platforms, while simultaneously concealing the complexities of physical scheduling from developers. At the heart of UEM lies the seamless and efficient integration of data encryption techniques, which are designed to ensure data integrity and guarantee the freshness of data upon access. Our research meticulously examines the security deficiencies present in existing heterogeneous memory system designs. By advancing centralized security enforcement strategies, we aim to achieve efficient object-centric data protection. Through extensive evaluations conducted across a variety of memory configurations and tasks, our findings highlight the effectiveness of UEM. The security features of UEM introduce low and acceptable overheads, and UEM outperforms conventional security measures in terms of speed and space efficiency.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {155},
numpages = {29},
keywords = {data confidentiality, data integrity, heterogeneous memory architecture, memory security, unified memory management}
}

@article{10.1145/3654984,
author = {Chen, Kaiwen and Koudas, Nick},
title = {Unstructured Data Fusion for Schema and Data Extraction},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654984},
doi = {10.1145/3654984},
abstract = {Recently, there has been significant interest in extracting actionable insights from the abundance of unstructured textual data. In this paper, we introduce a novel problem, which we term Semistructured Schema and Data Extraction (SDE). This task aims to enhance and complete tables using information discovered from textual repositories, given partial table specifications in the form of queries. To effectively solve SDE, several challenges must be overcome, which involve transforming the partial table specifications into effective queries, retrieving relevant documents, discerning values for partially specified attributes, inferring additional attributes, and constructing an enriched output table while mitigating the influence of false positives from the retrieval.We propose an end-to-end pipeline for SDE, which consists of a retrieval component and an augmentation component, to address each of the challenges. In the retrieval component, we serialize the partial table specifications into a query and employ a dense passage retrieval algorithm to extract the top-k relevant results from the text repository. Subsequently, the augmentation component ingests the output documents from the retrieval phase and generates an enriched table. We formulate this table enrichment task as a unique sequence-to-sequence task, distinct from traditional approaches, as it operates on multiple documents during generation. Utilizing an interpolation mechanism on the encoder output, our model maintains a nearly constant context length while automatically prioritizing the importance of documents during the generation. Due to the novelty of SDE, we establish a validation methodology, adapting and expanding existing benchmarks with the use of powerful large language models. Our extensive experiments show that our method achieves high accuracy in enriching query tables through multi-document fusion, while also surpassing baseline methods in both accuracy and computational efficiency.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {181},
numpages = {26},
keywords = {data fusion, information extraction, schema extraction}
}

@article{10.1145/3654992,
author = {Wu, Yang and Wan, Yao and Zhang, Hongyu and Sui, Yulei and Wei, Wucai and Zhao, Wei and Xu, Guandong and Jin, Hai},
title = {Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654992},
doi = {10.1145/3654992},
abstract = {The Natural Language to Visualization (NL2Vis) task aims to transform natural-language descriptions into visual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently, many deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts made by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning multiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models (LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and explore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore the ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and analyze which table content contributes most to the NL2Vis. Our findings suggest that transforming structured tabular data into programs is effective, and it is essential to consider the table schema when formulating prompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The experimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting performance improvements, at times even surpassing fine-tuned models when provided with certain few-shot demonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose to iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter. The experimental results confirm the efficacy of iterative updates and hold great potential for future study.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {115},
numpages = {28},
keywords = {code generation, data analysis, data visualization, exploratory study, large language models, natural language processing}
}

@article{10.1145/3655630,
author = {Pai, Yu-Tung and Sun, Nien-En and Li, Cheng-Te and Lin, Shou-de},
title = {Incremental Data Drifting: Evaluation Metrics, Data Generation, and Approach Comparison},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3655630},
doi = {10.1145/3655630},
abstract = {Incremental data drifting is a common problem when employing a machine-learning model in industrial applications. The underlying data distribution evolves gradually, e.g., users change their buying preferences on an E-commerce website over time. The problem needs to be addressed to obtain high performance. Right now, studies regarding incremental data drifting suffer from several issues. For one thing, there is a lack of clear-defined incremental drift datasets for examination. Existing efforts use either collected real datasets or synthetic datasets that show two obvious limitations. One is in particular when and of which type of drifts the distribution undergoes is unknown, and the other is that a simple synthesized dataset cannot reflect the complex representation we would normally face in the real world. For another, there lacks a well-defined protocol to evaluate a learner’s knowledge transfer capability on an incremental drift dataset. To provide a holistic discussion on these issues, we create approaches to generate datasets with specific drift types, and define a novel protocol for evaluation. Besides, we investigate recent advances in the transfer learning field, including Domain Adaptation and Lifelong Learning, and examine how they perform in the presence of incremental data drifting. The results unfold the relationships among drift types, knowledge preservation, and learning approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {71},
numpages = {26},
keywords = {Concept drift, incremental data drift, data generation}
}

@article{10.1145/3656338,
author = {Pioli, La\'{e}rcio and de Macedo, Douglas D. J. and Costa, Daniel G. and Dantas, Mario A. R.},
title = {Intelligent Edge-powered Data Reduction: A Systematic Literature Review},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3656338},
doi = {10.1145/3656338},
abstract = {The development of the Internet of Things (IoT) paradigm and its significant spread as an affordable data source has brought many challenges when pursuing efficient data collection, distribution, and storage. Since such hierarchical logical architecture can be inefficient and costly in many cases, Data Reduction (DR) solutions have arisen to allow data preprocessing before actual transmission. To increase DR performance, researchers are using Artificial Intelligence (AI) techniques and models toward reducing sensed data volume. AI for DR on the edge is investigated in this study in the form of a Systematic Literature Review (SLR) encompassing major issues such as data heterogeneity and AI-based techniques to reduce data, architectures, and contexts of usage. An SLR is conducted to map the state of the art in this area, highlighting the most common challenges and potential research trends in addition to a proposed taxonomy.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {234},
numpages = {39},
keywords = {Internet of things, artificial intelligence, data reduction, intelligent edge, systematic literature review, edge computing}
}

@article{10.1145/3656341,
author = {Sun, Weisong and Fang, Chunrong and Ge, Yifei and Hu, Yuling and Chen, Yuchen and Zhang, Quanjun and Ge, Xiuting and Liu, Yang and Chen, Zhenyu},
title = {A Survey of Source Code Search: A 3-Dimensional Perspective},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3656341},
doi = {10.1145/3656341},
abstract = {(Source) code search is widely concerned by software engineering researchers because it can improve the productivity and quality of software development. Given a functionality requirement usually described in a natural language sentence, a code search system can retrieve code snippets that satisfy the requirement from a large-scale code corpus, e.g., GitHub. To realize effective and efficient code search, many techniques have been proposed successively. These techniques improve code search performance mainly by optimizing three core components, including query understanding component, code understanding component, and query-code matching component. In this article, we provide a 3-dimensional perspective survey for code search. Specifically, we categorize existing code search studies into query-end optimization techniques, code-end optimization techniques, and match-end optimization techniques according to the specific components they optimize. These optimization techniques are proposed to enhance the performance of specific components, and thus the overall performance of code search. Considering that each end can be optimized independently and contributes to the code search performance, we treat each end as a dimension. Therefore, this survey is 3-dimensional in nature, and it provides a comprehensive summary of each dimension in detail. To understand the research trends of the three dimensions in existing code search studies, we systematically review 68 relevant literatures. Different from existing code search surveys that only focus on the query end or code end or introduce various aspects shallowly (including codebase, evaluation metrics, modeling technique, etc.), our survey provides a more nuanced analysis and review of the evolution and development of the underlying techniques used in the three ends. Based on a systematic review and summary of existing work, we outline several open challenges and opportunities at the three ends that remain to be addressed in future work.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {166},
numpages = {51},
keywords = {Source code search, deep learning, query-end optimization, code-end optimization, match-end optimization}
}

@article{10.1145/3656643,
author = {Abouelhamayed, Ahmed and Cui, Angela and Fernandez-marques, Javier and Lane, Nicholas and Abdelfattah, Mohamed},
title = {PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3656643},
doi = {10.1145/3656643},
abstract = {Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs), especially convolutional neural networks (CNNs). Recently, product quantization (PQ) has been applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. To better understand the efficiency tradeoffs of product-quantized DNNs (PQ-DNNs), we create a custom hardware accelerator to parallelize and accelerate nearest-neighbor search and dot-product lookups. Additionally, we perform an empirical study to investigate the efficiency–accuracy tradeoffs of different PQ parameterizations and training methods. We identify PQ configurations that improve performance-per-area for ResNet20 by up to 3.1\texttimes{}, even when compared to a highly optimized conventional DNN accelerator, with similar improvements on two additional compact DNNs. When comparing to recent PQ solutions, we outperform prior work by 4\texttimes{} in terms of performance-per-area with a 0.6\% accuracy degradation. Finally, we reduce the bitwidth of PQ operations to investigate the impact on both hardware efficiency and accuracy. With only 2–6-bit precision on three compact DNNs, we were able to maintain DNN accuracy eliminating the need for DSPs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {6},
numpages = {29},
keywords = {Deep neural network (DNN), product quantization, FPGA acceleration, low arithmetic precision}
}

@article{10.1145/3657294,
author = {Vaz, Bruno and Figueira, \'{A}lvaro},
title = {GANs in the Panorama of Synthetic Data Generation Methods},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3657294},
doi = {10.1145/3657294},
abstract = {This article focuses on the creation and evaluation of synthetic data to address the challenges of imbalanced datasets in machine learning (ML) applications, using fake news detection as a case study. We conducted a thorough literature review on generative adversarial networks (GANs) for tabular data, synthetic data generation methods, and synthetic data quality assessment. By augmenting a public news dataset with synthetic data generated by different GAN architectures, we demonstrate the potential of synthetic data to improve ML models’ performance in fake news detection. Our results show a significant improvement in classification performance, especially in the underrepresented class. We also modify and extend a data usage approach to evaluate the quality of synthetic data and investigate the relationship between synthetic data quality and data augmentation performance in classification tasks. We found a positive correlation between synthetic data quality and performance in the underrepresented class, highlighting the importance of high-quality synthetic data for effective data augmentation.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {3},
numpages = {28},
keywords = {Synthetic data, generative adversarial networks, imbalanced datasets, fake news detection, data augmentation quality}
}

@article{10.1145/3658189,
author = {Yang, Lingchen and Zoss, Gaspard and Chandran, Prashanth and Gross, Markus and Solenthaler, Barbara and Sifakis, Eftychios and Bradley, Derek},
title = {Learning a Generalized Physical Face Model From Data},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658189},
doi = {10.1145/3658189},
abstract = {Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {94},
numpages = {14},
keywords = {differentiable physics, deep learning, physically-based facial animation, digital humans}
}

@article{10.1145/3659207,
author = {Liu, Qunyou and Huang, Darong and Costero, Luis and Zapater, Marina and Atienza, David},
title = {Intermediate Address Space: virtual memory optimization of heterogeneous architectures for cache-resident workloads},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3659207},
doi = {10.1145/3659207},
abstract = {The increasing demand for computing power and the emergence of heterogeneous computing architectures have driven the exploration of innovative techniques to address current limitations in both the compute and memory subsystems. One such solution is the use of Accelerated Processing Units (APUs), processors that incorporate both a central processing unit (CPU) and an integrated graphics processing unit (iGPU). However, the performance of both APU and CPU systems can be significantly hampered by address translation overhead, leading to a decline in overall performance, especially for cache-resident workloads. To address this issue, we propose the introduction of a new intermediate address space (IAS) in both APU and CPU systems. IAS serves as a bridge between virtual address (VA) spaces and physical address (PA) spaces, optimizing the address translation process. In the case of APU systems, our research indicates that the iGPU suffers from significant translation look-aside buffer (TLB) misses in certain workload situations. Using an IAS, we can divide the initial address translation into front- and back-end phases, effectively shifting the bottleneck in address translation from the cache side to the memory controller side, a technique that proves to be effective for cache-resident workloads. Our simulations demonstrate that implementing IAS in the CPU system can boost performance by up to 40\% compared to conventional CPU systems. Furthermore, we evaluate the effectiveness of APU systems, comparing the performance of IAS-based systems with traditional systems, showing up to a 185\% improvement in APU system performance with our proposed IAS implementation. Furthermore, our analysis indicates that over 90\% of TLB misses can be filtered by the cache, and employing a larger cache within the system could potentially result in even greater improvements. The proposed IAS offers a promising and practical solution to enhance the performance of both APU and CPU systems, contributing to state-of-the-art research in the field of computer architecture.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {50},
numpages = {23},
keywords = {Computer architecture, CPU, GPU, virtual memory, TLB, cache}
}

@article{10.1145/3659589,
author = {Zhou, Yexu and Zhao, Haibin and Huang, Yiran and R\"{o}ddiger, Tobias and Kurnaz, Murat and Riedel, Till and Beigl, Michael},
title = {AutoAugHAR: Automated Data Augmentation for Sensor-based Human Activity Recognition},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659589},
doi = {10.1145/3659589},
abstract = {Sensor-based HAR models face challenges in cross-subject generalization due to the complexities of data collection and annotation, impacting the size and representativeness of datasets. While data augmentation has been successfully employed in domains like natural language and image processing, its application in HAR remains underexplored. This study presents AutoAugHAR, an innovative two-stage gradient-based data augmentation optimization framework. AutoAugHAR is designed to take into account the unique attributes of candidate augmentation operations and the unique nature and challenges of HAR tasks. Notably, it optimizes the augmentation pipeline during HAR model training without substantially extending the training duration. In evaluations on eight inertial-measurement-units-based benchmark datasets using five HAR models, AutoAugHAR has demonstrated superior robustness and effectiveness compared to other leading data augmentation frameworks. A salient feature of AutoAugHAR is its model-agnostic design, allowing for its seamless integration with any HAR model without the need for structural modifications. Furthermore, we also demonstrate the generalizability and flexible extensibility of AutoAugHAR on four datasets from other adjacent domains. We strongly recommend its integration as a standard protocol in HAR model training and will release it as an open-source tool1.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {48},
numpages = {27},
keywords = {automated data augmentation, human activity recognition, machine learning}
}

@article{10.1145/3659610,
author = {Garcia, Kimberly and Vontobel, Jonathan and Mayer, Simon},
title = {A Digital Companion Architecture for Ambient Intelligence},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659610},
doi = {10.1145/3659610},
abstract = {Ambient Intelligence (AmI) focuses on creating environments capable of proactively and transparently adapting to users and their activities. Traditionally, AmI focused on the availability of computational devices, the pervasiveness of networked environments, and means to interact with users. In this paper, we propose a renewed AmI architecture that takes into account current technological advancements while focusing on proactive adaptation for assisting and protecting users. This architecture consist of four phases: Perceive, Interpret, Decide, and Interact. The AmI systems we propose, called Digital Companions (DC), can be embodied in a variety of ways (e.g., through physical robots or virtual agents) and are structured according to these phases to assist and protect their users. We further categorize DCs into Expert DCs and Personal DCs, and show that this induces a favorable separation of concerns in AmI systems, where user concerns (including personal user data and preferences) are handled by Personal DCs and environment concerns (including interfacing with environmental artifacts) are assigned to Expert DCs; this separation has favorable privacy implications as well. Herein, we introduce this architecture and validate it through a prototype in an industrial scenario where robots and humans collaborate to perform a task.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {66},
numpages = {26},
keywords = {ambient intelligence, architecture, connected devices, digital companion systems, industrial environments, knowledge graph, mixed reality, scene graph generation algorithm}
}

@article{10.1145/3659620,
author = {Hou, Weiying and Wu, Chenshu},
title = {RFBoost: Understanding and Boosting Deep WiFi Sensing via Physical Data Augmentation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659620},
doi = {10.1145/3659620},
abstract = {Deep learning shows promising performance in wireless sensing. However, deep wireless sensing (DWS) heavily relies on large datasets. Unfortunately, building comprehensive datasets for DWS is difficult and costly, because wireless data depends on environmental factors and cannot be labeled offline. Despite recent advances in few-shot/cross-domain learning, DWS is still facing data scarcity issues. In this paper, we investigate a distinct perspective of radio data augmentation (RDA) for WiFi sensing and present a data-space solution. Our key insight is that wireless signals inherently exhibit data diversity, contributing more information to be extracted for DWS. We present RFBoost, a simple and effective RDA framework encompassing novel physical data augmentation techniques. We implement RFBoost as a plug-and-play module integrated with existing deep models and evaluate it on multiple datasets. Experimental results demonstrate that RFBoost achieves remarkable average accuracy improvements of 5.4\% on existing models without additional data collection or model modifications, and the best-boosted performance outperforms 11 state-of-the-art baseline models without RDA. RFBoost pioneers the study of RDA, an important yet currently underexplored building block for DWS, which we expect to become a standard DWS component of WiFi sensing and beyond. RFBoost is released at https://github.com/aiot-lab/RFBoost.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {58},
numpages = {26},
keywords = {Data Augmentation, Deep Learning, Wireless Sensing}
}

@article{10.1145/3660525,
author = {Wu, Ke and Dong, Dezun and Xu, Weixia},
title = {COER: A Network Interface Offloading Architecture for RDMA and Congestion Control Protocol Codesign},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3660525},
doi = {10.1145/3660525},
abstract = {RDMA (Remote Direct Memory Access) networks require efficient congestion control to maintain their high throughput and low latency characteristics. However, congestion control protocols deployed at the software layer suffer from slow response times due to the communication overhead between host hardware and software. This limitation has hindered their ability to meet the demands of high-speed networks and applications. Harnessing the capabilities of rapidly advancing Network Interface Cards (NICs) can drive progress in congestion control. Some simple congestion control protocols have been offloaded to RDMA NICs to enable faster detection and processing of congestion. However, offloading congestion control to the RDMA NIC faces a significant challenge in integrating the RDMA transport protocol with advanced congestion control protocols that involve complex mechanisms. We have observed that reservation-based proactive congestion control protocols share strong similarities with RDMA transport protocols, allowing them to integrate seamlessly and combine the functionalities of the transport layer and network layer. In this article, we present COER, an RDMA NIC architecture that leverages the functional components of RDMA to perform reservations and completes the scheduling of congestion control during the scheduling process of the RDMA protocol. COER facilitates the streamlined development of offload strategies for congestion control techniques —specifically, proactive congestion control —on RDMA NICs. We use COER to design offloading schemes for 11 congestion control protocols, which we implement and evaluate using a network emulator with a cycle-accurate RDMA NIC model that can load Message Passing Interface (MPI) programs. The evaluation results demonstrate that the architecture of COER does not compromise the original characteristics of the congestion control protocols. Compared with a layered protocol stack approach, COER enables the performance of RDMA networks to reach new heights.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {49},
numpages = {26},
keywords = {Congestion control, RDMA, NIC, offloading}
}

@article{10.1145/3660634,
author = {Bellandi, Valerio and Ceravolo, Paolo and Maggesi, Jonatan and Maghool, Samira},
title = {Data management for continuous learning in EHR systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1533-5399},
url = {https://doi.org/10.1145/3660634},
doi = {10.1145/3660634},
abstract = {To gain a comprehensive understanding of a patient’s health, advanced analytics must be applied to the data collected by electronic health record (EHR) systems. However, managing and curating this data requires carefully designed workflows. While digitalization and standardization enable continuous health monitoring, missing data values and technical issues can compromise the consistency and timeliness of the data. In this paper, we propose a workflow for developing prognostic models that leverages the SMART BEAR infrastructure and the capabilities of the Big Data Analytics (BDA) engine to homogenize and harmonize data points. Our workflow improves the quality of the data by evaluating different imputation algorithms and selecting one that maintains the distribution and correlation of features similar to the raw data. We applied this workflow to a subset of the data stored in the SMART BEAR repository and examined its impact on the prediction of emerging health states such as cardiovascular disease and mild depression. We also discussed the possibility of model validation by clinicians in the SMART BEAR project, the transmission of subsequent actions in the decision support system, and the estimation of the required number of data points.},
note = {Just Accepted},
journal = {ACM Trans. Internet Technol.},
month = may,
keywords = {Internet of Things, Electronic Health Records, Data Management, Continuous Learning}
}

@article{10.1145/3660639,
author = {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan, Naren},
title = {Neural Methods for Data-to-text Generation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3660639},
doi = {10.1145/3660639},
abstract = {The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text (D2T) generation. This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that focus not only on the design of linguistically capable systems but also on systems that exhibit fairness and accountability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {89},
numpages = {46},
keywords = {Narration, data-to-text, data-to-text generation, natural language generation}
}

@article{10.1145/3660647,
author = {Don, Asitha Kottahachchi Kankanamge and Khalil, Ibrahim},
title = {Q-SupCon: Quantum-Enhanced Supervised Contrastive Learning Architecture within the Representation Learning Framework},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660647},
doi = {10.1145/3660647},
abstract = {In the evolving landscape of data privacy regulations, the challenge of providing extensive data for robust deep classification models arises. The accuracy of these models relies on the amount of training data, due to the multitude of parameters that require tuning. Unfortunately, obtaining such ample data proves challenging, particularly in domains like medical applications, where there is a pressing need for robust models for early disease detection but a shortage of labeled data. Nevertheless, the classical supervised contrastive learning models, have shown the potential to address this challenge up to a certain limit, by utilizing deep encoder models. However, recent advancements in quantum machine learning enable the extraction of meaningful representations from extremely limited and simple data. Thus, replacing classical counterparts in classical or hybrid quantum-classical supervised contrastive models enhances feature learning capability with minimal data. Therefore, this work proposes the Q-SupCon model, a fully quantum-powered supervised contrastive learning model comprising a quantum data augmentation circuit, quantum encoder, quantum projection head, and quantum variational classifier, enabling efficient image classification with minimal labeled data. Furthermore, the novel model attains 80\%, 60\%, and 80\% test accuracy on MNIST, KMNIST, and FMNIST datasets, marking a significant advancement in addressing the data scarcity challenge.},
note = {Just Accepted},
journal = {ACM Transactions on Quantum Computing},
month = apr,
keywords = {Quantum Machine Learning, Representation Learning, Semi-Supervised Learning, Supervised Contrastive Learning}
}

@article{10.1145/3661997,
author = {Lai, Chengtao and Zhang, Wei},
title = {gem5-NVDLA: A Simulation Framework for Compiling, Scheduling, and Architecture Evaluation on AI System-on-Chips},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3661997},
doi = {10.1145/3661997},
abstract = {Recent years have seen an increasing trend in designing AI accelerators together with the rest of the system, including CPUs and memory hierarchy. This trend calls for high-quality simulators or analytical models that enable such kind of co-exploration. Currently, the majority of such exploration is supported by AI accelerator analytical models. But such models usually overlook the non-trivial impact of congestion of shared resources, non-ideal hardware utilization and non-zero CPU scheduler overhead, which could only be modeled by cycle-level simulators. However, most simulators with full-stack toolchains are proprietary to corporations, and the few open-source simulators are suffering from either weak compilers or limited space of modeling. This framework resolves these issues by proposing a compilation and simulation flow to run arbitrary Caffe neural network models on the NVIDIA Deep Learning Accelerator (NVDLA) with gem5, a cycle-level simulator, and by adding more building blocks including scratchpad allocation, multi-accelerator scheduling, tensor-level prefetching mechanisms, and a DMA-aided embedded buffer to map workload to multiple NVDLAs. The proposed framework has been tested and verified on a set of convolution neural networks, showcasing the capability of modeling complex buffer management strategies, scheduling policies, and hardware architectures. As a case study of this framework, we demonstrate the importance of adopting different buffering strategies for activation and weight tensors in AI accelerators to acquire remarkable speedup.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = sep,
articleno = {84},
numpages = {20},
keywords = {Neural networks, accelerators, cycle-level simulators}
}

@article{10.1145/3662179,
author = {Sandeepa, Chamara and Siniarski, Bartlomiej and Kourtellis, Nicolas and Wang, Shen and Liyanage, Madhusanka},
title = {A Survey on Privacy of Personal and Non-Personal Data in B5G/6G Networks},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3662179},
doi = {10.1145/3662179},
abstract = {The upcoming Beyond 5G (B5G) and 6G networks are expected to provide enhanced capabilities such as ultra-high data rates, dense connectivity, and high scalability. It opens many possibilities for a new generation of services driven by Artificial Intelligence (AI) and billions of interconnected smart devices. However, with this expected massive upgrade, the privacy of people, organisations, and states is becoming a rising concern. The recent introduction of privacy laws and regulations for personal and non-personal data signals that global awareness is emerging in the current privacy landscape. Yet, many gaps need to be identified in the case of two data types. If not detected, then they can lead to significant privacy leakages and attacks that will affect billions of people and organisations who utilise B5G/6G. This survey is a comprehensive study of personal and non-personal data privacy in B5G/6G to identify the current progress and future directions to ensure data privacy. We provide a detailed comparison of the two data types and a set of related privacy goals for B5G/6G. Next, we bring data privacy issues with possible solutions. This article also provides future directions to preserve personal and non-personal data privacy in future networks.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {266},
numpages = {37},
keywords = {Non-personal data, personal data, privacy, beyond 5G/6G}
}

@article{10.1145/3663369,
author = {Li, Na and Qi, Yiyang and Li, Chaoran and Zhao, Zhiming},
title = {Active Learning for Data Quality Control: A Survey},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3663369},
doi = {10.1145/3663369},
abstract = {Data quality plays a vital role in scientific research and decision-making across industries. Thus, it is crucial to incorporate the data quality control (DQC) process, which comprises various actions and operations to detect and correct data errors. The increasing adoption of machine learning (ML) techniques in different domains has raised concerns about data quality in the ML field. Conversely, ML’s capability to uncover complex patterns makes it suitable for addressing challenges involved in the DQC process. However, supervised learning methods demand abundant labeled data, while unsupervised learning methods heavily rely on the underlying distribution of the data. Active learning (AL) provides a promising solution by proactively selecting data points for inspection, thus reducing the burden of data labeling for domain experts. Therefore, this survey focuses on applying AL to DQC. Starting with a review of common data quality issues and solutions in the ML field, we aim to enhance the understanding of current quality assessment methods. We then present two scenarios to illustrate the adoption of AL into the DQC systems on the anomaly detection task, including pool-based and stream-based approaches. Finally, we provide the remaining challenges and research opportunities in this field.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {45},
keywords = {Data Quality Control, Active Learning, query strategy, anomaly detection, Machine Learning}
}

@article{10.1145/3663482,
author = {Gilman, Ekaterina and Bugiotti, Francesca and Khalid, Ahmed and Mehmood, Hassan and Kostakos, Panos and Tuovinen, Lauri and Ylipulli, Johanna and Su, Xiang and Ferreira, Denzil},
title = {Addressing Data Challenges to Drive the Transformation of Smart Cities},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3663482},
doi = {10.1145/3663482},
abstract = {Cities serve as vital hubs of economic activity and knowledge generation and dissemination. As such, cities bear a significant responsibility to uphold environmental protection measures while promoting the welfare and living comfort of their residents. There are diverse views on the development of smart cities, from integrating Information and Communication Technologies into urban environments for better operational decisions to supporting sustainability, wealth, and comfort of people. However, for all these cases, data are the key ingredient and enabler for the vision and realization of smart cities. This article explores the challenges associated with smart city data. We start with gaining an understanding of the concept of a smart city, how to measure that the city is a smart one, and what architectures and platforms exist to develop one. Afterwards, we research the challenges associated with the data of the cities, including availability, heterogeneity, management, analysis, privacy, and security. Finally, we discuss ethical issues. This article aims to serve as a “one-stop shop” covering data-related issues of smart cities with references for diving deeper into particular topics of interest.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {88},
numpages = {65},
keywords = {Big data, smart city, urban computing, machine learning, data analysis}
}

@article{10.1145/3663572,
author = {Tang, Shengeng and Xue, Feng and Wu, Jingjing and Wang, Shuo and Hong, Richang},
title = {Gloss-driven Conditional Diffusion Models for Sign Language Production},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3663572},
doi = {10.1145/3663572},
abstract = {Sign Language Production (SLP) aims to convert text or audio sentences into sign language videos corresponding to their semantics, which is challenging due to the diversity and complexity of sign languages, and cross-modal semantic mapping issues. In this work, we propose a Gloss-driven Conditional Diffusion Model (GCDM) for SLP. The core of the GCDM is a diffusion model architecture, in which the sign gloss sequence is encoded by a Transformer-based encoder and input into the diffusion model as a semantic prior condition. In the process of sign pose generation, the textual semantic priors carried in the encoded gloss features are integrated into the embedded Gaussian noise via cross-attention. Subsequently, the model converts the fused features into sign language pose sequences through T-round denoising steps. During the training process, the model uses the ground-truth labels of sign poses as the starting point, generates Gaussian noise through T rounds of noise, and then performs T rounds of denoising to approximate the real sign language gestures. The entire process is constrained by the MAE loss function to ensure that the generated sign language gestures are as close as possible to the real labels. In the inference phase, the model directly randomly samples a set of Gaussian noise, generates multiple sign language gesture sequence hypotheses under the guidance of the gloss sequence, and outputs a high-confidence sign language gesture video by averaging multiple hypotheses. Experimental results on the Phoenix2014T dataset show that the proposed GCDM method achieves competitiveness in both quantitative performance and qualitative visualization.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
keywords = {Sign Language Production, Gloss Semantic Encoding, Diffusion Model, Deep Learning}
}

@article{10.1145/3663759,
author = {Paproki, Anthony and Salvado, Olivier and Fookes, Clinton},
title = {Synthetic Data for Deep Learning in Computer Vision \&amp; Medical Imaging: A Means to Reduce Data Bias},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3663759},
doi = {10.1145/3663759},
abstract = {Deep-learning (DL) performs well in computer-vision and medical-imaging automated decision-making applications. A bottleneck of DL stems from the large amount of labelled data required to train accurate models that generalise well. Data scarcity and imbalance are common problems in imaging applications that can lead DL models towards biased decision making. A solution to this problem is synthetic data. Synthetic data is an inexpensive substitute to real data for improved accuracy and generalisability of DL models. This survey reviews the recent methods published in relation to the creation and use of synthetic data for computer-vision and medical-imaging DL applications. The focus will be on applications that utilised synthetic data to improve DL models by either incorporating an increased diversity of data that is difficult to obtain in real life, or by reducing a bias caused by class imbalance. Computer-graphics software and generative networks are the most popular data generation techniques encountered in the literature. We highlight their suitability for typical computer-vision and medical-imaging applications, and present promising avenues for research to overcome their computational and theoretical limitations.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {271},
numpages = {37},
keywords = {Synthetic data, machine learning, generative adversarial network, deep learning}
}

@article{10.1145/3664612,
author = {Not, Elena and Leonardi, Chiara and L\'{o}pez-De-Ipi\~{n}a, Diego and Silva Palacios, Daniel and S\'{a}nchez-Corcuera, Ruben and Kazhamiakin, Raman and Gerosa, Matteo},
title = {Designing a Digital Environment to Support the Co-production of Public Services: Balancing Multiple Requirements and Governance Concepts},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3664612},
doi = {10.1145/3664612},
abstract = {This paper investigates the challenges of designing a computer supported collaborative environment aimed at facilitating co-production processes, i.e., those collaborative processes between Public Administrations, private stakeholders and citizens that aim at the design of public services, their implementation, and their shared delivery to the community. We argue that, for such a digital platform, different types of socio-technical requirements should be considered, i.e., those related to governance models and associated collaboration dynamics; requirements that may emerge from the specific type of public service to design; as well as user and technical requirements common to all e-government platforms. This research informed the development and testing of a digital collaboration platform that offers guidance on how to organize a co-production initiative and a network of stakeholders, functionalities to support collaborative work, and enablers (in the form of reusable knowledge and digital resources) to perform the sequence of steps to produce a public service. The lessons learned from the iterative platform development process and a preliminary evaluation study conducted with three Public Administrations in different European countries pointed at specific functionalities that are perceived as most crucial and at different appropriation practices that depend on the organizational structure of the involved Public Administrations and related multi-stakeholder networks. The innovation that is brought about with respect to general-purpose platforms for computer supported cooperative work is represented by the operationalization of co-production processes, with step-by-step guidance and potential reuse (with adaptation) of ready to use resources and processes. Based on the results of our research, general guidelines are also proposed for the design of future digital platforms supporting co-production.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {24},
numpages = {30},
keywords = {ICT-enabled co-production, digital collaboration environment, digital platforms design, co-design with public administrations, public services}
}

@article{10.1145/3664923,
author = {Deng, Bobin and Nadendla, Bhargava and Suo, Kun and Xie, Yixin and Lo, Dan Chia-Tien},
title = {Fixed-point Encoding and Architecture Exploration for Residue Number Systems},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3664923},
doi = {10.1145/3664923},
abstract = {Residue Number Systems (RNS) demonstrate the fascinating potential to serve integer addition/ multiplication-intensive applications. The complexity of Artificial Intelligence (AI) models has grown enormously in recent years. From a computer system’s perspective, ensuring the training of these large-scale AI models within an adequate time and energy consumption has become a big concern. Matrix multiplication is a dominant subroutine in many prevailing AI models, with an addition/multiplication-intensive attribute. However, the data type of matrix multiplication within machine learning training typically requires real numbers, which indicates that RNS benefits for integer applications cannot be directly gained by AI training. The state-of-the-art RNS real-number encodings, including floating-point and fixed-point, have defects and can be further enhanced. To transform default RNS benefits to the efficiency of large-scale AI training, we propose a low-cost and high-accuracy RNS fixed-point representation: Single RNS Logical Partition (S-RNS-Logic-P) representation with Scaling-down Postprocessing Multiplication (SD-Post-Mul). Moreover, we extend the implementation details of the other two RNS fixed-point methods: Double RNS Concatenation and S-RNS-Logic-P representation with Scaling-down Preprocessing Multiplication. We also design the architectures of these three fixed-point multipliers. In empirical experiments, our S-RNS-Logic-P representation with SD-Post-Mul method achieves less latency and energy overhead while maintaining good accuracy. Furthermore, this method can easily extend to the Redundant Residue Number System to raise the efficiency of error-tolerant domains, such as improving the error correction efficiency of quantum computing.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {53},
numpages = {27},
keywords = {Residue number system, RNS, fixed-point encoding and computation, RNS multiplier architectures}
}

@article{10.1145/3665138,
author = {Heuillet, Alexandre and Nasser, Ahmad and Arioui, Hichem and Tabia, Hedi},
title = {Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3665138},
doi = {10.1145/3665138},
abstract = {In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS (Differentiable ARchitecTure Search), one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focused specifically on DNAS and reviewed recent approaches in this field. Furthermore, we proposed a novel challenge-based taxonomy to classify DNAS methods. We also discussed the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we concluded by giving some insights into future research directions for the DNAS field.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {270},
numpages = {36},
keywords = {Deep learning, neural architecture search, meta learning}
}

@article{10.1145/3665281,
author = {Volpe, Deborah and Cirillo, Giovanni and Zamboni, Maurizio and Graziano, Mariagrazia and Turvani, Giovanna},
title = {Improving the exploitability of Simulated Adiabatic Bifurcation through a flexible and open-source digital architecture},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665281},
doi = {10.1145/3665281},
abstract = {Combinatorial Optimization (CO) problems exhibit exponential complexity, constraining classical computers from providing fast and satisfactory outcomes. Quantum Computers (QCs) can effectively find optimal or near-optimal solutions by exploring the solutions space of a problem encoded in a qubits system, exploiting principles of quantum mechanics. However, non-idealities and high costs limit their availability. These can be overcome by emulating QCs on cheaper and more accessible classical computing platforms, like Field-Programmable Gate Arrays (FPGAs). This article presents a digital architecture, implementing the Ising-compatible Simulated Adiabatic Bifurcation algorithm. It mimics the quantum adiabatic evolution of a network of non-linear Kerr oscillators. The architecture, described in VHDL and targeting FPGAs, consists of processing elements for computing the Kerr oscillators’ evolution, a set of units considering their Ising-related interactions and an evolution variables update unit. The proposed approach includes a speedup-targeting approximation of the algorithm, a method for handling single-variable constraints, and a software model that allows architecture customization for specific problems. Tests were conducted using an Altera Cyclone V SoC with FPGA logic and the Nios II processor for interface purposes. The results demonstrate the functionality of the architecture and its scalability with the problem size, making it suitable for real-world applications.},
note = {Just Accepted},
journal = {ACM Transactions on Quantum Computing},
month = may,
keywords = {Ising machine, QUBO, optimization, Kerr oscillator, Simulated Bifurcation, Adiabacity}
}

@article{10.1145/3665324,
author = {Bruch, Sebastian and Nardini, Franco Maria and Ingber, Amir and Liberty, Edo},
title = {Bridging Dense and Sparse Maximum Inner Product Search},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3665324},
doi = {10.1145/3665324},
abstract = {Maximum inner product search (MIPS) over dense and sparse vectors have progressed independently in a bifurcated literature for decades; the latter is better known as top- (k)  retrieval in Information Retrieval. This duality exists because sparse and dense vectors serve different end goals. That is despite the fact that they are manifestations of the same mathematical problem. In this work, we ask if algorithms for dense vectors could be applied effectively to sparse vectors, particularly those that violate the assumptions underlying top- (k)  retrieval methods. We study clustering-based approximate MIPS where vectors are partitioned into clusters and only a fraction of clusters are searched during retrieval. We conduct a comprehensive analysis of dimensionality reduction for sparse vectors, and examine standard and spherical k-means for partitioning. Our experiments demonstrate that clustering-based retrieval serves as an efficient solution for sparse MIPS. As byproducts, we identify two research opportunities and explore their potential. First, we cast the clustering-based paradigm as dynamic pruning and turn that insight into a novel organization of the inverted index for approximate MIPS over general sparse vectors. Second, we offer a unified regime for MIPS over vectors that have dense and sparse subspaces, that is robust to query distributions.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {151},
numpages = {38},
keywords = {Maximum inner product search, top-k retrieval, sparse vectors, dense vectors, hybrid vectors, sketching, clustering-based approximate nearest neighbor search}
}

@article{10.1145/3665331,
author = {Gramoli, Lysa and Cumin, Julien and Lacoche, J\'{e}r\'{e}my and Foulonneau, Anthony and Arnaldi, Bruno and Gouranton, Val\'{e}rie},
title = {Generating and Evaluating Data of Daily Activities with an Autonomous Agent in a Virtual Smart Home},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3665331},
doi = {10.1145/3665331},
abstract = {Training machine learning models to identify human behavior is a difficult yet essential task to develop autonomous and adaptive systems such as smart homes. These models require large and diversified amounts of labeled data to be trained effectively. Due to the high variety of home environments and occupant behaviors, collecting datasets that are representative of all possible homes is a major challenge. In addition, privacy and cost are major hurdles to collect real home data. To avoid these difficulties, one solution consists of training these models using purely synthetic data, which can be generated through the simulation of home and their occupants. Two challenges arise from this approach: designing a methodology with a simulation able to generate credible simulated data and evaluating this credibility. In this article, we explain the methodology used to generate diversified synthetic data of daily activities, through the combination of an agent model to simulate an occupant and a simulated 3D house enriched with sensors and effectors to produce such data. We demonstrate the credibility of the generated synthetic data by comparing their efficacy for training human context understanding models against the efficacy generated by real data. To achieve this, we replicate a real dataset collection setting with our smart home simulator. The occupant is replaced by an autonomous agent following the same experimental protocol used for the real dataset collection. This agent is a BDI-based model enhanced with a scheduler designed to offer a balance between control and autonomy. This balance is useful in synthetic data generation since strong constraints can be imposed on the agent to simulate desired situations while allowing autonomous behaviors outside these constraints to generate diversified data. In our case, the constraints are those imposed during the real dataset collection that we want to replicate. The simulated sensors and effectors were configured to react to the agent’s behaviors similarly to the real ones. We experimentally show that data generated from this simulation are valuable for two human context understanding tasks: current human activity recognition and future human activity prediction. In particular, we show that models trained solely with simulated data can give reasonable predictions about real situations occurring in the original dataset. We also report experimental results regarding statistical analysis and C2ST to assess the credibility of generated data. We discuss the generality of our approach for evaluating the credibility of simulated data from their use as training data.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {13},
numpages = {25},
keywords = {Synthetic data, autonomous agent, activities of daily living, human activity recognition}
}

@article{10.1145/3665498,
author = {Wu, Xu and Lai, Zhihui and Zhou, Jie and Hou, Xianxu and Pedrycz, Witold and Shen, Linlin},
title = {Light-Aware Contrastive Learning for Low-Light Image Enhancement},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {9},
issn = {1551-6857},
url = {https://doi.org/10.1145/3665498},
doi = {10.1145/3665498},
abstract = {Low-Light Image Enhancement (LLIE) presents challenges due to texture information loss and uneven illumination, which can distort feature distribution and reduce the quality of the enhanced images. However, current deep learning methods for LLIE only use supervised information from clear images to extract low-light image features, while disregarding the negative information in low-light images (i.e., low illumination and noise). To address these challenges, we propose a novel LLIE method, LACR-VAE, by leveraging the negative information and considering the uneven illumination. In particular, a Light-Aware Contrastive Regularization (LACR) based on contrastive learning is designed to exploit information from both clear and low-light images. The LACR aims to align latent variables of enhanced images with clear images, away from those of low-light images. This allows the method to prioritize essential elements for LLIE and minimize noise and lighting variations. Furthermore, considering the uneven illumination with diverse region sizes and shapes, a Region-CAlibrated Module (RCAM) is present to learn local and global illumination relations among image regions, and an Attention-guided Multi-Scale Module (AMSM) is designed to extract multi-scale features that improve the model’s representation capability. Extensive experiments show that our method achieves superior performance than previous works. Specifically, our method yields a significant enhancement in the National Aeronautics and Space Administration (NASA) testset, achieving an improvement of at least 0.99 in PSNR and 0.0409 in SSIM. Codes and datasets are available at .},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {276},
numpages = {24},
keywords = {Low-light image enhancement, variational autoencoder, contrastive learning, transformer}
}

@article{10.1145/3665643,
author = {Rasch, Ari},
title = {(De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/3665643},
doi = {10.1145/3665643},
abstract = {Data-parallel computations, such as linear algebra routines and stencil computations, constitute one of the most relevant classes in parallel computing, e.g., due to their importance for deep learning. Efficiently de-composing such computations for the memory and core hierarchies of modern architectures and re-composing the computed intermediate results back to the final result—we say (de/re)-composition for short—is key to achieve high performance for these computations on, e.g., GPU and CPU. Current high-level approaches to generating data-parallel code are often restricted to a particular subclass of data-parallel computations and architectures (e.g., only linear algebra routines on only GPU or only stencil computations), and/or the approaches rely on a user-guided optimization process for a well-performing (de/re)-composition of computations, which is complex and error prone for the user.We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of Multi-Dimensional Homomorphisms (MDHs). Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking, and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc.), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world datasets and for a variety of data-parallel computations, including linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.},
journal = {ACM Trans. Program. Lang. Syst.},
month = oct,
articleno = {10},
numpages = {74},
keywords = {Code generation, data parallelism, auto-tuning, GPU, CPU, OpenMP, CUDA, OpenCL, linear algebra, stencils computation, quantum chemistry, data mining, deep learning}
}

@article{10.1145/3665794,
author = {Qiu, Bing and Huo, Jiahao},
title = {Quantitative Stylistic Analysis of Middle Chinese Texts Based on the Dissimilarity of Evolutive Core Word Usage},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3665794},
doi = {10.1145/3665794},
abstract = {Stylistic analysis enables open-ended and exploratory observation of languages. To fill the gap in the quantitative analysis of the stylistic systems of Middle Chinese, we construct lexical features based on the evolutive core word usage and scheme a Bayesian method for feature parameters estimation. The lexical features are from the Swadesh list, each of which has different word forms along with the language evolution during the Middle Ages. We thus count the varied word of those entries along with the language evolution as the linguistic features. With the Bayesian formulation, the feature parameters are estimated to construct a high-dimensional random feature vector to obtain the pair-wise dissimilarity matrix of all the texts based on different distance measures. Finally, we perform the spectral embedding and clustering to visualize, categorize, and analyze the linguistic styles of Middle Chinese texts. The quantitative result agrees with the existing qualitative conclusions and, furthermore, betters our understanding of the linguistic styles of Middle Chinese from both the inter-category and intra-category aspects. It also helps unveil the special styles induced by the indirect language contact.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {97},
numpages = {22},
keywords = {Stylistic analysis, middle chinese, swadesh list, lexical feature}
}

@article{10.1145/3665932,
author = {D\"{o}rpinghaus, Jens and Binnewitt, Johanna and Samray, David and Hein, Kristine},
title = {Understanding Informatics in Continuing Vocational Education and Training Data in Germany},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
url = {https://doi.org/10.1145/3665932},
doi = {10.1145/3665932},
abstract = {Objectives. The purpose of this study is to reveal the importance of informatics in continuing vocational education in Germany. The labour market is a field with diverse data structures and multiple applications, for example connecting jobseekers and trainings or jobs. The labour market heavily relies on vocational education and training and advanced vocational qualification to meet challenges, e.g., digitalization. Study Methods. Since continuing vocational education and training (CVET) is a structurally important lever for the digital transformation of work, this article presents a methodological procedure for content analysis that provides information about the significance of computer science in unregulated continuing education offerings and in formal continuing education regulations. Findings. The question of the extent to which continuing education programs include informaticss topics is investigated, assuming that they can be found in continuing education as cross-cutting topics in a wide variety of thematic contexts. Our results indicating the need for training in computing education. At the same time, computing education offers the highest share of unregulated CVET programs. This could reflect the fact that training and further education regulations in Germany are designed open to technology. Conclusions. We present a novel and unique approach to analyze the importance of informatics and digitalization in CVET advertisements and official regulations for the same.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {36},
numpages = {22},
keywords = {Labor market research, CVET advertisements, educational data mining}
}

@article{10.1145/3670697,
author = {Peng, Yanguo and Liu, Rongqiao and Guo, Jingjing and Gao, Xiyue and Huang, Luyuan and Tu, Yaofeng},
title = {SecCT: Secure and Scalable Count Query Models on Encrypted Genomic Data},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3670697},
doi = {10.1145/3670697},
abstract = {Recently, due to the continued reduction in DNA sequencing cost, large-scale genetic samples are being gathered for accelerating predispositions to specific diseases, tailoring treatment of efficient drugs and therapies, and the like. Massive genetic samples are encrypted-and-then-delegated to a public cloud to both save investment and maintenance costs and prevent the potential leakage of sensitive information. However, such a manner compromises the serviceability of a public cloud, since encryption inevitably breaks the semantic information of genetic samples. Secure count query of single-nucleotide polymorphisms (SNPs), as a kernel component for GWASs and related genomic analysis, is attracting much more attention.Existing methods lack provable security, suffer low efficiency caused by multiple interactions with the cloud, and so on. In this paper, a secure virtual CT-Tree (secure vCT-Tree) is carefully constructed to confuse the tree structure by introducing a hash function and a Paillier system. Furthermore, by delegating the secure vCT-Tree to the cloud, concrete models (i.e., SecCT and SecCT+) are presented to resolve secure count query problems on the fly. SecCT+ is a solution based on trusted execution environment while SecCT is a pure software solution. Both models advance the provable security of genetic research and are proven to be secure under the adaptive chosen keyword (query) attack (IND-CKA2) model. Furthermore, massive experiments are evaluated on realistic data to show the superiority of SecCT and SecCT+.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {21},
numpages = {25},
keywords = {Secure count query, genotypic and phenotypic data, IND-CKA security, scalability}
}

@article{10.1145/3670995,
author = {Jiang, Yiheng and Xu, Yuanbo and Yang, Yongjian and Yang, Funing and Wang, Pengyang and Li, Chaozhuo and Zhuang, Fuzhen and Xiong, Hui},
title = {TriMLP: A Foundational MLP-Like Architecture for Sequential Recommendation},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3670995},
doi = {10.1145/3670995},
abstract = {In this work, we present TriMLP as a foundational MLP-like architecture for the sequential recommendation, simultaneously achieving computational efficiency and promising performance. First, we empirically study the incompatibility between existing purely MLP-based models and sequential recommendation, that the inherent fully-connective structure endows historical user–item interactions (referred as tokens) with unrestricted communications and overlooks the essential chronological order in sequences. Then, we propose the MLP-based Triangular Mixer to establish ordered contact among tokens and excavate the primary sequential modeling capability under the standard auto-regressive training fashion. It contains (1) a global mixing layer that drops the lower-triangle neurons in MLP to block the anti-chronological connections from future tokens and (2) a local mixing layer that further disables specific upper-triangle neurons to split the sequence as multiple independent sessions. The mixer serially alternates these two layers to support fine-grained preferences modeling, where the global one focuses on the long-range dependency in the whole sequence, and the local one calls for the short-term patterns in sessions. Experimental results on 12 datasets of different scales from 4 benchmarks elucidate that TriMLP consistently attains favorable accuracy/efficiency tradeoff over all validated datasets, where the average performance boost against several state-of-the-art baselines achieves up to 14.88\%, and the maximum reduction of inference time reaches 23.73\%. The intriguing properties render TriMLP a strong contender to the well-established RNN-, CNN-, and Transformer-based sequential recommenders. Code is available at .},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {157},
numpages = {34},
keywords = {Sequential recommendation, data mining, multi-layer perceptron}
}

@article{10.1145/3671147,
author = {Ameer, Safwa and Praharaj, Lopamudra and Sandhu, Ravi and Bhatt, Smriti and Gupta, Maanak},
title = {ZTA-IoT: A Novel Architecture for Zero-Trust in IoT Systems and an Ensuing Usage Control Model},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {2471-2566},
url = {https://doi.org/10.1145/3671147},
doi = {10.1145/3671147},
abstract = {Recently, several researchers motivated the need to integrate Zero Trust (ZT) principles when designing and implementing authentication and authorization systems for IoT. An integrated Zero Trust IoT system comprises the network infrastructure (physical and virtual) and operational policies in place for IoT as a product of a ZT architecture plan. This article proposes a novel Zero Trust architecture for IoT systems called ZTA-IoT. Additionally, based on different types of interactions between various layers and components in this architecture, we present ZTA-IoT-ACF, an access control framework that recognizes different interactions that need to be controlled in IoT systems. Within this framework, the article then refines its focus to object-level interactions, i.e., interactions where the target resource is a device (equivalently a thing) or an information file generated or stored by a device. Building on the recently proposed Zero Trust score-based authorization framework (ZT-SAF), we develop the object-level Zero Trust score-based authorization framework for IoT systems, denoted as ZTA-IoT-OL-SAF, to govern access requests in this context. With this machinery in place, we finally develop a novel usage control model for users-to-objects and devices-to-objects interactions, denoted as UCON  (_{IoT}) . We give formal definitions, illustrative use cases, and a proof-of-concept implementation of UCON (_{IoT}) . This article is a first step toward establishing a rigorous formally defined score-based access control framework for Zero Trust IoT systems.},
journal = {ACM Trans. Priv. Secur.},
month = aug,
articleno = {22},
numpages = {36},
keywords = {Access control, zero trust, cloud-enabled IoT, UCON}
}

@article{10.1145/3672076,
author = {Mamish, John and Alharbi, Rawan and Sen, Sougata and Holla, Shashank and Kamath, Panchami and Sangar, Yaman and Alshurafa, Nabil and Hester, Josiah},
title = {NIR-sighted: A Programmable Streaming Architecture for Low-Energy Human-Centric Vision Applications},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3672076},
doi = {10.1145/3672076},
abstract = {Human studies often rely on wearable lifelogging cameras that capture videos of individuals and their surroundings to aid in visual confirmation or recollection of daily activities like eating, drinking, and smoking. However, this may include private or sensitive information that may cause some users to refrain from using such monitoring devices. Also, short battery lifetime and large form factors reduce applicability for long-term capture of human activity. Solving this triad of interconnected problems is challenging due to wearable embedded systems’ energy, memory, and computing constraints. Inspired by this critical use case and the unique design problem, we developed NIR-sighted, an architecture for wearable video cameras that navigates this design space via three key ideas: (i)&nbsp;reduce storage and enhance privacy by discarding masked pixels and frames, (ii)&nbsp;enable programmers to generate effective masks with low computational overhead, and (iii)&nbsp;enable the use of small MCUs by moving masking and compression off-chip. Combined together in an end-to-end system, NIR-sighted’s masking capabilities and off-chip compression hardware shrinks systems, stores less data, and enables programmer-defined obfuscation to yield privacy enhancement. The user’s privacy is enhanced significantly as nowhere in the pipeline is any part of the image stored before it is obfuscated. We design a wearable camera called NIR-sightedCam based on this architecture; it is compact and can record IR and grayscale video at 16 and 20+ fps, respectively, for 26 hours nonstop (59 hours with IR disabled) at a fraction of comparable platforms power draw. NIR-sightedCam includes a low-power Field Programmable Gate Array that implements our mJPEG compress/obfuscate hardware, Blindspot. We additionally show the potential for privacy-enhancing function and clinical utility via an in-lab eating study, validated by a nutritionist.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {101},
numpages = {26},
keywords = {Human-Centric Vision Applications}
}

@article{10.1145/3672082,
author = {Serra, Flavia and Peralta, Ver\'{o}nika and Marotta, Adriana and Marcel, Patrick},
title = {Use of Context in Data Quality Management: A Systematic Literature Review},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3672082},
doi = {10.1145/3672082},
abstract = {The importance of context in data quality (DQ) was shown many years ago and nowadays is widely accepted. Early approaches and surveys defined DQ as fitness for use and showed the influence of context on DQ. This article presents a Systematic Literature Review (SLR) for investigating how context is taken into account in recent proposals for DQ management (DQM). We specifically present the planning and execution of the SLR, the analysis criteria and our results reflecting the relationship between context and DQ in the state-of-the-art and, particularly, how this context is defined and used for DQM. The SLR is instrumental to the identification of context components and the design of a context formal model.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {19},
numpages = {41},
keywords = {Systematic literature review, data quality, context, data quality methodology}
}

@article{10.1145/3672614,
author = {Mathieu, Claire and Rajaraman, Rajmohan and Young, Neal E. and Yousefi, Arman},
title = {Competitive Data-Structure Dynamization},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1549-6325},
url = {https://doi.org/10.1145/3672614},
doi = {10.1145/3672614},
abstract = {Data-structure dynamization is a general approach for making static data structures dynamic. It is used extensively in geometric settings and in the guise of so-called merge (or compaction) policies in big-data databases such as LevelDB and Google Bigtable. Previous theoretical work is based on worst-case analyses for uniform inputs—insertions of one item at a time and non-varying read rate. In practice, merge policies must not only handle batch insertions and varying read/write ratios, they can take advantage of such non-uniformity to reduce cost on a per-input basis. To model this, we initiate the study of data-structure dynamization through the lens of competitive analysis via two new online set-cover problems. For each, the input is a sequence of disjoint sets of weighted items. The sets are revealed one at a time. The algorithm must respond to each with a set cover that covers all items revealed so far. It obtains the cover incrementally from the previous cover by adding one or more sets and optionally removing existing sets. For each new set the algorithm incurs build cost equal to the weight of the items in the set. In the first problem the objective is to minimize total build cost plus total query cost, where the algorithm incurs a query cost at each time  (t)  equal to the current cover size. In the second problem, the objective is to minimize the build cost while keeping the query cost from exceeding  (k)  (a given parameter) at any time. We give deterministic online algorithms for both variants, with competitive ratios of  (Theta(log^{*}n))  and  (k) , respectively. The latter ratio is optimal for the second variant.},
journal = {ACM Trans. Algorithms},
month = oct,
articleno = {37},
numpages = {28},
keywords = {Online algorithms, competitive analysis, data-structure dynamization, log-structured merge-tree, compaction}
}

@article{10.1145/3673226,
author = {Uhrmacher, Adelinde M and Frazier, Peter and H\"{a}hnle, Reiner and Kl\"{u}gl, Franziska and Lorig, Fabian and Lud\"{a}scher, Bertram and Nenzi, Laura and Ruiz-Martin, Cristina and Rumpe, Bernhard and Szabo, Claudia and Wainer, Gabriel and Wilsdorf, Pia},
title = {Context, Composition, Automation, and Communication: The C2AC Roadmap for Modeling and Simulation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3673226},
doi = {10.1145/3673226},
abstract = {Simulation has become, in many application areas, a sine qua non. Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.},
journal = {ACM Trans. Model. Comput. Simul.},
month = aug,
articleno = {23},
numpages = {51},
keywords = {Modeling, simulation, state of the art, open challenges, reuse, composition, communication, reproducibility, automation, intelligent modeling and simulation lifecycle}
}

@article{10.1145/3673653,
author = {Olgun, Ataberk and Bostanci, F. Nisa and Francisco de Oliveira Junior, Geraldo and Tugrul, Yahya Can and Bera, Rahul and Yaglikci, Abdullah Giray and Hassan, Hasan and Ergin, Oguz and Mutlu, Onur},
title = {Sectored DRAM: A Practical Energy-Efficient and High-Performance Fine-Grained DRAM Architecture},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3673653},
doi = {10.1145/3673653},
abstract = {Modern computing systems access data in main memory at coarse granularity (e.g., at 512-bit cache block granularity). Coarse-grained access leads to wasted energy because the system does not use all individually accessed small portions (e.g., words, each of which typically is 64 bits) of a cache block. In modern DRAM-based computing systems, two key coarse-grained access mechanisms lead to wasted energy: large and fixed-size (i) data transfers between DRAM and the memory controller and (ii) DRAM row activations. We propose Sectored DRAM, a new, low-overhead DRAM substrate that reduces wasted energy by enabling fine-grained DRAM data transfer and DRAM row activation. To retrieve only useful data from DRAM, Sectored DRAM exploits the observation that many cache blocks are not fully utilized in many workloads due to poor spatial locality. Sectored DRAM predicts the words in a cache block that will likely be accessed during the cache block’s residency in cache and (i) transfers only the predicted words on the memory channel by dynamically tailoring the DRAM data transfer size for the workload and (ii) activates a smaller set of cells that contain the predicted words by carefully operating physically isolated portions of DRAM rows (i.e., mats). Activating a smaller set of cells on each access relaxes DRAM power delivery constraints and allows the memory controller to schedule DRAM accesses faster.We evaluate Sectored DRAM using 41 workloads from widely used benchmark suites. Compared to a system with coarse-grained DRAM, Sectored DRAM reduces the DRAM energy consumption of highly memory intensive workloads by up to (on average) 33\% (20\%) while improving their performance by up to (on average) 36\% (17\%). Sectored DRAM’s DRAM energy savings, combined with its system performance improvement, allows system-wide energy savings of up to 23\%. Sectored DRAM’s DRAM chip area overhead is 1.7\% of the area of a modern DDR4 chip. Compared to state-of-the-art fine-grained DRAM architectures, Sectored DRAM greatly reduces DRAM energy consumption, does not reduce DRAM bandwidth, and can be implemented with low hardware cost. Sectored DRAM provides 89\% of the performance benefits of, consumes 12\% less DRAM energy than, and takes up 34\% less DRAM chip area than a high-performance state-of-the-art fine-grained DRAM architecture (Half-DRAM). It is our hope and belief that Sectored DRAM’s ideas and results will help to enable more efficient and high-performance memory systems. To this end, we open source Sectored DRAM at https://github.com/CMU-SAFARI/Sectored-DRAM.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {60},
numpages = {29},
keywords = {High-performance, DRAM, energy efficiency, fine-grained DRAM access}
}

@article{10.1145/3674502,
author = {Charrondi\`{e}re, Rapha\"{e}l and Neukirch, S\'{e}bastien and Bertails-Descoubes, Florence},
title = {MERCI: Mixed Curvature-Based Elements for Computing Equilibria of Thin Elastic Ribbons},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3674502},
doi = {10.1145/3674502},
abstract = {Thin elastic ribbons represent a class of intermediary objects lying in-between thin elastic plates and thin elastic rods. Although the two latter families of thin structures have received much interest from the Computer Graphics community over the last decades, ribbons have seldom been considered and modelled numerically so far, in spite of a growing number of applications in Computer Design. In this article, starting from the reduced developable ribbon models&nbsp;[Sadowsky 1929; Wunderlich 1962] recently popularised in Soft Matter Physics, we propose a both accurate and efficient algorithm for computing the statics of thin elastic ribbons. Inspired by the super-clothoid model for thin elastic rods, our method relies on compact ribbon elements whose normal curvature varies linearly with respect to arc length s, while their geodesic torsion is quadratic in s. In contrast, however, for the sake of efficiency, our algorithm avoids building a fully reduced kinematic chain and instead treats each element independently, gluing them only at the final solving stage through well-chosen bilateral constraints.Thanks to this mixed variational strategy, which yields a banded Hessian, our algorithm recovers the linear complexity of low-order models while preserving the high-order convergence of curvature-based models. As a result, our approach is scalable to a large number of elements, and suitable for various boundary conditions and unilateral contact constraints, making it possible to handle challenging scenarios such as confined buckling experiments or M\"{o}bius bands with contact. Remarkably, our mixed algorithm proves an order of magnitude faster compared to Discrete Elastic Ribbon models of the literature while achieving, in a few seconds only, high accuracy levels that remain out of reach for such low-order models. Additionally, our numerical model can incorporate various ribbon energies, including the RibExt model for quasi-developable ribbons recently introduced in Physics&nbsp;[Audoly and Neukirch 2021], which allows to transition smoothly between a rectangular Kirchhoff rod and a (developable) Sadowsky ribbon. Our numerical scheme is carefully validated against demanding experiments of the Physics literature, which demonstrates its accuracy, efficiency, robustness, and versatility.Our Merci code is publicly available at https://gitlab.inria.fr/elan-public-code/merci for the sake of reproducibility and future benchmarking.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {160},
numpages = {26},
keywords = {Thin elastic ribbon, curvature-based element, constraints, contact, m\"{o}bius band}
}

@article{10.1145/3674628,
author = {Elsman, Martin},
title = {Double-Ended Bit-Stealing for Algebraic Data Types},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ICFP},
url = {https://doi.org/10.1145/3674628},
doi = {10.1145/3674628},
abstract = {Algebraic data types are central to the development and evaluation of most functional programs. It is therefore important for compilers to choose compact and efficient representations of such types, in particular to achieve good memory footprints for applications.
 
 
 

 
 
 
Algebraic data types are most often represented using blocks of memory where the first word is used as a so-called tag, carrying information about the constructor, and the following words are used for carrying the constructor's arguments. As an optimisation, lists are usually represented more compactly, using a technique called bit-stealing, which, in its simplest form, uses the word-alignment property of pointers to byte-addressed allocated memory to discriminate between the nil constructor (often represented as 0x1) and the cons constructor (aligned pointer to allocated pair). Because the representation supports that all values can be held uniformly in one machine word, possibly pointing to blocks of memory, type erasure is upheld.
 
 
 

 
 
 
However, on today's 64-bit architectures, memory addresses (pointers) are represented using only a subset of the 64 bits available in a machine word, which leave many bits unused. In this paper, we explore the use, not only of the least-significant bits of pointers, but also of the most-significant bits, for representing algebraic data types in a full ML compiler. It turns out that, with such a particular utilisation of otherwise unused bits, which we call double-ended bit-stealing, it is possible to choose unboxed representations for a large set of data types, while still not violating the principle of uniform data-representations. Examples include Patricia trees, union-find data structures, stream data types, internal language representations for types and expressions, and mutually recursive ASTs for full language definitions.
 
 
 

 
 
 
The double-ended bit-stealing technique is implemented in the MLKit compiler and speedup ranges from 0 to 26 percent on benchmarks that are influenced by the technique. For MLKit, which uses abstract data types extensively, compilation speedups of around 9 percent are achieved for compiling MLton (another Standard ML compiler) and for compiling MLKit itself.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {239},
numpages = {33},
keywords = {Compilation, Data-type representations, Functional languages, Unboxing}
}

@article{10.1145/3674726,
author = {Zhu, Zhouruixing and Lee, Cheryl and Tang, Xiaoying and He, Pinjia},
title = {HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3674726},
doi = {10.1145/3674726},
abstract = {Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {200},
numpages = {25},
keywords = {Root cause analysis, microservices, traces, metrics}
}

@article{10.1145/3674734,
author = {Antoniou, Georgia and Bartolini, Davide and Volos, Haris and Kleanthous, Marios and Wang, Zhe and Kalaitzidis, Kleovoulos and Rollet, Tom and Li, Ziwei and Mutlu, Onur and Sazeides, Yiannakis and Haj Yahya, Jawad},
title = {Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3674734},
doi = {10.1145/3674734},
abstract = {Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70\% with limited performance degradation (at most 2\%).},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {66},
numpages = {26},
keywords = {Power management, idle states, C-states, datacenters, microservices}
}

@article{10.1145/3674849,
author = {Wakigami, Kazuya and Machida, Fumio and Phung-Duc, Tuan},
title = {Empirical Architecture Comparison of Two-input Machine Learning Systems for Vision Tasks},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3674849},
doi = {10.1145/3674849},
abstract = {As machine learning models have been deployed in many vision systems, including autonomous vehicles and robots, designing architectures for machine learning systems (MLSs) has emerged as a critical concern. Previous studies have shown that enhancing the reliability of MLS outputs can be achieved by comparing multiple inference results on distinct inputs. Nevertheless, the architectures facilitating multiple inferences incur non-negligible performance overhead and energy consumption that have been less investigated. This article delves into the trade-offs among reliability, performance, and energy efficiency of architectures for two-input MLSs through real experiments conducted on image classification and object detection tasks. Specifically, we scrutinize the comparison between parallel- and shared-type architectures of two-input MLSs for vision tasks. The experiments confirm that the shared-type architecture can achieve a shorter response time and smaller energy consumption by using a shared machine learning module for both image classification and object detection tasks. However, the parallel-type architecture can benefit the redundant machine learning modules for improving throughput and fault tolerance. Our empirical results also show the service time distributions of image classification and object detection tasks fit well with a log-normal distribution and a mixture of the Gaussian model, respectively.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {22},
numpages = {19},
keywords = {Energy consumption, machine learning system, performance, reliability, simulation}
}

@article{10.1145/3675393,
author = {Perera, Judith and Tempero, Ewan and Tu, Yu-Cheng and Blincoe, Kelly},
title = {A Systematic Mapping Study Exploring Quantification Approaches to Code, Design, and Architecture Technical Debt},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3675393},
doi = {10.1145/3675393},
abstract = {To effectively manage Technical Debt (TD), we need reliable means to quantify it. We conducted a Systematic Mapping Study (SMS) where we identified 39 quantification approaches for Code, Design, and Architecture TD. We analyzed concepts and metrics discussed in these quantification approaches by classifying the quantification approaches based on a set of abstract TD Quantification (TDQ) concepts and their high-level themes, process/time, cost, benefit, probability, and priority, which we developed during our SMS. This helped identify gaps in the literature and to propose future research directions. Among the abstract TDQ concepts discussed in the different quantification approaches, TD item, TD remediation cost, TD interest, and Benefit of remediating TD were the most frequently discussed concepts. They were also supported by some form of measurement. However, some TDQ concepts were poorly examined, for example, the benefit of taking TD. It was evident that cost concepts were more frequently quantified among the approaches, while benefit concepts were not. Most of the approaches focused on remediating TD in retrospect rather than quantifying TD to strategically use it during software development. This raises the question of whether existing approaches reliably quantify TD and suggests the need to further explore TDQ.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {177},
numpages = {44},
keywords = {Technical debt management, technical debt quantification, technical debt measurement, software quality}
}

@article{10.1145/3676279,
author = {Konstantinidis, Ioannis and Kapantai, Eleni and Michailidis, Alexios and Deligiannis, Athanasios and Berberidis, Christos and Magnisalis, Ioannis and Peristeras, Vassilios},
title = {From document-centric to data-centric public service provision},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3676279},
doi = {10.1145/3676279},
abstract = {The profound digitization of public administration over recent decades has not eliminated information exchange via paper or electronic documents and certificates. We argue that a paradigm shift from document-centric to data-centric public service provision is needed and is feasible today with the exploitation of emerging technologies. We explore frameworks, architectures, benefits, and challenges in transforming document-centric administration processes into integrated, granular data exchange. A conceptual architecture for public service provision is proposed to extract preconditions from legislation, map the needed evidence to the service requirements, standardize evidence types, and integrate authoritative data sources. While promoting efficiency, privacy, and innovation, this shift faces technical and organizational challenges as the analysis of the “National Registry of Administrative Public Services” in Greece reveals. Further research on aligning policies, upholding trust, and coordinating institutional processes is warranted.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {28},
numpages = {27},
keywords = {data, public service, digital transformation, AI, LLMs, Knowledge Graphs}
}

@article{10.1145/3676961,
author = {Yang, Xu and Rajbahadur, Gopi krishnan and Lin, Dayi and Wang, Shaowei and Jiang, Zhen Ming (Jack)},
title = {SimClone: Detecting Tabular Data Clones Using Value Similarity},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3676961},
doi = {10.1145/3676961},
abstract = {Data clones are defined as multiple copies of the same data among datasets. The presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. The majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this article, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20\% in terms of both F1-score and AUC. In addition, SimClone’s visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {17},
numpages = {27},
keywords = {Data clone, Tabular clone, Value Similarity, Machine learning datasets}
}

@article{10.1145/3677057,
author = {Hougaard, Bastian Ils\o{} and Knoche, Hendrik},
title = {Aiming, Pointing, Steering: A Core Task Analysis Framework for Gameplay},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3677057},
doi = {10.1145/3677057},
abstract = {Underneath their compelling audiovisual surface, games require players to carry out mundane interaction work, such as pointing, typing, or steering. However, many of these underlying building blocks are not defined rigorously, hampering synthesis and analysis. We elaborate on the origin of tasks within human-computer interaction (HCI) and define tasks' relationship to game terminology (game mechanics, goals, and actions). Our proposed framework draws on systemic-structural theory of activity to aid systematic analysis and exploration of game design by mapping gameplay to abstract core tasks. The framework contains four task tools, applicable when 1) uncovering design properties, 2) designing experimental manipulation, 3) creating behavioral measurements, and 4) describing gameplay in literature reviews of game genres and design techniques. We evaluated our framework as a lens to design purposeful games in three case studies within a scientific education. We invite researchers and practitioners to employ the framework as a microscope, to describe and design games rigorously.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {292},
numpages = {48},
keywords = {abstraction, action, activity theory, core task, design landscape, feedback, game design, gameplay, imperative goals, mechanics, ontology, task analysis, task definition}
}

@article{10.1145/3677107,
author = {Hammad, Noor and Harpstead, Erik and Hammer, Jessica},
title = {Towards a Design Framework for Data-Driven Game Streaming: A Multi-Stakeholder Approach},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3677107},
doi = {10.1145/3677107},
abstract = {Research on live streaming systems that incorporate real-time data, such as game or viewer data, have been a topic of HCI research for some time. Despite the potential of data-driven game streaming interfaces, translating this research into practice faces two key challenges. First, the design space afforded by data-driven game streaming systems is not yet well understood, making it difficult to identify how designs might meet users' existing and potential needs. Second, adoption of these systems relies on engagement with the entire streaming ecosystem, which includes developers, streamers, moderators, and viewers, rather than with just one group. Through a two-phase design study, we investigate the expectations, desires, and experiences of streaming stakeholders, shedding light on how data-driven game streaming systems can meet their needs. Building upon these insights and drawing upon previous research, we propose a design framework aimed at analyzing and generating data-driven game streaming designs, thereby moving toward formalizing the design and development of such systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {342},
numpages = {28},
keywords = {game design, game development, live streaming, participatory design, twitch}
}

@article{10.1145/3677139,
author = {Ma, Lei and Cao, Lei and VanNostrand, Peter M. and Hofmann, Dennis M. and Su, Yao and Rundensteiner, Elke A.},
title = {Pluto: Sample Selection for Robust Anomaly Detection on Polluted Log Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3677139},
doi = {10.1145/3677139},
abstract = {Log anomaly detection, critical in identifying system failures and preempting security breaches, finds irregular patterns within large volumes of log data. Modern log anomaly detectors rely on training deep learning models on clean anomaly-free log data. However, such clean log data requires expensive and tedious human labeling. In this paper, we thus propose a robust log anomaly detection framework, PlutoNOSPACE, that automatically selects a clean representative sample subset of the polluted log sequence data to train a Transformer-based anomaly detection model. Pluto features three innovations. First, due to localized concentrations of anomalies inherent in the embedding space of log data, Pluto partitions the sequence embedding space generated by the model into regions that then allow it to identify and discard regions that are highly polluted by our pollution level estimation scheme, based on our pollution quantification via Gaussian mixture modeling. Second, for the remaining more slightly polluted regions, we select samples that maximally purify the eigenvector spectrum, which can be transformed into the NP-hard facility location problem; allowing us to leverage its greedy solution with a (1-(1/e)) approximation guarantee in optimality. Third, by iteratively alternating between the above subset selection, a model re-training on the latest subset, and a subset filtering using dynamic training artifacts generated by the latest model, the data selected is progressively refined. The final sample set is used to retrain the final anomaly detection model. Our experiments on four real-world log benchmark datasets demonstrate that by retaining 77.7\% (BGL) to 96.6\% (ThunderBird) of the normal sequences while effectively removing 90.3\% (BGL) to 100.0\% (ThunderBird, HDFS) of the anomalies, Pluto provides a significant absolute F-1 improvement up to 68.86\% (2.16\% → 71.02\%) compared to the state-of-the-art sample selection methods. The implementation of this work is available at https://github.com/LeiMa0324/Pluto-SIGMOD25.},
journal = {Proc. ACM Manag. Data},
month = sep,
articleno = {203},
numpages = {25},
keywords = {anomaly detection, log sequence, polluted data, sample selection}
}

@article{10.1145/3677318,
author = {Wang, Xingbin and Zhao, Boyan and Su, Yulan and Zhang, Sisi and Yuan, Fengkai and Zhang, Jun and Meng, Dan and Hou, Rui},
title = {A Hybrid Sparse-dense Defensive DNN Accelerator Architecture against Adversarial Example Attacks},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3677318},
doi = {10.1145/3677318},
abstract = {Understanding how to defend against adversarial attacks is crucial for ensuring the safety and reliability of these systems in real-world applications. Various adversarial defense methods are proposed, which aim at improving the robustness of neural networks against adversarial attacks by changing the model structure, adding detection networks, and adversarial purification network. However, deploying adversarial defense methods in existing DNN accelerators or defensive accelerators leads to many key issues. To address these challenges, this article proposes sDNNGuard, an elastic heterogeneous DNN accelerator architecture that can efficiently orchestrate the simultaneous execution of original (target) DNN networks and the detect algorithm or network. It not only supports for dense DNN detect algorithms, but also allows for sparse DNN defense methods and other mixed dense-sparse (e.g., dense-dense and sparse-dense) workloads to fully exploit the benefits of sparsity. sDNNGuard with a CPU core also supports the non-DNN computing and allows the special layer of the neural network, and used for the conversion for sparse storage format for weights and activation values. To reduce off-chip traffic and improve resources utilization, a new hardware abstraction with elastic on-chip buffer/computing resource management is proposed to achieve dynamical resource scheduling mechanism. We propose an extended AI instruction set for neural networks synchronization, task scheduling and efficient data interaction. Experiment results show that sDNNGuard can effectively validate the legitimacy of the input samples in parallel with the target DNN model, achieving an average 1.42\texttimes{} speedup compared with the state-of-the-art accelerators.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {79},
numpages = {28},
keywords = {DNN, DNN accelerator, adversarial example defense, instruction set}
}

@article{10.1145/3678006,
author = {Liu, Changxu and Zhou, Hao and Dai, Patrick and Shang, Li and Yang, Fan},
title = {PriorMSM: An Efficient Acceleration Architecture for Multi-Scalar Multiplication},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3678006},
doi = {10.1145/3678006},
abstract = {Multi-Scalar Multiplication (MSM) is a computationally intensive task that operates on elliptic curves based on GF(P). It is commonly used in zero-knowledge proof (ZKP), where it accounts for a significant portion of the computation time required for proof generation. In this article, we present PriorMSM, an efficient acceleration architecture for MSM. We propose a Priority-Based Scheduling Mechanism (PBSM) based on a multi-FIFO and multi-bank architecture to accelerate the implementation of MSM. By increasing the pairing success rate of internal points, PBSM reduces the number of bubbles in the pipeline of point addition (PADD), consequently improving the data throughput of the pipeline. We also introduce an advanced parallel bucket aggregation algorithm, leveraging PADD’s fully pipelined characteristics to significantly accelerate the implementation of bucket aggregation. We perform a sensitivity analysis on the crucial parameter of window size in MSM. The results indicate that the window size of the MSM significantly impacts its latency. Area-Time Product (ATP) metric is introduced to guide the selection of the optimal window size, balancing the performance and cost for practical applications of subsequent MSM implementations. PriorMSM is evaluated using the TSMC 28 nm process. It achieves a maximum speedup of 10.9\texttimes{} compared to the previous custom hardware implementations and a maximum speedup of 3.9\texttimes{} compared to the GPU implementations.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {77},
numpages = {26},
keywords = {Multi-scalar&nbsp;multiplication, zero-knowledge proof, privacy-preserving&nbsp;computing, ASIC&nbsp;design}
}

@article{10.1145/3678009,
author = {Wu, Jiang and Zhang, Zhuo and Yang, Deheng and Xu, Jianjun and He, Jiayu and Mao, Xiaoguang},
title = {Time-Aware Spectrum-Based Bug Localization for Hardware Design Code with Data Purification},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3678009},
doi = {10.1145/3678009},
abstract = {The verification of hardware design code is a critical aspect in ensuring the quality and reliability of hardware products. Finding bugs in hardware design code is important for hardware development and is frequently considered as a notoriously challenging and time-consuming activity while being an essential aspect of verification. Thus, bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, there exists an unpredictable time span between the precise origin of a bug and its detected manifestation in prior work without costly formal verification. Locating the bug responsible for the exposed discrepancy between expected and exhibited design behavior remains a major challenge. In this work, we propose Tartan, a Time-aware spectrum-based bug localization with data purification for hardware design code to address these limitations. Tartan integrates hardware-specific timing information with the spectrum and captures the changes of executed statements when the state of the circuit changes to effectively locate bugs. Further, Tartan purifies the spectrum data from the simulation and evaluates the suspiciousness of the statements in the design to indicate the likelihood of being buggy. To evaluate the effectiveness of Tartan, we conduct large-scale experiments on 69 versions of 15 hardware projects by the state-of-the-art bug localization techniques. The experimental results clearly show that Tartan is statistically more effective than the baselines. It provides a new perspective on hardware design code bug localization and brings fresh insights to the community.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {64},
numpages = {25},
keywords = {Automatic bug localization, time-aware, spectrum, hardware design code, data purification}
}

@article{10.1145/3680277,
author = {Zhao, Yao and Qu, Youyang and Xiang, Yong and Uddin, Md Palash and Peng, Dezhong and Gao, Longxiang},
title = {A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals and Future Trends},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3680277},
doi = {10.1145/3680277},
abstract = {Recent advances in edge computing&nbsp;(EC) have pushed cloud-based data caching services to edge; however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV), which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {8},
numpages = {34},
keywords = {Edge data integrity verification, edge computing, security, Internet of Things}
}

@article{10.1145/3680547,
author = {Yi, Chenglong and Liu, Jintong and Wan, Shenggang and Fang, Juntao and Sun, Bin and Zhang, Liqiang},
title = {Data Deduplication Based on Content Locality of Transactions to Enhance Blockchain Scalability},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3680547},
doi = {10.1145/3680547},
abstract = {Blockchain is a promising infrastructure for the internet and digital economy, but it has serious scalability problems, that is, long block synchronization time and high storage cost. Conventional coarse-grained data deduplication schemes (block or file level) are proved to be ineffective on improving the scalability of blockchains. Based on comprehensive analysis on typical blockchain workloads, we propose two new locality concepts (economic and argument locality) and a novel fine-grained data deduplication scheme (transaction level) named Alias-Chain. Specifically, Alias-Chain replaces frequently used data, for example, smart contract arguments, with much shorter aliases to reduce the block sizes, which results in both shorter synchronization time and lower storage cost. Furthermore, to solve the potential consistency issue in Alias-Chain, we propose two complementary techniques: one is generating aliases from history blocks with high consistency, and the other is speeding up the generation of aliases via a specific algorithm. Our simulation results show: (1) the average transfer and SC-call transaction (a transaction used to call the smart contracts in the blockchain) sizes can be significantly reduced by up to 11.03\% and 79.44\% in native Ethereum, and up to 39.29\% and 81.84\% in Ethereum optimized by state-of-the-art techniques; and (2) the two complementary techniques well address the inconsistency risk with very limited impact on the benefit of Alias-Chain. Prototyping-based experiments are further conducted on a testbed consisting of up to 3200 miners. The results demonstrate the effectiveness and efficiency of Alias-Chain on reducing block synchronization time and storage cost under typical real-world workloads.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {74},
numpages = {24},
keywords = {Blockchain, scalability, data deduplication}
}

@article{10.1145/3685929,
author = {Protick, Taufiq Islam and Sabir, Aafaq and Abhinaya, Sb and Bartlett, Aiden and Das, Anupam},
title = {Unveiling Users’ Security and Privacy Concerns Regarding Smart Home IoT Products from Online Reviews},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3685929},
doi = {10.1145/3685929},
abstract = {The Internet of Things (IoT) has revolutionized the global market with lifestyle products such as fitness trackers (FT), smart home speakers (SHS), and surveillance and security camera systems (SSCS). While offering convenience, these products also introduce potential security and privacy (S&amp;P) risks to buyers, often going unnoticed. Consumers’ incomplete mental models of the risks involved and the information asymmetry between buyers and sellers only add to the problem. Understanding consumer concerns in online product reviews can play a crucial role in bridging the gap of such information asymmetry. By establishing a balanced flow of information between buyers and sellers, manufacturers can leverage genuine concerns expressed in reviews to enhance product features while educating users about misinformation in reviews. In this study, we collected FT, SHS, and SSCS product reviews from three Amazon domains: the US, the UK, and India. Using a keyword-based search method focused on S&amp;P concerns, we discovered a considerable number of reviews expressing notable concerns regarding security and privacy. Our qualitative analysis revealed that data security is a common concern across all product types. Further, our quantitative analysis exposed significant geographic variations, with the concern ratio being higher in the US than in the UK for all device types and higher than in the Indian domain for security cameras. These findings highlight the need for tailored security measures and user awareness campaigns in different parts of the world to address the identified concerns effectively.},
journal = {ACM J. Comput. Sustain. Soc.},
month = nov,
articleno = {44},
numpages = {41},
keywords = {IoT devices, mixed-method analysis, security and privacy concerns}
}

@article{10.1145/3687124,
author = {Khraisat, Ansam and Alazab, Ammar and Singh, Sarabjot and Jan, Tony and Jr. Gomez, Alfredo},
title = {Survey on Federated Learning for Intrusion Detection System: Concept, Architectures, Aggregation Strategies, Challenges, and Future Directions},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3687124},
doi = {10.1145/3687124},
abstract = {Intrusion Detection Systems (IDS) are essential for securing computer networks by identifying and mitigating potential threats. However, traditional IDS face challenges related to scalability, privacy, and computational demands as network data complexity increases. Federated Learning (FL) has emerged as a promising solution, enabling collaborative model training on decentralized data sources while preserving data privacy. Each participant retains local data repositories, ensuring data sovereignty and precluding data sharing. Leveraging the FL framework, participants locally train machine learning models on their respective datasets, subsequently transmitting model updates to a central server for aggregation. The central server then disseminates the aggregated model updates to individual participants, collectively striving to bolster intrusion detection capabilities. This article presents a comprehensive survey of FL applications in IDS, covering core concepts, architectural approaches, and aggregation strategies. We evaluate the strengths and limitations of various FL methodologies for IDS, addressing privacy and security concerns and exploring privacy-preserving techniques and security protocols. Our examination of aggregation strategies within the FL framework for IDS aims to highlight their effectiveness, limitations, and potential enhancements.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {7},
numpages = {38},
keywords = {Intrusion detection systems, federated learning, privacy preservation, network security}
}

@article{10.1145/3687265,
author = {Gao, Yicheng and Casale, Giuliano and Singhal, Rekha},
title = {Performance Modeling of Distributed Data Processing in Microservice Applications},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3687265},
doi = {10.1145/3687265},
abstract = {Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = oct,
articleno = {14},
numpages = {30},
keywords = {Microservice, performance modeling, data placement, data caching, layered network}
}

@article{10.1145/3687301,
author = {Goedegebuure, Abel and Kumara, Indika and Driessen, Stefan and Van Den Heuvel, Willem-Jan and Monsieur, Geert and Tamburri, Damian Andrew and Nucci, Dario Di},
title = {Data Mesh: A Systematic Gray Literature Review},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3687301},
doi = {10.1145/3687301},
abstract = {Data mesh is an emerging domain-driven decentralized data architecture that aims to minimize or avoid operational bottlenecks associated with centralized, monolithic data architectures in enterprises. The topic has piqued the practitioners’ interest, and considerable gray literature exists. At the same time, we observe a lack of academic attempts at defining and building upon the concept. Hence, in this article, we aim to start from the foundations and characterize the data mesh architecture regarding its design principles, architectural components, capabilities, and organizational roles. We systematically collected, analyzed, and synthesized 114 industrial gray literature articles. The resulting review provides insights into practitioners’ perspectives on the four key principles of data mesh: data as a product, domain ownership of data, self-serve data platform, and federated computational governance. Moreover, due to the comparability of data mesh and SOA (service-oriented architecture), we mapped the findings from the gray literature into the reference architectures from the SOA academic literature to create the reference architectures for describing three key dimensions of data mesh: organization of capabilities and roles, development, and runtime. Finally, we discuss open research issues in data mesh, partially based on the findings from the gray literature.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {11},
numpages = {36},
keywords = {Data mesh, principles, reference architectures, research challenges, gray literature review, data architecture, data management}
}

@article{10.1145/3687303,
author = {Madan, Anjum and Kumar, Devender},
title = {CNN-Based Models for Emotion and Sentiment Analysis Using Speech Data},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3687303},
doi = {10.1145/3687303},
abstract = {The study aims to present an in-depth Sentiment Analysis (SA) grounded by the presence of emotions in the speech signals. Nowadays, all kinds of web-based applications ranging from social media platforms and video-sharing sites to e-commerce applications provide support for Human–Computer Interfaces (HCIs). These media applications allow users to share their experiences in all forms such as text, audio, video, GIF, and so on. The most natural and fundamental form of expressing oneself is through speech. Speech-Based Sentiment Analysis (SBSA) is the task of gaining insights into speech signals. It aims to classify the statement as neutral, negative, or positive. On the other hand, Speech Emotion Recognition (SER) categorizes speech signals into the following emotions: disgust, fear, sadness, anger, happiness, and neutral. It is necessary to recognize the sentiments along with the profoundness of the emotions in the speech signals. To cater to the above idea, a methodology is proposed defining a text-oriented SA model using the combination of CNN and Bi-LSTM techniques along with an embedding layer, applied to the text obtained from speech signals; achieving an accuracy of 84.49\%. Also, the proposed methodology suggests an Emotion Analysis (EA) model based on the CNN technique highlighting the type of emotion present in the speech signal with an accuracy measure of 95.12\%. The presented architecture can also be applied to different other domains like product review systems, video recommendation systems, education, health, security, and so on.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {142},
numpages = {24},
keywords = {Speech recognition, emotion detection, deep learning, sentiment analysis}
}

@article{10.1145/3687464,
author = {Li, Keyi and Yang, Sen and Sullivan, Travis M. and Burd, Randall S. and Marsic, Ivan},
title = {ProcessGAN: Generating Privacy-Preserving Time-Aware Process Data with Conditional Generative Adversarial Nets},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {9},
issn = {1556-4681},
url = {https://doi.org/10.1145/3687464},
doi = {10.1145/3687464},
abstract = {Process data constructed from event logs provides valuable insights into procedural dynamics over time. The confidential information in process data, together with the data’s intricate nature, makes the datasets not sharable and challenging to collect. Consequently, research is limited using process data and analytics in the process mining domain. In this study, we introduced a synthetic process data generation task to address the limitation of sharable process data. We introduced a generative adversarial network, called ProcessGAN, to generate process data with activity sequences and corresponding timestamps. ProcessGAN consists of a transformer-based network as the generator, and a time-aware self-attention network as the discriminator. It can generate privacy-preserving process data from random noise. ProcessGAN considers the duration of the process and time intervals between activities to generate realistic activity sequences with timestamps. We evaluated ProcessGAN on five real-world datasets, two that are public and three collected in medical domains that are private. To evaluate the synthetic data, in addition to statistical metrics, we trained a supervised model to score the synthetic processes. We also used process mining to discover workflows for synthetic medical processes and had domain experts evaluate the clinical applicability of the synthetic workflows. ProcessGAN outperformed the existing generative models in generating complex processes with valid parallel pathways. The synthetic process data generated by ProcessGAN better represented the long-range dependencies between activities, a feature relevant to complicated medical and other processes. The timestamps generated by the ProcessGAN model showed similar distributions with the authentic timestamps. In addition, we trained a transformer-based network to generate synthetic contexts (e.g., patient demographics) that were associated with the synthetic processes. The synthetic contexts generated by our model outperformed the baseline models, with the distributions similar to the authentic contexts. We conclude that ProcessGAN can generate sharable synthetic process data indistinguishable from authentic data. Our source code is available in .},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {228},
numpages = {31},
keywords = {Synthetic data generation, Process mining, Sequential data, Generative adversarial networks, Data privacy, Time aware}
}

@article{10.1145/3687483,
author = {Hafezan, Mohammad Hassan and Atoofian, Ehsan},
title = {Transient Fault Detection in Tensor Cores for Modern GPUs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3687483},
doi = {10.1145/3687483},
abstract = {Deep neural networks (DNNs) have emerged as an effective solution for many machine learning applications. However, the great success comes with the cost of excessive computation. The Volta graphics processing unit (GPU) from NVIDIA introduced a specialized hardware unit called tensor core (TC) aiming at meeting the growing computation demand needed by DNNs. Most previous studies on TCs have focused on performance improvement through the utilization of the TC's high degree of parallelism. However, as DNNs are deployed into security-sensitive applications such as autonomous driving, the reliability of TCs is as important as performance.In this work, we exploit the unique architectural characteristics of TCs and propose a simple and implementation-efficient hardware technique called fault detection in tensor core (FDTC) to detect transient faults in TCs. In particular, FDTC exploits the zero-valued weights that stem from network pruning as well as sparse activations arising from the common ReLU operator to verify tensor operations. The high level of sparsity in tensors allows FDTC to run original and verifying products simultaneously, leading to zero performance penalty. For applications with a low sparsity rate, FDTC relies on temporal redundancy to re-execute effectual products. FDTC schedules the execution of verifying products only when multipliers are idle. Our experimental results reveal that FDTC offers 100\% fault coverage with no performance penalty and small energy overhead in TCs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = aug,
articleno = {82},
numpages = {29},
keywords = {Deep neural networks, graphics processing unit, tensor core, reliability}
}

@article{10.1145/3687484,
author = {Jiang, Zijing and Ding, Qun and Wang, An},
title = {Efficient Multi-Byte Power Analysis Architecture Focusing on Bitwise Linear Leakage},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3687484},
doi = {10.1145/3687484},
abstract = {As the most commonly used side-channel analysis method, Correlation Power Analysis (CPA) usually uses the divide-and-conquer strategy to guess the single-byte key in the scenario of block cipher parallel implementation. However, this method cannot effectively use the power consumption information, resulting in a large number of power consumption traces. Therefore, genetic algorithm-based CPA is proposed, which can efficiently extract keys by multi-byte power analysis. However, genetic algorithm-based CPA tends to sacrifice computational cost to achieve a high key guessing success rate. To solve the above problems, this article focuses on bitwise linear leakage and proposes a multi-byte power analysis architecture based on the raindrop ripple algorithm. First, we propose to complete the key initialization by multiple linear regression. Second, we propose a novel swarm intelligence algorithm, the raindrop ripple algorithm, tailored for multi-byte power analysis based on the principles of “family planning” and “eugenics,” which greatly improves the probability of producing individuals with high fitness values. Third, we further enhance the possibility of the correct key being recovered by traversing the candidate key space in specific conditions. To verify the key guessing efficiency of the multi-byte power analysis architecture based on the raindrop ripple algorithm, comparative experiments are conducted on SAKURA-G with three power analysis methods based on genetic algorithms. Experimental results show that our proposal not only has the efficient power information utilization of multi-byte power analysis but also has a convergence speed comparable to or even faster than that of single-byte CPA. Its efficiency of key guessing is improved by 85.64\% compared to EfficiencyGa-CPA, and its convergence speed is even faster than that of single-byte CPA at 725 power traces, and 83.87\% faster than single-byte CPA at 1000 power traces, which is astonishing as a multi-byte power analysis.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {102},
numpages = {25},
keywords = {Block cipher algorithm 1, Correlation power analysis 2, FPGA 3, Genetic algorithm 4, Swarm intelligence 5}
}

@article{10.1145/3687946,
author = {Trusty, Ty and Fei, Yun (Raymond) and Levin, David and Kaufman, Danny},
title = {Trading Spaces: Adaptive Subspace Time Integration for Contacting Elastodynamics},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3687946},
doi = {10.1145/3687946},
abstract = {We construct a subspace simulator that adaptively balances solution improvement against system size. The core components of our simulator are an adaptive subspace oracle, model, and parallel time-step solver algorithm. Our in-time-step adaptivity oracle continually assesses subspace solution quality and candidate update proposals while accounting for temporal variations in deformation and spatial variations in material. In turn our adaptivity model is subspace agnostic. It allows application across subspace representations and expresses unrestricted deformations independent of subspace choice. We couple our oracle and model with a custom-constructed parallel time-step solver for our enriched systems that exposes a pair of user tolerances which provide controllable simulation quality. As tolerances are tightened our model converges to full-space solutions (with expected cost increases). On the other hand, as tolerances are relaxed we obtain output-bound simulation costs. We demonstrate the efficacy of our approach across a wide range of challenging nonlinear materials models, material stiffnesses, heterogeneities, dynamic behaviors, and frictionally contacting conditions, obtaining scalable and efficient simulations of complex elastodynamic scenarios.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {227},
numpages = {16},
keywords = {adaptive subspace simulation, nonlinear elastodynamics}
}

@article{10.1145/3688003,
author = {Wang, ZiXuan and Wang, Pan and Sun, Zhixin and Zhou, Xiaokang and Fu, MengYi and Liu, MinYao and Wang, XinTong and Chen, Lu},
title = {FASDSA: A Flexible Adaptive and Secure Data Sharing Architecture},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3688003},
doi = {10.1145/3688003},
abstract = {With the development of Web 3.0 and Metaverse technologies, the ability of autonomous vehicles has been dramatically improved. These technologies have decentralized features that break the traditional data-sharing mode, grant users control over their data, and achieve benefits through data sharing, promoting the widespread circulation of data. To ensure data exchange security, flexibility, and reliability, this paper proposes FASDSA: A Flexible, Adaptive, and Secure Data Sharing Architecture for CAVs with Web 3.0 and Metaverse. This architecture has three advantages: First, it adopts a decentralized, federated learning and CAV role division method, which allows different computational power CAVs to participate in data sharing according to their roles, achieving flexible data privacy protection. Second, it has the ability of tampered model detection based on interpretable analysis, which can effectively ensure that the model is not tampered with. Third, it has a reward mechanism based on work contribution and trust assessment, which uses blockchain technology to ensure the continuous security operation of this architecture. To verify the performance of FASDSA, we used the UNSW-NB15 dataset to conduct three experiments. The experimental results indicate that compared to traditional methods, FASDSA possesses greater flexibility and security while maintaining similar or even superior model performance.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
keywords = {Cooperative Intrusion Detection, Federated Learning, Blockchain, Connected Autonomous Vehicles, Web3.0}
}

@article{10.1145/3688393,
author = {Alzahrani, Naif and Ca\l{}a, Jacek and Missier, Paolo},
title = {Experience: A Comparative Analysis of Multivariate Time-Series Generative Models: A Case Study on Human Activity Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3688393},
doi = {10.1145/3688393},
abstract = {Human activity recognition (HAR) is an active research field that has seen great success in recent years due to advances in sensory data collection methods and activity recognition systems. Deep artificial intelligence (AI) models have contributed to the success of HAR systems lately, although still suffering from limitations such as data scarcity, the high costs of labelling data instances, and datasets’ imbalance and bias. The temporal nature of human activity data, represented as time series data, impose an additional challenge to using AI models in HAR, because most state-of-the-art models do not account for the time component of the data instances. These limitations have inspired the time-series research community to design generative models for sequential data, but very little work has been done to evaluate the quality of such models. In this work, we conduct a comparative quality analysis of three generative models for time-series data, using a case study in which we aim to generate sensory human activity data from a seed public dataset. Additionally, we adapt and clearly explain four evaluation methods of synthetic time-series data from the literature and apply them to assess the quality of the synthetic activity data we generate. We show experimentally that high-quality human activity data can be generated using deep generative models, and the synthetic data can thus be used in HAR systems to augment real activity data. We also demonstrate that the chosen evaluation methods effectively ensure that the generated data meets the essential quality benchmarks of realism, diversity, coherence, and utility. Our findings suggest that using deep generative models to produce synthetic human activity data can potentially address challenges related to data scarcity, biases, and expensive labeling. This holds promise for enhancing the efficiency and reliability of HAR systems.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {18},
numpages = {18},
keywords = {Human activity recognition, multivariate time series, generative modeling}
}

@article{10.1145/3688569,
author = {Wang, Anqi and Dong, Jiahua and Lee, Lik-Hang and Shen, Jiachuan and Hui, Pan},
title = {A Survey on Deep Learning for Design and Generation of Virtual Architecture},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3688569},
doi = {10.1145/3688569},
abstract = {Three-dimensional (3D) shape generation techniques leveraging deep learning have garnered significant interest from both computer vision and architectural design communities, promising to enrich the content in the virtual environment. However, research on virtual architectural design remains limited, particularly regarding designer-AI collaboration and deep learning-assisted design. In our survey, we reviewed 149 related articles (81.2\% of articles published between 2019 and 2023) covering architectural design, 3D shape techniques, and virtual environments. Through scrutinizing the literature, we first identify the principles of virtual architecture and illuminate its current production challenges, including datasets, multimodality, design intuition, and generative frameworks. We then introduce the latest approaches to designing and generating virtual buildings leveraging 3D shape generation and summarize four characteristics of various approaches to virtual architecture. Based on our analysis, we expound on four research agendas, including agency, communication, user consideration, and integrating tools. Additionally, we highlight four important enablers of ubiquitous interaction with immersive systems in deep learning-assisted architectural generation. Our work contributes to fostering understanding between designers and deep learning techniques, broadening access to designer-AI collaboration. We advocate for interdisciplinary efforts to address this timely research topic, facilitating content designing and generation in the virtual environment.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {29},
numpages = {41},
keywords = {AI-assisted architectural design, 3D shape generation, virtual architecture, designer-AI collaboration, AIGC}
}

@article{10.1145/3688610,
author = {Li, Chunfeng and Shi, Feng and Yin, Fei and Soliman, Karim and Wei, Jin},
title = {A High Scalability Memory NoC with Shared-Inside Hierarchical-Groupings for Triplet-Based Many-Core Architecture},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3688610},
doi = {10.1145/3688610},
abstract = {Innovative processor architecture designs are shifting towards Many-Core Architectures (MCAs) to meet the future demands of high-performance computing as the limits of Moore’s Law have almost been reached. Many-core processors utilize shared memory hierarchies to achieve high-speed memory systems, improving memory access efficiency. However, as the number of cores multiplies, the scalability of this system is significantly constrained by the increased proportion of long-distance and Non-Uniform Memory Access (NUMA). Improving the scalability of MCAs is crucial for achieving large/super-scale general-purpose many-core processors. This work proposes a high scalability memory Network-on-Chip (NoC) for Triplet-Based Many-Core Architecture (TriBA), named TriBA-mNoC. TriBA-mNoC maintains a consistent core-to-core spacing as the network scale increases, effectively preventing increased long-distance memory access latency. Moreover, it leverages an inherent advantage of shared-inside hierarchical-groupings, alleviating common NUMA issues in the NoC design. Evaluations of static network characteristics show that TriBA-mNoC outperforms most classical NoCs in network diameter, average distance, and cost. TriBA-mNoC can be integrated with TriBA in the same silicon die with a tile-like floorplan, forming a novel NoC called TriBA-NoC, which can combine the strengths of both networks to maximize the architecture performance. We evaluated the memory access performance and scalability of TriBA-NoC using the mathematical evaluation models and actual simulations with real traffic (PARSEC 3.0 and SPLASH-2) at different network scales. The mathematical evaluation results indicate that TriBA-NoC achieves an aggregate speedup of approximately 3x compared with 2D-Mesh for a similar number of cores. Furthermore, TriBA-NoC’s single-core speedup efficiency remains stable as the number of cores increases under the same cache hit ratio, while 2D-Mesh experiences a rapid decline, highlighting TriBA-NoC’s exceptional scalability. Finally, the actual traffic simulation results show that TriBA-NoC achieves an average memory access latency and time reduction of 25.90\% − 40.50\% and 5.61\% − 31.69\% respectively, compared with 2D-Mesh.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
keywords = {many-core processor, network-on-chip, cache structure, scalability, triplet-based many-core architecture}
}

@article{10.1145/3688612,
author = {Lu, Xiaobo and Fang, Jianbin and Peng, Lin and Huang, Chun and Du, Zidong and Zhao, Yongwei and Wang, Zheng},
title = {Mentor: A Memory-Efficient Sparse-dense Matrix Multiplication Accelerator Based on Column-Wise Product},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3688612},
doi = {10.1145/3688612},
abstract = {Sparse-dense matrix multiplication (SpMM) is the performance bottleneck of many high-performance and deep-learning applications, making it attractive to design specialized SpMM hardware accelerators. Unfortunately, existing hardware solutions do not take full advantage of data reuse opportunities of the input and output matrices or suffer from irregular memory access patterns. Their strategies increase the off-chip memory traffic and bandwidth pressure, leaving much room for improvement. We present Mentor, a new approach to designing SpMM accelerators. Our key insight is that column-wise dataflow, while rarely exploited in prior works, can address these issues in SpMM computations. Mentor is a software-hardware co-design approach for leveraging column-wise dataflow to improve data reuse and regular memory accesses of SpMM. On the software level, Mentor incorporates a novel streaming construction scheme to preprocess the input matrix for enabling a streaming access pattern. On the hardware level, it employs a fully pipelined design to unlock the potential of column-wise dataflow further. The design of Mentor is underpinned by a carefully designed analytical model to find the tradeoff between performance and hardware resources. We have implemented an FPGA prototype of Mentor. Experimental results show that Mentor achieves speedup by geomean 2.05\texttimes{} (up to 3.98\texttimes{}), reduces the memory traffic by geomean 2.92\texttimes{} (up to 4.93\texttimes{}), and improves bandwidth utilization by geomean 1.38\texttimes{} (up to 2.89\texttimes{}), compared with the state-of-the-art hardware solutions.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {79},
numpages = {25},
keywords = {SpMM, sparse linear algebra}
}

@article{10.1145/3689042,
author = {Arzberger, Anne and Lupetti, Maria Luce and Giaccardi, Elisa},
title = {Reflexive Data Curation: Opportunities and Challenges for Embracing Uncertainty in Human–AI Collaboration},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3689042},
doi = {10.1145/3689042},
abstract = {This article presents findings from a Research through Design investigation focusing on a reflexive approach to data curation and the use of generative AI in design and creative practices. Using binary gender categories manifested in children’s toys as a context, we examine three design experiments aimed at probing how designers can cultivate a reflexive human-AI practice to confront and challenge their internalized biases. Our goal is to underscore the intricate interplay between the designer, AI technology, and publicly held imaginaries and to offer an initial set of tactics for how personal biases and societal norms can be illuminated through interactions with AI. We conclude by proposing that designers not only bear the responsibility of grappling critically with the complexities of AI but also possess the opportunity to creatively harness the limitations of technology to craft a reflexive data curation that encourages profound reflections and awareness within design processes.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {74},
numpages = {33},
keywords = {human-AI collaboration, machine-learning uncertainty, Research through Design, reflexive data curation, human-AI reflexive practices}
}

@article{10.1145/3689043,
author = {Reed, Courtney N. and Benito, Adan L. and Caspe, Franco and McPherson, Andrew P.},
title = {Shifting Ambiguity, Collapsing Indeterminacy: Designing with Data as Baradian Apparatus},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3689043},
doi = {10.1145/3689043},
abstract = {This article examines how digital systems designers distil the messiness and ambiguity of the world into concrete data that can be processed by computing systems. Using Karen Barad's agential realism as a guide, we explore how data is fundamentally entangled with the tools and theories of its measurement. We examine data-enabled artefacts acting as Baradian apparatuses: they do not exist independently of the phenomenon they seek to measure but rather collect and co-produce observations from within their entangled state: the phenomenon and the apparatus co-constitute one another. Connecting Barad's quantum view of indeterminacy to the prevailing HCI discourse on the opportunities and challenges of ambiguity, we suggest that the very act of trying to stabilise a conceptual interpretation of data within an artefact has the paradoxical effect of amplifying and shifting ambiguity in interaction. We illustrate these ideas through three case studies from our own practices of designing digital musical instruments (DMIs). DMIs necessarily encode symbolic and music-theoretical knowledge as part of their internal operation, even as conceptual knowledge is not their intended outcome. In each case, we explore the nature of the apparatus, what phenomena it co-produces, and where the ambiguity lies to suggest approaches for design using these abstract theoretical frameworks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {73},
numpages = {41},
keywords = {ambiguity, research through design, agential realism, entanglement, mapping, digital musical instruments}
}

@article{10.1145/3689437,
author = {Li, Guangyan and Ye, Zewen and Chen, Donglong and Dai, Wangchen and Mao, Gaoyu and Huang, Kejie and Cheung, Ray C. C.},
title = {ProgramGalois: A Programmable Generator of Radix-4 Discrete Galois Transformation Architecture for Lattice-Based Cryptography},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1936-7406},
url = {https://doi.org/10.1145/3689437},
doi = {10.1145/3689437},
abstract = {Lattice-based cryptography (LBC) has been established as a prominent research field, with particular attention on post-quantum cryptography (PQC) and fully homomorphic encryption (FHE). As the implementing bottleneck of PQC and FHE, number theoretic transform (NTT) has been extensively studied. However, current works struggled with scalability, hindering their adaptation to various parameters, such as bit width and polynomial length. In this article, we proposed a novel Discrete Galois Transformation (DGT) algorithm utilizing the radix-4 variant to achieve a higher level of parallelism to the existing NTT. Furthermore, to implement the efficient radix-4 DGT adapting more LBCs, we proposed a set of scalable building blocks, including a modified Barrett modular multiplier accepting arbitrary modulus with only one integer multiplier, a radix-4 DGT butterfly unit, and a stream permutation network. The proposed modules are implemented on the Xilinx Virtex-7 and U250 FPGA to evaluate resource utilization and performance. Lastly, a design space exploration framework is proposed to generate optimized radix-4 DGT hardware constrained by polynomial and platform parameters. The sensitivity analysis showcases the generated hardware’s performance and scalability. The implementation results on the Xilinx Virtex-7 and U250 FPGA show significant performance improvements over the state-of-the-art works, which reached at least 35\%, 192\%, and 68\% area-time product improvements in terms of LUTs, BRAMs, and DSPs, respectively.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = nov,
articleno = {53},
numpages = {32},
keywords = {Lattice-based Cryptography, Number Theoretic Transform (NTT), Discrete Galois Transform (DGT), FPGA architecture}
}

@article{10.1145/3689627,
author = {Sharief, Farhana and Ijaz, Humaira and Shojafar, Mohammad and Naeem, Muhammad Asif},
title = {Multi-Class Imbalanced Data Handling with Concept Drift in Fog Computing: A Taxonomy, Review, and Future Directions},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3689627},
doi = {10.1145/3689627},
abstract = {A network of actual physical objects or “IoT components” linked to the internet and equipped with sensors, electronics, software, and network connectivity is known as the Internet of Things (IoT). This ability of the IoT components to gather and share data is made possible by this network connectivity. Many IoT devices are currently operating, which generate a lot of data. When these IoT devices started collecting data, the cloud was the only place to analyze, filter, pre-process, and aggregate it. However, when it comes to IoT, the cloud has restrictions regarding latency and a more centralized method of distributing programs. A new form of computing called Fog computing has been proposed to address the shortcomings of current cloud computing. In an IoT context, sensors regularly communicate signal information, and edge devices process the data obtained from these sensors using Fog computing. The sensors’ internal or external problems, security breaches, or the integration of heterogeneous equipment contribute to the imbalanced data, i.e., comparatively speaking, one class has more instances than the other. As a result of this data, the pattern extraction is imbalanced. Recent attempts have concentrated heavily on binary-class imbalanced concerns with exactly two classes. However, the classification of multi-class imbalanced data is an issue that needs to be fixed in Fog computing, even if it is widespread in other fields, including text categorization, human activity detection, and medical diagnosis. The study intends to deal with this problem. It presents a systematic, thorough, and in-depth comparative analysis of several binary-class and multi-class imbalanced data handling strategies for batch and streaming data in IoT networks and Fog computing. There are five major objectives in this study. First, reviewing the Fog computing concept. Second, outlining the optimization metric used in Fog computing. Third, focusing on binary and multi-class batch data handling for IoT networks and Fog computing. Fourth, reviewing and comparing the current imbalanced data handling methodologies for multi-class data streams. Fifth, explaining how to cope with the concept drift, including novel and recurring classes, targeted optimization measures, and evaluation tools. Finally, the best performance metrics and tools for concept drift, binary-class (batch and stream) data, and multi-class (batch and stream) data are highlighted.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {16},
numpages = {48},
keywords = {Cloud computing, fog computing, Internet of Things (IoT), multi-class imbalanced data stream, concept drift}
}

@article{10.1145/3689775,
author = {Le, Callista and Gopinathan, Kiran and Lee, Koon Wen and Gilbert, Seth and Sergey, Ilya},
title = {Concurrent Data Structures Made Easy},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689775},
doi = {10.1145/3689775},
abstract = {Design of an efficient thread-safe concurrent data structure is a balancing act between its implementation complexity and performance. Lock-based concurrent data structures, which are relatively easy to derive from their sequential counterparts and to prove thread-safe, suffer from poor throughput under even light multi-threaded workload. At the same time, lock-free concurrent structures allow for high throughput, but are notoriously difficult to get right and require careful reasoning to formally establish their correctness.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
In this work, we explore a solution to this conundrum based on a relatively old idea of batch parallelism---an approach for designing high-throughput concurrent data structures via a simple insight: efficiently processing a batch of a priori known operations in parallel is easier than optimising performance for a stream of arbitrary asynchronous requests. Alas, batch-parallel structures have not seen wide practical adoption due to (i) the inconvenience of having to structure multi-threaded programs to explicitly group operations and (ii) the lack of a systematic methodology to implement batch-parallel structures as simply as lock-based ones.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
We present OBatcher---a Multicore OCaml library that streamlines the design, implementation, and usage of batch-parallel structures. OBatcher solves the first challenge (how to use) by suggesting a new lightweight implicit batching design pattern that is built on top of generic asynchronous programming mechanisms. The second challenge (how to implement) is addressed by identifying a family of strategies for converting common sequential structures into the corresponding efficient batch-parallel versions, and by providing a library of functors that embody those strategies. We showcase OBatcher with a diverse set of benchmarks ranging from Red-Black and AVL trees to van Emde Boas trees, skip lists, and a thread-safe implementation of a Datalog solver. Our evaluation of all the implementations on large asynchronous workloads shows that (a) they consistently outperform the corresponding coarse-grained lock-based implementations---the only ones available in OCaml to date, and that (b) their throughput scales reasonably with the number of processors.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {335},
numpages = {29},
keywords = {Multicore OCaml, batch parallelism, shared-memory concurrency}
}

@article{10.1145/3690825,
author = {Wang, Shaobu and Zhang, Guangyan and Wei, Junyu and Wang, Yang and Wu, Jiesheng and Luo, Qingchao},
title = {Understanding Silent Data Corruption in Processors for Mitigating its Effects},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3690825},
doi = {10.1145/3690825},
abstract = {Silent Data Corruption (SDC) in processors can lead to various application-level issues, such as incorrect calculations and even data loss. Since traditional techniques are not effective in detecting these errors, it is very hard to address problems caused by SDCs in processors. For the same reason, knowledge about these SDCs in the wild is limited.In this article, we conduct an extensive study on CPU SDCs in a large production CPU population, encompassing over one million processors. In addition to collecting overall statistics, we perform a detailed study to understand (1) whether certain processor features are particularly vulnerable and their potential impacts on applications; (2) the reproducibility of CPU SDCs and the triggering conditions (e.g., temperature) of those less reproducible SDCs; and (3) the challenges to mitigate and handle CPU SDCs.We further investigate the implications that our observations obtained from the above researches have on the SDC fault models, SDC mitigation strategies, and the future research fields. In addition, we design an efficient SDC mitigation approach called Farron, which uses prioritized testing to detect highly reproducible SDCs and temperature control to mitigate less-reproducible SDCs. Our experimental results indicate that Farron can achieve better coverage of CPU SDCs with lower overall overhead, compared to the baseline used in Alibaba Cloud. This demonstrates that our observations are able to assist in SDC mitigation.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {84},
numpages = {27},
keywords = {Processor, silent data corruption, reliability, fault tolerance}
}

@article{10.1145/3691348,
author = {Maroto-G\'{o}mez, Marcos and Lewis, Matthew and Castro-Gonz\'{a}lez, \'{A}lvaro and Malfaz, Mar\'{\i}a and Salichs, Miguel \'{A}ngel and Ca\~{n}amero, Lola},
title = {Adapting to My User, Engaging with My Robot: An Adaptive Affective Architecture for a Social Assistive Robot},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3691348},
doi = {10.1145/3691348},
abstract = {Affective feedback from social robots is a useful technique for communicating to people whether they are interacting “well” with the robot or not. However, some users, such as people with physical or cognitive difficulties, may not be able to interact in all the desired ways. In these cases, affective feedback from the robot could be excessively negative—an “unhappy” robot, leading to an unrewarding experience for the user. This article presents a motivation-based architecture for an autonomous multimodal social robot, that incorporates an affective feedback mechanism which generates an affective state by combining the internal needs of the robot and the social interaction quality. The balance between these two factors can dynamically change, allowing the robot to adapt its affective feedback to the user's interaction style and capabilities. We have implemented this architecture in a simulation and in a MiRo social robot, and report experiments examining the behavior of the system in interactions with different experimental user profiles. The results show that the adaptive mechanism allows the robot to change its affective feedback to give more positive encouragement to users than in non-adaptive cases.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {125},
numpages = {28},
keywords = {Affective Cognitive Robot Architecture, Biologically-Inspired Robot Motivations and Emotions, Decision Making, Human–Robot Interaction, Social Assistive Robots}
}

@article{10.1145/3691352,
author = {Gupta, Rajan and Pandey, Gaurav and Pal, Saibal Kumar},
title = {Automating Government Report Generation: A Generative AI Approach for Efficient Data Extraction, Analysis, and Visualization},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691352},
doi = {10.1145/3691352},
abstract = {This application paper introduces a transformative solution to address the labour-intensive manual report generation, data searching \&amp; report revision process in government entities. Traditional methods of data extraction, analysis, and graph creation for annual reports are not only time-consuming but also prone to human errors. To mitigate these challenges, we propose an innovative system leveraging Generative Artificial Intelligence (GenAI), with a specific focus on large language models (LLMs). Our solution incorporates automated data extraction from diverse sources designated as internal knowledge base, text analysis, summarization using advanced language models, and the generation as well as revisions of informative graphs. Different LLMs like Google's Gemini Pro and OpenAI's GPT 4.0 has been used to read different data visualisation graphs and fetching the information from internal knowledge base, respectively, to update the graphs in automated manner. Solution implementation using Python language on the World Economic Situation and Prospects report by Department of Economic \&amp; Social Affairs, United Nation shows that the early result produces almost 0.87\% to 17.46\% average error rates in the task of factual data visualisation graph. Key benefit of this approach include improved time efficiency, consistency in report format, enhanced insights, and a user-friendly interface. A review and approval workflow facilitate user feedback, contributing to continuous model performance improvement.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
keywords = {Generative AI, Gemini Pro, GPT, OpenAI, Knowledge update, Data visualisation, Automated graph revision, Data fetching, Data generation}
}

@article{10.1145/3694687,
author = {Bambini, Giovanni and Ottaviano, Alessandro and Conficoni, Christian and Tilli, Andrea and Benini, Luca and Bartolini, Andrea},
title = {Modeling and Controlling Many-Core HPC Processors: an Alternative to PID and Moving Average Algorithms},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3694687},
doi = {10.1145/3694687},
abstract = {The race towards performance increase and computing power has led to chips with heterogeneous and complex designs, integrating an ever-growing number of cores on the same monolithic chip or chiplet silicon die. Higher integration density, compounded with the slowdown of technology-driven power reduction, implies that power and thermal management become increasingly relevant. Unfortunately, existing research lacks a detailed analysis and modeling of thermal, power, and electrical coupling effects and how they have to be jointly considered to perform dynamic control of complex and heterogeneous mpsoc. To close the gap, in this work, we first provide a detailed thermal and power model targeting a modern hpc mpsoc. We consider real-world coupling effects such as actuators’ non-idealities and the exponential relation between the dissipated power, the temperature state, and the voltage level in a single processing element. We analyze how these factors affect the control algorithm behavior and the type of challenges that they pose. Based on the analysis, we propose a thermal capping strategy inspired by Fuzzy control theory to replace the state-of-the-art PID controller, as well as a root-finding iterative method to optimally choose the shared voltage value among cores grouped in the same voltage domain. We evaluate the proposed controller with model-in-the-loop and hardware-in-the-loop co-simulations. We show an improvement over state-of-the-art methods of up to  (5times)  the maximum exceeded temperature while providing an average of  (3.56\%)  faster application execution runtime across all the evaluation scenarios.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = sep,
keywords = {Modeling, Control, Nonlinear systems}
}

@article{10.1145/3695463,
author = {Yu, Chunqiang and Cheng, Shichao and Zhang, Xianquan and Zhang, Xinpeng and Tang, Zhenjun},
title = {Reversible Data Hiding in Shared JPEG Images},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {12},
issn = {1551-6857},
url = {https://doi.org/10.1145/3695463},
doi = {10.1145/3695463},
abstract = {Reversible data hiding (RDH) in encrypted images has emerged as an effective technique for securely storing and managing confidential images in the cloud. However, most RDH methods in shared images (RDHSI) are designed for uncompressed images and cannot be applied for JPEG images. To address this issue, we propose a novel RDH in shared JPEG images. Our method consists of JPEG image sharing and data hiding in JPEG shares, which are both conducted on JPEG bit-stream. Specifically, the DC appended bits (DCA) and AC appended bits (ACA) derived from the original JPEG bit-stream are shared by ( (k) ,  (n) ) threshold Chinese remainder theorem-based secret sharing (CRTSS) with two different constraints, one for DC sharing and another for AC sharing. The constraint of DC sharing ensures that the DC coefficient shares do not overflow. The constraint of AC sharing ensures the sizes of ACA shares are less than the sizes of the original ACA so that the embedding room can be vacated from each shared JPEG bit-stream. Each data-hider can embed the secret data into the personal JPEG share. The original JPEG image can be recovered losslessly from any  (k)  JPEG shares. The proposed sharing and data hiding are both well compatible with the JPEG standard. Experimental results demonstrate that the proposed method not only well preserves the file size whether the JPEG shares or marked JPEG shares but also achieves outstanding security performance and a high embedding capacity.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
articleno = {372},
numpages = {24},
keywords = {Reversible data hiding, Image sharing, JPEG image, high embedding capacity}
}

@article{10.1145/3695466,
author = {Cui, Cu},
title = {Acceleration of Tensor-Product Operations with Tensor Cores},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3695466},
doi = {10.1145/3695466},
abstract = {In this article, we explore the acceleration of tensor product operations in finite element methods, leveraging the computational power of the NVIDIA A100 GPU Tensor Cores. We provide an accessible overview of the necessary mathematical background and discuss our implementation strategies. Our study focuses on two common programming approaches for NVIDIA Tensor Cores: the C++ Warp Matrix Functions in nvcuda::wmma and the inline Parallel Thread Execution (PTX) instructions mma.sync.aligned. A significant focus is placed on the adoption of the versatile inline PTX instructions combined with a conflict-free shared memory access pattern, a key to unlocking superior performance. When benchmarked against traditional CUDA Cores, our approach yields a remarkable 2.3-fold increase in double-precision performance, achieving 8 TFLOPS/s—45\% of the theoretical maximum. Furthermore, in half-precision computations, numerical experiments demonstrate a fourfold enhancement in solving the Poisson equation using the flexible GMRES (FGMRES) method, preconditioned by a multigrid method in 3D. This is achieved while maintaining the same discretization error as observed in double-precision computations. These results highlight the considerable benefits of using Tensor Cores for finite element operators with tensor products, achieving an optimal balance between computational speed and precision.},
journal = {ACM Trans. Parallel Comput.},
month = nov,
articleno = {15},
numpages = {24},
keywords = {Multigrid method, discontinuous galerkin method, matrix free method, GPU, tensor core, mixed precision}
}

@article{10.1145/3695987,
author = {Oliveria de Souza, Leandro and Santana de Almeida, Eduardo and Silveira Neto, Paulo Anselmo da Mota and Barr, Earl T. and Petke, Justyna},
title = {Software Product Line Engineering via Software Transplantation },
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695987},
doi = {10.1145/3695987},
abstract = {Software Product Lines (SPLs) improve time-to-market, enhance software quality, and reduce maintenance costs. Current SPL re-engineering practices are largely manual and require domain knowledge. Thus, adopting and, to a lesser extent, maintaining SPLs are expensive tasks, preventing many companies from enjoying their benefits. To address these challenges, we introduce Foundry, an approach utilizing software transplantation to reduce the manual effort of SPL adoption and maintenance. Foundry enables integrating features across different codebases, even codebases that are unaware that they are contributing features to a software product line. Each product produced by Foundry is pure code, without variability annotation, unlike feature flags, which eases variability management and reduces code bloat.We realise Foundry in prodScalpel, a tool that transplants multiple organs (i.e., a set of interesting features) from donor systems into an emergent product line for codebases written in C. Given tests and lightweight annotations identifying features and implantation points, prodScalpel automates feature extraction and integration. To evaluate its effectiveness, our evaluation compares feature transplantation using prodScalpel to the current state of practice: on our dataset, prodScalpel’s use speeds up feature migration by an average of 4.8 times when compared to current practice.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
keywords = {Software Product Lines, Software Transplantation, Genetic Improvement}
}

@article{10.1145/3696014,
author = {Buchta, Robin and Gkoktsis, George and Heine, Felix and Kleiner, Carsten},
title = {Advanced Persistent Threat Attack Detection Systems: A Review of Approaches, Challenges, and Trends},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3696014},
doi = {10.1145/3696014},
abstract = {Advanced persistent threat (APT) attacks present a significant challenge for any organization, as they are difficult to detect due to their elusive nature and characteristics. In this article, we conduct a comprehensive literature review to investigate the various APT attack detection systems and approaches and classify them based on their threat model and detection method. Our findings reveal common obstacles in APT attack detection, such as correctly attributing anomalous behavior to APT attack activities, limited availability of public datasets and inadequate evaluation methods, challenges with detection procedures, and misinterpretation of requirements. Based on our findings, we propose a reference architecture to enhance the comparability of existing systems and provide a framework for classifying detection systems. In addition, we look in detail at the problems encountered in current evaluations and other scientific gaps, such as a neglected consideration of integrating the systems into existing security architectures and their adaptability and durability. While no one-size-fits-all solution exists for APT attack detection, this review shows that graph-based approaches hold promising potential. However, further research is required for real-world usability, considering the systems’ adaptability and explainability.},
journal = {Digital Threats},
month = dec,
articleno = {39},
numpages = {37},
keywords = {Cybersecurity, APT, attack detection, machine learning, artificial intelligence}
}

@article{10.1145/3696391,
author = {Diamant, Jonathan and Landau Feibish, Shir},
title = {SetD4: Sets With Deletions and Decay in the Data Plane},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT4},
url = {https://doi.org/10.1145/3696391},
doi = {10.1145/3696391},
abstract = {Sets are a fundamental data type in Computer Science. Data structures used to maintain sets need to enable the insertion and deletion of keys from the set and support a lookup operation to check if a key belongs to the set. Recent advances in programmable networks allow performing fine-grained network telemetry and other network functions right in the data plane, many of which utilize sets. One of the most common data structure in use for maintaining sets in the data plane is the Bloom Filter (BF). Existing implementations of BFs in the data plane support key insertion and lookup, yet due to the harsh processing restrictions of the data plane, they do not support deletions. We present SetD4, the first data structure for maintaining sets in the data plane that supports insertion, lookup and deletion. SetD4 maintains a modified BF, which holds the set, as well as two auxiliary structures that allow the safe removal of keys from the set. In addition, we present a variant of SetD4 that also supports decay, which allows the automatic removal of keys from the structure after a predefined time interval. We analyze SetD4 and show precise error rates for both the decaying and non-decaying structures. We have implemented SetD4 on the Tofino programmable switch and show that it can achieve high accuracy with limited overhead when compared to the current state-of-the-art set-membership data structures in the data plane with better false-positive and false-negative error rates.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {34},
numpages = {22},
keywords = {dynamic set membership, network algorithms, programmable switches}
}

@article{10.1145/3696421,
author = {Tan, Chang and Liu, Zhewei and Li, Zhengdao and Jia, Jingyu and Lv, Siyi and Li, Tong and Liu, Zheli},
title = {EdgeSyn: Privacy-preserving Data Publishing on Edge Network over Infinite Multimedia Data Stream},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3696421},
doi = {10.1145/3696421},
abstract = {To privately publish sensitive multimedia data in an edge network with fog devices, one of the best privacy-preserving solutions is to use differential privacy (DP) mechanisms. However, existing DP data publication mechanisms for the infinite data stream of edge networks mainly focus on publishing data with specific types of data or a set of predetermined queries. This approach is not suitable for multimedia data with numerous features that require a more flexible data publishing mechanism. In this paper, we propose EdgeSyn, a novel mechanism for accurately publishing multimedia data over infinite data streams in an edge network. It allocates privacy budgets with a sliding window, adopting data synthesis mechanisms to support dynamic publishing without loss of accuracy. In more detail, EdgeSyn addresses the limitations associated with data types in prior data stream publishing approaches and introduces a privacy budget management strategy that optimally allocates budgets for the implementation of data synthesis mechanisms over an infinite data stream. The experimental results show that EdgeSyn performs well under different privacy budgets and various lengths of active windows.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
keywords = {Privacy, Multimedia data, Differential privacy}
}

@article{10.1145/3697008,
author = {P\'{e}rez, Beatriz and Rubio, \'{A}ngel Luis and Zapata, Mar\'{\i}a A.},
title = {PROV-IDEA: Supporting Interoperable Schema and Data Provenance within Database Evolution},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697008},
doi = {10.1145/3697008},
abstract = {Database evolution and data provenance are two closely related research fields. On the one hand, the registry (via provenance) of the schema evolution allows the maintenance of its version record. On the other hand, the origin of the data (i.e. its provenance) will always be affected by modifications (i.e. the evolution) in the schema on which they are based. Despite these interrelationships, there are few works in the literature that have proposed advances in that direction. In particular, to the best of our knowledge, there is no research that has resulted in a general and interoperable solution to the problem of managing database evolution using provenance. In this paper we present PROV-IDEA: a PROV-Interoperable Database Evolution Approach. This is a proposal that allows the simultaneous management of the provenance of schemas (of relational databases) and data, using the PROV standard as a way to guarantee interoperability. Furthermore, it is an adaptable and expandable approach (by using PROV templates), which allows a non-intrusive and seamless integration with existing applications, as well as different aspects of provenance information generation. These properties are demonstrated in the article by presenting a proof of concept built on top of a third-party relational database evolution tool.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep
}

@article{10.1145/3698195,
author = {Hussein, Dina and Belkhouja, Taha and Bhat, Ganapati and Doppa, Jana},
title = {Sensor-Aware Data Imputation for Time-Series Machine Learning on Low-Power Wearable Devices},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3698195},
doi = {10.1145/3698195},
abstract = {Wearable devices that have low-power sensors, processors, and communication capabilities are gaining wide adoption in several health applications. The machine learning algorithms on these devices assume that data from all sensors are available during runtime. However, data from one or more sensors may be unavailable due to energy or communication challenges. This loss of sensor data can result in accuracy degradation of the application. Prior approaches to handle missing data, such as generative models or training multiple classifiers for each combination of missing sensors are not suitable for low-energy wearable devices due to their high overhead at runtime. In contrast to prior approaches, we present an energy-efficient approach, referred to as Sensor-Aware iMputation&nbsp;(SAM), to accurately impute missing data at runtime and recover application accuracy. SAM first uses unsupervised clustering to obtain clusters of similar sensor data patterns. Next, it learns inter-relationship between clusters to obtain imputation patterns for each combination of clusters using a principled sensor-aware search algorithm. Using sensor data for clustering before choosing imputation patterns ensures that the imputation is aware of sensor data observations. Experiments on seven diverse wearable sensor-based time-series datasets demonstrate that SAM is able to maintain accuracy within 5\% of the baseline with no missing data when one sensor is missing. We also compare SAM against generative adversarial imputation networks&nbsp;(GAIN), transformers, and k-nearest neighbor methods. Results show that SAM outperforms all three approaches on average by more than&nbsp;25\% when two sensors are missing with negligible overhead compared to the baseline.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {2},
numpages = {27},
keywords = {Human activity recognition, wearable electronics, missing data detection, data imputation, clustering, health monitoring}
}

@article{10.1145/3698811,
author = {Yan, Mengyi and Wang, Yaoshu and Wang, Yue and Miao, Xiaoye and Li, Jianxin},
title = {GIDCL: A Graph-Enhanced Interpretable Data Cleaning Framework with Large Language Models},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698811},
doi = {10.1145/3698811},
abstract = {Data quality is critical across many applications. The utility of data is undermined by various errors, making rigorous data cleaning a necessity. Traditional data cleaning systems depend heavily on predefined rules and constraints, which necessitate significant domain knowledge and manual effort. Moreover, while configuration-free approaches and deep learning methods have been explored, they struggle with complex error patterns, lacking interpretability, requiring extensive feature engineering or labeled data. This paper introduces GIDCL (Graph-enhanced Interpretable Data Cleaning with Large language models), a pioneering framework that harnesses the capabilities of Large Language Models (LLMs) alongside Graph Neural Network (GNN) to address the challenges of traditional and machine learning-based data cleaning methods. By converting relational tables into graph structures, GIDCL utilizes GNN to effectively capture and leverage structural correlations among data, enhancing the model's ability to understand and rectify complex dependencies and errors. The framework's creator-critic workflow innovatively employs LLMs to automatically generate interpretable data cleaning rules and tailor feature engineering with minimal labeled data. This process includes the iterative refinement of error detection and correction models through few-shot learning, significantly reducing the need for extensive manual configuration. GIDCL not only improves the precision and efficiency of data cleaning but also enhances its interpretability, making it accessible and practical for non-expert users. Our extensive experiments demonstrate that GIDCL significantly outperforms existing methods, improving F1-scores by 10\% on average while requiring only 20 labeled tuples.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {236},
numpages = {29},
keywords = {data quality, graph neural network, interpretable, large language models}
}

@article{10.1145/3698812,
author = {Boeschen, Nils and Ziegler, Tobias and Binnig, Carsten},
title = {GOLAP: A GPU-in-Data-Path Architecture for High-Speed OLAP},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698812},
doi = {10.1145/3698812},
abstract = {In this paper, we suggest a novel GPU-in-data-path architecture that leverages a GPU to accelerate the I/O path and thus can achieve almost in-memory bandwidth using SSDs. In this architecture, the main idea is to stream data in heavy-weight compressed blocks from SSDs directly into the GPU and decompress it on-the-fly as part of the table scan to inflate data before processing it by downstream query operators. Furthermore, we employ novel GPU-optimized pruning techniques that help us further inflate the perceived read bandwidth. In our evaluation, we show that the GPU-in-data-path architecture can achieve an effective bandwidth of up to 100 GiB/s, surpassing existing in-memory systems' capabilities.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {237},
numpages = {26},
keywords = {dbms gpu-acceleration, olap, ssd storage}
}

@article{10.1145/3698831,
author = {Gao, Haotian and Cai, Shaofeng and Dinh, Tien Tuan Anh and Huang, Zhiyong and Ooi, Beng Chin},
title = {CtxPipe: Context-aware Data Preparation Pipeline Construction for Machine Learning},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698831},
doi = {10.1145/3698831},
abstract = {Machine learning models are only as good as their training data. Simple models trained on well-chosen features extracted from the raw data often outperform complex models trained directly on the raw data. Data preparation pipelines, which clean and derive features from the data, are therefore important for machine learning applications. However, constructing such pipelines is a resource-intensive process that involves deep human expertise.Our goal is to design an efficient framework for automatically finding high-quality data preparation pipelines. The main challenge is how to explore a large search space of pipeline components with the objective of computing features that maximize the performance of the downstream models. Existing solutions are limited in terms of feature quality, which results in low accuracies of the downstream models, while incurring significant runtime overhead. We present CtxPipe, a novel framework that addresses the limitations of previous works by leveraging contextual information to improve the pipeline construction process. Specifically, it uses pre-trained embedding models to capture the data semantics, which are then used to guide the selection of pipeline components. We implement CtxPipe with deep reinforcement learning and evaluate it against state-of-the-art automated pipeline construction solutions. Our comprehensive experiments demonstrate that CtxPipe outperforms all of the baselines in both model performance and runtime cost.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {231},
numpages = {27},
keywords = {data analytics pipeline, data preparation}
}

@article{10.1145/3699730,
author = {Lyu, Feng and Zhang, Jie and Lu, Huali and Wu, Huaqing and Wu, Fan and Zhang, Yongmin and Zhang, Yaoxue},
title = {SynthCAT: Synthesizing Cellular Association Traces with Fusion of Model-Based and Data-Driven Approaches},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699730},
doi = {10.1145/3699730},
abstract = {The scarcity of publicly available cellular association traces hinders user location-based research and various data-driven services, highlighting the importance of data synthesis in this field. In this paper, we investigate the cellular association trace synthesis (CATS) problem, aiming to generate diverse and realistic cellular association traces based on road segment-based trajectories and corresponding departure times. To substantiate our research, we first gather substantial data, including road segment-based trajectories, base station (BS) distribution, and ground truths of cellular association traces. We then perform systematic data analysis to reveal technical challenges such as disparity in geographic spaces, complex and dynamic BS handover, and poor performance of single-dimension approaches. To address these challenges, we propose SynthCAT, a novel scheme that fuses model-based and data-driven approaches. Specifically, SynthCAT includes: i) A model-based coarse-grained cellular association trace generation component, encompassing GPS reference generation, weighted historical average time generation, Bayesian decision, and time mapping modules. This component establishes a unified GPS space to map road and BS spaces, generates initial time information, synthesizes coarse-grained spatial cellular association traces by following explicit BS handover rules, and maps the corresponding arrival time for each trace point; ii) A fine-grained cellular association trace generation component, which combines model-based and data-driven approaches. This employs a two-stage Autoencoder Generative Adversarial Network (AEGAN) to refine cellular association traces based on the coarse-grained ones. Extensive field experiments validate the efficacy of SynthCAT in terms of trace similarity to ground truths and its efficiency in supporting practical downstream applications.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {161},
numpages = {24},
keywords = {Autoencoder Generative Adversarial Network, Downstream applications support, Model-based, Synthesizing cellular association traces, data-driven fusion}
}

@article{10.1145/3699754,
author = {Ling, Yue and Zhao, Dong and Deng, Kaikai and Yin, Kangwen and Zheng, Wenxin and Ma, Huadong},
title = {Uranus: Empowering Generalized Gesture Recognition with Mobility through Generating Large-scale mmWave Radar Data},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699754},
doi = {10.1145/3699754},
abstract = {Millimeter-wave radar shows great sensing capabilities for pervasive and privacy-preserving gesture recognition. However, the lack of large-scale, dynamic radar datasets hinders the advancement of deep learning models for generalized gesture recognition in dynamic scenes. To address this problem, we opt for designing a system that employs wealthy dynamic 2D videos to generate realistic radar data, but it confronts two challenges including i) simulating the complex signal reflection characteristics of humans and the background and ii) extracting elusive gesture-relevant features from dynamic radar data. To this end, we design Uranus with two key components: (i) a dynamic data generation network (DDG-Net) combines several key modules, human reflection model, background reflection extractor, and data fitting model to simulate the signal reflection characteristics of humans and the background, followed by fitting the number and global distribution of points in point clouds to generate realistic radar data; (ii) a dynamic gesture recognition network (DGR-Net) combines two modules, spatial feature extraction and global feature fusion, to extract spatial and global features of points in point clouds, respectively, to achieve generalized gesture recognition. We implement and evaluate Uranus with dynamic video data from public video sources and self-collected radar data, demonstrating that Uranus outperforms the state-of-the-art approaches for gesture recognition in dynamic scenes.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {204},
numpages = {28},
keywords = {2D videos, dynamic data generation, gesture recognition, radar sensing}
}

@article{10.1145/3699757,
author = {Sui, Yueyuan and Zhao, Minghui and Xia, Junxi and Jiang, Xiaofan and Xia, Stephen},
title = {TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699757},
doi = {10.1145/3699757},
abstract = {We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state-of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3\% in Perceptual Evaluation of Speech Quality (PESQ) and 1.8\% in Short-Time Objective Intelligibility (STOI), with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160\% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {205},
numpages = {29},
keywords = {Audio Super Resolution, Bone Conduction Enhancement, Earable Computing, Mamba, Speech Enhancement, Transformer}
}

@article{10.1145/3700144,
author = {\r{A}kesson, Alfred and Gehrmann, Christian and Hedin, G\"{o}rel and Johnsson, Bj\"{o}rn A. and Magnusson, Boris and Nordahl, Mattias and Ramezanian, Sara and Stankovski Wagner, Paul},
title = {A Trust Establishment and Key Management Architecture for Hospital-at-Home},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700144},
doi = {10.1145/3700144},
abstract = {The landscape of healthcare is experiencing a digitalization shift, transferring many medical activities to the patients’ homes, a phenomenon commonly referred to as Hospital-at-Home. While Internet of Things (IoT) devices facilitate the building of such systems, there is a need for powerful middleware that encapsulates device-to-device communication, and enables the construction of user-friendly, secure, and robust Hospital-at-Home systems. A key challenge for such middleware is to build a trustworthy and lightweight key management system allowing different devices in the system to exchange messages securely. In this paper we present a simple, easily manageable and scalable such architecture which, in addition, supports long term data protection using post-quantum cryptographic primitives. Our proposed solution utilizes a Merkle tree to enable the IoT devices to establish trust between each other automatically, even in the absence of Internet connection. We have implemented the architecture and present performance figures as well as a security analysis of our approach.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
keywords = {Hospital-at-Home, IoT Middleware, Key Management, Trust Establishment, Post Quantum Cryptography}
}

@article{10.1145/3700593,
author = {Kolokasis, Iacovos G. and Evdorou, Giannos and Akram, Shoaib and Kozanitis, Christos and Papagiannis, Anastasios and Zakkak, Foivos S. and Pratikakis, Polyvios and Bilas, Angelos},
title = {TeraHeap: Exploiting Flash Storage for Mitigating DRAM Pressure in Managed Big Data Frameworks},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3700593},
doi = {10.1145/3700593},
abstract = {Big data analytics frameworks, such as Spark and Giraph, need to process and cache massive datasets that do not always fit on the managed heap. Therefore, frameworks temporarily move long-lived objects outside the heap (off-heap) on a fast storage device. However, this practice results in (1) high serialization/deserialization (S/D) cost and (2) high memory pressure when off-heap objects are moved back for processing.In this article, we propose TeraHeap, a system that eliminates S/D overhead and expensive GC scans for a large portion of objects in analytics frameworks. TeraHeap relies on three concepts: (1) It eliminates S/D by extending the managed runtime (JVM) to use a second high-capacity heap (H2) over a fast storage device. (2) It offers a simple hint-based interface, allowing analytics frameworks to leverage object knowledge to populate H2. (3) It reduces GC cost by fencing the collector from scanning H2 objects while maintaining the illusion of a single managed heap, ensuring memory safety.We implement TeraHeap in OpenJDK8 and OpenJDK17 and evaluate it with fifteen widely used applications in two real-world big data frameworks, Spark and Giraph. We find that for the same DRAM size, TeraHeap improves performance by up to 73\% and 28\% compared to native Spark and Giraph. Also, it can still provide better performance by consuming up to  (4.6times)  and  (1.2times)  less DRAM than native Spark and Giraph, respectively. TeraHeap can also be used for in-memory frameworks and applying it to the Neo4j Graph Data Science library improves its performance by up to 26\%. Finally, it outperforms Panthera, a state-of-the-art garbage collector for hybrid DRAM-NVM memories, by up to 69\%.},
journal = {ACM Trans. Program. Lang. Syst.},
month = dec,
articleno = {12},
numpages = {37},
keywords = {Java Virtual Machine (JVM), large analytics, serialization, large managed heaps, memory management, garbage collection, memory hierarchy, fast storage devices}
}

@article{10.1145/3700640,
author = {Jiang, Bin and Feng, Jiacheng and Cui, Xuerong and Wang, Jian and Liu, Yongxin and Song, Houbing},
title = {Security and Reliability of Internet of Underwater Things: Architecture, Challenges, and Opportunities},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3700640},
doi = {10.1145/3700640},
abstract = {The Internet of Underwater Things (IoUT) pertains to a system that utilizes technology of Internet of Things (IoT) for data collection, communication, and control in the underwater environment. The monitoring and management of various parameters in the underwater domain are gathered through the deployment of underwater sensors, communication devices, and controllers. It is crucial in emerging ocean engineering. However, due to the instability of the underwater environment and the particularity of the underwater communication transmission medium, it is vulnerable to security threats, which may damage the system or cause data errors. In this survey, we will discuss the challenges, solutions, and future directions of IoUT from security and reliability, respectively. To ensure the normal operation of IoUT, we analyze the underwater security problems and solutions of the IoUT. Then, we discuss the reliability issue and improved strategies of IoUT system in detail. Finally, we come up with our views about the theories, challenges, and future prospects of IoUT security after the comparative analysis.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {67},
numpages = {37},
keywords = {Internet of Underwater Things, underwater wireless network, marine network security, malicious attack threats, underwater network reliability}
}

@article{10.1145/3700878,
author = {Xiang, Linhua and Wang, Zengfu},
title = {Joint Mixing Data Augmentation for Skeleton-based Action Recognition},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3700878},
doi = {10.1145/3700878},
abstract = {Skeleton-based action recognition is beneficial for understanding human behavior in videos, and thus has received much attention in recent years as an important research area in action recognition. Current research focuses on designing more advanced algorithms to better extract spatio-temporal information from skeleton data. However, due to the small amount of data in the existing skeleton dataset and the lack of effective data augmentation methods, it is easy to lead to overfitting in model training. To address this challenge, we propose a mix-based data augmentation method, Joint Mixing Data Augmentation (JMDA), which can generally improve the effectiveness and robustness of various skeleton-based action recognition algorithms. In terms of spatial information, we introduce SpatialMix (SM), a method that projects the original 3D skeleton discrete information into a 2D space. Then, SM mixes the projected spatial information between two random samples during the training process to achieve the spatial-based mixing data augmentation. Concerning temporal information, we propose TemporalMix (TM). Leveraging the temporal continuity in skeleton data, we perform a temporal resize operation on the original skeleton data, and then merge two random samples during training to achieve the temporal-based mixed data augmentation. Additionally, we analyze the Feature Mismatch (FM) problem caused by introducing mix-based data augmentation into skeleton data. Then we propose a new data preprocessing method called Feature Alignment (FA) to effectively address this problem and improve model performance. Moreover, we propose a novel training pipeline, Joint Training Strategy (JTS), which combines multiple mix-based data augmentation methods for further improvement of model performance. Specifically, our proposed JMDA is plug-and-play and widely applicable to skeleton-based action recognition models. At the same time, the application of JMDA does not increase the model parameters and there is almost no additional training cost. We conduct extensive experiments on NTU RGB+D 60 and NTU RGB+D 120 datasets to demonstrate the effectiveness and robustness of the proposed JMDA on several mainstream skeleton-based action recognition algorithms.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
keywords = {skeleton-based action recognition, data augmentation, graph convolutional networks}
}

@article{10.1145/3701032,
author = {Sharma, Sonam and Roy, Dipanjan and Pawar, Digambar},
title = {PROTECTS: Progressive Rtl Obfuscation with ThrEshold Control Technique during architectural Synthesis},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3701032},
doi = {10.1145/3701032},
abstract = {Due to the supply chain globalization of the semiconductor industry, securing heterogeneous System-on-Chip (SoC) is becoming necessary. A malicious alteration, inserting Hardware Trojan, infringement, or counterfeiting of design via Reverse Engineering (RE) is the primary reason. As RE allows attackers to uncover proprietary algorithms, design specifications, and other intellectual property, exploiting the design becomes easier. This has a havoc impact on the manufacturer’s revenue as well as erodes consumer trust in the authenticity of the devices. This enforces a robust framework from the topmost design abstraction level to protect against RE attacks. This article proposed a robust, architectural synthesis-driven dual-phase functional obfuscation framework for securing Register Transfer Level design. In this framework, obfuscation is achieved for both the datapath (DP) and control unit (CU) of design. Further, the robustness of obfuscated design is tested against sophisticated DP and CU level attacks. Moreover, to protect the design from brute force attack, a Consecutive Design Mis-Authentication Prevention Mechanism (CDMAP) is proposed. The proposed framework is validated using six standard Hardware Accelerator (HA) benchmarks and evaluated based on design overhead and robustness for different key sizes. A significant improvement is achieved in terms of security (∼1800 times) and (∼5.2 times) and strength of obfuscation (∼1.87 \texttimes{} 1056 times) and (∼5.94 \texttimes{} 1033 times) at a lower design cost of around (∼20.4\%) and (∼10.4\%) compared to two closely related approaches.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {7},
numpages = {34},
keywords = {Architectural synthesis, reverse engineering, hardware security, hardware obfuscation, hardware accelerator}
}

@article{10.1145/3701034,
author = {Xu, Jiahong and Liu, Haikun and Peng, Xiaoyang and Duan, Zhuohui and Liao, Xiaofei and Jin, Hai},
title = {A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3701034},
doi = {10.1145/3701034},
abstract = {Emerging resistive random-access memory (ReRAM) based processing-in-memory (PIM) accelerators have been increasingly explored in recent years because they can efficiently perform in-situ matrix-vector multiplication (MVM) operations involved in a wide spectrum of artificial neural networks. However, there remain significant challenges to apply existing ReRAM-based PIM accelerators to the most popular Transformer neural networks. Since Transformers involve a series of matrix-matrix multiplication (MatMul) operations with data dependencies, they should write intermediate results of MatMuls to ReRAM crossbar arrays for further processing. Conventional ReRAM-based PIM accelerators often suffer from high latency of ReRAM writes and intra-layer pipeline stalls.In this paper, we propose ReCAT, a ReRAM-based PIM accelerator designed particularly for Transformers. ReCAT exploits transimpedance amplifiers (TIAs) to cascade a pair of crossbar arrays for MatMul operations involved in the self-attention mechanism. The intermediate result of a MatMul generated by one crossbar array can be directly mapped to another crossbar array, avoiding costly analog-to-digital conversions. In this way, ReCAT allows MVM operations to overlap with the corresponding data mapping, hiding the high latency of ReRAM writes. Furthermore, we propose an analog-to-digital converter (ADC) virtualization scheme to dynamically share scarce ADCs among a group of crossbar arrays, and thus significantly improve the utilization of ADCs to eliminate the performance bottleneck of MVM operations. Experimental results show that ReCAT achieves 207.3\texttimes{}, 2.11\texttimes{}, and 3.06\texttimes{} performance improvement on average compared with other Transformer acceleration solutions—GPUs, ReBert, and ReTransformer, respectively.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {10},
numpages = {23},
keywords = {Transformer, ReRAM, PIM, analog-to-digital conversion}
}

@article{10.1145/3701232,
author = {Jiang, He and Zou, Peiyu and Li, Xiaochen and Zhou, Zhide and Zhao, Xu and Zhang, Yi and Guo, Shikai},
title = {DeLoSo: Detecting Logic Synthesis Optimization Faults Based on Configuration Diversity},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3701232},
doi = {10.1145/3701232},
abstract = {Logic synthesis tools are the core components of digital circuit design, which convert programs written in hardware description languages into gate-level netlists and optimize the netlists. However, the netlist optimization is complex, with numerous optimization parameters to be configured. Any minor optimization faults in logic synthesis tools may cause circuit diagrams to significantly deviate from the original design, posing risks in target systems. We propose DeLoSo, the Detector of Logic Synthesis optimization faults, the first method specifically designed to identify potential faults in the optimization processes of logic synthesis tools. DeLoSo relies on netlist differences and parameter variations to guide the generation of diverse Logic Synthesis Optimization Configuration (LSOC) combinations to thoroughly test the optimization process. DeLoSo consists of three components: LSOC generator, which generates diverse LSOC combinations through configuration recombination and mutation; LSOC diversity evaluator, which assesses the diversity of optimization configurations; and LSOC validator, which validates the generated LSOC combinations to discover optimization faults. DeLoSo identified 19 faults in two established logic synthesis tools (i.e., Vivado and Yosys); 15 of them have been fixed by vendors. Particularly, the community maintainers of Yosys have considered incorporating DeLoSo into Yosys’ existing test suite.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {12},
numpages = {26},
keywords = {Logic synthesis tools, optimization configurations, fault detection}
}

@article{10.1145/3702322,
author = {Giaccardi, Elisa and Murray-Rust, Dave and Redstr\"{o}m, Johan and Caramiaux, Baptiste},
title = {Prototyping with Uncertainties: Data, Algorithms, and Research through Design},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3702322},
doi = {10.1145/3702322},
abstract = {Seen both as a resource and an obstacle to clarity, uncertainty is a concept that permeates many areas of design. As the concept gains prominence in Human-Computer Interaction (HCI), this special issue specifically explores the interplay between uncertainty and prototyping in Research through Design (RtD). We first outline three histories of uncertainty in design, in relation to its philosophical significance, its role in statistical and algorithmic processes, and its importance in prototyping. The convergence of these aspects is crucial as design evolves toward more agentive and entangled systems, introducing challenges such as Design as a Probabilistic Outcome. We then investigate the design spaces for engaging with “being uncertain” that emerge from the papers: from nuancing the relationship between designers and quantitative data to blurring the line between humans, fungi, and algorithms. Finally, we illuminate some preliminary threads for how RtD can navigate and engage with these shifting technological and design landscapes thoughtfully.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {68},
numpages = {21},
keywords = {Algorithms, Data, Prototyping, Research through Design, Uncertaintities}
}

@article{10.1145/3703353,
author = {Gao, Jianhua and Liu, Zeming and Wang, Yizhuo and Ji, Weixing},
title = {RaNAS: Resource-Aware Neural Architecture Search for Edge Computing},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3703353},
doi = {10.1145/3703353},
abstract = {Neural architecture search (NAS) for edge devices is often time-consuming because of long-latency deploying and testing on edge devices. The ability to accurately predict the computation cost and memory requirement for convolutional neural networks (CNNs) in advance holds substantial value. Existing work primarily relies on analytical models, which can result in high prediction errors. This paper proposes a resource-aware NAS (RaNAS) model based on various features. Additionally, a new graph neural network is introduced to predict inference latency and maximum memory requirements for CNNs on edge devices. Experimental results show that, within the error bound of ±1\%, RaNAS achieves an accuracy improvement of approximately 8\% for inference latency prediction and about 25\% for maximum memory occupancy prediction over the state-of-the-art approaches.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
keywords = {Neural architecture search, edge devices, graph neural network, computational resource}
}

@article{10.1145/3703593,
author = {Ionescu, Bogdan and Patras, Ioannis and M\"{u}ller, Henning and Del Bimbo, Alberto},
title = {Introduction to the Special Issue on Realistic Synthetic Data: Generation, Learning, Evaluation},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3703593},
doi = {10.1145/3703593},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {1},
numpages = {7},
keywords = {synthetic data, Generative Adversarial Networks, Diffusion Models, data augmentation, data generation, datasets}
}

@article{10.1145/3703626,
author = {Wang, Tao and Zhang, Yushu and Qi, Shuren and Zhao, Ruoyu and Xia, Zhihua and Weng, Jian},
title = {Security and Privacy on Generative Data in AIGC: A Survey},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3703626},
doi = {10.1145/3703626},
abstract = {The advent of artificial intelligence-generated content (AIGC) represents a pivotal moment in the evolution of information technology. With AIGC, it can be effortless to generate high-quality data that is challenging for the public to distinguish. Nevertheless, the proliferation of generative data across cyberspace brings security and privacy issues, including privacy leakages of individuals and media forgery for fraudulent purposes. Consequently, both academia and industry begin to emphasize the trustworthiness of generative data, successively providing a series of countermeasures for security and privacy. In this survey, we systematically review the security and privacy on generative data in AIGC, particularly for the first time analyzing them from the perspective of information security properties. Specifically, we reveal the successful experiences of state-of-the-art countermeasures in terms of the foundational properties of privacy, controllability, authenticity, and compliance, respectively. Finally, we show some representative benchmarks, present a statistical analysis, and summarize the potential exploration directions from each of these properties.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {82},
numpages = {34},
keywords = {Information security, AIGC, generative data, privacy, controllability, authenticity, compliance}
}

@article{10.1145/3703913,
author = {Zhou, Yuxuan and Mingyang, Li and Jingze, Tong and Linlin, Li and Zhiwei, Yang},
title = {SD-Meta: The Software-Defined Network of Human-Centric Metaverse for Multi-Lead or Multi-Media Data in Spread Spectrum Communications},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3703913},
doi = {10.1145/3703913},
abstract = {In this paper, the novel adjacent two-end link or network multi-end link concept of software-defined network is presented to support the data transmission of electroencephalogram, audio and video streaming in spread spectrum communications. Instead of solving the problem of software-defined prioritization scheme in networking calculation and communication, the southbound-northbound structure of existing neural or information network is found in control and perceptual interactions. The proposed framework of multi-lead and multi-media structures allow the human-centric Metaverse to execute different operations for local and global planning schemes with the high-speed lead or media data transmission of spread spectrum streaming, depending on the kind of brain-computer interaction, and similar to the system of multi-modal perception in the routing or switching. Firstly, this research designed the filed programmable gate array for conventional routing nodes in the computation of different neural networks. Secondly, this study demonstrates its enhanced applicability with software core for multiple routers in the computing power network of the Internet of brains in different brain-computer smart terminals. Experiment results also show the benefits of proposed model and structure, and exposing the different running indicators in the simulation.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
keywords = {Software-defined network, human-centric Metaverse, multi-lead and multi-media, Internet of brains}
}

@article{10.1145/3704437,
author = {Lautrup, Anton Danholt and Hyrup, Tobias and Zimek, Arthur and Schneider-Kamp, Peter},
title = {Systematic Review of Generative Modelling Tools and Utility Metrics for Fully Synthetic Tabular Data},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704437},
doi = {10.1145/3704437},
abstract = {Sharing data with third parties is essential for advancing science, but it is becoming more and more difficult with the rise of data protection regulations, ethical restrictions, and growing fear of misuse. Fully synthetic data, which transcends anonymisation, may be the key to unlocking valuable untapped insights stored away in secured data vaults. This review examines current synthetic data generation methods and their utility measurement. We found that more traditional generative models such as Classification and Regression Tree models alongside Bayesian Networks remain highly relevant and are still capable of surpassing deep learning alternatives like Generative Adversarial Networks. However, our findings also display the same lack of agreement on metrics for evaluation, uncovered in earlier reviews, posing a persistent obstacle to advancing the field. We propose a tool for evaluating the utility of synthetic data and illustrate how it can be applied to three synthetic data generation models. By streamlining evaluation and promoting agreement on metrics, researchers can explore novel methods and generate compelling results that will convince data curators and lawmakers to embrace synthetic data. Our review emphasises the potential of synthetic data and highlights the need for greater collaboration and standardisation to unlock its full potential.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {90},
numpages = {38},
keywords = {Synthetic data, generative modelling, tabular data, models and metrics, utility and privacy, model benchmark, privacy enhancing technologies}
}

@article{10.1145/3704923,
author = {Nagrecha, Kabir and Liu, Lingyi and Delgado, Pablo},
title = {Reinforcement Learning for Intra- \&amp; Inter-Node Recommender Data Pipeline Optimization},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704923},
doi = {10.1145/3704923},
abstract = {Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved for DLRM training, driving new interest in cost- \&amp; time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning (DL) training jobs are dominated by model execution times, the most important factor in DLRM training performance is often online data ingestion.In this paper, we study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and identify shortfalls in existing data pipeline optimizers. Our studies lead us to design a new solution for data pipeline optimization, InTuneX. InTuneX&nbsp;is designed for production-scale multi-node recommender data pipelines. It unifies \&amp; tackles the challenges of both intra- and inter- node pipeline optimization. We achieve this with a multi-agent reinforcement learning (RL) design, simultaneously optimizing node assignments at the cluster level \&amp; CPU assignments within nodes. Our experiments show that InTuneX&nbsp;can build optimized data pipeline configurations within minutes. We apply InTuneX&nbsp;to our cluster, and find that it increases single-node data ingestion throughput by as much as 2.29X versus state-of-the-art optimizers, while improving the cost-efficiency of multi-node pipelines by 15-25\%.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = dec,
keywords = {data processing, recommendation systems, deep learning, parallel computing, resource allocation}
}

@article{10.1145/3705313,
author = {Dang, Xiaochao and Ding, GuoZhen and Dong, Xiaohui and Li, Fengfang and Gao, Shiwei and Wang, Yue},
title = {UIE-Based Relational Extraction Task for Mine Hoist Fault Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3705313},
doi = {10.1145/3705313},
abstract = {Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59\% to 92.51\%.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
keywords = {joint extraction, mechanical problem, mining sector, prompt learning}
}

@article{10.1145/3705863,
author = {Hamed, Naeima and Rana, Omer and Orozco-terWengel, Pablo and Goossens, Beno\^{\i}t and Perera, Charith},
title = {A Comparison of Open Data Observatories},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3705863},
doi = {10.1145/3705863},
abstract = {Open Data Observatories refer to online platforms that provide real-time and historical data for a particular application context, e.g., urban/non-urban environments or a specific application domain. They are generally developed to facilitate collaboration within one or more communities through reusable datasets, analysis tools, and interactive visualizations. Open Data Observatories collect and integrate various data from multiple disparate data sources—some providing mechanisms to support real-time data capture and ingest. Data types can include sensor data (soil, weather, traffic, pollution levels) and satellite imagery. Data sources can include Open Data providers, interconnected devices, and services offered through the Internet of Things. The continually increasing volume and variety of such data require timely integration, management, and analysis, yet presented in a way that end-users can easily understand. Data released for open access preserve their value and enable a more in-depth understanding of real-world choices. This survey compares thirteen Open Data Observatories and their data management approaches - investigating their aims, design, and types of data. We conclude with research challenges that influence the implementation of these observatories, outlining some strengths and limitations for each one and recommending areas for improvement. Our goal is to identify best practices learned from the selected observatories to aid the development of new Open Data Observatories.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = nov,
keywords = {Urban and non-urban data observatories, FAIR Open Data principles, Data integration, Data platforms.}
}

@article{10.1145/3706063,
author = {Carvalho, Jason and Daga, Enrico and Mulholland, Paul and Asprino, Luigi and Uwasomba, Chukwudi and Daquino, Marilena and Gangemi, Aldo and Maguire, Mark and Stoneman, Adam},
title = {Integrating Citizen Experiences in Cultural Heritage Archives with a Linked Non-Open Data Hub},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3706063},
doi = {10.1145/3706063},
abstract = {This article explores and presents innovative methods and technologies for supporting citizen curation of cultural heritage. Relevant outcomes of the Social Participation, Cohesion and Inclusion through Cultural Engagement (SPICE) project are presented, focusing on enhancing the state of content management and delivery strategies in museums and memory institutions. We argue that citizen curation requires a principled way of managing and integrating citizen responses, contributions and dataflows in the domain of cultural heritage, raising challenges and opportunities such as integration of distributed and diverse data sources, authoritativeness, interdependence, privacy, data reuse and rights management. The solution is a Linked Data Hub (LDH), which integrates museum collections and user-generated content and repurposes them to end-user systems tailored to specific use cases for citizens and museum practitioners. Such LDH must be non-open, by offering an approach that gives citizens and organisations, such as museums or engagement agencies, meaningful control over their data by implementing user-tailored policies and negotiated access and terms of use. Additionally, our solution addresses privacy violations in user-contributed content by offering a near-real-time content monitoring framework. We present the LDH discussing pilot applications within the EU-funded project SPICE, including ‘Deep Viewpoints’, which currently supports the Irish Museum of Modern Art (IMMA) in citizen curation activities. Overall, this article serves as a critical milestone in closing functional gaps and advancing the state of technology in managing citizen responses and contributions in the cultural heritage domain.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {71},
numpages = {39},
keywords = {Cultural Heritage, Linked Data, Citizen Curation, Data Management, Data Integration, RDF, SPARQL}
}

@article{10.1145/3706582,
author = {Busch, Norbert Rudolf and Zalewski, Andrzej},
title = {A Systematic Literature Review of Enterprise Architecture Evaluation Methods},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3706582},
doi = {10.1145/3706582},
abstract = {Enterprise Architecture (EA) is a systematic and holistic approach to designing and managing an organization's information systems components, aiding in optimizing resources, managing risk, and facilitating change. It weighs different architectural quality attributes against each other to achieve the most advantageous architecture. However, the evaluation of EA lacks a systematic approach. This study employs a Systematic Literature Review, analyzing, in detail, 109 articles carefully selected from 3644 papers published since 2005. The key outcome of the research reveals that a crucial factor for the extensive worldwide adoption of EA evaluation methods lies in the automation of the assessment and architecture modeling processes, particularly emphasizing the facet of data collection. The automation of EA evaluation will empower organizations to streamline their processes, make data-driven decisions, and respond more effectively to change, ultimately contributing to their competitiveness and long-term success in the global market. The study identifies diverse evaluation methods, determines evaluation criteria, examines the extent to which these methods have been verified in practice, and provides directions for further research and advancement.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = nov,
keywords = {Enterprise architecture, evaluation, assessment, analysis, measurement, systematic literature review}
}

@article{10.1145/3706587,
author = {Valente, Hugo and de Miguel, Miguel and P\'{e}rez-Mu\~{n}oz, \'{A}ngel and Alonso, Alejandro and Zamorano, Juan and de la Puente, Juan},
title = {Model-based Toolchain for Core Flight System (cFS) Embedded Systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1539-9087},
url = {https://doi.org/10.1145/3706587},
doi = {10.1145/3706587},
abstract = {The space domain is experiencing a paradigm shift with the rise of micro and nanosatellites. Historically, launching a satellite required a big financial risk only sustained by governments or big companies. Nowadays, with the miniaturization of satellites, there has been a significant reduction in costs and, as a consequence, a greater opportunity for universities and smaller businesses to launch satellites into space.Companies are taking advantage of this reduction in launch and manufacturing costs to gain a competitive edge by adopting what is known as “Agile Space”, which emphasizes rapid iterations. To facilitate this high development pace, specialized toolchains and frameworks are designed for satellite software development.In this article, we provide a solution to reduce the development time of embedded software systems by ensuring consistency between the design and the implementation. We have integrated the core Flight System (cFS), a message-oriented framework developed by NASA based on a publish-subscribe architecture, with TASTE, a toolset from the European Space Agency. This integration combines modeling capabilities and automatic code generation, reducing error-prone repetitive tasks. It ensures consistency across different development stages allowing the end-user to focus on the implementation-specific details. To demonstrate the feasibility and advantages of this model-based toolchain, we present a case study of the UPMSat-2 microsatellite. This study demonstrates how this approach can be used to successfully support the development of embedded software systems.},
note = {Just Accepted},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
keywords = {Model-Driven Engineering, Component-Based Software Development, New Space, Flight Software}
}

@article{10.1145/3707449,
author = {Zhang, Qin and Zheng, Mengqi and Chen, Shangsi and Liu, Han and Fang, Meng},
title = {Self Data Augmentation for Open Domain Question Answering},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3707449},
doi = {10.1145/3707449},
abstract = {Information Retrieval (IR) constitutes a vital facet of Open-Domain Question Answering (ODQA) systems, focusing on the exploration of pertinent information within extensive collections of passages, such as Wikipedia, to facilitate subsequent reader processing. Historically, information retrieval relied on textual overlaps for relevant context retrieval, employing methods like BM25 and TF-IDF, which, however, lacked natural language understanding. The advent of deep learning ushered in a new era, leading to the introduction of Dense Passage Retrievers (DPR), shows superiority over traditional sparse retrievers. These dense retrievers leverage Pre-trained Language Models (PLMs) to initialize context encoders, enabling the extraction of natural language representations. They utilize the distance between latent vectors of contexts as a metric for assessing similarity. However, DPR methods are heavily reliant on large volumes of meticulously labeled data, such as Natural Questions. The process of data labeling is both costly and time-intensive. In this paper, we propose a novel data augmentation methodology SDA (Self Data Augmentation) that employs DPR models to automatically annotate unanswered questions. Specifically, we initiate the process by retrieving relevant pseudo passages for these unlabeled questions. We subsequently introduce three distinct passage selection methods to annotate these pseudo passages. Ultimately, we amalgamate the pseudo-labeled passages with the unanswered questions to create augmented data. Our experimental evaluations conducted on two extensive datasets (Natural Questions and TriviaQA), alongside a reletively small dataset (WebQuestions), utilizing three diverse base models, illustrate the significant enhancement achieved through the incorporation of freshly augmented data. Moreover, our proposed data augmentation method exhibits remarkable flexibility, which is readily adaptable to various dense retrievers. Additionally, we have conducted a comprehensive human study on the augmented data, which further supports our conclusions.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = dec,
keywords = {Open Domain Question Answering, Deep Neural Networks, Data Augmentation, Distant Supervision}
}

@article{10.1145/3707455,
author = {Jiang, Yingjie and Mo, Ran and Zhan, Wenjing and Wang, Dongyu and Li, Zengyang and Ma, Yutao},
title = {Leveraging Modular Architecture for Bug Characterization and Analysis in Automated Driving Software},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707455},
doi = {10.1145/3707455},
abstract = {With the rapid advancement of automated driving technology, numerous manufacturers deploy vehicles with auto-driving features. This highlights the importance of ensuring the quality of automated driving software. To achieve this, characterizing bugs in automated driving software is important, as it can facilitate bug detection and bug fixes, thereby ensuring software quality. Automated driving software typically has a modular architecture, where software is divided into multiple modules, each designed for its own functionality for automated driving. This may lead to varying bug characteristics. Additionally, our recent study has shown a correlation between bugs caused by code clones and the functionalities of modules in automated driving software. Hence, we consider the modular structure when analyzing bug characteristics. In this paper, we analyze 3,078 bugs from two representative open-source Level-4 automated driving systems, Apollo and Autoware. By analyzing the bug report description, title, and developers’ discussions, we have identified 20 bug symptoms and 17 bug-fixing strategies, and analyzed their relationships with the respective modules. Our analysis achieves 12 main findings offering a comprehensive view of bug characteristics in automated driving software. We believe our findings can help developers better understand and manage bugs in automated driving software, thereby improving software quality and reliability.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Automated Driving Software, Software Modules, Bug Characterization, Bug Analysis}
}

@article{10.1145/3707457,
author = {Vitale, Antonio and Oliveto, Rocco and Scalabrino, Simone},
title = {A Catalog of Data Smells for Coding Tasks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707457},
doi = {10.1145/3707457},
abstract = {Large Language Models (LLMs) are increasingly becoming fundamental in supporting software developers in coding tasks. The massive datasets used for training LLMs are often collected automatically, leading to the introduction of data smells. Previous work addressed this issue by using quality filters to handle some specific smells. Still, the literature lacks a systematic catalog of the data smells for coding tasks currently known. This paper presents a Systematic Literature Review (SLR) focused on articles that introduce LLMs for coding tasks. We first extracted the quality filters adopted for training and testing such LLMs, inferred the root problem behind their adoption (data smells for coding tasks), and defined a taxonomy of such smells. Our results highlight discrepancies in the adoption of quality filters between pre-training and fine-tuning stages and across different coding tasks, shedding light on areas for improvement in LLM-based software development support.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {LLMs for coding tasks, data smells, data quality, Systematic Literature Review}
}

@article{10.1145/3707459,
author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Lavista Ferres, Juan M. and Banerjee, Arindam},
title = {TorchGeo: Deep Learning With Geospatial Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2374-0353},
url = {https://doi.org/10.1145/3707459},
doi = {10.1145/3707459},
abstract = {Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that may have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for uncurated geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.},
note = {Just Accepted},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = dec,
keywords = {deep learning, computer vision, remote sensing, earth observation, satellite imagery, geospatial, datasets, samplers, transforms, models}
}

@article{10.1145/3707647,
author = {Candela, Gustavo},
title = {Browsing Linked Open Data in Cultural Heritage: a shareable visual configuration approach},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3707647},
doi = {10.1145/3707647},
abstract = {Over the last decade Cultural Heritage (CH) organizations have been exploring new ways to make their content available and browsable using the Semantic Web and Linked Open Data (LOD). The objective of the present study was to introduce a framework to browse LOD in CH institutions using a shareable visual configuration approach. The framework was then applied to a selection of LOD repositories. Common features and best practices were identified. A detailed analysis, that can be useful for organizations interested in publishing and explore their datasets, is provided. Open issues requiring further work are outlined.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = dec,
keywords = {Semantic Web, Linked Open Data, Data visualisation, Cultural Heritage}
}

@article{10.1145/3707693,
author = {Caroprese, Luciano and Pisani, Francesco and Veloso, Bruno Miguel and Konig, Matthias and Manco, Giuseppe and Hoos, Holger and Gama, Joao},
title = {Modelling Concept Drift in Dynamic Data Streams for Recommender Systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707693},
doi = {10.1145/3707693},
abstract = {Recommendation systems play a crucial role in modern e-commerce and streaming services. However, the limited availability of public datasets hampers the rapid development of more efficient and accurate recommendation algorithms within the research community. This work introduces a stream-based data generator designed to generate user preferences for a set of items while accommodating progressive changes in user preferences. The underlying principle involves using user/item embeddings to derive preferences by exploring the proximity of these embeddings. Whether randomly generated or learned from a real finite data stream, these embeddings serve as the basis for generating new preferences. We investigate how this fundamental model can adapt to shifts in user behavior over time; in our framework, changes correspond to alterations in the structure of the tripartite graph, reflecting modifications in the underlying embeddings. Through an analysis of real-life data streams, we demonstrate that the proposed model is effective in capturing actual preferences and the changes that they can exhibit over time. Thus, we characterize these changes and develop a generalized method capable of simulating realistic data, thereby generating streams with similar yet controllable drift dynamics.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = dec,
keywords = {Recommender Systems, Collaborative Filtering, Concept Drift Adaptation, Data Generation}
}

@article{10.1145/3708346,
author = {Chen, Yipei and Yuen, Hua and Ma, Baojun and Wang, Limin and Qian, Yu},
title = {Beyond Songs: Analyzing User Sentiment Through Music Playlists and Multimodal Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3708346},
doi = {10.1145/3708346},
abstract = {The automatic recognition of user sentiments through their music listening behavior is an important research task in cognitive studies. Whereas prior studies were conducted to identify the sentiment conveyed (or evoked) by a song that a user listens to at a particular time, we argue that a more effective method would be to identify the user’s induced sentiment based on the comprehensive list of songs they have listened to (e.g., the sequence of music being played). However, recognizing the sentiment information induced by a playlist using machine learning techniques is much more challenging than identifying the sentiment induced by a single song, as it is difficult to obtain accurately labeled training samples for playlists. In this study, we developed the List-Song Relationship Factorization (LSRF) model with the objective of efficiently identifying sentiments induced by playlists. This model employs two side information constraints: the sentiment similarity between songs, based on multimodal information, and the co-occurrence of songs in playlists. These constraints enable the simultaneous co-clustering of songs and playlists. The experimental results demonstrate that the proposed model efficiently and consistently identifies sentiment information evoked by either playlists or individual songs.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
keywords = {Music sentiment, playlist, matrix factorization, multimodality data, co-clustering}
}

@article{10.1145/3708469,
author = {Ni, Ziying and Khalid, Ayesha and Liu, Weiqiang and O'Neill, M\'{a}ire},
title = {A Highly Hardware Efficient ML-KEM Accelerator with Optimised Architectural Layers},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3708469},
doi = {10.1145/3708469},
abstract = {The Module-Lattice-Based Key encapsulation Mechanism (ML-KEM) scheme, which is currently being standardised, is a quantum attack resistant KEM that is based on CRYSTALS-Kyber. CRYSTALS-Kyber is the only Public-key Encryption (PKE)/ KEM scheme selected in the first set of successful candidates as part of the NIST initiated Post-Quantum Cryptography (PQC) process. ML-KEM scheme includes three different security levels, namely security level 1, 3, and 5. In this research, we propose a highly area-time efficient hardware ML-KEM architecture. The architecture comprises three computational layers. The first layer comprises a hash and sampling module; the second layer includes a number theoretic transform (NTT), its inverse (INTT) and a point-wise multiplication (PWM) module; and the third layer comprises addition, compressing and encoding. Intra-layer pipelining and out-of-layer scheduling ensures that either layer 1 or layer 2 operate in the shortest time. In the reduction module, we propose a novel hybrid architecture to obtain the final result within 2 cycles with low area consumption. In the NTT module, the PWM pipelining method is modified and an optimised iterative FIFO access method is adopted to reduce the size of FIFO units by 55\% over previous research. Look-up tables are also used to replace the first-stage of the NTT to reduce 8 cycles. Furthermore, the memory unit uses only FIFOs, the size are optimised based on the requirements of the most resource-intensive function in ML-KEM (ML-KEM.CPA.Dec). The results show that the proposed architecture has a 48.2\%, 41.2\%, and 78.1\% reduction in computational time in comparison to previous work for security levels 1, 3, and 5, respectively. In addition, the area of proposed optimised ML-KEM designs is reduced by 73\%, 70\%, 76\% and resulting in an improved area-time (AT) product of 15.8\%, 10.7\%, and 11.3\%, for the Level 1, 3, and 5&nbsp;security levels respectively, compared with state-of-the-art designs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jan,
articleno = {25},
numpages = {24},
keywords = {Post-quantum Cryptography (PQC), Key Exchange Mechanism (KEM), Module-Lattice-Based (ML)-KEM, CRYSTALS-Kyber, hardware architecture}
}

@article{10.1145/3708539,
author = {Karunakar, Shruthi and Kalayappan, Rajshekar and Chandran, Sandeep},
title = {Consequence-based Clustered Architecture},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3708539},
doi = {10.1145/3708539},
abstract = {We recognize that the execution of many dynamic instructions have no consequence on the overall execution of the program. For example, the execution of a correctly predicted conditional branch instruction, as well as all the instructions leading up to it, are inconsequential. We propose a clustered architecture that steers consequential instructions to the primary cluster, and inconsequential ones to the secondary one called the I-Pipe that is less capable and thereby, more area and power efficient. The proposed architecture also entails minimal inter-cluster communication, thereby greatly reducing the complexities of inter-cluster result buses. Such a steering policy also helps increase the performance as the consequential instructions do not face any interference from the inconsequential ones. We demonstrate a  (42\% )  area reduction as compared to a baseline single cluster (Tigerlake-based) architecture, a  (18.5\% )  power reduction in the SPEC CPU2017 suite ( (13.7\% )  power reduction in GAPBS), and a  (5.15\% )  performance uplift in the SPEC CPU2017 suite ( (10.22\% )  in the GAPBS suite).},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
keywords = {Microarchitecture, Execution clusters, Parallelism, Consequentiality}
}

@article{10.1145/3709158,
author = {Singh, Akarsh and Sural, Shounak and Sengupta, Tirthankar and Sural, Shamik},
title = {AVChain: Trusted Sharing of Autonomous Vehicle Crash Incident Data using Interoperating HyperLedger Fabric Networks and IPFS},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709158},
doi = {10.1145/3709158},
abstract = {Autonomous vehicles (AVs) are gaining in popularity over the years as a viable cab service apps as well as for personal use. However, incidents of crashes involving AVs continue to occur, adversely affecting their prospects for widespread acceptance by both end users and regulatory authorities. While such cases are routinely investigated, in the absence of a human to testify on what caused the crash, one has to rely solely on available data. It is therefore imperative that the data logged by AVs is accessible to the concerned parties in a trustworthy manner. In this paper, we present AVChain - a novel framework for using a permissioned blockchain like HyperLedger Fabric (HLF) to record and share AV data comprised of sensors, actuators, maps, planning algorithms and machine learning models so that the data stays immutable even in the face of cross blaming among involved parties. Since the data volume is extremely large, we appropriately compress and down sample the same before storing in a distributed file system, namely, IPFS (Inter-Planetary File System). The hashes of such IPFS data called Content Ids (CIDs) are committed to the HLF network for making them tamper proof. The HLF ledger can later be queried to obtain the CIDs, which are then further used to retrieve and un-compress the original data from IPFS. Effectiveness and usability of AVChain is demonstrated by generating autonomous vehicle data from CARLA, which is a widely used open source AV simulator. For sharing AV data across organizations like sensor and actuator suppliers, map service providers, machine learning model developers and law enforcement authorities, the Weaver tool has been used to make multiple HLF networks interoperate. We have also developed a web application to demonstrate the working of AVChain. Results of an extensive set of experiments establish the efficacy of our approach.},
note = {Just Accepted},
journal = {Distrib. Ledger Technol.},
month = dec,
keywords = {Autonomous vehicles, Trusted data sharing, HyperLedger Fabric, IPFS, CARLA, Blockchain interoperability}
}

@article{10.1145/3711118,
author = {Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia},
title = {Data-centric Artificial Intelligence: A Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3711118},
doi = {10.1145/3711118},
abstract = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jan,
keywords = {Artificial intelligence, machine learning, data-centric AI}
}

@article{10.14778/2556549.2556559,
author = {Huai, Yin and Ma, Siyuan and Lee, Rubao and O'Malley, Owen and Zhang, Xiaodong},
title = {Understanding insights into the basic structure and essential issues of table placement methods in clusters},
year = {2013},
issue_date = {September 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/2556549.2556559},
doi = {10.14778/2556549.2556559},
abstract = {A table placement method is a critical component in big data analytics on distributed systems. It determines the way how data values in a two-dimensional table are organized and stored in the underlying cluster. Based on Hadoop computing environments, several table placement methods have been proposed and implemented. However, a comprehensive and systematic study to understand, to compare, and to evaluate different table placement methods has not been done. Thus, it is highly desirable to gain important insights into the basic structure and essential issues of table placement methods in the context of big data processing infrastructures.In this paper, we present such a study. The basic structure of a data placement method consists of three core operations: row reordering, table partitioning, and data packing. All the existing placement methods are formed by these core operations with variations made by the three key factors: (1) the size of a horizontal logical subset of a table (or the size of a row group), (2) the function of mapping columns to column groups, and (3) the function of packing columns or column groups in a row group into physical blocks. We have designed and implemented a benchmarking tool to provide insights into how variations of each factor affect the I/O performance of reading data of a table stored by a table placement method. Based on our results, we give suggested actions to optimize table reading performance. Results from large-scale experiments have also confirmed that our findings are valid for production workloads. Finally, we present ORC File as a case study to show the effectiveness of our findings and suggested actions.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1750–1761},
numpages = {12}
}

@article{10.14778/3137765.3137828,
author = {Eldawy, Ahmed and Mokbel, Mohamed F.},
title = {The era of big spatial data},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137828},
doi = {10.14778/3137765.3137828},
abstract = {In this tutorial, we present the recent work in the database community for handling Big Spatial Data. This topic became very hot due to the recent explosion in the amount of spatial data generated by smart phones, satellites and medical devices, among others. This tutorial goes beyond the use of existing systems as-is (e.g., Hadoop, Spark or Impala), and digs deep into the core components of big systems (e.g., indexing and query processing) to describe how they are designed to handle big spatial data. During this 90-minute tutorial, we review the state-of-the-art work in the area of Big Spatial Data while classifying the existing research efforts according to the implementation approach, underlying architecture, and system components. In addition, we provide case studies of full-fledged systems and applications that handle Big Spatial Data which allows the audience to better comprehend the whole tutorial.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1992–1995},
numpages = {4}
}

@article{10.14778/3342263.3342634,
author = {Fang, Yuanwei and Zou, Chen and Chien, Andrew A.},
title = {Accelerating raw data analysis with the ACCORDA software and hardware architecture},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342634},
doi = {10.14778/3342263.3342634},
abstract = {The data science revolution and growing popularity of data lakes make efficient processing of raw data increasingly important. To address this, we propose the ACCelerated Operators for Raw Data Analysis (ACCORDA) architecture. By extending the operator interface (subtype with encoding) and employing a uniform runtime worker model, ACCORDA integrates data transformation acceleration seamlessly, enabling a new class of encoding optimizations and robust high-performance raw data processing. Together, these key features preserve the software system architecture, empowering state-of-art heuristic optimizations to drive flexible data encoding for performance. ACCORDA derives performance from its software architecture, but depends critically on the acceleration of the Unstructured Data Processor (UDP) that is integrated into the memory-hierarchy, and accelerates data transformation tasks by 16x-21x (parsing, decompression) to as much as 160x (deserialization) compared to an x86 core.We evaluate ACCORDA using TPC-H queries on tabular data formats, exercising raw data properties such as parsing and data conversion. The ACCORDA system achieves 2.9x-13.2x speedups when compared to SparkSQL, reducing raw data processing overhead to a geomean of 1.2x (20\%). In doing so, ACCORDA robustly matches or outperforms prior systems that depend on caching loaded data, while computing on raw, unloaded data. This performance benefit is robust across format complexity, query predicates, and selectivity (data statistics). ACCORDA's encoding-extended operator interface unlocks aggressive encoding-oriented optimizations that deliver 80\% average performance increase over the 7 affected TPC-H queries.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1568–1582},
numpages = {15}
}

@article{10.14778/3352063.3352138,
author = {Cao, Lei and Tao, Wenbo and An, Sungtae and Jin, Jing and Yan, Yizhou and Liu, Xiaoyu and Ge, Wendong and Sah, Adam and Battle, Leilani and Sun, Jimeng and Chang, Remco and Westover, Brandon and Madden, Samuel and Stonebraker, Michael},
title = {Smile: a system to support machine learning on EEG data at scale},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352138},
doi = {10.14778/3352063.3352138},
abstract = {In order to reduce the possibility of neural injury from seizures and sidestep the need for a neurologist to spend hours on manually reviewing the EEG recording, it is critical to automatically detect and classify "interictal-ictal continuum" (IIC) patterns from EEG data. However, the existing IIC classification techniques are shown to be not accurate and robust enough for clinical use because of the lack of high quality labels of EEG segments as training data. Obtaining high-quality labeled data is traditionally a manual process by trained clinicians that can be tedious, time-consuming, and error-prone. In this work, we propose Smile, an industrial scale system that provides an end-to-end solution to the IIC pattern classification problem. The core components of Smile include a visualization-based time series labeling module and a deep-learning based active learning module. The labeling module enables the users to explore and label 350 million EEG segments (30TB) at interactive speed. The multiple coordinated views allow the users to examine the EEG signals from both time domain and frequency domain simultaneously. The active learning module first trains a deep neural network that automatically extracts both the local features with respect to each segment itself and the long term dynamics of the EEG signals to classify IIC patterns. Then leveraging the output of the deep learning model, the EEG segments that can best improve the model are selected and prompted to clinicians to label. This process is iterated until the clinicians and the models show high degree of agreement. Our initial experimental results show that our Smile system allows the clinicians to label the EEG segments at will with a response time below 500 ms. The accuracy of the model is progressively improved as more and more high quality labels are acquired over time.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2230–2241},
numpages = {12}
}

@article{10.14778/3476249.3476264,
author = {Min, Seung Won and Wu, Kun and Huang, Sitao and Hidayeto\u{g}lu, Mert and Xiong, Jinjun and Ebrahimi, Eiman and Chen, Deming and Hwu, Wen-mei},
title = {Large graph convolutional network training with GPU-oriented data communication architecture},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476264},
doi = {10.14778/3476249.3476264},
abstract = {Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92\% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2087–2100},
numpages = {14}
}

@article{10.14778/3503585.3503600,
author = {Paul, Debjyoti and Cao, Jie and Li, Feifei and Srikumar, Vivek},
title = {Database workload characterization with query plan encoders},
year = {2021},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503600},
doi = {10.14778/3503585.3503600},
abstract = {Smart databases are adopting artificial intelligence (AI) technologies to achieve instance optimality, and in the future, databases will come with prepackaged AI models within their core components. The reason is that every database runs on different workloads, demands specific resources, and settings to achieve optimal performance. It prompts the necessity to understand workloads running in the system along with their features comprehensively, which we dub as workload characterization.To address this workload characterization problem, we propose our query plan encoders that learn essential features and their correlations from query plans. Our pretrained encoders captures the structural and the computational performance of queries independently. We show that our pretrained encoders are adaptable to workloads that expedites the transfer learning process. We performed independent assessments of structural encoder and performance encoders with multiple downstream tasks. For the overall evaluation of our query plan encoders, we architect two downstream tasks (i) query latency prediction and (ii) query classification. These tasks show the importance of feature-based workload characterization. We also performed extensive experiments on individual encoders to verify the effectiveness of representation learning, and domain adaptability.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {923–935},
numpages = {13}
}

@article{10.14778/3598581.3598601,
author = {Sudhir, Sivaprasad and Tao, Wenbo and Laptev, Nikolay and Habis, Cyrille and Cafarella, Michael and Madden, Samuel},
title = {Pando: Enhanced Data Skipping with Logical Data Partitioning},
year = {2023},
issue_date = {May 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3598581.3598601},
doi = {10.14778/3598581.3598601},
abstract = {With enormous volumes of data, quickly retrieving data that is relevant to a query is essential for achieving high performance. Modern cloud-based database systems often partition the data into blocks and employ various techniques to skip irrelevant blocks during query execution. Several algorithms, often based on historical properties of a workload of queries run over the data, have been proposed to tune the physical layout of data to reduce the number of blocks accessed. The effectiveness of these methods at skipping blocks depends on what metadata is stored and how well the physical data layout aligns with the queries. Existing work on automatic physical database design misses significant opportunities in skipping blocks because it ignores logical predicates in the workload that exhibit strongly correlated results. In this paper, we present Pando which enables significantly better block skipping than past methods by informing physical layout decisions with correlation-aware logical partitioning. Across a range of benchmark and real-world workloads, Pando attains up to 2.8X reduction in the number of blocks scanned and up to 2.3X speedup in end-to-end query execution time over the state-of-the-art techniques.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {2316–2329},
numpages = {14}
}

@article{10.14778/3603581.3603599,
author = {Rong, Kexin and Budiu, Mihai and Skiadopoulos, Athinagoras and Suresh, Lalith and Tai, Amy},
title = {Scaling a Declarative Cluster Manager Architecture with Query Optimization Techniques},
year = {2023},
issue_date = {June 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3603581.3603599},
doi = {10.14778/3603581.3603599},
abstract = {Cluster managers play a crucial role in data centers by distributing workloads among infrastructure resources. Declarative Cluster Management (DCM) is a new cluster management architecture that enables users to express placement policies declaratively using SQL-like queries. This paper presents our experiences in scaling this architecture from moderate-sized enterprise clusters (102 - 103 nodes) to hyperscale clusters (104 nodes) via query optimization techniques. First, we formally specify the syntax and semantics of DCM's declarative language, C-SQL, a SQL variant used to express constraint optimization problems. We showcase how constraints on the desired state of the cluster system can be succinctly represented as C-SQL programs, and how query optimization techniques like incremental view maintenance and predicate pushdown can enhance the execution of C-SQL programs. We evaluate the effectiveness of our optimizations through a case study of building Kubernetes schedulers using C-SQL. Our optimizations demonstrated an almost 3000\texttimes{} speed up in database latency and reduced the size of optimization problems by as much as 1/300 of the original, without affecting the quality of the scheduling solutions.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {2618–2631},
numpages = {14}
}

@article{10.14778/3603581.3603604,
author = {Pedreira, Pedro and Erling, Orri and Karanasos, Konstantinos and Schneider, Scott and McKinney, Wes and Valluri, Satya R and Zait, Mohamed and Nadeau, Jacques},
title = {The Composable Data Management System Manifesto},
year = {2023},
issue_date = {June 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3603581.3603604},
doi = {10.14778/3603581.3603604},
abstract = {The requirement for specialization in data management systems has evolved faster than our software development practices. After decades of organic growth, this situation has created a siloed landscape composed of hundreds of products developed and maintained as monoliths, with limited reuse between systems. This fragmentation has resulted in developers often reinventing the wheel, increased maintenance costs, and slowed down innovation. It has also affected the end users, who are often required to learn the idiosyncrasies of dozens of incompatible SQL and non-SQL API dialects, and settle for systems with incomplete functionality and inconsistent semantics. In this vision paper, considering the recent popularity of open source projects aimed at standardizing different aspects of the data stack, we advocate for a paradigm shift in how data management systems are designed. We believe that by decomposing these into a modular stack of reusable components, development can be streamlined while creating a more consistent experience for users. Towards that goal, we describe the state-of-the-art, principal open source technologies, and highlight open questions and areas where additional research is needed. We hope this work will foster collaboration, motivate further research, and promote a more composable future for data management.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {2679–2685},
numpages = {7}
}

@article{10.14778/3611540.3611569,
author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},
title = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611569},
doi = {10.14778/3611540.3611569},
abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3848–3860},
numpages = {13}
}

@article{10.14778/3611540.3611611,
author = {Liu, Yushi and Yuan, Liwei and Chen, Zhihao and Yu, Yekai and Zhang, Zhao and Jin, Cheqing and Yan, Ying},
title = {ChainDash: An Ad-Hoc Blockchain Data Analytics System},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611611},
doi = {10.14778/3611540.3611611},
abstract = {The emergence of digital asset applications, driven by Web 3.0 and powered by blockchain technology, has led to a growing demand for blockchain-specific graph analytics to unearth the insights. However, current blockchain data analytics systems are unable to perform efficient ad-hoc graph analytics over both live and past time windows due to their inefficient data synchronization and slow graph snapshots retrieval capability. To address these issues, we propose ChainDash, a blockchain data analytics system that dedicates a highly-parallelized data synchronization component and a retrieval-optimized temporal graph store. By leveraging these techniques, ChainDash supports efficient ad-hoc graph analytics of smart contract activities over arbitrary time windows. In the demonstration, we showcase the interactive visualization interfaces of ChainDash, where attendees will execute customized queries for ad-hoc graph analytics of blockchain data.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4022–4025},
numpages = {4}
}

@article{10.14778/3625054.3625056,
author = {Chiosa, Monica and Preu\ss{}er, Thomas B. and Blott, Michaela and Alonso, Gustavo},
title = {AMNES: Accelerating the Computation of Data Correlation Using FPGAs},
year = {2023},
issue_date = {September 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3625054.3625056},
doi = {10.14778/3625054.3625056},
abstract = {A widely used approach to characterize input data in both databases and ML is computing the correlation between attributes. The operation is supported by all major database engines and ML platforms. However, it is an expensive operation as the number of attributes involved grows. To address the issue, in this paper we introduce AMNES, a stream analytics system offloading the correlation operator into an FPGA-based network interface card. AMNES processes data at network line rate and the design can be used in combination with smart storage or SmartNICs to implement near data or in-network data processing. AMNES design goes beyond matrix multiplication and offers a customized solution for correlation computation bypassing the CPU. Our experiments show that AMNES can sustain streams arriving at 100 Gbps over an RDMA network, while requiring only ten milliseconds to compute the correlation coefficients among 64 streams, an order of magnitude better than competing CPU or GPU designs.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {4174–7187},
numpages = {3014}
}

@article{10.14778/3641204.3641211,
author = {Xing, Junjie and Wang, Xinyu and Jagadish, H. V.},
title = {Data-Driven Insight Synthesis for Multi-Dimensional Data},
year = {2024},
issue_date = {January 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3641204.3641211},
doi = {10.14778/3641204.3641211},
abstract = {Exploratory data analysis can uncover interesting data insights from data. Current methods utilize "interestingness measures" designed based on system designers' perspectives, thus inherently restricting the insights to their defined scope. These systems, consequently, may not adequately represent a broader range of user interests. Furthermore, most existing approaches that formulate "interestingness measure" are rule-based, which makes them inevitably brittle and often requires holistic re-design when new user needs are discovered.This paper presents a data-driven technique for deriving an "interestingness measure" that learns from annotated data. We further develop an innovative annotation algorithm that significantly reduces the annotation cost, and an insight synthesis algorithm based on the Markov Chain Monte Carlo method for efficient discovery of interesting insights. We consolidate these ideas into a system. Our experimental outcomes and user studies demonstrate that DAISY can effectively discover a broad range of interesting insights, thereby substantially advancing the current state-of-the-art.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1007–1019},
numpages = {13}
}

@article{10.14778/3648160.3648182,
author = {Zhang, Huayi and Yan, Binwei and Cao, Lei and Madden, Samuel and Rundensteiner, Elke},
title = {MetaStore: Analyzing Deep Learning Meta-Data at Scale},
year = {2024},
issue_date = {February 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3648160.3648182},
doi = {10.14778/3648160.3648182},
abstract = {The process of training deep learning models produces a huge amount of meta-data, including but not limited to losses, hidden feature embeddings, and gradients. Model diagnosis tools have been developed to analyze losses and feature embeddings with the aim to improve the performance of these models. However, gradients, despite carrying rich information that is potentially relevant for model interpretation and data debugging, have yet to be fully explored due to their size and complexity. Each single gradient has a size as large as the number of parameters of the neural net - often measured in the tens of millions. This makes it extremely challenging to efficiently collect, store, and analyze large numbers of gradients in these models. In this work, we develop MetaStore to fill this gap. MetaStore leverages our observation that storing certain compact intermediate results produced in the back propagation process, namely, the prefix and suffix gradients, is sufficient for the exact restoration of the original gradient. These prefix and suffix gradients are much more compact than the original gradients, thus allowing us to address the gradient collection and storage challenges. Furthermore, MetaStore features a rich set of analytics operators that allow the users to analyze the gradients for data debugging or model interpretation. Rather than first having to restore the original gradients and then run analytics on top of this decompressed view, MetaStore directly executes these operators on the compact prefix and suffix structures, making gradient-based analytics efficient and scalable. Our experiments on popular deep learning models such as VGG, BERT, and ResNet and benchmark image and text datasets demonstrate that MetaStore outperforms strong baseline methods from 4 to 678x in storage costs and from 2 to 1000x in running time.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1446–1459},
numpages = {14}
}

@article{10.14778/3659437.3659446,
author = {Wu, Biao and Huang, Qiang and Tung, Anthony K. H.},
title = {From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying},
year = {2024},
issue_date = {April 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3659437.3659446},
doi = {10.14778/3659437.3659446},
abstract = {Safeguarding the Intellectual Property (IP) of data has become critically important as machine learning applications continue to proliferate, and their success heavily relies on the quality of training data. While various mechanisms exist to secure data during storage, transmission, and consumption, fewer studies have been developed to detect whether they are already leaked for model training without authorization. This issue is particularly challenging due to the absence of information and control over the training process conducted by potential attackers.In this paper, we concentrate on the domain of tabular data and introduce a novel methodology, Local Distribution Shifting Synthesis (LDSS), to detect leaked data that are used to train classification models. The core concept behind LDSS involves injecting a small volume of synthetic data-characterized by local shifts in class distribution-into the owner's dataset. This enables the effective identification of models trained on leaked data through model querying alone, as the synthetic data injection results in a pronounced disparity in the predictions of models trained on leaked and modified datasets. LDSS is model-oblivious and hence compatible with a diverse range of classification models. We have conducted extensive experiments on seven types of classification models across five real-world datasets. The comprehensive results affirm the reliability, robustness, fidelity, security, and efficiency of LDSS. Extending LDSS to regression tasks further highlights its versatility and efficacy compared with baseline methods.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1898–1910},
numpages = {13}
}

@article{10.14778/3659437.3659455,
author = {Mohapatra, Shubhankar and Zong, Jianqiao and Kerschbaum, Florian and He, Xi},
title = {Differentially Private Data Generation with Missing Data},
year = {2024},
issue_date = {April 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3659437.3659455},
doi = {10.14778/3659437.3659455},
abstract = {Despite several works that succeed in generating synthetic data with differential privacy (DP) guarantees, they are inadequate for generating high-quality synthetic data when the input data has missing values. In this work, we formalize the problems of DP synthetic data with missing values and propose three effective adaptive strategies that significantly improve the utility of the synthetic data on four real-world datasets with different types and levels of missing data and privacy requirements. We also identify the relationship between privacy impact for the complete ground truth data and incomplete data for these DP synthetic data generation algorithms. We model the missing mechanisms as a sampling process to obtain tighter upper bounds for the privacy guarantees to the ground truth data. Overall, this study contributes to a better understanding of the challenges and opportunities for using private synthetic data generation algorithms in the presence of missing data.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {2022–2035},
numpages = {14}
}

@article{10.14778/3675034.3675054,
author = {Luo, Wensheng and Fang, Yixiang and Lin, Chunxu and Zhou, Yingli},
title = {Efficient Parallel D-Core Decomposition at Scale},
year = {2024},
issue_date = {June 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3675034.3675054},
doi = {10.14778/3675034.3675054},
abstract = {Directed graphs are prevalent in social networks, web networks, and communication networks. A well-known concept of the directed graph is the D-core, or (k, l)-core, which is the maximal subgraph in which each vertex has an in-degree not less than k and an out-degree not less than l. Computing the non-empty D-cores for all possible values of k and l, a.k.a. D-core decomposition, has found versatile applications spanning social network analysis, community search, and graph visualization. However, existing algorithms of D-core decomposition suffer from efficiency and scalability issues on large graphs, because serial peeling-based algorithms are limited by single-core utilization, while skyline coreness-based methods exhibit notably high time complexity. To tackle these issues, in this paper, we propose efficient parallel algorithms for D-core decomposition by leveraging the computational prowess of multicore CPUs. Specifically, we first propose a novel algorithm that computes the D-cores for each possible k value, by exploiting an implicit level-by-level vertex removal strategy, which not only diminishes dependencies between vertices but also maintains a time complexity akin to that of sequential algorithms. We further develop an advanced algorithm by introducing a novel concept of D-shell, which allows us to curtail redundant computations by reducing the necessary k values when computing corresponding D-cores, and deriving D-cores with larger k values from the D-cores currently computed based on D-shell. Extensive experiments on ten real-world large graphs show that our algorithms are highly efficient and scalable, and the advanced algorithm is up to two orders of magnitude faster than the state-of-the-art parallel decomposition algorithm with 32 threads.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2654–2667},
numpages = {14}
}

@article{10.14778/3681954.3681957,
author = {Campos, David and Yang, Bin and Kieu, Tung and Zhang, Miao and Guo, Chenjuan and Jensen, Christian S.},
title = {QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681957},
doi = {10.14778/3681954.3681957},
abstract = {We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes. It is thus attractive to be able to deploy machine learning models, e.g., for classification, on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers. To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits. The resulting quantized models are then calibrated using back-propagation with the full training data to ensure accuracy. This one-time calibration works for deployments in static environments. However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions with the original training data. The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus cannot be assumed to be always available on edge devices. The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive. We propose QCore to enable continual calibration on the edge. First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths. We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data. Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation. An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2708–2721},
numpages = {14}
}

@article{10.14778/3681954.3681965,
author = {Tian, Anxin and Zhou, Alexander and Wang, Yue and Jian, Xun and Chen, Lei},
title = {Efficient Index for Temporal Core Queries over Bipartite Graphs},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681965},
doi = {10.14778/3681954.3681965},
abstract = {Many real-world binary relations can be modelled as bipartite graphs, which can be inherently temporal and each edge is associated with a timestamp. The (α, β)-core, a popular structure that requires minimum degrees over two layers of vertices, is useful for understanding the organisation of bipartite networks. However, the temporal property has rarely been considered in cohesive subgraph mining in bipartite graphs. This gap prevents the finding of time-sensitive (α, β)-cores in real-world applications. In this paper, we aim at finding (α, β)-cores within any time window over a temporal bipartite graph. To address this problem, we propose a novel DAG (Directed Acyclic Graph)-like hierarchy with qualified time windows to describe the temporal containment property of the (α, β)-core. Furthermore, we construct the superior-optimized index which significantly optimizes space complexity and guarantees efficient query performance. We also propose a maintenance approach that can efficiently update the index by removing stale information and incorporating newly inserted temporal edges. Extensive experiments are conducted on eight real-world graphs and the results show the effectiveness and efficiency of our indexes.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2813–2825},
numpages = {13}
}

@article{10.14778/3681954.3681976,
author = {Sheng, Zeang and Zhang, Wentao and Tao, Yangyu and Cui, Bin},
title = {OUTRE: An OUT-of-Core De-REdundancy GNN Training Framework for Massive Graphs within A Single Machine},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681976},
doi = {10.14778/3681954.3681976},
abstract = {Sampling-based Graph Neural Networks (GNNs) have become the de facto standard for handling various graph learning tasks on large-scale graphs. As the graph size grows larger and even exceeds the standard host memory size of a single machine, out-of-core sampling-based GNN training has gained attention from the community. For out-of-core sampling-based GNN training, the performance bottleneck is the data preparation process that includes sampling neighbor lists and gathering node features from external storage. Based on this observation, existing out-of-core GNN training frameworks try to accomplish larger percentages of data requests without inquiring the external storage by designing better in-memory caches. However, the enormous overall requested data volume is unchanged under this approach. In this paper, we present a new perspective on reducing the overall requested data volume. Through a quantitative analysis, we find that Neighborhood Redundancy and Temporal Redundancy exist in out-of-core sampling-based GNN training. To reduce these two kinds of data redundancies, we propose OUTRE, an OUT-of-core de-REdundancy GNN training framework. OUTRE incorporates two new designs, partition-based batch construction and historical embedding cache, to reduce the corresponding data redundancies. Moreover, we propose automatic cache space management to automatically organize available memory for different caches. Evaluation results on four public large-scale graph datasets show that OUTRE achieves 1.52\texttimes{} to 3.51\texttimes{} speedup against the SOTA framework.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2960–2973},
numpages = {14}
}

@article{10.14778/3681954.3681983,
author = {Takagi, Shun and Xiong, Li and Kato, Fumiyuki and Cao, Yang and Yoshikawa, Masatoshi},
title = {HRNet: Differentially Private Hierarchical and Multi-Resolution Network for Human Mobility Data Synthesization},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681983},
doi = {10.14778/3681954.3681983},
abstract = {Human mobility data offers valuable insights for many applications such as urban planning and pandemic response, but its use also raises privacy concerns. In this paper, we introduce the Hierarchical and Multi-Resolution Network (HRNet), a novel deep generative model specifically designed to synthesize realistic human mobility data while guaranteeing differential privacy. We first identify the key difficulties inherent in learning human mobility data under differential privacy. In response to these challenges, HRNet integrates three components: a hierarchical location encoding mechanism, multi-task learning across multiple resolutions, and private pre-training. These elements collectively enhance the model's ability under the constraints of differential privacy. Through extensive comparative experiments utilizing a real-world dataset, HRNet demonstrates a marked improvement over existing methods in balancing the utility-privacy trade-off.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3058–3071},
numpages = {14}
}

@article{10.14778/3681954.3682005,
author = {Zhang, Yi and Chen, Peter Baile and Ives, Zachary G.},
title = {Searching Data Lakes for Nested and Joined Data},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682005},
doi = {10.14778/3681954.3682005},
abstract = {Exploratory data science is driving new platforms that assist data scientists with everyday tasks, such as integration and wrangling, to assemble training datasets. Such tools take scientists' work-in-progress data as a search object (table or JSON) and find relevant supplementary data from an organizational data lake, which can be unioned or joined with the current data. Existing data lake search tools find single, relational tables to match or join with a search object. Yet many data science applications revolve around hierarchical data, which can only be matched by creating views that simultaneously join and transform several tables in the data lake. In this paper, we extend the Juneau data lake search system [46] for this broader class of matches at scale. Our contribution is a general framework for efficiently merging ranked results to match hierarchical data, leveraging novel techniques for indexing and sketching, and incorporating existing single-table search techniques and ranking functions. We experimentally validate our methods' benefits and broad applicability using real data from data science computational notebooks. Our results indicate that, with different ranking functions, our approach can return the optimal set of views up to 4.8x faster and 43\% more related compared to heuristics, and increase the data domain coverage by up to 28\%. In a case study to show the utility of our results to data science downstream tasks, we reduce regression error by up to 6.6\%, and improve classification accuracy by up to 19.5\%.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3346–3359},
numpages = {14}
}

@article{10.14778/3681954.3682007,
author = {Zhang, William and Lim, Wan Shen and Butrovich, Matthew and Pavlo, Andrew},
title = {The Holon Approach for Simultaneously Tuning Multiple Components in a Self-Driving Database Management System with Machine Learning via Synthesized Proto-Actions},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682007},
doi = {10.14778/3681954.3682007},
abstract = {Existing machine learning (ML) approaches to automatically optimize database management systems (DBMSs) only target a single configuration space at a time (e.g., knobs, query hints, indexes). Simultaneously tuning multiple configuration spaces is challenging due to the combined space's complexity. Previous tuning methods work around this by sequentially tuning individual spaces with a pool of tuners. However, these approaches struggle to coordinate their tuners and get stuck in local optima.This paper presents the Proto-X framework that holistically tunes multiple configuration spaces. The key idea of Proto-X is to identify similarities across multiple spaces, encode them in a high-dimensional model, and then synthesize "proto-actions" to navigate the organized space for promising configurations. We evaluate Proto-X against state-of-the-art DBMS tuning frameworks on tuning PostgreSQL for analytical and transactional workloads. By reasoning about configuration spaces that are orders of magnitude more complex than other frameworks (both in terms of quantity and variety), Proto-X discovers configurations that improve PostgreSQL's performance by up to 53\% over the next best approach.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3373–3387},
numpages = {15}
}

@article{10.14778/3681954.3682022,
author = {Wang, Zuozhi and Huang, Yicong and Ni, Shengquan and Kumar, Avinash and Alsudais, Sadeem and Liu, Xiaozhen and Lin, Xinyuan and Ding, Yunyan and Li, Chen},
title = {Texera: A System for Collaborative and Interactive Data Analytics Using Workflows},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682022},
doi = {10.14778/3681954.3682022},
abstract = {Domain experts play an important role in data science, as their knowledge can unlock valuable insights from data. As they often lack technical skills required to analyze data, they need collaborations with technical experts. In these joint efforts, productive collaborations are critical not only in the phase of constructing a data science task, but more importantly, during the execution of a task. This need stems from the inherent complexity of data science, which often involves user-defined functions or machine-learning operations. Consequently, collaborators want various interactions during runtime, such as pausing/resuming the execution, inspecting an operator's state, and modifying an operator's logic. To achieve the goal, in the past few years we have been developing an open-source system called Texera to support collaborative data analytics using GUI-based workflows as cloud services. In this paper, we present a holistic view of several important design principles we followed in the design and implementation of the system. We focus on different methods of sending messages to running workers, how these methods are adopted to support various runtime interactions from users, and their trade-offs on both performance and consistency. These principles enable Texera to provide powerful user interactions during a workflow execution to facilitate efficient collaborations in data analytics.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3580–3588},
numpages = {9}
}

@article{10.14778/3685800.3685810,
author = {Yi, Peng and Liang, Lei and Zhang, Da and Chen, Yong and Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin and Lin, Hao and Qiu, Leijie and Zhou, Jun},
title = {KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685810},
doi = {10.14778/3685800.3685810},
abstract = {Based on the diversified application scenarios at Ant Group, we built the Ant Knowledge Graph Platform (AKGP). It has constructed numerous domain-specific knowledge graphs related to merchants, companies, accounts, products, and more. AKGP manages trillions of structured knowledge graphs, serving search, recommendation, risk control and other businesses. However, as the demand increasing for various workloads such as graph pattern matching, graph representation learning, and cross-domain knowledge reuse, the existing warehouse systems based on relational DBMS or graph databases are unable to meet the requirements. To address these issues, we propose KGFabric, an industrial-scale knowledge graph management system built on the distributed file system (DFS). KGFabric offers a nearline knowledge storage engine that utilizes a Semantic-enhanced Programmable Graph (SPG) model, which is compatible with the Labeled Property Graph (LPG) model. The data is persistently stored in DFS, such as HDFS, which leverages the POSIX file system API, making it suitable for deployment in multi-cloud environment at low cost. KGFabric provides a native graph-based and hybrid storage format that can serve as a shared backend for parallel graph computing systems, significantly accelerating the analysis of multi-workload. Additionally, KGFabric includes a graph fabric framework that minimizes data duplication and guarantees data security.KGFabric is able to manage Peta-scale data and has supported graph fabric and analysis with over 100 billion relations at Ant Group. We conduct experiments on various datasets to evaluate the performance of KGFabric. Compared with popular relational DBMS and graph databases, the storage space for semantic relations is reduced by over 90\%. The performance of graph fabric improves by 21\texttimes{} in real-world workloads. In multi-hop semantic graph analysis, KGFabric enhances performance by 100\texttimes{}.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {3841–3854},
numpages = {14}
}

@article{10.14778/3685800.3685834,
author = {Okolnychyi, Anton and Sun, Chao and Tanimura, Kazuyuki and Spitzer, Russell and Blue, Ryan and Ho, Szehon and Gu, Yufei and Lakkundi, Vishwanath and Tsai, DB},
title = {Petabyte-Scale Row-Level Operations in Data Lakehouses},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685834},
doi = {10.14778/3685800.3685834},
abstract = {Data lakehouses combine the almost infinite scale and diverse tooling of a data lake with the reliability and functionality of a data warehouse. This paper presents extensions that enhance data lake-houses using Apache Iceberg and Apache Spark with performant petabyte-scale row-level operations. The framework is capable of handling both high-density and sparse modifications by either materializing changes at the file level during writes or producing equality and position deletes that are lazily merged with existing data during reads. The paper also outlines essential improvements in determining and applying row-level changes: eliminating expensive shuffles with storage-partitioned joins, minimizing write amplification with runtime filtering, and optimizing the layout of output data with adaptive writes. Our evaluation demonstrates the relative strengths and weaknesses of the various materialization strategies, highlighting the use cases that require each technique. We also show an order of magnitude improvement in performance after our enhancements.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4159–4172},
numpages = {14}
}

@article{10.14778/3685800.3685843,
author = {Nawab, Faisal and Sadoghi, Mohammad},
title = {Consensus in Data Management: With Use Cases in Edge-Cloud and Blockchain Systems},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685843},
doi = {10.14778/3685800.3685843},
abstract = {Consensus is a fundamental problem in distributed systems, involving the challenge of achieving agreement among distributed nodes. It plays a critical role in various distributed data management problems. This tutorial aims to provide a comprehensive primer for data management researchers on the topic of consensus and its fundamental and modern applications in data management. We begin by exploring the basic principles of consensus, including the problem statement, system models, failure scenarios, and various consensus algorithms such as Paxos and its variants. The tutorial then delves into the applications of consensus in distributed data management, focusing on distributed atomic commitment and data replication. We explain how consensus is integral to these areas and present examples of research and industry work that apply consensus to data management. The tutorial extends to modern use cases of consensus in the evolving landscapes of edge-cloud systems and blockchain technology. We discuss how consensus mechanisms are being adapted and applied in these areas, highlighting their growing importance in emerging areas of data management. By exploring these cutting-edge applications, participants will gain insights into how consensus is shaping ongoing and future research on distributed data management.The tutorial builds on the authors' recent book "Consensus in Data Management: from Distributed Commit to Blockchain". The book will serve as the foundation and reading material for the tutorial. This tutorial targets data management researchers and practitioners to equip them with the knowledge and perspective needed to innovate in these emerging fields. This includes graduate students and junior researchers starting their careers in the area of distributed data management. Also, it includes researchers in other areas of data management who wish to explore the area of distributed data management with the goal of utilizing it in their own fields.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4233–4236},
numpages = {4},
keywords = {atomic commit, blockchain, consensus, data management, replication}
}

@article{10.14778/3685800.3685847,
author = {Pedreira, Pedro and Majeti, Deepak and Erling, Orri},
title = {Composable Data Management: An Execution Overview},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685847},
doi = {10.14778/3685800.3685847},
abstract = {The trend of decomposing monolithic data management systems into a stack of reusable components has quickly gained momentum across the industry. Although a series of open-source projects have emerged targeting different layers of the stack, execution engines are of special importance due to the complexity they encapsulate, and the demand to optimize price-performance. In this tutorial, we will survey the space of composability in data management, focusing on the execution layer. We will discuss the main APIs, integration with existing and novel data management systems, and how specialized behavior can be accommodated by using extensibility APIs. With an emphasis on analytics, we will take a deeper dive into performance, discussing modern aspects of vectorization, compressed (encoding-aware) execution, and adaptivity. While the presentation is contextualized using real-world examples and experience while developing the Velox open-source execution engine and integrations with existing systems like Presto (Prestissimo) and Spark (Gluten), the concepts and techniques discussed are generally applicable to other execution engines. Finally, we will discuss future trends and ongoing work regarding novel file formats, compressed execution opportunities, and nascent hardware acceleration efforts, highlighting current challenges and open questions. With a survey of the state-of-the-art in this space, we hope this tutorial will help motivate individuals and organizations to embrace composability and promote collaborations across related projects.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4249–4252},
numpages = {4}
}

@article{10.14778/3685800.3685876,
author = {Xue, Siqiao and Qi, Danrui and Jiang, Caigao and Cheng, Fangyin and Chen, Keting and Zhang, Zhiping and Zhang, Hongyang and Wei, Ganglin and Zhao, Wang and Zhou, Fan and Yi, Hong and Liu, Shaodong and Yang, Hongjun and Chen, Faqiang},
title = {Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685876},
doi = {10.14778/3685800.3685876},
abstract = {The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4365–4368},
numpages = {4}
}

@article{10.14778/3685800.3685893,
author = {Zhu, Yiding and Zhang, Hongwei and Zhang, Jiayao and Liu, Jinfei and Ren, Kui},
title = {DataPrice: An Interactive System for Pricing Datasets in Data Marketplaces},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685893},
doi = {10.14778/3685800.3685893},
abstract = {With the flourishing of data-driven applications, data marketplaces, which can dramatically facilitate data utilization, have emerged recently. However, determining the appropriate price for datasets presents a significant challenge due to the intangible nature of data. DataPrice alleviates the challenge by providing an interactive system based on a pricing model trained on real datasets collected from commercial data marketplaces. The pricing model can estimate an appropriate price according to the dataset description as a reference for both data sellers and data buyers. By leveraging Shapley values to evaluate the contribution of each attribute in metadata on the estimated price, DataPrice offers an explanation of the pricing process in a text form to enhance user trust.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4433–4436},
numpages = {4}
}

@article{10.14778/3685800.3685901,
author = {Gao, Chang and Zhang, Tianlong and Zeng, Yuxiang and Xu, Yi and Li, Shuyuan and Zhang, Yuanyuan},
title = {Swift: A Data-Driven Flight Planning System at Scale},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685901},
doi = {10.14778/3685800.3685901},
abstract = {Flight planning, a pivotal challenge in the airline industry, strives to achieve economic and flexible scheduling of airplanes to serve designated flight itineraries. As the demand for air transportation soars, traditional planning methods can be inefficient in managing large-scale flights. Thus, we introduce Swift, a data-driven system tailored to enhance the scalability and effectiveness of flight planning. Swift primarily employs the bipartite graph model to derive optimal and economic flight plans for airlines. Our method not only minimizes the number of required planes but also ensures a balanced workload across these planes. Furthermore, Swift offers the capability of dynamic updates to flight plans in response to unexpected incidents at airports, such as bad weather conditions. Besides, Swift incorporates other functionalities like predicting future flight demand and monitoring real-time flight trajectories. Conference participants can interact with this system and explore our flight planning solution in real-world scenarios.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4465–4468},
numpages = {4}
}

@article{10.14778/3685800.3685913,
author = {Amer-Yahia, Sihem},
title = {Intelligent Agents for Data Exploration},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685913},
doi = {10.14778/3685800.3685913},
abstract = {Data Exploration is an incremental process that helps users express what they want through a conversation with the data. Reinforcement Learning (RL) is one of the most notable approaches to automate data exploration and several solutions have been proposed. We first summarize some RL solutions that were built for different applications. In this context, various data exploration operators are leveraged including traditional roll-up and drill-down operations and text-based operations. An RL agent is trained to generate the best policy according to a hand-crafted reward function.The benefit of training RL policies for specific data exploration tasks has been demonstrated more than once for exploring finding a needle in a haystack, for serendipitous galaxy exploration, for helping a customer land on a satisfactory product, for helping a conference chair build a program committee in a stepwise fashion, for summarizing large datasets, etc.With the advent of Large Language Models and their ability to reason sequentially, it has become legitimate to ask the question: would LLMs and AI planning outperform an RL policy in data exploration? More specifically, would LLMs help circumvent retraining for new tasks and striking a balance between specificity and generality? This led us to designing LLM-powered approaches that introduce a new way of thinking about data exploration.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4521–4530},
numpages = {10}
}

@article{10.1613/jair.1.13267,
author = {Chen, Zengjian and Xu, Jin and Liao, Meng and Xue, Tong and He, Kun},
title = {Two-phase Multi-document Event Summarization on Core Event Graphs},
year = {2022},
issue_date = {Sep 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {74},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13267},
doi = {10.1613/jair.1.13267},
abstract = {Succinct event description based on multiple documents is critical to news systems as well as search engines. Different from existing summarization or event tasks, Multi-document Event Summarization (MES) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. Identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. To address these challenges, we propose a two-phase framework for the MES task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. For experiments in the new task, we construct two large-scale real-world datasets for training and assessment. Extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.},
journal = {J. Artif. Int. Res.},
month = sep,
numpages = {21}
}

@article{10.1613/jair.1.13646,
author = {Francis, Jonathan and Kitamura, Nariaki and Labelle, Felix and Lu, Xiaopeng and Navarro, Ingrid and Oh, Jean},
title = {Core Challenges in Embodied Vision-Language Planning},
year = {2022},
issue_date = {Sep 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {74},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13646},
doi = {10.1613/jair.1.13646},
abstract = {Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.},
journal = {J. Artif. Int. Res.},
month = sep,
numpages = {57}
}

@article{10.5555/3122009.3242083,
author = {Athreya, Avanti and Fishkind, Donniell E. and Tang, Minh and Priebe, Carey E. and Park, Youngser and Vogelstein, Joshua T. and Levin, Keith and Lyzinski, Vince and Qin, Yichen},
title = {Statistical inference on random dot product graphs: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {8393–8484},
numpages = {92},
keywords = {adjacency spectral embedding, laplacian spectral embedding, multi-sample graph hypothesis testing, random dot product graph, semiparametric modeling}
}

@article{10.5555/3542805.3542807,
author = {Choi, Young Mi},
title = {Applying tangible augmented reality for product usability assessment},
year = {2019},
issue_date = {August 2019},
publisher = {Usability Professionals' Association},
address = {Bloomingdale, IL},
volume = {14},
number = {4},
abstract = {When developing a new product, it is common for designers to feel that they do not have enough information about users' needs. This is especially true at the front end of a new product design process. An important component of this process is testing and validating potential design concepts. The aim of this study was to explore the validity of augmented reality (AR) and tangible augmented reality (TAR) as tools for evaluating the usability of a product. For this study, 70 college students were recruited to perform a usability evaluation of a space heater product and equivalent AR and TAR representations of it. The results indicate that overall TAR can be a reliable method for evaluating the usability of a fully realized product, especially for products with physical interface controls. However, TAR was not found to be reliable with respect to Ease of Use. Overall, AR was not found to be as reliable as TAR or with respect to any specific aspect of usability that was measured. Applications, limitations, and areas for further study are discussed in this article as well.},
journal = {J. Usability Studies},
month = aug,
pages = {187–200},
numpages = {14},
keywords = {augmented reality, product design evaluation, tangible augmented reality, usability assessment}
}

@article{10.5555/3546258.3546445,
author = {Bonnevie, Rasmus and Schmidt, Mikkel N.},
title = {Matrix product states for inference in discrete probabilistic models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {When faced with problems involving inference in discrete domains, solutions often involve appeals to conditional independence structure or mean-field approximations. We argue that this is insufficient for a number of interesting Bayesian problems, including mixture assignment posteriors and probabilistic relational models (e.g. the stochastic block model). These posteriors exhibit no conditional independence structure, precluding the use of graphical model methods, yet exhibit dependency between every single element of the posterior, making mean-field methods a poor fit. We propose using an expressive yet tractable approximation inspired by tensor factorization methods, alternately known as the tensor train or the matrix product state, and which can be construed of as a direct extension of the mean-field approximation to higher-order dependencies. We give a comprehensive introduction to the application of matrix product state in probabilistic inference, and illustrate how to efficiently perform marginalization, conditioning, sampling, normalization, some expectations, and approximate variational inference in our proposed model.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {187},
numpages = {48},
keywords = {variational inference, matrix product states, tensor trains, discrete models, symmetry}
}

@article{10.5555/3580523.3580536,
author = {Conrad, Susan S.},
title = {Integrating Data Privacy Principles into Product Design: Teaching "Privacy by Design" to Application Developers and Data Scientists},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {3},
issn = {1937-4771},
abstract = {Data privacy and data security are terms often used interchangeably but strong data security does not guarantee data privacy. Data privacy focuses on protecting the rights of an individual to control access and keep personal data private whereas data security focuses on keeping data safe from threats and vulnerabilities. With the proliferation of online data systems, users often provide their information online and have no idea how the data will be protected or shared. New laws dictate that privacy online begins with integrating privacy measures in the design phase of IT ecosystem products and services however few system developers have the training to understand how this is accomplished. This paper discusses the privacy-by-design approach to system design and offers suggestions about how this data must be protected throughout the data lifecycle. It looks at how the NIST privacy framework and enhanced security techniques can be applied to the collection, processing, disclosure, retention, and deletion of personal data and discusses strategies for educators and developers to add privacy provisions to their products},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {132–142},
numpages = {11}
}

@article{10.5555/3636971.3636975,
author = {Peng, Bin and Cigas, John},
title = {Digital Circuit Projects for an Accelerated Online Undergraduate Computer Architecture Course},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {2},
issn = {1937-4771},
abstract = {Digital Logic is an essential topic in the Computer Science curriculum. This paper introduces a set of circuit-building projects to teach digital logic and circuits in an eight-week online version of an undergraduate computer architecture course. Those projects cover circuit design and building in a software simulator by progressing through gate-level design, sub-circuits, and circuit component-level integration. Students commented positively on gaining a deeper understanding from those hands-on projects. Student performance and feedback are presented and discussed.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {38–48},
numpages = {11}
}

@article{10.5555/3636988.3637008,
author = {Akerele, Abimbola and Leppert, William and Somerville, Shionta and Amoussou, Guy-Alain},
title = {The Digital Twins Incident Response To Improve the Security of Power System Critical Infrastructure},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {3},
issn = {1937-4771},
abstract = {Digitalizing real-world assets has numerous benefits, including the algorithmic analysis of physical objects. Digital twin (DT) technology has evolved to further contribute with continuous real-time data valuable flow to the analysis. With this analysis comes a greater understanding of a problem, which, in turn, provides a more comprehensive solution. We will examine the definition of critical infrastructure, to understand better how DT technology can be applied in a large-scale physical environment. We then turn our focus to the components within the energy sector of critical infrastructure, including various Industrial Control Systems (ICS) components with a focus on sensor networks and Supervisory Control and Data Acquisition (SCADA) systems. With this understanding, we examine research pertinent to the key features of a DT and how that may be applied to the sensor and ICS networks. Field sensors within ICS are susceptible to semantic attacks leading to disruption of horizontal and lateral operations and systems. With this possibility, incident response algorithms can be implemented in a digital space of an energy grid to maintain security and operability during adverse conditions. This review intends to outline these core components for additional lab testing and research, analyzing the effectiveness of digital twins in securing operations of Industrial Control Systems in the energy sector.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {86–99},
numpages = {14}
}

@article{10.5555/3648699.3648714,
author = {Vadillo, Jon and Santana, Roberto and Lozano, Jose A.},
title = {Extending adversarial attacks to produce adversarial class probability distributions},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Despite the remarkable performance and generalization levels of deep learning models in a wide range of artificial intelligence tasks, it has been demonstrated that these models can be easily fooled by the addition of imperceptible yet malicious perturbations to natural inputs. These altered inputs are known in the literature as adversarial examples. In this paper, we propose a novel probabilistic framework to generalize and extend adversarial attacks in order to produce a desired probability distribution for the classes when we apply the attack method to a large number of inputs. This novel attack paradigm provides the adversary with greater control over the target model, thereby exposing, in a wide range of scenarios, threats against deep learning models that cannot be conducted by the conventional paradigms. We introduce four different strategies to efficiently generate such attacks, and illustrate our approach by extending multiple adversarial attack algorithms. We also experimentally validate our approach for the spoken command classification task and the Tweet emotion classification task, two exemplary machine learning problems in the audio and text domain, respectively. Our results demonstrate that we can closely approximate any probability distribution for the classes while maintaining a high fooling rate and even prevent the attacks from being detected by label-shift detection methods.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {15},
numpages = {42},
keywords = {adversarial examples, deep neural networks, robust classification, class probability distributions, linear programming}
}
