@article{10.1007/s00165-021-00566-z,
author = {Luteberget, Bj\o{}rnar and Johansen, Christian},
title = {Drawing with SAT: four methods and A tool for producing railway infrastructure schematics},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00566-z},
doi = {10.1007/s00165-021-00566-z},
abstract = {Schematic drawings showing railway tracks and equipment are commonly used
to visualize railway operations and to communicate system specifications and
construction blueprints.  Recent advances in on-line collaboration and modeling
tools have raised the expectations for quickly making changes to models,
resulting in frequent changes to layouts, text, and/or symbols in schematic
drawings.  Automating the creation of high-quality schematic views from
geographical and topological models can help engineers produce and update
drawings efficiently. This paper introduces four methods for automatically producing
schematic railway drawings with increasing level of quality and control
over the result.  The final method, implemented in the open-source tool that we have developed,
can use any combination of the following optimization criteria, which can have
different priorities in different use cases: width and height of the drawing,
the diagonal line lengths, and the number of bends. We show how to encode schematic railway drawings  as an optimization
problem over Boolean and numerical domains, using combinations of unary number
encoding, lazy difference constraints, and numerical optimization into an
incremental SAT formulation. We compare drawings resulting from each of the four methods, applied to
models of real-world engineering projects and existing railway infrastructure.
We also show how to add symbols and labels to the track plan, which is
important for the usefulness of the final outputs.  Since the proposed tool is
customizable and efficiently produces high-quality drawings from railML 2.x
models, it can be used (as it is or extended) both as an integrated module in
an industrial design tool like RailCOMPLETE, or by researchers for
visualization purposes.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {829–854},
numpages = {26},
keywords = {Quality, Optimization, Railplot, Maps, Railway, Schematics, SAT}
}

@article{10.1109/TASLP.2018.2842146,
author = {Paleologu, Constantin and Benesty, Jacob and Ciochina, Silviu},
title = {Linear System Identification Based on a Kronecker Product Decomposition},
year = {2018},
issue_date = {October 2018},
publisher = {IEEE Press},
volume = {26},
number = {10},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2842146},
doi = {10.1109/TASLP.2018.2842146},
abstract = {Linear system identification is a key problem in many important applications, among which echo cancelation is a very challenging one. Due to the long length impulse responses i.e., echo paths to be identified, there is always room and needs to improve the performance of the echo cancelers, especially in terms of complexity, convergence rate, robustness, and accuracy. In this paper, we propose a new way to address the system identification problem from the echo cancelation perspective, by exploiting an optimal approximation of the impulse response based on the nearest Kronecker product decomposition. Also, we make a first step toward this direction, by developing an iterative Wiener filter based on this approach. As compared to the conventional Wiener filter, the proposed solution is much more attractive since its gain is twofold. First, the matrices to be inverted or, preferably, linear systems to be solved are smaller as compared to the conventional approach. Second, as a consequence, the iterative Wiener filter leads to a good estimate of the impulse response, even when a small amount of data is available for the estimation of the statistics. Simulation results support the theoretical findings and indicate the good results of the proposed approach, for the identification of different network and acoustic impulse responses.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1793–1808},
numpages = {16}
}

@article{10.1109/TASLP.2019.2895241,
author = {Cohen, Israel and Benesty, Jacob and Chen, Jingdong},
title = {Differential Kronecker Product Beamforming},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2895241},
doi = {10.1109/TASLP.2019.2895241},
abstract = {Differential beamformers have attracted much interest over the past few decades. In this paper, we introduce differential Kronecker product beamformers that exploit the structure of the steering vector to perform beamforming differently from the well-known and studied conventional approach. We consider a class of microphone arrays that enable to decompose the steering vector as a Kronecker product of two steering vectors of smaller virtual arrays. In the proposed approach, instead of directly designing the differential beamformer, we break it down following the decomposition of the steering vector, and show how to derive differential beamformers using the Kronecker product formulation. As demonstrated, the Kronecker product decomposition facilitates further flexibility in the design of differential beamformers and in the tradeoff control between the directivity factor and the white noise gain.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {892–902},
numpages = {11}
}

@article{10.1109/TASLP.2020.3015659,
author = {Yu, Kai and Ma, Rao and Shi, Kaiyu and Liu, Qi},
title = {Neural Network Language Model Compression With Product Quantization and Soft Binarization},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3015659},
doi = {10.1109/TASLP.2020.3015659},
abstract = {Large memory consumption of the neural network language models (NN LMs) prohibits their use in many resource-constrained scenarios. Hence, effective NN LM compression approaches that are independent of NN structures are of great interest. However, previous approaches usually achieve a high compression ratio at the cost of obvious performance loss. In this paper, two recently proposed quantization approaches, product quantization (PQ) and soft binarization are effectively combined to address the issue. PQ decomposes word embedding matrices into a Cartesian product of low dimensional subspaces and quantizes each subspace separately. Soft binarization uses a small number of float scalars and the knowledge distillation technique to recover the performance loss during the binarization. Experiments show that the proposed approaches can achieve a high compression ratio, from 70 to over 100, while still maintaining comparable performance to the uncompressed NN LM on both PPL and word error rate criteria.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2438–2449},
numpages = {12}
}

@article{10.1109/TASLP.2021.3092825,
author = {Wang, Xianghui and Chen, Jie and Chen, Xiaoyi and Guo, Jing and Xiang, Qian},
title = {Multichannel Iterative Noise Reduction Filters in the Short-Time-Fourier-Transform Domain Based on Kronecker Product Decomposition},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3092825},
doi = {10.1109/TASLP.2021.3092825},
abstract = {In this paper, the design of multichannel noise reduction filters in the short-time-Fourier-transform (STFT) domain is addressed. By investigating the structure of the linear filter, a set of multichannel iterative noise reduction filters are developed based on the Kronecker product decomposition. Instead of computing a long noise reduction filter, we compute three much shorter sub-filters which are separately applied in the spatial, temporal and frequency dimensions. Consequently, compared with the traditional multichannel STFT-domain noise reduction filters, the proposed approaches have two advantages: 1) significantly lower computational complexity; 2) less past observations are needed to construct the iterative filters, which leads to better tracking ability for the temporal/spatial signal nonstationarity. Experimental results demonstrate the advantages of the developed iterative filters over the traditional ones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2725–2740},
numpages = {16}
}

@article{10.1109/TASLP.2021.3120603,
author = {Wang, Jianyu and Guan, Shanzheng and Liu, Shupei and Zhang, Xiao-Lei},
title = {Minimum-Volume Multichannel Nonnegative Matrix Factorization for Blind Audio Source Separation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120603},
doi = {10.1109/TASLP.2021.3120603},
abstract = {Multichannel blind audio source separation aims to recover the latent sources from their multichannel mixtures without supervised information. One state-of-the-art blind audio source separation method, named independent low-rank matrix analysis (ILRMA), unifies independent vector analysis (IVA) and nonnegative matrix factorization (NMF). However, the spectra matrix produced from NMF may not find a compact spectral basis. It may not guarantee the identifiability of each source as well. To address this problem, here we propose to enhance the identifiability of the source model by a minimum-volume prior distribution. We further regularize a multichannel NMF (MNMF) and ILRMA respectively with the minimum-volume regularizer. The proposed methods maximize the posterior distribution of the separated sources, which ensures the stability of the convergence. Experimental results demonstrate the effectiveness of the proposed methods compared with auxiliary independent vector analysis, MNMF, ILRMA and its extensions. The source code is available at &lt;uri&gt;https://github.com/alexwang9654/m-ILRMA&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3089–3103},
numpages = {15}
}

@article{10.1109/TASLP.2024.3446242,
author = {Wang, Wupeng and Pan, Zexu and Li, Xinke and Wang, Shuai and Li, Haizhou},
title = {Speech Separation With Pretrained Frontend to Minimize Domain Mismatch},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3446242},
doi = {10.1109/TASLP.2024.3446242},
abstract = {Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {4184–4198},
numpages = {15}
}

@article{10.1109/TCBB.2018.2875692,
author = {Bakhteh, Somayeh and Ghaffari-Hadigheh, Alireza and Chaparzadeh, Nader},
title = {Identification of Minimum Set of Master Regulatory Genes in Gene Regulatory Networks},
year = {2020},
issue_date = {May-June 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2875692},
doi = {10.1109/TCBB.2018.2875692},
abstract = {Identification of master regulatory genes is one of the primary challenges in systems biology. The minimum dominating set problem is a powerful paradigm in analyzing such complex networks. In these models, genes stand as nodes and their interactions are assumed as edges. Here, members of a minimal dominating set could be regarded as master genes. As finitely many minimum dominating sets may exist in a network, it is difficult to identify which one represents the most appropriate set of master genes. In this paper, we develop a weighted gene regulatory network problem with two objectives as a version of the dominating set problem. Collective influence of each gene is considered as its weight. The first objective aims to find a master regulatory genes set with minimum cardinality, and the second objective identifies the one with maximum weight. The model is converted to a single objective using a parameter varying between zero and one. The model is implemented on three human networks, and the results are reported and compared with the existing model of weighted network. Parametric programming in linear optimization and logistic regression are also implemented on the arisen relaxed problem to provide a deeper understanding of the results. Learned from computational results in parametric analysis, for some ranges of priorities in objectives, the identified master regulatory genes are invariant, while some of them are identified for all priorities. This would be an indication that such genes have higher degree of being master regulatory ones, specially on the noisy networks.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {999–1009},
numpages = {11}
}

@article{10.1109/TCBB.2024.3366240,
author = {K. S., Sheena and Nair, Madhu S.},
title = {GenCoder: A Novel Convolutional Neural Network Based Autoencoder for Genomic Sequence Data Compression},
year = {2024},
issue_date = {May-June 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3366240},
doi = {10.1109/TCBB.2024.3366240},
abstract = {Revolutionary advances in DNA sequencing technologies fundamentally change the nature of genomics. Today's sequencing technologies have opened into an outburst in genomic data volume. These data can be used in various applications where long-term storage and analysis of genomic sequence data are required. Data-specific compression algorithms can effectively manage a large volume of data. In recent times, deep learning has achieved great success in many compression tools and is gradually being used in genomic sequence compression. Significantly, autoencoder has been applied in dimensionality reduction, compact representations of data, and generative model learning. It can use convolutional layers to learn essential features from input data, which is better for image and series data. Autoencoder reconstructs the input data with some loss of information. Since accuracy is critical in genomic data, compressed genomic data must be decompressed without any information loss. We introduce a new scheme to address the loss incurred in the decompressed data of the autoencoder. This paper proposes a novel algorithm called GenCoder for reference-free compression of genomic sequences using a convolutional autoencoder and regenerating the genomic sequences from a latent code produced by the autoencoder, and retrieving original data losslessly. Performance evaluation is conducted on various genomes and benchmarked datasets. The experimental results on the tested data demonstrate that the deep learning model used in the proposed compression algorithm generalizes well for genomic sequence data and achieves a compression gain of 27% over the best state-of-the-art method.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {405–415},
numpages = {11}
}

@article{10.1109/TNET.2020.2971770,
author = {Sun, Yahui and Rehfeldt, Daniel and Brazil, Marcus and Thomas, Doreen and Halgamuge, Saman},
title = {A Physarum-Inspired Algorithm for Minimum-Cost Relay Node Placement in Wireless Sensor Networks},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2971770},
doi = {10.1109/TNET.2020.2971770},
abstract = {Relay node placement, which aims to connect pre-deployed sensor nodes to base stations, is essential in minimizing the costs of wireless sensor networks. In this paper, we formulate the new Node-Weighted Partial Terminal Steiner Tree Problem (NWPTSTP) for minimum-cost relay node placement in two-tiered wireless sensor networks. The objective is to minimize the sum of heterogeneous production and placement costs of relay nodes and the sum of outage probabilities of transmission routes in a routing tree simultaneously. This extends the previous work that considers the costs of relay nodes to be homogeneous. After formulating NWPTSTP for this purpose, we prove that it can be transformed to the existing node-weighted Steiner tree problem. Subsequently, we conduct some theoretical analyses on the emerging Physarum-inspired algorithms to reveal their potential of computing Steiner trees. Based on these analyses, we propose a new Physarum-inspired algorithm for solving NWPTSTP. We conduct computational trials to show that: 1) in comparison to a state-of-the-art approximation algorithm for solving the node-weighted Steiner tree problem, our Physarum-inspired algorithm can produce better solutions in a smaller amount of time; and 2) in comparison to two state-of-the-art relay node placement algorithms, our Physarum-inspired algorithm can design wireless sensor networks with 25% lower relay cost and similar quality of service (specifically, 5% shorter network lifetime, 2% longer delay, and 0% loss of goodput). This indicates the usefulness of our Physarum-inspired algorithm for minimum-cost relay node placement in budget-limited scenarios.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {681–694},
numpages = {14}
}

@article{10.1109/TNET.2020.2977198,
author = {Liu, Shengxin and Joe-Wong, Carlee and Chen, Jiasi and Brinton, Christopher G. and Tan, Chee Wei and Zheng, Liang},
title = {Economic Viability of a Virtual ISP},
year = {2020},
issue_date = {April 2020},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2977198},
doi = {10.1109/TNET.2020.2977198},
abstract = {Growing mobile data usage has led to end users paying substantial data costs, while Internet service providers (ISPs) struggle to upgrade their networks to keep up with demand and maintain high quality-of-service (QoS). This problem is particularly severe for smaller ISPs with less capital. Instead of simply upgrading their network infrastructure, ISPs can pool their networks to provide a good QoS and attract more users. Such a &lt;italic&gt;vISP&lt;/italic&gt; (virtual ISP), for example, Google’s Project Fi, allows users to access any of its partner ISPs’ networks. We provide the first systematic analysis of a vISP’s economic impact, showing that the vISP provides a viable solution for smaller ISPs attempting to attract more users, but may not maintain a positive profit if users’ data demands evolve. To do so, we consider users’ decisions of whether to defect from their current ISP to the vISP, as well as existing ISPs’ decisions on whether to partner with the vISP. We derive the vISP’s dependence on user behavior and partner ISPs: users with very light or very heavy usage are the most likely to defect, while ISPs with heavy-usage customers can benefit from declining to partner with the vISP. Our analytical results are verified with extensive numerical simulations.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {902–916},
numpages = {15}
}

@article{10.1109/TNET.2022.3192167,
author = {Majidi, Akbar and Gao, Xiaofeng and Zhu, Shunjia and Jahanbakhsh, Nazila and Zheng, Jiaqi and Chen, Guihai},
title = {MiFi: Bounded Update to Optimize Network Performance in Software-Defined Data Centers},
year = {2022},
issue_date = {Feb. 2023},
publisher = {IEEE Press},
volume = {31},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3192167},
doi = {10.1109/TNET.2022.3192167},
abstract = {A controller needs to solve the multi-commodity flow problem and globally update the network under tight time constraints to maintain optimal network configurations. This centralized optimization in data centers involves many variables and constraints, which has a slow convergence speed and little scalability. In this paper, we propose MiFi, which aims to Minimize Flow cost or intuitively average transmission delay (delay or latency of flows), under reconfiguration budget constraints in data centers. Thus, we formulate this optimization problem as a constrained Markov Decision Process and propose a set of algorithms to solve it in a scalable manner. We first develop a propagation algorithm to identify the flows mostly affected in terms of latency and configuration in the next update. Then, we set a limitation range (the subset of switches requiring network updates) for updating them to improve adaptability and scalability by updating a less number of flows each time to achieve fast operations. Further, based on the Drift-Plus-Penalty method in Lyapunov theory, we propose a heuristic policy without prior information of flow demand and a renewal policy with a performance guarantee to minimize the additive optimality gap. To the best of our knowledge, MiFi is the first paper that studies the range and frequency of flow reconfigurations, which has both theoretical and practical significance in the area. Emulations and numerical simulations, which are much better than the estimated theoretical bound, show that MiFi outperforms the state of the art algorithms in terms of latency by over 45% while making improvements in adaptability and scalability.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {322–335},
numpages = {14}
}

@article{10.1109/TNET.2022.3201545,
author = {Wang, En and Zhang, Mijia and Liu, Wenbin and Xiong, Haoyi and Yang, Bo and Yang, Yongjian and Wu, Jie},
title = {Outlier-Concerned Data Completion Exploiting Intra- and Inter-Data Correlations in Sparse CrowdSensing},
year = {2022},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {31},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3201545},
doi = {10.1109/TNET.2022.3201545},
abstract = {Mobile CrowdSensing (MCS) is a popular data collection paradigm which usually faces the problem of sparse sensed data because of the limited sensing cost. In order to address the situation of sparse data, sparse MCS recruits users to sense important areas and infers completed data by data completion, which is crucial in sparse MCS for urban sensing applications (e.g. enhancing data expression, improving urban analysis, guiding city planning, etc.) To achieve accurate completion results, previous methods usually utilize the universal similarity and conventional tendency while incorporating only a single dataset to infer the full map. However, in real-world scenarios, there may exist many kinds of data (inter-data), that could help to complement each other. Moreover, for each kind of data (intra-data), there usually exist a few but important outliers caused by the special events (e.g., parking peak, traffic congestion, or festival parade), which may behave in a different way as the statistical patterns. These outliers cannot be ignored, while it is difficult to detect and recover them in data completion because of the following challenges: 1) the infrequency and unpredictability of outliers’ occurrence, 2) the large deviations against the means compared to normal values, and 3) the complex spatiotemporal relations among inter-data. To this end, focusing on spatiotemporal data with both intra- and inter-data correlations, we propose a matrix completion method that takes outliers’ effects into consideration and exploits both intra- and inter-data correlations for enhancing performance. Specifically, we first conduct the Deep Matrix Factorization (DMF) with multiple auxiliary Neural Networks, which named Stacked Deep Matrix Factorization (SDMF). Note that the loss function of SDMF is no longer the previous MSE loss function, but replaced with an Outlier Value Loss (OVL) function to effectively detect and recover the outliers. Moreover, a spatiotemporal outlier value memory network is added for further enhancing the outlier inference. Finally, we take extensive qualitative and quantitative experiments on two popular datasets each with two types of sensing data, and the experimental results indicate the advantages of our method that outperforms the state-of-the-art methods.},
journal = {IEEE/ACM Trans. Netw.},
month = sep,
pages = {648–663},
numpages = {16}
}

@article{10.1109/TNET.2023.3265127,
author = {Wang, Chen and Hu, Qin and Yu, Dongxiao and Cheng, Xiuzhen},
title = {Online Learning for Failure-Aware Edge Backup of Service Function Chains With the Minimum Latency},
year = {2023},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {31},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3265127},
doi = {10.1109/TNET.2023.3265127},
abstract = {Virtual network functions (VNFs) have been widely deployed in mobile edge computing (MEC) to flexibly and efficiently serve end users running resource-intensive applications, which can be further serialized to form service function chains (SFCs), providing customized networking services. To ensure the availability of SFCs, it turns out to be effective to place redundant SFC backups at the edge for quickly recovering from any failures. The existing research largely overlooks the influences of SFC popularity, backup completeness, and failure rate on the optimal deployment of SFC backups on edge servers. In this paper, we comprehensively consider from the perspectives of both the end users and edge system to backup SFCs for providing popular services with the lowest latency. To overcome the challenges resulted from unknown SFC popularity and failure rate, as well as the known system parameter constraints, we take advantage of the online bandit learning technique to cope with the uncertainty issue. Combining the Prim -inspired method with the greedy strategy, we propose a Real-Time Selection and Deployment (RTSD) algorithm. Extensive simulation experiments are conducted to demonstrate the superiority of our proposed algorithms.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {2730–2744},
numpages = {15}
}

@article{10.1109/TNET.2023.3265276,
author = {Cao, Peirui and Zhao, Shizhen and Zhang, Dai and Liu, Zhuotao and Xu, Mingwei and Teh, Min Yee and Liu, Yunzhuo and Wang, Xinbing and Zhou, Chenghu},
title = {Threshold-Based Routing-Topology Co-Design for Optical Data Center},
year = {2023},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {31},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3265276},
doi = {10.1109/TNET.2023.3265276},
abstract = {Despite the bandwidth scaling limit of electrical switching and the high cost of building Clos data center networks (DCNs), the adoption of optical DCNs is still limited. There are two reasons. First, existing optical DCN designs usually face high deployment complexity. Second, these designs are not full-optical and the performance benefit over the non-blocking Clos DCN is not clear. After exploring the design tradeoffs of the existing optical DCN designs, we propose TROD (Threshold Routing based Optical Datacenter), a low-complexity optical DCN with superior performance than other optical DCNs. There are two novel designs in TROD that contribute to its success. First, TROD performs robust topology optimization based on the recurring traffic patterns and thus does not need to react to every traffic change, which lowers deployment and management complexity. Second, TROD introduces tVLB (threshold-based Valiant Load Balance), which can avoid network congestion as much as possible even under unexpected traffic bursts. We conduct simulation based on both Facebook’s real DCN traces and our synthesized highly bursty DCN traces. TROD reduces flow completion time (FCT) by about 1.15-&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$2.16times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; compared to Google’s Jupiter DCN, at least &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$2times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; compared to other optical DCN designs, and about 2.4-&lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$3.2times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; compared to expander graph DCN. Compared with the non-blocking Clos, TROD reduces the hop count of the majority packets by one, and could even outperform the non-blocking Clos with proper bandwidth over-provision at the optical layer. Note that TROD can be built with commercially available hardware and does not require host modifications.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {2870–2885},
numpages = {16}
}

@article{10.1109/TNET.2023.3323522,
author = {Guo, Xianwei and Huang, Fangwan and Yang, Dingqi and Tu, Chunyu and Yu, Zhiyong and Guo, Wenzhong},
title = {Spatiotemporal Fracture Data Inference in Sparse Mobile Crowdsensing: A Graph- and Attention-Based Approach},
year = {2023},
issue_date = {April 2024},
publisher = {IEEE Press},
volume = {32},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2023.3323522},
doi = {10.1109/TNET.2023.3323522},
abstract = {Mobile Crowdsensing (MCS) is a sensing paradigm that enables large-scale smart city applications, such as environmental sensing and traffic monitoring. However, traditional MCS often suffers from performance degradation due to the limited spatiotemporal coverage of collected data. In this context, Sparse MCS has been proposed, which utilizes data inference algorithms to recover full data from sparse data collected by users. However, existing Sparse MCS approaches often overlook spatiotemporal fractures, where no data is observed either for a sensing subarea across all sensing time slots (temporal fracture), or for a sensing time slot in all sensing subarea (spatial fracture). Such spatiotemporal fractures pose great challenges to the data inference algorithms, as it is difficult to capture the complex spatiotemporal correlations of the sensing data from very limited observations. To address this issue, we propose a Graph- and Attention-based Matrix Completion (GAMC) method for the spatiotemporal fracture data inference problem in Sparse MCS. Specifically, we first pre-fill the general missing values using the classical Matrix Factorization (MF) technique. Then, we propose a neural network architecture based on Graph Attention Networks (GAT) and Transformer to capture complex spatiotemporal dependencies in the sensing data. Finally, we recover the complete data with a projection layer. We conduct extensive experiments on three real-world urban sensing datasets. The experimental results show the effectiveness of the proposed method.},
journal = {IEEE/ACM Trans. Netw.},
month = oct,
pages = {1631–1644},
numpages = {14}
}

@article{10.1109/TNET.2024.3361324,
author = {Chen, Xiang and Liu, Hongyan and Xiao, Qingjiang and Huang, Qun and Zhang, Dong and Zhou, Haifeng and Zhou, Boyang and Wu, Chunming and Liu, Xuan and Yang, Qiang},
title = {Hermes: Low-Overhead Inter-Switch Coordination in Network-Wide Data Plane Program Deployment},
year = {2024},
issue_date = {Aug. 2024},
publisher = {IEEE Press},
volume = {32},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3361324},
doi = {10.1109/TNET.2024.3361324},
abstract = {Network administrators usually realize network functions in data plane programs. They employ the network-wide program deployment that decomposes input programs into match-action tables (MATs) while deploying each MAT on a specific switch. Since MATs may be deployed on different switches, existing solutions propose the inter-switch coordination that uses the per-packet header space to deliver crucial packet processing information among switches. However, such coordination incurs non-trivial per-packet byte overhead, leading to end-to-end performance degradation. We propose Hermes, a framework that aims to minimize the per-packet byte overhead. The key idea is to formulate network-wide program deployment as a mixed-integer programming (MIP) problem with the objective of minimizing the per-packet byte overhead. Also, Hermes offers a greedy-based heuristic that solves the problem in a near-optimal and timely manner. We have implemented Hermes on Tofino switches. Compared to existing frameworks, Hermes decreases the per-packet byte overhead by 156bytes while preserving end-to-end performance in terms of flow completion time and goodput.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {2842–2857},
numpages = {16}
}

@article{10.1109/TNET.2024.3365815,
author = {Nguyen, Chi-Hieu and Saputra, Yuris Mulya and Hoang, Dinh Thai and Nguyen, Diep N. and Nguyen, Van-Dinh and Xiao, Yong and Dutkiewicz, Eryk},
title = {Encrypted Data Caching and Learning Framework for Robust Federated Learning-Based Mobile Edge Computing},
year = {2024},
issue_date = {June 2024},
publisher = {IEEE Press},
volume = {32},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3365815},
doi = {10.1109/TNET.2024.3365815},
abstract = {Federated Learning (FL) plays a pivotal role in enabling artificial intelligence (AI)-based mobile applications in mobile edge computing (MEC). However, due to the resource heterogeneity among participating mobile users (MUs), delayed updates from slow MUs may deteriorate the learning speed of the MEC-based FL system, commonly referred to as the straggling problem. To tackle the problem, this work proposes a novel privacy-preserving FL framework that utilizes homomorphic encryption (HE) based solutions to enable MUs, particularly resource-constrained MUs, to securely offload part of their training tasks to the cloud server (CS) and mobile edge nodes (MENs). Our framework first develops an efficient method for packing batches of training data into HE ciphertexts to reduce the complexity of HE-encrypted training at the MENs/CS. On that basis, the mobile service provider (MSP) can incentivize straggling MUs to encrypt part of their local datasets that are uploaded to certain MENs or the CS for caching and remote training. However, caching a large amount of encrypted data at the MENs and CS for FL may not only overburden those nodes but also incur a prohibitive cost of remote training, which ultimately reduces the MSP’s overall profit. To optimize the portion of MUs’ data to be encrypted, cached, and trained at the MENs/CS, we formulate an MSP’s profit maximization problem, considering all MUs’ and MENs’ resource capabilities and data handling costs (including encryption, caching, and training) as well as the MSP’s incentive budget. We then show that the problem is convex and can be efficiently solved using an interior point method. Extensive simulations on a real-world human activity recognition dataset show that our proposed framework can achieve much higher model accuracy (improving up to 24.29%) and faster convergence rate (by 2.86 times) than those of the conventional FedAvg approach when the straggling probability varies between 20% and 80%. Moreover, the proposed framework can improve the MSP’s profit up to 2.84 times compared with other baseline FL approaches without MEN-assisted training.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {2705–2720},
numpages = {16}
}

@article{10.1109/TNET.2024.3382546,
author = {Liu, Yang and Wang, Xi and Wang, Xiaoqi and Wang, Zhen},
title = {Fast Outbreak Sense and Effective Source Inference via Minimum Observer Set},
year = {2024},
issue_date = {Aug. 2024},
publisher = {IEEE Press},
volume = {32},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3382546},
doi = {10.1109/TNET.2024.3382546},
abstract = {This paper addresses the Fast outbreak Sensing and Effective diffusion source Inferring (FSEI) problem, which assumes that the state of nodes in a particularly chosen observer set can be monitored if necessary and aims to optimize the observer set such that outbreaks can be timely detected and their sources can be effectively targeted. We propose three approaches to tackle the FSEI problem: Greedy Strategy (GS), Network-Topology-based Method (NTM), and Hybrid Method (HM). Among them, GS relies on collected outbreaks and constructs the observer set by iteratively choosing and removing the node that minimizes the product of sensing time and source targeting cost of the remaining network. For NTM, we also consider the remaining network and introduce a novel strategy to optimize its topology via simultaneously minimizing the adjoining component size and ratio of the first and second moments. HM is a combination of GS and NTM, considering the submodular property of GS on the minimization of the sensing time and well approximation of the component size on the optimization of the source targeting. We perform extensive experiments on over 200 empirical networks, using various diffusion models, to validate the proposed methods. The results demonstrate that our approaches consistently outperform the state-of-the-art. We believe that the model and methodology presented in this paper can be readily applied to real-world scenarios such as combating misinformation and controlling diffusions of information or disease.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {3111–3125},
numpages = {15}
}

@article{10.1109/TNET.2024.3403671,
author = {Hu, Jinbin and He, Yi and Luo, Wangqing and Huang, Jiawei and Wang, Jin},
title = {Enhancing Load Balancing With In-Network Recirculation to Prevent Packet Reordering in Lossless Data Centers},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3403671},
doi = {10.1109/TNET.2024.3403671},
abstract = {Many existing load balancing mechanisms work effectively in lossy datacenter networks (DCNs), but they suffer from serious packet reordering in lossless Ethernet DCNs deployed with the hop-by-hop Priority-based Flow Control (PFC). The key reason is that the prior solutions are not able to perceive PFC triggering correctly and in a timely manner when making load balancing decisions. Once the forwarding path pauses transmission due to PFC triggering, the packets allocated on it are blocked, inevitably leading to out-of-order packets and retransmission. In this paper, we present an Reordering-robust Load Balancing (RLB) scheme with PFC prediction in lossless DCNs. At its heart, RLB leverages the derivative of ingress queue length to predict PFC triggering and proactively notifies the upstream switches to choose an appropriate rerouting path or perform packet recirculation to avoid reordering. Furthermore, under switch failure scenarios, RLB adjusts the recirculation threshold adaptively to mitigate the risk of packets over-recirculation. We have implemented RLB in the hardware programmable switch. As a building block for existing load balancing mechanisms, we have integrated RLB into Presto, LetFlow, Hermes and DRILL. The evaluation results show that the RLB-enhanced solutions deliver significant performance by avoiding packet reordering. For example, it reduces the &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$99^{th}$ &lt;/tex-math&gt;&lt;/inline-formula&gt; percentile flow completion time (FCT) by up to 72%, 67%, 58% and 54% over DRILL, Presto, LetFlow and Hermes, respectively.},
journal = {IEEE/ACM Trans. Netw.},
month = may,
pages = {4114–4127},
numpages = {14}
}

@article{10.1109/TNET.2024.3404011,
author = {Cong, Yinchuan and Xie, Kun and Wen, Jigang and Zhang, Jiwei and Yin, Yansong and Zhang, Qingyi and Xie, Gaogang and Liang, Wei},
title = {Per-Packet Traffic Measurement in Storage, Computation and Bandwidth Limited Data Plane},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3404011},
doi = {10.1109/TNET.2024.3404011},
abstract = {Packet level measurement in the data plane provides a microscopic view of the network’s state. Although advances in programmable switches and routers make it possible to measure the Sequence of Packet Lengths and Arrival Times (SPLT) in the data plane, collecting this information remains challenging due to limited storage, processing resources, and bandwidth. To address this issue, we propose MES, which Measures and Encodes Simultaneously the packet length and timestamp when each packet passes through the Network Processor (NP) in the switch/router. We design the packet length compression and timestamp compression algorithms to be lightweight and implement the designed algorithms using simple operations supported by the network processor, while taking into account the computation constraints of the NP. Through extensive experiments on five packet traces, we demonstrate that our MES achieves high precision SPLT measurements (up to 99.82% cosine similarity) while reducing storage and bandwidth overhead by up to 87%. Simulations conducted on the BMV2 P4 software switch demonstrate that our designed SPLT measurement mechanism imposes little impact on network throughput and delay.},
journal = {IEEE/ACM Trans. Netw.},
month = may,
pages = {3730–3742},
numpages = {13}
}

@article{10.1109/TNET.2024.3422264,
author = {Ruan, Yichen and Zhang, Xiaoxi and Joe-Wong, Carlee},
title = {How Valuable is Your Data? Optimizing Client Recruitment in Federated Learning},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Press},
volume = {32},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2024.3422264},
doi = {10.1109/TNET.2024.3422264},
abstract = {Federated learning allows distributed clients to train a shared machine learning model while preserving user privacy. In this framework, user devices (i.e., clients) perform local iterations of the learning algorithm on their data. These updates are periodically aggregated to form a shared model. Thus, a client represents the bundle of the user data, the device, and the user’s willingness to participate: since participating in federated learning requires clients to expend resources and reveal some information about their data, users may require some form of compensation to contribute to the training process. Recruiting more users generally results in higher accuracy, but slower completion time and higher cost. We propose the first work to theoretically analyze the resulting performance tradeoffs in deciding which clients to recruit for the federated learning algorithm. Our framework accounts for both accuracy (training and testing) and efficiency (completion time and cost) metrics. We provide solutions to this NP-Hard optimization problem and verify the value of client recruitment in experiments on synthetic and real-world data. The results of this work can serve as a guideline for the real-world deployment of federated learning and an initial investigation of the client recruitment problem.},
journal = {IEEE/ACM Trans. Netw.},
month = jul,
pages = {4207–4221},
numpages = {15}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {type checking, theorem proving, static analysis, software product line, software analysis, program family, model checking, Product-line analysis}
}

@article{10.1145/2629662,
author = {Shan, Mengfan and Chen, Guihai and Luo, Dijun and Zhu, Xiaojun and Wu, Xiaobing},
title = {Building Maximum Lifetime Shortest Path Data Aggregation Trees in Wireless Sensor Networks},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/2629662},
doi = {10.1145/2629662},
abstract = {In wireless sensor networks, the spanning tree is usually used as a routing structure to collect data. In some situations, nodes do in-network aggregation to reduce transmissions, save energy, and maximize network lifetime. Because of the restricted energy of sensor nodes, how to build an aggregation tree of maximum lifetime is an important issue. It has been proved to be NP-complete in previous works. As shortest path spanning trees intuitively have short delay, it is imperative to find an energy-efficient shortest path tree for time-critical applications. In this article, we first study the problem of building maximum lifetime shortest path aggregation trees in wireless sensor networks. We show that when restricted to shortest path trees, building maximum lifetime aggregation trees can be solved in polynomial time. We present a centralized algorithm and design a distributed protocol for building such trees. Simulation results show that our approaches greatly improve the lifetime of the network and are very effective compared to other solutions. We extend our discussion to networks without aggregation and present interesting results.},
journal = {ACM Trans. Sen. Netw.},
month = jul,
articleno = {11},
numpages = {24},
keywords = {media access control, maximum lifetime, load balance, Wireless sensor networks}
}

@article{10.1145/2700386,
author = {Xie, Hong and Lui, John C. S.},
title = {Mathematical Modeling and Analysis of Product Rating with Partial Information},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2700386},
doi = {10.1145/2700386},
abstract = {Many Web services like Amazon, Epinions, and TripAdvisor provide historical product ratings so that users can evaluate the quality of products. Product ratings are important because they affect how well a product will be adopted by the market. The challenge is that we only have partial information on these ratings: each user assigns ratings to only a small subset of products. Under this partial information setting, we explore a number of fundamental questions. What is the minimum number of ratings a product needs so that one can make a reliable evaluation of its quality? How may users’ misbehavior, such as cheating in product rating, affect the evaluation result? To answer these questions, we present a probabilistic model to capture various important factors (e.g., rating aggregation rules, rating behavior) that may influence the product quality assessment under the partial information setting. We derive the minimum number of ratings needed to produce a reliable indicator on the quality of a product. We extend our model to accommodate users’ misbehavior in product rating. We derive the maximum fraction of misbehaving users that a rating aggregation rule can tolerate and the minimum number of ratings needed to compensate. We carry out experiments using both synthetic and real-world data (from Amazon and TripAdvisor). We not only validate our model but also show that the “average rating rule” produces more reliable and robust product quality assessments than the “majority rating rule” and the “median rating rule” in aggregating product ratings. Last, we perform experiments on two movie rating datasets (from Flixster and Netflix) to demonstrate how to apply our framework to improve the applications of recommender systems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {26},
numpages = {33},
keywords = {true quality, rating aggregation rule, misbehavior, minimum number of ratings, bias, Product rating}
}

@article{10.1145/2753764,
author = {Xu, Silei and Lui, John C. S.},
title = {Product Selection Problem: Improve Market Share by Learning Consumer Behavior},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/2753764},
doi = {10.1145/2753764},
abstract = {It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers’ requirements and how consumers make their purchase decisions so that the new products will be competitive in the market. In this paper, we first present a general distance-based product adoption model to capture consumers’ purchase behavior. Using this model, various distance metrics can be used to describe different real life purchase behavior. We then provide a learning algorithm to decide which set of distance metrics one should use when we are given some accessible historical purchase data. Based on the product adoption model, we formalize the k most marketable products (or k-MMP) selection problem and formally prove that the problem is NP-hard. To tackle this problem, we propose an efficient greedy-based approximation algorithm with a provable solution guarantee. Using submodularity analysis, we prove that our approximation algorithm can achieve at least 63\% of the optimal solution. We apply our algorithm on both synthetic datasets and real-world datasets (TripAdvisor.com), and show that our algorithm can easily achieve five or more orders of speedup over the exhaustive search and achieve about 96\% of the optimal solution on average. Our experiments also demonstrate the robustness of our distance metric learning method, and illustrate how one can adopt it to improve the accuracy of product selection.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {25},
keywords = {submodular set function, model learning, consumer behavior, approximation algorithm, Product selection}
}

@article{10.1145/2795229,
author = {Jung, Dongha and Lee, Hokyoon and Kim, Seon Wook},
title = {Lowering Minimum Supply Voltage for Power-Efficient Cache Design by Exploiting Data Redundancy},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/2795229},
doi = {10.1145/2795229},
abstract = {Voltage scaling is known to be an efficient way of saving power and energy within a system, and large caches such as LLCs are good candidates for voltage scaling considering their constantly increasing size. However, the VCCMIN problem, in which the lower bound of scalable voltage is limited by process variation, has made it difficult to exploit the benefits of voltage scaling. Lowering VCCMIN incurs multibit faults, which cannot be efficiently resolved by current technologies due to their high complexity and power consumption. We overcame the limitation by exploiting the data redundancy of memory hierarchy. For example, cache coherence states and several layers of cache organization naturally expose the existence of redundancy within cache blocks. If blocks have redundant copies, their VCCMIN can be lowered; although more faults can occur in the blocks, they can be efficiently detected by simple error detection codes and recovered by reloading the redundant copies. Our scheme requires only minor modifications to the existing cache design. We verified our proposal on a cycle accurate simulator with SPLASH-2 and PARSEC benchmark suites and found that the VCCMIN of a 2MB L2 cache can be further lowered by 0.1V in 32nm technology with negligible degradation in performance. As a result, we could achieve 15.6\% of reduction in dynamic power and 15.4\% of reduction in static power compared to the previous minimum power.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {11},
numpages = {24},
keywords = {cache replacement policy, VCCMIN, SRAM reliability, Dynamic voltage scaling}
}

@article{10.1145/2842629,
author = {Zhao, Wayne Xin and Wang, Jinpeng and He, Yulan and Wen, Ji-Rong and Chang, Edward Y. and Li, Xiaoming},
title = {Mining Product Adopter Information from Online Reviews for Improving Product Recommendation},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/2842629},
doi = {10.1145/2842629},
abstract = {We present in this article an automated framework that extracts product adopter information from online reviews and incorporates the extracted information into feature-based matrix factorization for more effective product recommendation. In specific, we propose a bootstrapping approach for the extraction of product adopters from review text and categorize them into a number of different demographic categories. The aggregated demographic information of many product adopters can be used to characterize both products and users in the form of distributions over different demographic categories. We further propose a graph-based method to iteratively update user- and product-related distributions more reliably in a heterogeneous user--product graph and incorporate them as features into the matrix factorization approach for product recommendation. Our experimental results on a large dataset crawled from JingDong, the largest B2C e-commerce website in China, show that our proposed framework outperforms a number of competitive baselines for product recommendation.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {29},
numpages = {23},
keywords = {product recommendation, product adopter, matrix factorisation, Online review}
}

@article{10.1145/2857054,
author = {Bing, Lidong and Wong, Tak-Lam and Lam, Wai},
title = {Unsupervised Extraction of Popular Product Attributes from E-Commerce Web Sites by Considering Customer Reviews},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2857054},
doi = {10.1145/2857054},
abstract = {We develop an unsupervised learning framework for extracting popular product attributes from product description pages originated from different E-commerce Web sites. Unlike existing information extraction methods that do not consider the popularity of product attributes, our proposed framework is able to not only detect popular product features from a collection of customer reviews but also map these popular features to the related product attributes. One novelty of our framework is that it can bridge the vocabulary gap between the text in product description pages and the text in customer reviews. Technically, we develop a discriminative graphical model based on hidden Conditional Random Fields. As an unsupervised model, our framework can be easily applied to a variety of new domains and Web sites without the need of labeling training samples. Extensive experiments have been conducted to demonstrate the effectiveness and robustness of our framework.},
journal = {ACM Trans. Internet Technol.},
month = apr,
articleno = {12},
numpages = {17},
keywords = {product attribute, customer reviews, conditional random fields, Information extraction}
}

@article{10.1145/2907611,
author = {Lee, Jinyong and Heo, Ingoo and Lee, Yongje and Paek, Yunheung},
title = {Efficient Security Monitoring with the Core Debug Interface in an Embedded Processor},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/2907611},
doi = {10.1145/2907611},
abstract = {For decades, various concepts in security monitoring have been proposed. In principle, they all in common in regard to the monitoring of the execution behavior of a program (e.g., control-flow or dataflow) running on the machine to find symptoms of attacks. Among the proposed monitoring schemes, software-based ones are known for their adaptability on the commercial products, but there have been concerns that they may suffer from nonnegligible runtime overhead. On the other hand, hardware-based solutions are recognized for their high performance. However, most of them have an inherent problem in that they usually mandate drastic changes to the internal processor architecture. More recent ones have strived to minimize such modifications by employing external hardware security monitors in the system. However, these approaches intrinsically suffer from the overhead caused by communication between the host and the external monitor. Our solution also relies on external hardware for security monitoring, but unlike the others, ours tackles the communication overhead by using the core debug interface (CDI), which is readily available in most commercial processors for debugging. We build our system simply by plugging our monitoring hardware into the processor via CDI, precluding the need for altering the processor internals. To validate the effectiveness of our approach, we implement two well-known monitoring techniques on our proposed framework: dynamic information flow tracking and branch regulation. The experimental results on our FPGA prototype show that our external hardware monitors efficiently perform monitoring tasks with negligible performance overhead, mainly with thanks to the support of CDI, which helps us reduce communication costs substantially.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = may,
articleno = {8},
numpages = {29},
keywords = {security monitoring, dynamic information flow tracking (DIFT), code reuse attack detection, Core debug interface (CDI)}
}

@article{10.1145/2937752,
author = {Zhang, Yating and Jatowt, Adam and Tanaka, Katsumi},
title = {Causal Relationship Detection in Archival Collections of Product Reviews for Understanding Technology Evolution},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2937752},
doi = {10.1145/2937752},
abstract = {Technology progress is one of the key reasons behind today's rapid changes in lifestyles. Knowing how products and objects evolve can not only help with understanding the evolutionary patterns in our society but can also provide clues on effective product design and can offer support for predicting the future. We propose a general framework for analyzing technology's impact on our lives through detecting cause--effect relationships, where causes represent changes in technology while effects are changes in social life, such as new activities or new ways of using products. We address the challenge of viewing technology evolution through the “social impact lens” by mining causal relationships from the long-term collections of product reviews. In particular, we first propose dividing vocabulary into two groups: terms describing product features (called physical terms) and terms representing product usage (called conceptual terms). We then search for two kinds of changes related to the appearance of terms: frequency-based and context-based changes. The former indicate periods when a word was significantly more frequently used, whereas the latter indicate periods of high change in the word's context. Based on the detected changes, we then search for causal term pairs such that the change in the physical term triggers the change in the conceptual term. We next extend our approach to finding causal relationships between word groups such as a group of words representing the same technology and causing a given conceptual change or group of words representing two different technologies that simultaneously “co-cause” a conceptual change. We conduct experiments on different product types using the Amazon Product Review Dataset, which spans 1995 to 2013, and we demonstrate that our approaches outperform state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {3},
numpages = {41},
keywords = {social influence, product evolution analysis, causality detection, Technology evolution analysis}
}

@article{10.1145/2972950,
author = {Tian, Yuanyuan and \"{O}zcan, Fatma and Zou, Tao and Goncalves, Romulo and Pirahesh, Hamid},
title = {Building a Hybrid Warehouse: Efficient Joins between Data Stored in HDFS and Enterprise Warehouse},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/2972950},
doi = {10.1145/2972950},
abstract = {The Hadoop Distributed File System (HDFS) has become an important data repository in the enterprise as the center for all business analytics, from SQL queries and machine learning to reporting. At the same time, enterprise data warehouses (EDWs) continue to support critical business analytics. This has created the need for a new generation of a special federation between Hadoop-like big data platforms and EDWs, which we call the hybrid warehouse. There are many applications that require correlating data stored in HDFS with EDW data, such as the analysis that associates click logs stored in HDFS with the sales data stored in the database. All existing solutions reach out to HDFS and read the data into the EDW to perform the joins, assuming that the Hadoop side does not have efficient SQL support.In this article, we show that it is actually better to do most data processing on the HDFS side, provided that we can leverage a sophisticated execution engine for joins on the Hadoop side. We identify the best hybrid warehouse architecture by studying various algorithms to join database and HDFS tables. We utilize Bloom filters to minimize the data movement and exploit the massive parallelism in both systems to the fullest extent possible. We describe a new zigzag join algorithm and show that it is a robust join algorithm for hybrid warehouses that performs well in almost all cases. We further develop a sophisticated cost model for the various join algorithms and show that it can facilitate query optimization in the hybrid warehouse to correctly choose the right algorithm under different predicate and join selectivities.},
journal = {ACM Trans. Database Syst.},
month = nov,
articleno = {21},
numpages = {38},
keywords = {query push-down, join on Hadoop, hybrid warehouse, federation, cost model, SQL-on-Hadoop, Distributed join, Bloom filter}
}

@article{10.1145/2996452,
author = {Teflioudi, Christina and Gemulla, Rainer},
title = {Exact and Approximate Maximum Inner Product Search with LEMP},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/2996452},
doi = {10.1145/2996452},
abstract = {We study exact and approximate methods for maximum inner product search, a fundamental problem in a number of data mining and information retrieval tasks. We propose the LEMP framework, which supports both exact and approximate search with quality guarantees. At its heart, LEMP transforms a maximum inner product search problem over a large database of vectors into a number of smaller cosine similarity search problems. This transformation allows LEMP to prune large parts of the search space immediately and to select suitable search algorithms for each of the remaining problems individually. LEMP is able to leverage existing methods for cosine similarity search, but we also provide a number of novel search algorithms tailored to our setting. We conducted an extensive experimental study that provides insight into the performance of many state-of-the-art techniques—including LEMP—on multiple real-world datasets. We found that LEMP often was significantly faster or more accurate than alternative methods.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {5},
numpages = {49},
keywords = {top-k search, recommender systems, indexing, Maximum inner product search (MIPS)}
}

@article{10.1145/3022185,
author = {Liu, Guannan and Fu, Yanjie and Chen, Guoqing and Xiong, Hui and Chen, Can},
title = {Modeling Buying Motives for Personalized Product Bundle Recommendation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3022185},
doi = {10.1145/3022185},
abstract = {Product bundling is a marketing strategy that offers several products/items for sale as one bundle. While the bundling strategy has been widely used, less efforts have been made to understand how items should be bundled with respect to consumers’ preferences and buying motives for product bundles. This article investigates the relationships between the items that are bought together within a product bundle. To that end, each purchased product bundle is formulated as a bundle graph with items as nodes and the associations between pairs of items in the bundle as edges. The relationships between items can be analyzed by the formation of edges in bundle graphs, which can be attributed to the associations of feature aspects. Then, a probabilistic model BPM (Bundle Purchases with Motives) is proposed to capture the composition of each bundle graph, with two latent factors node-type and edge-type introduced to describe the feature aspects and relationships respectively. Furthermore, based on the preferences inferred from the model, an approach for recommending items to form product bundles is developed by estimating the probability that a consumer would buy an associative item together with the item already bought in the shopping cart. Finally, experimental results on real-world transaction data collected from well-known shopping sites show the effectiveness advantages of the proposed approach over other baseline methods. Moreover, the experiments also show that the proposed model can explain consumers’ buying motives for product bundles in terms of different node-types and edge-types.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {28},
numpages = {26},
keywords = {recommendation, probabilistic graphical model, buying motives, Product bundle}
}

@article{10.1145/3039241,
author = {Chakrabarty, Deeparnab and Dixit, Kashyap and Jha, Madhav and Seshadhri, C.},
title = {Property Testing on Product Distributions: Optimal Testers for Bounded Derivative Properties},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1549-6325},
url = {https://doi.org/10.1145/3039241},
doi = {10.1145/3039241},
abstract = {The primary problem in property testing is to decide whether a given function satisfies a certain property or is far from any function satisfying it. This crucially requires a notion of distance between functions. The most prevalent notion is the Hamming distance over the uniform distribution on the domain. This restriction to uniformity is rather limiting, and it is important to investigate distances induced by more general distributions.In this article, we provide simple and optimal testers for bounded derivative properties over arbitrary product distributions. Bounded derivative properties include fundamental properties, such as monotonicity and Lipschitz continuity. Our results subsume almost all known results (upper and lower bounds) on monotonicity and Lipschitz testing over arbitrary ranges.We prove an intimate connection between bounded derivative property testing and binary search trees (BSTs). We exhibit a tester whose query complexity is the sum of expected depths of optimal BSTs for each marginal. Furthermore, we show that this sum-of-depths is also a lower bound. A technical contribution of our work is an optimal dimension reduction theorem for all bounded derivative properties that relates the distance of a function from the property to the distance of restrictions of the function to random lines. Such a theorem has been elusive even for monotonicity, and our theorem is an exponential improvement to the previous best-known result.},
journal = {ACM Trans. Algorithms},
month = mar,
articleno = {20},
numpages = {30},
keywords = {monotonicity, Property testing, Lipschitz continuity}
}

@article{10.1145/3039243,
author = {Fomin, Fedor V. and Lokshtanov, Daniel and Panolan, Fahad and Saurabh, Saket},
title = {Representative Families of Product Families},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1549-6325},
url = {https://doi.org/10.1145/3039243},
doi = {10.1145/3039243},
abstract = {A subfamily F′ of a set family F is said to q-represent F if for every A ∈ F and B of size q such that A∩B = ∅ there exists a set A′ ∈ F′ such that A′ ∩ B = ∅. Recently, we provided an algorithm that, for a given family F of sets of size p together with an integer q, efficiently computes a q-representative family F′ of F of size approximately (p+q p). In this article, we consider the efficient computation of q-representative families for product families F. A family F is a product family if there exist families A and B such that F = { A, ∪, B : A ∈ A, B ∈ B, A, ∩, B = ∅}. Our main technical contribution is an algorithm that, given A, B and q, computes a q-representative family F′ of F. The running time of our algorithm is sublinear in |F| for many choices of A, B, and q that occur naturally in several dynamic programming algorithms. We also give an algorithm for the computation of q-representative families for product families F in the more general setting where q-representation also involves independence in a matroid in addition to disjointness. This algorithm considerably outperforms the naive approach where one first computes F from A and B and then computes the q-representative family F′ from F.We give two applications of our new algorithms for computing q-representative families for product families. The first is a 3.8408knO(1) deterministic algorithm for the Multilinear Monomial Detection (k-MlD) problem. The second is a significant improvement of deterministic dynamic programming algorithms for “connectivity problems” on graphs of bounded treewidth.},
journal = {ACM Trans. Algorithms},
month = mar,
articleno = {36},
numpages = {29},
keywords = {tree-width bounded graphs, representative families, parameterized algorithms, multi-linear monomial detection, Matroids}
}

@article{10.1145/3050437,
author = {Pathania, Anuj and Venkataramani, Vanchinathan and Shafique, Muhammad and Mitra, Tulika and Henkel, J\"{o}rg},
title = {Defragmentation of Tasks in Many-Core Architecture},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3050437},
doi = {10.1145/3050437},
abstract = {Many-cores can execute multiple multithreaded tasks in parallel. A task performs most efficiently when it is executed over a spatially connected and compact subset of cores so that performance loss due to communication overhead imposed by the task’s threads spread across the allocated cores is minimal. Over a span of time, unallocated cores can get scattered all over the many-core, creating fragments in the task mapping. These fragments can prevent efficient contiguous mapping of incoming new tasks leading to loss of performance. This problem can be alleviated by using a task defragmenter, which consolidates smaller fragments into larger fragments wherein the incoming tasks can be efficiently executed. Optimal defragmentation of a many-core is an NP-hard problem in the general case. Therefore, we simplify the original problem to a problem that can be solved optimally in polynomial time. In this work, we introduce a concept of exponentially separable mapping (ESM), which defines a set of task mapping constraints on a many-core. We prove that an ESM enforcing many-core can be defragmented optimally in polynomial time.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {2},
numpages = {21},
keywords = {task defragmentation, multiagent systems, Many-core}
}

@article{10.1145/3057271,
author = {Kakar, Adarsh Kumar},
title = {Investigating the Relationships Between the Use Contexts, User Perceived Values, and Loyalty to a Software Product},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3057271},
doi = {10.1145/3057271},
abstract = {In this study, we propose that software products provide three types of value—utilitarian, hedonic, and social—that impact user loyalty. Although the Technology Acceptance Model (TAM) has focused on the user impacts of utilitarian and hedonic values provided by utilitarian and hedonic software products on system use, the impact of social value provided by the software products in general have been largely ignored. The results of a longitudinal study with actual users of three types of software products show that all three types of software products—utilitarian (Producteev), hedonic (Kerbal), and social (Facebook)—provide significant but varying degrees of all three types of values. Further, the value derived by the users’ primary use context moderated the impact of the secondary values provided by the software product to the users on their loyalty for the product.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {3},
numpages = {23},
keywords = {utilitarian value, user loyalty, social value, Hedonic value}
}

@article{10.1145/3075618,
author = {Stanic, Milan and Palomar, Oscar and Hayes, Timothy and Ratkovic, Ivan and Cristal, Adrian and Unsal, Osman and Valero, Mateo},
title = {An Integrated Vector-Scalar Design on an In-Order ARM Core},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3075618},
doi = {10.1145/3075618},
abstract = {In the low-end mobile processor market, power, energy, and area budgets are significantly lower than in the server/desktop/laptop/high-end mobile markets. It has been shown that vector processors are a highly energy-efficient way to increase performance; however, adding support for them incurs area and power overheads that would not be acceptable for low-end mobile processors. In this work, we propose an integrated vector-scalar design for the ARM architecture that mostly reuses scalar hardware to support the execution of vector instructions. The key element of the design is our proposed block-based model of execution that groups vector computational instructions together to execute them in a coordinated manner. We implemented a classic vector unit and compare its results against our integrated design. Our integrated design improves the performance (more than 6\texttimes{}) and energy consumption (up to 5\texttimes{}) of a scalar in-order core with negligible area overhead (only 4.7\% when using a vector register with 32 elements). In contrast, the area overhead of the classic vector unit can be significant (around 44\%) if a dedicated vector floating-point unit is incorporated. Our block-based vector execution outperforms the classic vector unit for all kernels with floating-point data and also consumes less energy. We also complement the integrated design with three energy/performance-efficient techniques that further reduce power and increase performance. The first proposal covers the design and implementation of chaining logic that is optimized to work with the cache hierarchy through vector memory instructions, the second proposal reduces the number of reads/writes from/to the vector register file, and the third idea optimizes complex memory access patterns with the memory shape instruction and unified indexed vector load.},
journal = {ACM Trans. Archit. Code Optim.},
month = may,
articleno = {17},
numpages = {26},
keywords = {mobile processors, low-power, energy efficiency, Vector processors}
}

@article{10.1145/3105910,
author = {Grover, Shuchi and Basu, Satabdi and Bienkowski, Marie and Eagle, Michael and Diana, Nicholas and Stamper, John},
title = {A Framework for Using Hypothesis-Driven Approaches to Support Data-Driven Learning Analytics in Measuring Computational Thinking in Block-Based Programming Environments},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
url = {https://doi.org/10.1145/3105910},
doi = {10.1145/3105910},
abstract = {Systematic endeavors to take computer science (CS) and computational thinking (CT) to scale in middle and high school classrooms are underway with curricula that emphasize the enactment of authentic CT skills, especially in the context of programming in block-based programming environments. There is, therefore, a growing need to measure students’ learning of CT in the context of programming and also support all learners through this process of learning computational problem solving. The goal of this research is to explore hypothesis-driven approaches that can be combined with data-driven ones to better interpret student actions and processes in log data captured from block-based programming environments with the goal of measuring and assessing students’ CT skills. Informed by past literature and based on our empirical work examining a dataset from the use of the Fairy Assessment in the Alice programming environment in middle schools, we present a framework that formalizes a process where a hypothesis-driven approach informed by Evidence-Centered Design effectively complements data-driven learning analytics in interpreting students’ programming process and assessing CT in block-based programming environments. We apply the framework to the design of Alice tasks for high school CS to be used for measuring CT during programming.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {14},
numpages = {25},
keywords = {hypothesis-driven, evidence-centered design, data-driven, computational psychometrics, block-based programming environments, K-12 computer science education, Blended learning analytics}
}

@article{10.1145/3119909,
author = {Shen, Li-Yong and Goldman, Ron},
title = {Implicitizing Rational Tensor Product Surfaces Using the Resultant of Three Moving Planes},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3119909},
doi = {10.1145/3119909},
abstract = {Implicitizing rational surfaces is a fundamental computational task in Computer Graphics and Computer Aided Design. Ray tracing, collision detection, and solid modeling all benefit from implicitization procedures for rational surfaces. The univariate resultant of two moving lines generated by a μ-basis for a rational curve represents the implicit equation of the rational curve. But although the multivariate resultant of three moving planes corresponding to a μ-basis for a rational surface is guaranteed to contain the implicit equation of the surface as a factor, μ-bases for rational surfaces are difficult to compute. Moreover, μ-bases for a rational surface often have high degrees, so these resultants generally contain many extraneous factors. Here we develop fast algorithms to implicitize rational tensor product surfaces by computing the resultant of three moving planes corresponding to three syzygies with low degrees. These syzygies are easy to compute, and the resultants of the corresponding moving planes generally contain fewer extraneous factors than the resultants of the moving planes corresponding to μ-bases. We predict and compute all the possible extraneous factors that may appear in these resultants. Examples are provided to clarify and illuminate the theory.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {167},
numpages = {14},
keywords = {roots at infinity, moving plane, extraneous factor, base point, Implicitization}
}

@article{10.1145/3126533,
author = {Tr\"{u}b, Roman and Giannopoulou, Georgia and Tretter, Andreas and Thiele, Lothar},
title = {Implementation of Partitioned Mixed-Criticality Scheduling on a Multi-Core Platform},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3126533},
doi = {10.1145/3126533},
abstract = {Recent industrial trends favor the adoption of multi-core architectures for mixed-criticality applications. Although several mixed-criticality multi-core scheduling approaches have been proposed, currently there are few implementations on hardware that demonstrate efficient resource utilization and the ability to bound interference on shared resources. To address this necessity, we develop a mixed-criticality runtime environment on the Kalray MPPA-256 Andey many-core platform. The runtime environment implements a scheduling policy based on adaptive temporal partitioning. We develop models, methods and implementation principles to implement the necessary scheduling primitives, to achieve high platform utilization and to perform a compositional worst-case execution time analysis. The bounds account for scheduling overheads and for the inter-task interference on the platform’s shared memory. Using realistic benchmarks from avionics and signal processing, we validate the correctness and tightness of the bounds and demonstrate a high platform utilization.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {122},
numpages = {21},
keywords = {multi-core, mixed-criticality, adaptive temporal partitioning, MPPA-256}
}

@article{10.1145/3133560,
author = {Leech, Charles and Kumar, Charan and Acharyya, Amit and Yang, Sheng and Merrett, Geoff V. and Al-Hashimi, Bashir M.},
title = {Runtime Performance and Power Optimization of Parallel Disparity Estimation on Many-Core Platforms},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3133560},
doi = {10.1145/3133560},
abstract = {This article investigates the use of many-core systems to execute the disparity estimation algorithm, used in stereo vision applications, as these systems can provide flexibility between performance scaling and power consumption. We present a learning-based runtime management approach that achieves a required performance threshold while minimizing power consumption through dynamic control of frequency and core allocation. Experimental results are obtained from a 61-core Intel Xeon Phi platform for the aforementioned investigation. The same performance can be achieved with an average reduction in power consumption of 27.8\% and increased energy efficiency by 30.04\% when compared to Dynamic Voltage and Frequency Scaling control alone without runtime management.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
articleno = {41},
numpages = {19},
keywords = {power optimization, many-core platforms, computer vision, Runtime management}
}

@article{10.1145/3161885,
author = {Moreno, Sebastian and Neville, Jennifer and Kirshner, Sergey},
title = {Tied Kronecker Product Graph Models to Capture Variance in Network Populations},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3161885},
doi = {10.1145/3161885},
abstract = {Much of the past work on mining and modeling networks has focused on understanding the observed properties of single example graphs. However, in many real-life applications it is important to characterize the structure of populations of graphs. In this work, we analyze the distributional properties of probabilistic generative graph models (PGGMs) for network populations. PGGMs are statistical methods that model the network distribution and match common characteristics of real-world networks. Specifically, we show that most PGGMs cannot reflect the natural variability in graph properties observed across multiple networks because their edge generation process assumes independence among edges. Then, we propose the mixed Kronecker Product Graph Model (mKPGM), a scalable generalization of KPGMs that uses tied parameters to increase the variability of the sampled networks, while preserving the edge probabilities in expectation. We compare mKPGM to several other graph models. The results show that learned mKPGMs accurately represent the characteristics of real-world networks, while also effectively capturing the natural variability in network structure.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {35},
numpages = {40},
keywords = {social network analysis, kronecker product graph models, Graph generation models}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100\% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {vector angle--based evolutionary algorithm (VaEA), satisfiability (SAT) solvers, many-objective optimization, Optimal feature selection}
}

@article{10.1145/3195801,
author = {Wijesundera, Deshya and Prakash, Alok and Srikanthan, Thambipillai and Ihalage, Achintha},
title = {Framework for Rapid Performance Estimation of Embedded Soft Core Processors},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3195801},
doi = {10.1145/3195801},
abstract = {The large number of embedded soft core processors available today make it tedious and time consuming to select the best processor for a given application. This task is even more challenging due to the numerous configuration options available for a single soft core processor while optimizing for contradicting design requirements such as performance and area. In this article, we propose a generic framework for rapid performance estimation of applications on soft core processors. The proposed technique is scalable to the large number of configuration options available in modern soft core processors by relying on rapid and accurate estimation models instead of time-consuming FPGA synthesis and execution-based techniques. Experimental results on two leading commercial soft core processors executing applications from the widely used CHStone benchmark suite show an average error of less than 6\% while running in the order of minutes when compared to hours taken by synthesis-based techniques.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = jul,
articleno = {9},
numpages = {21},
keywords = {performance modeling and analysis, design methodologies, Soft processor}
}

@article{10.1145/3209882,
author = {Fan, Mingming and Truong, Khai N.},
title = {Guidelines for Creating Senior-Friendly Product Instructions},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3209882},
doi = {10.1145/3209882},
abstract = {Although older adults feel generally positive about technologies, many face difficulties when using them and need support during the process. One common form of support is the product instructions that come with devices. Unfortunately, when using them, older adults often feel confused, overwhelmed, or frustrated. In this work, we sought to address the issues that affect older adults’ ability to successfully complete tasks using product instructions. By observing how older adults used the product instructions of various devices and how they made modifications to simplify the use of the instructions, we identified 11 guidelines for creating senior-friendly product instructions. We validated the usability and effectiveness of the guidelines by evaluating how older adults used instruction manuals that were modified to adhere to these guidelines against the originals and those that were modified by interaction design researchers. Results show that, overall, participants had the highest task success rate and lowest task completion time when using guideline-modified user instructions. Participants also perceived these instructions to be the most helpful, the easiest to follow, the most complete, and the most concise among the three. We also compared the guidelines derived from this research to existing documentation guidelines and discussed potential challenges of applying them.},
journal = {ACM Trans. Access. Comput.},
month = jun,
articleno = {9},
numpages = {35},
keywords = {user-friendly, user-centered design, user manuals, technology support, seniors, senior-friendly, product instructions, older adults, instruction design, Guidelines}
}

@article{10.1145/3269981,
author = {Zhang, Irene and Sharma, Naveen Kr. and Szekeres, Adriana and Krishnamurthy, Arvind and Ports, Dan R. K.},
title = {Building Consistent Transactions with Inconsistent Replication},
year = {2018},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3269981},
doi = {10.1145/3269981},
abstract = {Application programmers increasingly prefer distributed storage systems with strong consistency and distributed transactions (e.g., Google’s Spanner) for their strong guarantees and ease of use. Unfortunately, existing transactional storage systems are expensive to use—in part, because they require costly replication protocols, like Paxos, for fault tolerance. In this article, we present a new approach that makes transactional storage systems more affordable: We eliminate consistency from the replication protocol, while still providing distributed transactions with strong consistency to applications.We present the Transactional Application Protocol for Inconsistent Replication (TAPIR), the first transaction protocol to use a novel replication protocol, called inconsistent replication, that provides fault tolerance without consistency. By enforcing strong consistency only in the transaction protocol, TAPIR can commit transactions in a single round-trip and order distributed transactions without centralized coordination. We demonstrate the use of TAPIR in a transactional key-value store, TAPIR-KV. Compared to conventional systems, TAPIR-KV provides better latency and better throughput.},
journal = {ACM Trans. Comput. Syst.},
month = dec,
articleno = {12},
numpages = {37},
keywords = {strict serializability, inconsistent replication, Distributed transactional storage}
}

@article{10.1145/3278720,
author = {Rodr\'{\i}guez, Ricardo J. and Tolosana-calasanz, Rafael and Rana, Omer F.},
title = {A Dynamic Data-throttling Approach to Minimize Workflow Imbalance},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3278720},
doi = {10.1145/3278720},
abstract = {Scientific workflows enable scientists to undertake analysis on large datasets and perform complex scientific simulations. These workflows are often mapped onto distributed and parallel computational infrastructures to speed up their executions. Prior to its execution, a workflow structure may suffer transformations to accommodate the computing infrastructures, normally involving task clustering and partitioning. However, these transformations may cause workflow imbalance because of the difference between execution task times (runtime imbalance) or because of unconsidered data dependencies that lead to data locality issues (data imbalance). In this article, to mitigate these imbalances, we enhance the workflow lifecycle process in use by introducing a workflow imbalance phase that quantifies workflow imbalance after the transformations. Our technique is based on structural analysis of Petri nets, obtained by model transformation of a data-intensive workflow, and Linear Programming techniques. Our analysis can be used to assist workflow practitioners in finding more efficient ways of transforming and scheduling their workflows. Moreover, based on our analysis, we also propose a technique to mitigate workflow imbalance by data throttling. Our approach is based on autonomic computing principles that determine how data transmission must be throttled throughout workflow jobs. Our autonomic data-throttling approach mainly monitors the execution of the workflow and recompute data-throttling values when certain watchpoints are reached and time derivation is observed. We validate our approach by a formal proof and by simulations along with the Montage workflow. Our findings show that a dynamic data-throttling approach is feasible, does not introduce a significant overhead, and minimizes the usage of input buffers and network bandwidth.},
journal = {ACM Trans. Internet Technol.},
month = may,
articleno = {32},
numpages = {21},
keywords = {optimization, linear programming, Scientific workflows, Petri nets}
}

@article{10.1145/3301413,
author = {Still, Jeremiah and Still, Mary},
title = {Influence of Visual Salience on Webpage Product Searches},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3301413},
doi = {10.1145/3301413},
abstract = {Visual salience can increase search efficiency in complex displays but does that influence persist when completing a specific search? In two experiments, participants were asked to search webpages for the prices of specific products. Those products were located near an area of high visual salience or low visual salience. In Experiment 1, participants were read the name of the product before searching; in Experiment 2, participants were shown an image of the exact product before searching. In both cases, participants completed their search more quickly in the high-salience condition. This was true even when there was no ambiguity about the visual characteristics of the product. Our findings suggest that salience guides users through complex displays under realistic, goal-driven task conditions. Designers can use this knowledge to create interfaces that are easier to search by aligning salience and task-critical elements.},
journal = {ACM Trans. Appl. Percept.},
month = feb,
articleno = {3},
numpages = {11},
keywords = {visual search, salience, eye movements, attention, Web design}
}

@article{10.1145/3306346.3323026,
author = {Preiner, Reinhold and Boubekeur, Tamy and Wimmer, Michael},
title = {Gaussian-product subdivision surfaces},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323026},
doi = {10.1145/3306346.3323026},
abstract = {Probabilistic distribution models like Gaussian mixtures have shown great potential for improving both the quality and speed of several geometric operators. This is largely due to their ability to model large fuzzy data using only a reduced set of atomic distributions, allowing for large compression rates at minimal information loss. We introduce a new surface model that utilizes these qualities of Gaussian mixtures for the definition and control of a parametric smooth surface. Our approach is based on an enriched mesh data structure, which describes the probability distribution of spatial surface locations around each vertex via a Gaussian covariance matrix. By incorporating this additional covariance information, we show how to define a smooth surface via a nonlinear probabilistic subdivision operator based on products of Gaussians, which is able to capture rich details at fixed control mesh resolution. This entails new applications in surface reconstruction, modeling, and geometric compression.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {35},
numpages = {11},
keywords = {triangulation, subdivision surfaces, gaussian mixtures, covariance mesh}
}

@article{10.1145/3319498,
author = {Izadpanah, Ramin and Allan, Benjamin A. and Dechev, Damian and Brandt, Jim},
title = {Production Application Performance Data Streaming for System Monitoring},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2376-3639},
url = {https://doi.org/10.1145/3319498},
doi = {10.1145/3319498},
abstract = {In this article, we present an approach to streaming collection of application performance data. Practical application performance tuning and troubleshooting in production high-performance computing (HPC) environments requires an understanding of how applications interact with the platform, including (but not limited to) parallel programming libraries such as Message Passing Interface (MPI). Several profiling and tracing tools exist that collect heavy runtime data traces either in memory (released only at application exit) or on a file system (imposing an I/O load that may interfere with the performance being measured). Although these approaches are beneficial in development stages and post-run analysis, a systemwide and low-overhead method is required to monitor deployed applications continuously. This method must be able to collect information at both the application and system levels to yield a complete performance picture.In our approach, an application profiler collects application event counters. A sampler uses an efficient inter-process communication method to periodically extract the application counters and stream them into an infrastructure for performance data collection. We implement a tool-set based on our approach and integrate it with the Lightweight Distributed Metric Service (LDMS) system, a monitoring system used on large-scale computational platforms. LDMS provides the infrastructure to create and gather streams of performance data in a low overhead manner. We demonstrate our approach using applications implemented with MPI, as it is one of the most common standards for the development of large-scale scientific applications.We utilize our tool-set to study the impact of our approach on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify patterns in the behavior of the application without source-level knowledge. We leverage LDMS to collect system-level performance data and explore the correlation between the system and application events. Also, we demonstrate how our tool-set can help detect anomalies with a low latency. We run tests on two different architectures: a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor. Our overhead study shows our method imposes at most 0.5\% CPU usage overhead on the application in realistic deployment scenarios.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = apr,
articleno = {8},
numpages = {25},
keywords = {performance data streaming, application profiling, Application and system monitoring}
}

@article{10.1145/3320277,
author = {Zhang, Mingyue and Wei, Xuan and Guo, Xunhua and Chen, Guoqing and Wei, Qiang},
title = {Identifying Complements and Substitutes of Products: A Neural Network Framework Based on Product Embedding},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320277},
doi = {10.1145/3320277},
abstract = {Complements and substitutes are two typical product relationships that deserve consideration in online product recommendation. One of the key objectives of recommender systems is to promote cross-selling, which heavily relies on recommending the appropriate type of products in specific scenarios. Research on consumer behavior has shown that consumers usually prefer substitutes in the browsing stage whereas complements in the purchasing stage. Thus, it is of great importance to identify the complementary and substitutable relationships between products. In this article, we design a neural network based framework that integrates the textual content and non-textual information of online reviews to mine product relationships. For the textual content, we utilize methods such as LDA topic modeling to represent products in a succinct form called “embedding.” To capture the semantics of complementary and substitutable relationships, we design a modeling process that transfers the product embeddings into semantic features and incorporates additional non-textual factors of product reviews. Extensive experiments are conducted to verify the effectiveness of the proposed product relationship mining model. The advantages and robustness of our model are discussed from various perspectives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {34},
numpages = {29},
keywords = {substitutes, product relationship, product recommendation, product embedding, online reviews, Complements}
}

@article{10.1145/3323917,
author = {Iturbe, Xabier and Venu, Balaji and Ozer, Emre and Poupat, Jean-Luc and Gimenez, Gregoire and Zurek, Hans-Ulrich},
title = {The Arm Triple Core Lock-Step (TCLS) Processor},
year = {2019},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0734-2071},
url = {https://doi.org/10.1145/3323917},
doi = {10.1145/3323917},
abstract = {The Arm Triple Core Lock-Step (TCLS) architecture is the natural evolution of Arm Cortex-R Dual Core Lock-Step (DCLS) processors to increase dependability, predictability, and availability in safety-critical and ultra-reliable applications. TCLS is simple, scalable, and easy to deploy in applications where Arm DCLS processors are widely used (e.g., automotive), as well as in new sectors where the presence of Arm technology is incipient (e.g., enterprise) or almost non-existent (e.g., space). Specifically in space, COTS Arm processors provide optimal power-to-performance, extensibility, evolvability, software availability, and ease of use, especially in comparison with the decades old rad-hard computing solutions that are still in use. This article discusses the fundamentals of an Arm Cortex-R5 based TCLS processor, providing key functioning and implementation details. The article shows that the TCLS architecture keeps the use of rad-hard technology to a minimum, namely, using rad-hard by design standard cell libraries only to protect the critical parts that account for less than 4\% of the entire TCLS solution. Moreover, when exposure to radiation is relatively low, such as in terrestrial applications or even satellites operating in Low Earth Orbits (LEO), the system could be implemented entirely using commercial cell libraries, relying on the radiation mitigation methods implemented on the TCLS to cope with sporadic soft errors in its critical parts. The TCLS solution allows thus to significantly reduce chip manufacturing costs and keep pace with advances in low power consumption and high density integration by leveraging commercial semiconductor processes, while matching the reliability levels and improving availability that can be achieved using extremely expensive rad-hard semiconductor processes. Finally, the article describes a TRL4 proof-of-concept TCLS-based System-on-Chip (SoC) that has been prototyped and tested to power the computer on-board an Airbus Defence and Space telecom satellite. When compared to the currently used processor solution by Airbus, the TCLS-based SoC results in a more than 5\texttimes{} performance increase and cuts power consumption by more than half.},
journal = {ACM Trans. Comput. Syst.},
month = jun,
articleno = {7},
numpages = {30},
keywords = {space avionics, soft error resilience, safety-critical, Arm}
}

@article{10.1145/3331151,
author = {Mostaeen, Golam and Roy, Banani and Roy, Chanchal and Schneider, Kevin},
title = {Designing for Real-Time Groupware Systems to Support Complex Scientific Data Analysis},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {EICS},
url = {https://doi.org/10.1145/3331151},
doi = {10.1145/3331151},
abstract = {Scientific Workflow Management Systems (SWfMSs) have become popular for accelerating the specification, execution, visualization, and monitoring of data-intensive scientific experiments. Unfortunately, to the best of our knowledge no existing SWfMSs directly support collaboration. Data is increasing in complexity, dimensionality, and volume, and the efficient analysis of data often goes beyond the realm of an individual and requires collaboration with multiple researchers from varying domains. In this paper, we propose a groupware system architecture for data analysis that in addition to supporting collaboration, also incorporates features from SWfMSs to support modern data analysis processes. As a proof of concept for the proposed architecture we developed SciWorCS - a groupware system for scientific data analysis. We present two real-world use-cases: collaborative software repository analysis and bioinformatics data analysis. The results of the experiments evaluating the proposed system are promising. Our bioinformatics user study demonstrates that SciWorCS can leverage real-world data analysis tasks by supporting real-time collaboration among users.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {9},
numpages = {28},
keywords = {workflow, scientific, data, collaboration, analysis}
}

@article{10.1145/3341980,
author = {Orth, Daniel and Thurgood, Clementine and Hoven, Elise Van Den},
title = {Designing Meaningful Products in the Digital Age: How Users Value Their Technological Possessions},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3341980},
doi = {10.1145/3341980},
abstract = {Devices such as phones, laptops and tablets have become central to the ways in which many people communicate with others, conduct business and spend their leisure time. This type of product uniquely contains both physical and digital components that affect how they are perceived and valued by users. This article investigates the nature of attachment in the context of technological possessions to better understand ways in which designers can create devices that are meaningful and kept for longer. Findings from our study of the self-reported associations and meaningfulness of technological possessions revealed that the digital contents of these possessions were often the primary source of meaning. Technological possessions were frequently perceived as systems of products rather than as singular devices. We identified several design opportunities for materialising the associations ascribed to the digital information contained within technological products to more meaningfully integrate their physical and digital components.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {34},
numpages = {28},
keywords = {technological products, qualitative methods, physical objects, digital media, devices, design, associations, Attachment}
}

@article{10.1145/3345640,
author = {Zhao, Ying and Wang, Lei and Li, Shijie and Zhou, Fangfang and Lin, Xiaoru and Lu, Qiang and Ren, Lei},
title = {A Visual Analysis Approach for Understanding Durability Test Data of Automotive Products},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3345640},
doi = {10.1145/3345640},
abstract = {People face data-rich manufacturing environments in Industry 4.0. As an important technology for explaining and understanding complex data, visual analytics has been increasingly introduced into industrial data analysis scenarios. With the durability test of automotive starters as background, this study proposes a visual analysis approach for understanding large-scale and long-term durability test data. Guided by detailed scenario and requirement analyses, we first propose a migration-adapted clustering algorithm that utilizes a segmentation strategy and a group of matching-updating operations to achieve an efficient and accurate clustering analysis of the data for starting mode identification and abnormal test detection. We then design and implement a visual analysis system that provides a set of user-friendly visual designs and lightweight interactions to help people gain data insights into the test process overview, test data patterns, and durability performance dynamics. Finally, we conduct a quantitative algorithm evaluation, case study, and user interview by using real-world starter durability test datasets. The results demonstrate the effectiveness of the approach and its possible inspiration for the durability test data analysis of other similar industrial products.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {70},
numpages = {23},
keywords = {visual analysis, smart manufacturing, durability test, automotive starter, Industry 4.0}
}

@article{10.1145/3351228,
author = {Adaimi, Rebecca and Thomaz, Edison},
title = {Leveraging Active Learning and Conditional Mutual Information to Minimize Data Annotation in Human Activity Recognition},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351228},
doi = {10.1145/3351228},
abstract = {A difficulty in human activity recognition (HAR) with wearable sensors is the acquisition of large amounts of annotated data for training models using supervised learning approaches. While collecting raw sensor data has been made easier with advances in mobile sensing and computing, the process of data annotation remains a time-consuming and onerous process. This paper explores active learning as a way to minimize the labor-intensive task of labeling data. We train models with active learning in both offline and online settings with data from 4 publicly available activity recognition datasets and show that it performs comparably to or better than supervised methods while using around 10\% of the training data. Moreover, we introduce a method based on conditional mutual information for determining when to stop the active learning process while maximizing recognition performance. This is an important issue that arises in practice when applying active learning to unlabeled datasets.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {70},
numpages = {23},
keywords = {Stopping Criterion, Human Activity Recognition, Data Annotation, Conditional Mutual Information, Active Learning}
}

@article{10.1145/3358212,
author = {Vashist, Abhishek and Keats, Andrew and Dinakarrao, Sai Manoj Pudukotai and Ganguly, Amlan},
title = {Unified Testing and Security Framework for Wireless Network-on-Chip Enabled Multi-Core Chips},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358212},
doi = {10.1145/3358212},
abstract = {On-chip wireless interconnects have been demonstrated to improve the performance and energy consumption of data communication in Network-on-Chips (NoCs). However, the wireless interfaces (WIs) can be defective, rendering these broken links severely affect the performance. This makes manufacturing test of the WIs critical. While analog testing of the transceivers is possible, such methodologies are impractical in a Wireless NoC (WiNoC) due to large overheads. In addition to testing, security is another prominent challenge in WiNoCs, as the security breach can happen due to embedded hardware Trojans or through external attacker exploiting the wireless medium. The typical security measures used in general wireless networks are not practical in a WiNoC due to unique network architectures and performance requirements of such a system. However, both testing and security defense can potentially leverage a basic monitoring framework which, can detect malfunctions or anomalies. Based on this idea, we propose a unified architecture for testing and attack detection and protection of on-chip wireless interconnects. We adopt a Built-In-Self Test (BIST) methodology to enable online monitoring of the wireless interconnects which can also be reused for monitoring the security threats. We focus on manufacturing defects of the WIs for testing and persistent jamming attack for the security measures, as this kind of attack is most likely on wireless communication systems. The BIST methodology is capable of detecting faults in the wireless links with a low aliasing probability of 2.32\texttimes{} 10−10. Additionally, the proposed unified architecture is able to detect the persistent jamming with an accuracy of 99.87\% and suffer &lt; 3\% communication bandwidth degradation even in the presence of attacks from either internal or external sources.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {75},
numpages = {20},
keywords = {wireless interconnect, security, machine learning, jamming, Network-on-chip, DoS}
}

@article{10.1145/3359313,
author = {Wang, Dakuo and Weisz, Justin D. and Muller, Michael and Ram, Parikshit and Geyer, Werner and Dugan, Casey and Tausczik, Yla and Samulowitz, Horst and Gray, Alexander},
title = {Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359313},
doi = {10.1145/3359313},
abstract = {The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {211},
numpages = {24},
keywords = {machine learning, human-in-the-loop ai, human-centered ai, human-ai collaboration, future of work, domain experts, data scientist, data science, automl, automation, autoai, ai design ai}
}

@article{10.1145/3361126,
author = {Cho, Bryden and Sun, Chengzheng and Ng, Agustina},
title = {Issues and Experiences in Building Heterogeneous Co-Editing Systems},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {GROUP},
url = {https://doi.org/10.1145/3361126},
doi = {10.1145/3361126},
abstract = {Most past research efforts in co-editing focused on homogeneous co-editing, which allows multiple users to use the same editor to edit shared documents in the same session, and nearly all real-world co-editors, such as Google Docs, are homogeneous co-editors. In this work, we explore issues and solutions in building heterogeneous co-editing systems, which allow multiple users to use different editors to edit shared documents in the same session. To drive our exploration, we built a prototype heterogeneous co-editing system, named CoVim+CoEmacs, which allows multiple users to use full functionalities and UI features of two comprehensive and rivalry text editors, Vim and Emacs, in the same co-editing session. In this paper, we focus on technical issues in designing and implementing heterogeneous co-editors in general and CoVim+CoEmacs in particular. We have motivated this work by potential usage benefits of heterogeneous co-editing systems and used working scenarios under the CoVim+CoEmacs prototype to illustrate some novel usages and inner workings of such systems, but left systematic user studies on heterogeneous co-editing to future work. We hope the insights and experiences drawn from this work can not only contribute to advancing state-of-the-art collaborative system design and implementation, but also provide inspiration to future heterogeneous collaborative application system designers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {245},
numpages = {28},
keywords = {transparent adaptation, operation transformation, computer-supported cooperative work, collaborative editing}
}

@article{10.1145/3361738,
author = {Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Croft, W. Bruce},
title = {Explainable Product Search with a Dynamic Relation Embedding Model},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361738},
doi = {10.1145/3361738},
abstract = {Product search is one of the most popular methods for customers to discover products online. Most existing studies on product search focus on developing effective retrieval models that rank items by their likelihood to be purchased. However, they ignore the problem that there is a gap between how systems and customers perceive the relevance of items. Without explanations, users may not understand why product search engines retrieve certain items for them, which consequentially leads to imperfect user experience and suboptimal system performance in practice. In this work, we tackle this problem by constructing explainable retrieval models for product search. Specifically, we propose to model the “search and purchase” behavior as a dynamic relation between users and items, and create a dynamic knowledge graph based on both the multi-relational product data and the context of the search session. Ranking is conducted based on the relationship between users and items in the latent space, and explanations are generated with logic inferences and entity soft matching on the knowledge graph. Empirical experiments show that our model, which we refer to as the Dynamic Relation Embedding Model (DREM), significantly outperforms the state-of-the-art baselines and has the ability to produce reasonable explanations for search results.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {relation embedding, knowledge graph, explainable model, Product search}
}

@article{10.1145/3363785,
author = {Selva, Manuel and Gruber, Fabian and Sampaio, Diogo and Guillon, Christophe and Pouchet, Louis-No\"{e}l and Rastello, Fabrice},
title = {Building a Polyhedral Representation from an Instrumented Execution: Making Dynamic Analyses of Nonaffine Programs Scalable},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3363785},
doi = {10.1145/3363785},
abstract = {The polyhedral model has been successfully used in production compilers. Nevertheless, only a very restricted class of applications can benefit from it. Recent proposals investigated how runtime information could be used to apply polyhedral optimization on applications that do not statically fit the model. In this work, we go one step further in that direction. We propose the folding-based analysis that, from the output of an instrumented program execution, builds a compact polyhedral representation. It is able to accurately detect affine dependencies, fixed-stride memory accesses, and induction variables in programs. It scales to real-life applications, which often include some nonaffine dependencies and accesses in otherwise affine code. This is enabled by a safe fine-grained polyhedral overapproximation mechanism. We evaluate our analysis on the entire Rodinia benchmark suite, enabling accurate feedback about the potential for complex polyhedral transformations.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {45},
numpages = {26},
keywords = {polyhedral model, loop transformations, instrumentation, dynamic dependency graph, compiler optimization, binary, Performance feedback}
}

@article{10.1145/3365005,
author = {Pandurangan, Gopal and Robinson, Peter and Scquizzato, Michele},
title = {A Time- and Message-Optimal Distributed Algorithm for Minimum Spanning Trees},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1549-6325},
url = {https://doi.org/10.1145/3365005},
doi = {10.1145/3365005},
abstract = {This article presents a randomized (Las Vegas) distributed algorithm that constructs a minimum spanning tree (MST) in weighted networks with optimal (up to polylogarithmic factors) time and message complexity. This algorithm runs in \~{O}(D + √ n) time and exchanges \~{O}(m) messages (both with high probability), where n is the number of nodes of the network, D is the hop-diameter, and m is the number of edges. This is the first distributed MST algorithm that matches simultaneously the time lower bound of Ω˜(D + √ n) [10] and the message lower bound of Ω (m) [31], which both apply to randomized Monte Carlo algorithms.The prior time and message lower bounds are derived using two completely different graph constructions; the existing lower-bound construction that shows one lower bound does not work for the other. To complement our algorithm, we present a new lower-bound graph construction for which any distributed MST algorithm requires both Ω˜(D + √ n) rounds and Ω (m) messages.},
journal = {ACM Trans. Algorithms},
month = nov,
articleno = {13},
numpages = {27},
keywords = {minimum spanning trees, Distributed algorithms}
}

@article{10.1145/3365006,
author = {Asathulla, Mudabir Kabir and Khanna, Sanjeev and Lahn, Nathaniel and Raghvendra, Sharath},
title = {A Faster Algorithm for Minimum-cost Bipartite Perfect Matching in Planar Graphs},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1549-6325},
url = {https://doi.org/10.1145/3365006},
doi = {10.1145/3365006},
abstract = {Given a weighted planar bipartite graph G(A ∪ B, E) where each edge has an integer edge cost, we give an \~{O}(n4/3log nC) time algorithm to compute minimum-cost perfect matching; here C is the maximum edge cost in the graph. The previous best-known planarity exploiting algorithm has a running time of O(n3/2log n) and is achieved by using planar separators (Lipton and Tarjan ’80).Our algorithm is based on the bit-scaling paradigm (Gabow and Tarjan ’89). For each scale, our algorithm first executes O(n1/3) iterations of Gabow and Tarjan’s algorithm in O(n4/3) time leaving only O(n2/3) vertices unmatched. Next, it constructs a compressed residual graph H with O(n2/3) vertices and O(n) edges. This is achieved by using an r-division of the planar graph G with r=n2/3. For each partition of the r-division, there is an edge between two vertices of H if and only if they are connected by a directed path inside the partition. Using existing efficient shortest-path data structures, the remaining O(n2/3) vertices are matched by iteratively computing a minimum-cost augmenting path, each taking \~{O}(n2/3) time. Augmentation changes the residual graph, so the algorithm updates the compressed representation for each partition affected by the change in \~{O}(n2/3) time. We bound the total number of affected partitions over all the augmenting paths by O(n2/3 log n). Therefore, the total time taken by the algorithm is \~{O}(n4/3).},
journal = {ACM Trans. Algorithms},
month = nov,
articleno = {2},
numpages = {30},
keywords = {scaling algorithms, primal-dual, Minimum-cost matching}
}

@article{10.1145/3365538,
author = {Kang, Yin and Zhou, Lina},
title = {Helpfulness Assessment of Online Reviews: The Role of Semantic Hierarchy of Product Features},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3365538},
doi = {10.1145/3365538},
abstract = {Effective use of online consumer reviews is hampered by uncertainty about their helpfulness. Despite a growing body of knowledge on indicators of review helpfulness, previous studies have overlooked rich semantic information embedded in review content. Following design science principles, this study introduces a semantic hierarchy of product features by probing the review text. Using the hierarchical framework as a guide, we develop a research model of review helpfulness assessment. In the model, we propose and conceptualize three new factors—breadth, depth, and redundancy, by building on and/or extending product uncertainty, information quality, signaling, and encoding variability theories. The model-testing results lend strong support to the proposed effects of those factors on review helpfulness. They also reveal interesting differences in the effects of redundancy and readability between different types of products. This study embodies knowledge moments of multiple genres of inquiry in design science research, which have multifold research and practical implications.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = nov,
articleno = {12},
numpages = {18},
keywords = {semantic hierarchy, redundancy, product feature, design science, depth, breadth, Review helpfulness}
}

@article{10.1145/3368618,
author = {Betcke, Timo and Scroggs, Matthew W. and undefinedmigaj, Wojciech},
title = {Product Algebras for Galerkin Discretisations of Boundary Integral Operators and their Applications},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3368618},
doi = {10.1145/3368618},
abstract = {Operator products occur naturally in a range of regularised boundary integral equation formulations. However, while a Galerkin discretisation only depends on the domain space and the test (or dual) space of the operator, products require a notion of the range. In the boundary element software package Bempp, we have implemented a complete operator algebra that depends on knowledge of the domain, range, and test space. The aim was to develop a way of working with Galerkin operators in boundary element software that is as close to working with the strong form on paper as possible, while hiding the complexities of Galerkin discretisations. In this article, we demonstrate the implementation of this operator algebra and show, using various Laplace and Helmholtz example problems, how it significantly simplifies the definition and solution of a wide range of typical boundary integral equation problems.},
journal = {ACM Trans. Math. Softw.},
month = mar,
articleno = {4},
numpages = {22},
keywords = {operator preconditioning, boundary element software, Boundary integral equations}
}

@article{10.1145/3384473,
author = {Ni, Li and Luo, Wenjian and Lu, Nannan and Zhu, Wenjie},
title = {Mining the Local Dependency Itemset in a Products Network},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3384473},
doi = {10.1145/3384473},
abstract = {Many studies have been conducted on market basket analysis such as association rules and dependent patterns. These studies mainly focus on mining all significant patterns or patterns directly associated with a given item in a dataset. The problem that has not been addressed is how to mine patterns associated with a given item from the local view. This problem becomes very meaningful when the market basket dataset is huge. To address this problem, in this study, first, a new idea called “local dependency itemset” is put forward, which refers to patterns associated with the given item. Second, a framework of mining the local dependency itemset is presented. The framework has two steps, which are executed iteratively. One is expanding the local dependency itemset that initially consists of only the given item; the other is updating the local products network. Third, this framework is implemented by three different dependence indicators and a typical local community detection algorithm. The experimental results confirm that the local dependency itemset is meaningful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = apr,
articleno = {3},
numpages = {31},
keywords = {products network, local dependency itemset, local community detection, Data mining}
}

@article{10.1145/3392877,
author = {Whiting, Mark E. and Gao, Irena and Xing, Michelle and Diarrassouba, N'godjigui Junior and Nguyen, Tonya and Bernstein, Michael S.},
title = {Parallel Worlds: Repeated Initializations of the Same Team to Improve Team Viability},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392877},
doi = {10.1145/3392877},
abstract = {A team's early interactions are influential: small behaviors cascade, driving the team either toward successful collaboration or toward fracture. Would a team be more viable if it could undo initial interactional missteps and try again? We introduce a technique that supports online and remote teams in creating multiple parallel worlds: the same team meets many times, led to believe that each convening is with a new team due to pseudonym masking while actual membership remains static. Afterward, the team moves forward with the parallel world with the highest viability by using the same pseudonyms and conversation history from that instance. In two experiments, we find that this technique improves team viability: teams that are reconvened from the highest-viability parallel world are significantly more viable than the same group meeting in a new parallel world. Our work suggests parallel worlds can help teams start off on the right foot - and stay there.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {67},
numpages = {22},
keywords = {team viability, team intervention, team dynamics, online teams}
}

@article{10.1145/3394113,
author = {Bai, Ruirui and Wang, Zhongqing and Kong, Fang and Li, Shoushan and Zhou, Guodong},
title = {Neural Co-training for Sentiment Classification with Product Attributes},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3394113},
doi = {10.1145/3394113},
abstract = {Sentiment classification aims to detect polarity from a piece of text. The polarity is usually positive or negative, and the text genre is usually product review. The challenges of sentiment classification are that it is hard to capture semantic of reviews, and the labeled data is hard to annotate. Therefore, we propose neural co-training to learn the semantic representation of each review using the neural network model, and learn the information from unlabeled data using a co-training framework. In particular, we use the attention-based bi-directional Gated Recurrent Unit (Att-BiGRU) to model the semantic content of each review and regard different categories of the target product as different views. We then use a co-training framework to learn and predict the unlabeled reviews with different views. Experiment results with the Yelp dataset demonstrate the effectiveness of our approach.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {74},
numpages = {17},
keywords = {product attributes, co-training, Semi-supervised sentiment classification}
}

@article{10.1145/3397518,
author = {Emiris, Ioannis Z. and Psarros, Ioannis},
title = {Products of Euclidean Metrics, Applied to Proximity Problems among Curves: Unified Treatment of Discrete Fr\'{e}chet and Dynamic Time Warping Distances},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/3397518},
doi = {10.1145/3397518},
abstract = {Approximate Nearest Neighbor (ANN) search is a fundamental computational problem that has benefited from significant progress in the past couple of decades. However, most work has been devoted to pointsets, whereas complex shapes have not been sufficiently addressed. Here, we focus on distance functions between discretized curves in Euclidean space: They appear in a wide range of applications, from road segments and molecular backbones to time-series in general dimension. For ℓp-products of Euclidean metrics, for any constant p, we propose simple and efficient data structures for ANN based on randomized projections: These data structures are of independent interest. Furthermore, they serve to solve proximity questions under a notion of distance between discretized curves, which generalizes both discrete Fr\'{e}chet and Dynamic Time Warping distance functions. These are two very popular and practical approaches to comparing such curves. We offer, for both approaches, the first data structures and query algorithms for ANN with arbitrarily good approximation factor, at the expense of increasing space usage and preprocessing time over existing methods. Query time complexity is comparable or significantly improved by our methods; our algorithm is especially efficient when the length of the curves is bounded. Finally, we focus on discrete Fr\'{e}chet distance when the ambient space is high dimensional and derive complexity bounds in terms of doubling dimension as well as an improved approximate near neighbor search.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jun,
articleno = {27},
numpages = {20},
keywords = {polygonal curves, dynamic time warping, Fr\'{e}chet distance, Approximate nearest neighbor}
}

@article{10.1145/3416126,
author = {Usui, Toshinori and Otsuki, Yuto and Ikuse, Tomonori and Kawakoya, Yuhei and Iwamura, Makoto and Miyoshi, Jun and Matsuura, Kanta},
title = {Automatic Reverse Engineering of Script Engine Binaries for Building Script API Tracers},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3416126},
doi = {10.1145/3416126},
abstract = {Script languages are designed to be easy-to-use and require low learning costs. These features provide attackers options to choose a script language for developing their malicious scripts. This diversity of choice in the attacker side unexpectedly imposes a significant cost on the preparation for analysis tools in the defense side. That is, we have to prepare for multiple script languages to analyze malicious scripts written in them. We call this unbalanced cost for script languages asymmetry problem.To solve this problem, we propose a method for automatically detecting the hook and tap points in a script engine binary that is essential for building a script Application Programming Interface (API) tracer. Our method allows us to reduce the cost of reverse engineering of a script engine binary, which is the largest portion of the development of a script API tracer, and build a script API tracer for a script language with minimum manual intervention. This advantage results in solving the asymmetry problem. The experimental results showed that our method generated the script API tracers for the three script languages popular among attackers (Visual Basic for Applications (VBA), Microsoft Visual Basic Scripting Edition (VBScript), and PowerShell). The results also demonstrated that these script API tracers successfully analyzed real-world malicious scripts.},
journal = {Digital Threats},
month = jan,
articleno = {5},
numpages = {31},
keywords = {reverse engineering, function enhancement, dynamic analysis, Malicious script}
}

@article{10.1145/3418686,
author = {Ardagna, Claudio A. and Asal, Rasool and Damiani, Ernesto and Ioini, Nabil El and Elahi, Mehdi and Pahl, Claus},
title = {From Trustworthy Data to Trustworthy IoT: A Data Collection Methodology Based on Blockchain},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3418686},
doi = {10.1145/3418686},
abstract = {Internet of Things (IoT) is composed of physical devices, communication networks, and services provided by edge systems and over-the-top applications. IoT connects billions of devices that collect data from the physical environment, which are pre-processed at the edge and then forwarded to processing services at the core of the infrastructure, on top of which cloud-based applications are built and provided to mobile end users. IoT comes with important advantages in terms of applications and added value for its users, making their world smarter and simpler. These advantages, however, are mitigated by the difficulty of guaranteeing IoT trustworthiness, which is still in its infancy. IoT trustworthiness is a must especially in critical domains (e.g., health, transportation) where humans become new components of an IoT system and their life is put at risk by system malfunctioning or breaches. In this article, we put forward the idea that trust in IoT can be boosted if and only if its automation and adaptation processes are based on trustworthy data. We therefore depart from a scenario that considers the quality of a single decision as the main goal of an IoT system and consider the trustworthiness of collected data as a fundamental requirement at the basis of a trustworthy IoT environment. We therefore define a methodology for data collection that filters untrusted data out according to trust rules evaluating the status of the devices collecting data and the collected data themselves. Our approach is based on blockchain and smart contracts and collects data whose trustworthiness and integrity are proven over time. The methodology balances trustworthiness and privacy and is experimentally evaluated in real-world and simulated scenarios using Hyperledger fabric blockchain.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = dec,
articleno = {11},
numpages = {26},
keywords = {trustworthiness, Internet of Things, Blockchain}
}

@article{10.1145/3419634,
author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n},
title = {A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419634},
doi = {10.1145/3419634},
abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {131},
numpages = {59},
keywords = {cloud computing in IoT, cloud IoT services, big data 2.0, V’s challenges for IoT big data, IoT big data survey, IoT big data}
}

@article{10.1145/3425398,
author = {Sanchez, Susan M.},
title = {Data Farming: Methods for the Present, Opportunities for the Future},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3425398},
doi = {10.1145/3425398},
abstract = {Data farming is a descriptive metaphor that captures the notion of generating data purposefully to maximize the information “yield” from simulation models. Large-scale designed experiments let us grow the simulation output efficiently and effectively. We can explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. Data farming has been used in the defense community over the past two decades, and has resulted in quantum leaps in the breadth, depth, and timeliness of the insights yielded by simulation models. In this article, we provide an overview of current data farming capabilities and their relationship to emerging techniques in data science and analytics. We use graphics to motivate insight into some of the benefits of a data farming approach. Finally, we share some thoughts about opportunities and challenges for further improving the state of the art, and transforming the state of the practice, in the data farming domain.},
journal = {ACM Trans. Model. Comput. Simul.},
month = nov,
articleno = {22},
numpages = {30},
keywords = {metamodeling, design of experiments, data mining, Simulation}
}

@article{10.1145/3428263,
author = {Grosser, Tobias and Theodoridis, Theodoros and Falkenstein, Maximilian and Pitchanathan, Arjun and Kruse, Michael and Rigger, Manuel and Su, Zhendong and Hoefler, Torsten},
title = {Fast linear programming through transprecision computing on small and sparse data},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428263},
doi = {10.1145/3428263},
abstract = {A plethora of program analysis and optimization techniques rely on linear programming at their heart. However, such techniques are often considered too slow for production use. While today’s best solvers are optimized for complex problems with thousands of dimensions, linear programming, as used in compilers, is typically applied to small and seemingly trivial problems, but to many instances in a single compilation run. As a result, compilers do not benefit from decades of research on optimizing large-scale linear programming. We design a simplex solver targeted at compilers. A novel theory of transprecision computation applied from individual elements to full data-structures provides the computational foundation. By carefully combining it with optimized representations for small and sparse matrices and specialized small-coefficient algorithms, we (1) reduce memory traffic, (2) exploit wide vectors, and (3) use low-precision arithmetic units effectively. We evaluate our work by embedding our solver into a state-of-the-art integer set library and implement one essential operation, coalescing, on top of our transprecision solver. Our evaluation shows more than an order-of-magnitude speedup on the core simplex pivot operation and a mean speedup of 3.2x (vs. GMP) and 4.6x (vs. IMath) for the optimized coalescing operation. Our results demonstrate that our optimizations exploit the wide SIMD instructions of modern microarchitectures effectively. We expect our work to provide foundations for a future integer set library that uses transprecision arithmetic to accelerate compiler analyses.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {195},
numpages = {28},
keywords = {Transprecision, Simplex, Presburger Arithmetic, Linear Programming}
}

@article{10.1145/3433543,
author = {Tian, Hui and Peng, Fang and Quan, Hanyu and Chang, Chin-Chen},
title = {Identity-Based Public Auditing for Cloud Storage of Internet-of-Vehicles Data},
year = {2023},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3433543},
doi = {10.1145/3433543},
abstract = {The Internet of Vehicles (IoV), with the help of cloud computing, can provide rich and powerful application services for vehicles and drivers by sharing and analysing various IoV data. However, how to ensure the integrity of IoV data with multiple sources and diversity outsourced in the cloud is still an open challenge. To address this concern, this paper first presents an identity-based public auditing scheme for cloud storage of IoV data, which can fully achieve the essential function and security requirements, such as classified auditing, multi-source auditing and privacy protection. Particularly, we design a new authenticated data structure, called data mapping table, to track the distribution of each type of IoV data to ensure fine and rapid audits. Moreover, our scheme can reduce the overheads for both the key management and the generation of block tags. We formally prove the security of the presented scheme and evaluate its performance by comprehensive comparisons with the state-of-the-art schemes designed for traditional scenarios. The theoretical analyses and experimental results demonstrate that our scheme can securely and efficiently realize public auditing for IoV data, and outperforms the previous ones in both the computation and communication overheads in most cases.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {88},
numpages = {24},
keywords = {data mapping table, cloud storage, internet of vehicles, data integrity, Public auditing}
}

@article{10.1145/3440016,
author = {Thakker, Urmish and Fedorov, Igor and Zhou, Chu and Gope, Dibakar and Mattina, Matthew and Dasika, Ganesh and Beu, Jesse},
title = {Compressing RNNs to Kilobyte Budget for IoT Devices Using Kronecker Products},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1550-4832},
url = {https://doi.org/10.1145/3440016},
doi = {10.1145/3440016},
abstract = {Micro-controllers (MCUs) make up most of the processors in the world with widespread applicability from automobile to medical devices. The Internet of Things promises to enable these resource-constrained MCUs with machine learning algorithms to provide always-on intelligence. Many Internet of Things applications consume time-series data that are naturally suitable for recurrent neural networks (RNNs) like LSTMs and GRUs. However, RNNs can be large and difficult to deploy on these devices, as they have few kilobytes of memory. As a result, there is a need for compression techniques that can significantly compress RNNs without negatively impacting task accuracy. This article introduces a method to compress RNNs for resource-constrained environments using the Kronecker product (KP). KPs can compress RNN layers by 16\texttimes{} to 38\texttimes{} with minimal accuracy loss. By quantizing the resulting models to 8 bits, we further push the compression factor to 50\texttimes{}. We compare KP with other state-of-the-art compression techniques across seven benchmarks spanning five different applications and show that KP can beat the task accuracy achieved by other techniques by a large margin while simultaneously improving the inference runtime. Sometimes the KP compression mechanism can introduce an accuracy loss. We develop a hybrid KP approach to mitigate this. Our hybrid KP algorithm provides fine-grained control over the compression ratio, enabling us to regain accuracy lost during compression by adding a small number of model parameters.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jul,
articleno = {46},
numpages = {18},
keywords = {IoT, model compression, Kronecker products, matrix decomposition, micro-controllers, Neural networks}
}

@article{10.1145/3440887,
author = {Xie, Hong and Zhong, Mingze and Li, Yongkun and Lui, John C. S.},
title = {Understanding Persuasion Cascades in Online Product Rating Systems: Modeling, Analysis, and Inference},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3440887},
doi = {10.1145/3440887},
abstract = {Online product rating systems have become an indispensable component for numerous web services such as Amazon, eBay, Google Play Store, and TripAdvisor. One functionality of such systems is to uncover the product quality via product ratings (or reviews) contributed by consumers. However, a well-known psychological phenomenon called “message-based persuasion” lead to “biased” product ratings in a cascading manner (we call this the persuasion cascade). This article investigates: 
(1) How does the persuasion cascade influence the product quality estimation accuracy? (2) Given a real-world product rating dataset, how to infer the persuasion cascade and analyze it to draw practical insights?
We first develop a mathematical model to capture key factors of a persuasion cascade. We formulate a high-order Markov chain to characterize the opinion dynamics of a persuasion cascade and prove the convergence of opinions. We further bound the product quality estimation error for a class of rating aggregation rules including the averaging scoring rule, via the matrix perturbation theory and the Chernoff bound. We also design a maximum likelihood algorithm to infer parameters of the persuasion cascade. We conduct experiments on both synthetic data and real-world data from Amazon and TripAdvisor. Experiment results show that our inference algorithm has a high accuracy. Furthermore, persuasion cascades notably exist, but the average scoring rule has a small product quality estimation error under practical scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {36},
numpages = {29},
keywords = {product quality estimation, high order Markov chain, persuasion cascades, Online rating systems}
}

@article{10.1145/3441451,
author = {Xia, Peike and Jiang, Wenjun and Wu, Jie and Xiao, Surong and Wang, Guojun},
title = {Exploiting Temporal Dynamics in Product Reviews for Dynamic Sentiment Prediction at the Aspect Level},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441451},
doi = {10.1145/3441451},
abstract = {Online reviews and ratings play an important role in shaping the purchase decisions of customers in e-commerce. Many researches have been done to make proper recommendations for users, by exploiting reviews, ratings, user profiles, or behaviors. However, the dynamic evolution of user preferences and item properties haven’t been fully exploited. Moreover, it lacks fine-grained studies at the aspect level. To address the above issues, we define two concepts of user maturity and item popularity, to better explore the dynamic changes for users and items. We strive to exploit fine-grained information at the aspect level and the evolution of users and items, for dynamic sentiment prediction. First, we analyze three real datasets from both the overall level and the aspect level, to discover the dynamic changes (i.e., gradual changes and sudden changes) in user aspect preferences and item aspect properties. Next, we propose a novel model of Aspect-based Sentiment Dynamic Prediction (ASDP), to dynamically capture and exploit the change patterns with uniform time intervals. We further propose the improved model ASDP+ with a bin segmentation algorithm to set the time intervals non-uniformly based on the sudden changes. Experimental results on three real-world datasets show that our work leads to significant improvements.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {68},
numpages = {29},
keywords = {temporal dynamics, sentiment prediction, opinion evolution, aspect level, Review mining}
}

@article{10.1145/3446429,
author = {Baharev, Ali and Schichl, Hermann and Neumaier, Arnold and Achterberg, Tobias},
title = {An Exact Method for the Minimum Feedback Arc Set Problem},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
issn = {1084-6654},
url = {https://doi.org/10.1145/3446429},
doi = {10.1145/3446429},
abstract = {A feedback arc set of a directed graph G is a subset of its arcs containing at least one arc of every cycle in G. Finding a feedback arc set of minimum cardinality is an NP-hard problem called the minimum feedback arc set problem. Numerically, the minimum set cover formulation of the minimum feedback arc set problem is appropriate as long as all simple cycles in G can be enumerated. Unfortunately, even those sparse graphs that are important for practical applications often have Ω (2n) simple cycles. Here we address precisely such situations: An exact method is proposed for sparse graphs that enumerates simple cycles in a lazy fashion and iteratively extends an incomplete cycle matrix. In all cases encountered so far, only a tractable number of cycles has to be enumerated until a minimum feedback arc set is found. The practical limits of the new method are evaluated on a test set containing computationally challenging sparse graphs, relevant for industrial applications. The 4,468 test graphs are of varying size and density and suitable for testing the scalability of exact algorithms over a wide range.},
journal = {ACM J. Exp. Algorithmics},
month = apr,
articleno = {1.4},
numpages = {28},
keywords = {tearing, minimum feedback vertex set, minimum feedback arc set, maximum acyclic subgraph, Linear ordering problem}
}

@article{10.1145/3449222,
author = {Azenkot, Shiri and Hanley, Margot J. and Baker, Catherine M.},
title = {How Accessibility Practitioners Promote the Creation of Accessible Products in Large Companies},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449222},
doi = {10.1145/3449222},
abstract = {Although some technology companies have made significant strides towards the accessibility of their products, most consumer-facing technology products still pose access barriers to people with disabilities. Prior research has established that accessibility expertise is limited to a small number of practitioners in companies, but we do not know how these practitioners can affect change across a large organization. We sought to address this gap and understand how large companies that produce consumer-facing technologies integrate accessibility into their product lifecycle. We conducted semi-structured interviews with 30 accessible technology practitioners working at 13 companies. We found accessibility expertise was centered in three main roles within the company: on a central accessibility team, in champions, and in accessibility teams embedded into large product teams. Much of the work of these practitioners centered around education and development of tools and resources to allow designers and developers throughout the organization to implement accessibility. Our study revealed current practices for embedding accessibility in large companies, highlighting the gap between accessibility research and practice. We conclude by presenting areas that need future research to understand how to better support accessibility practice.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {148},
numpages = {27},
keywords = {accessibility, design practice, translational science gap}
}

@article{10.1145/3453171,
author = {Rahman, Mohammad Saidur and Khalil, Ibrahim and Yi, Xun and Atiquzzaman, Mohammed and Bertino, Elisa},
title = {A Lossless Data-Hiding based IoT Data Authenticity Model in Edge-AI for Connected Living},
year = {2021},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3453171},
doi = {10.1145/3453171},
abstract = {Edge computing is an emerging technology for the acquisition of Internet-of-Things (IoT) data and provisioning different services in connected living. Artificial Intelligence (AI) powered edge devices (edge-AI) facilitate intelligent IoT data acquisition and services through data analytics. However, data in edge networks are prone to several security threats such as external and internal attacks and transmission errors. Attackers can inject false data during data acquisition or modify stored data in the edge data storage to hamper data analytics. Therefore, an edge-AI device must verify the authenticity of IoT data before using them in data analytics. This article presents an IoT data authenticity model in edge-AI for a connected living using data hiding techniques. Our proposed data authenticity model securely hides the data source’s identification number within IoT data before sending it to edge devices. Edge-AI devices extract hidden information for verifying data authenticity. Existing data hiding approaches for biosignal cannot reconstruct original IoT data after extracting the hidden message from it (i.e., lossy) and are not usable for IoT data authenticity. We propose the first lossless IoT data hiding technique in this article based on error-correcting codes (ECCs). We conduct several experiments to demonstrate the performance of our proposed method. Experimental results establish the lossless property of the proposed approach while maintaining other data hiding properties.},
journal = {ACM Trans. Internet Technol.},
month = dec,
articleno = {57},
numpages = {25},
keywords = {connected living, reversible IoT steganography, lossless data hiding, data authenticity, Edge-AI}
}

@article{10.1145/3453184,
author = {Werner, Martin},
title = {GloBiMapsAI: An AI-Enhanced Probabilistic Data Structure for Global Raster Datasets},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/3453184},
doi = {10.1145/3453184},
abstract = {In the last decade, more and more spatial data has been acquired on a global scale due to satellite missions, social media, and coordinated governmental activities. This observational data suffers from huge storage footprints and makes global analysis challenging. Therefore, many information products have been designed in which observations are turned into global maps showing features such as land cover or land use, often with only a few discrete values and sparse spatial coverage like only within cities. Traditional coding of such data as a raster image becomes challenging due to the sizes of the datasets and spatially non-local access patterns, for example, when labeling social media streams. This article proposes GloBiMap, a randomized data structure, based on Bloom filters, for modeling low-cardinality sparse raster images of excessive sizes in a configurable amount of memory with pure random access operations avoiding costly intermediate decompression. In addition, the data structure is designed to correct the inevitable errors of the randomized layer in order to have a fully exact representation. We show the feasibility of the approach on several real-world datasets including the Global Urban Footprint in which each pixel denotes whether a particular location contains a building at a resolution of roughly 10m globally as well as on a global Twitter sample of more than 220 million precisely geolocated tweets. In addition, we propose the integration of a denoiser engine based on artificial intelligence in order to reduce the amount of error correction information for extremely compressive GloBiMaps.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jun,
articleno = {18},
numpages = {24},
keywords = {machine learning, geographic information systems, randomized data structures, data sparsity and compression, Image representation}
}

@article{10.1145/3460890,
author = {Geissmann, Barbara and Gianinazzi, Lukas},
title = {Parallel Minimum Cuts in Near-linear Work and Low Depth},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/3460890},
doi = {10.1145/3460890},
abstract = {We present the first near-linear work and poly-logarithmic depth algorithm for computing a minimum cut in an undirected graph. Previous parallel algorithms with poly-logarithmic depth required at least quadratic work in the number of vertices.In a graph with n vertices and m edges, our randomized algorithm computes the minimum cut with high probability in O(m log4 n) work and O(log3 n) depth. This result is obtained by parallelizing a data structure that aggregates weights along paths in a tree, in addition exploiting the connection between minimum cuts and approximate maximum packings of spanning trees.In addition, our algorithm improves upon bounds on the number of cache misses incurred to compute a minimum cut.},
journal = {ACM Trans. Parallel Comput.},
month = aug,
articleno = {8},
numpages = {20},
keywords = {parallel algorithms, minimum path data structure, graph algorithms, cache-oblivious algorithms, Minimum cut}
}

@article{10.1145/3460948,
author = {Lutz, Neil},
title = {Fractal Intersections and Products via Algorithmic Dimension},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1942-3454},
url = {https://doi.org/10.1145/3460948},
doi = {10.1145/3460948},
abstract = {Algorithmic fractal dimensions quantify the algorithmic information density of individual points and may be defined in terms of Kolmogorov complexity. This work uses these dimensions to bound the classical Hausdorff and packing dimensions of intersections and Cartesian products of fractals in Euclidean spaces. This approach shows that two prominent, fundamental results about the dimension of Borel or analytic sets also hold for arbitrary sets.},
journal = {ACM Trans. Comput. Theory},
month = aug,
articleno = {14},
numpages = {15},
keywords = {fractal geometry, Kolmogorov complexity, Effective dimension}
}

@article{10.1145/3461015,
author = {Fugini, Mariagrazia and Finocchi, Jacopo},
title = {Data and Process Quality Evaluation in a Textual Big Data Archiving System},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3461015},
doi = {10.1145/3461015},
abstract = {The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.},
journal = {J. Comput. Cult. Herit.},
month = mar,
articleno = {2},
numpages = {19},
keywords = {content management, data quality, machine learning, text analytics, unstructured Big Data, Big Data analytics}
}

@article{10.1145/3462333,
author = {Mainardi, Nicholas and Barenghi, Alessandro and Pelosi, Gerardo},
title = {Privacy-aware Character Pattern Matching over Outsourced Encrypted Data},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3462333},
doi = {10.1145/3462333},
abstract = {Providing a method to efficiently search into outsourced encrypted data, without forsaking strong privacy guarantees, is a pressing concern rising from the separation of data ownership and data management typical of cloud-based applications. While several existing solutions allow a client to look up the occurrences of a substring in an outsourced document collection, the practical application requirements in terms of privacy and efficiency call for the improvement of such solutions. In this work, we present a privacy-preserving substring search protocol with a polylogarithmic communication cost and a limited computational effort on the server side. The proposed protocol provides search pattern and access pattern privacy, for both exact string search and character-pattern search with wildcards. Its extension to a multi-user setting shows significant savings in terms of outsourced storage w.r.t. a baseline solution where the whole dataset is replicated. The performance figures of an optimized implementation of our protocol, searching into a remotely stored genomic dataset, validate the practicality of the approach exhibiting a data transfer of less than 50 kiB to execute a query over a document of 40 MiB, with execution times on client and server in the range of a few seconds and a few minutes, respectively.},
journal = {Digital Threats},
month = oct,
articleno = {7},
numpages = {38},
keywords = {privacy-preserving protocol, homomorphic encryption, cryptography, Secure substring search}
}

@article{10.1145/3470566,
author = {Sun, Xiaoming and Woodruff, David P. and Yang, Guang and Zhang, Jialin},
title = {Querying a Matrix through Matrix-Vector Products},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1549-6325},
url = {https://doi.org/10.1145/3470566},
doi = {10.1145/3470566},
abstract = {We consider algorithms with access to an unknown matrix M ε F n\texttimes{}d via matrix-vector products, namely, the algorithm chooses vectors v1, ⃛ , vq, and observes Mv1, ⃛ , Mvq. Here the vi can be randomized as well as chosen adaptively as a function of Mv1, ⃛ , Mvi-1. Motivated by applications of sketching in distributed computation, linear algebra, and streaming models, as well as connections to areas such as communication complexity and property testing, we initiate the study of the number q of queries needed to solve various fundamental problems. We study problems in three broad categories, including linear algebra, statistics problems, and graph problems. For example, we consider the number of queries required to approximate the rank, trace, maximum eigenvalue, and norms of a matrix M; to compute the AND/OR/Parity of each column or row of M, to decide whether there are identical columns or rows in M or whether M is symmetric, diagonal, or unitary; or to compute whether a graph defined by M is connected or triangle-free. We also show separations for algorithms that are allowed to obtain matrix-vector products only by querying vectors on the right, versus algorithms that can query vectors on both the left and the right. We also show separations depending on the underlying field the matrix-vector product occurs in. For graph problems, we show separations depending on the form of the matrix (bipartite adjacency versus signed edge-vertex incidence matrix) to represent the graph.Surprisingly, very few works discuss this fundamental model, and we believe a thorough investigation of problems in this model would be beneficial to a number of different application areas.},
journal = {ACM Trans. Algorithms},
month = oct,
articleno = {31},
numpages = {19},
keywords = {sketching, linear algebra, Communication complexity}
}

@article{10.1145/3476046,
author = {Karizat, Nadia and Delmonaco, Dan and Eslami, Motahhare and Andalibi, Nazanin},
title = {Algorithmic Folk Theories and Identity: How TikTok Users Co-Produce Knowledge of Identity and Engage in Algorithmic Resistance},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476046},
doi = {10.1145/3476046},
abstract = {Algorithms in online platforms interact with users' identities in different ways. However, little is known about how users understand the interplay between identity and algorithmic processes on these platforms, and if and how such understandings shape their behavior on these platforms in return. Through semi-structured interviews with 15 US-based TikTok users, we detail users' algorithmic folk theories of the For You Page algorithm in relation to two inter-connected identity types: person and social identity. Participants identified potential harms that can accompany algorithms' tailoring content to their person identities. Further, they believed the algorithm actively suppresses content related to marginalized social identities based on race and ethnicity, body size and physical appearance, ability status, class status, LGBTQ identity, and political and social justice group affiliation. We propose a new algorithmic folk theory of social feeds-The Identity Strainer Theory-to describe when users believe an algorithm filters out and suppresses certain social identities. In developing this theory, we introduce the concept of algorithmic privilege as held by users positioned to benefit from algorithms on the basis of their identities. We further propose the concept of algorithmic representational harm to refer to the harm users experience when they lack algorithmic privilege and are subjected to algorithmic symbolic annihilation. Additionally, we describe how participants changed their behaviors to shape their algorithmic identities to align with how they understood themselves, as well as to resist the suppression of marginalized social identities and lack of algorithmic privilege via individual actions, collective actions, and altering their performances. We theorize our findings to detail the ways the platform's algorithm and its users co-produce knowledge of identity on the platform. We argue the relationship between users' algorithmic folk theories and identity are consequential for social media platforms, as it impacts users' experiences, behaviors, sense of belonging, and perceived ability to be seen, heard, and feel valued by others as mediated through algorithmic systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {305},
numpages = {44},
keywords = {algorithm, algorithmic identity, algorithmic privilege, algorithmic representational harm, algorithmic resistance, algorithmic symbolic annihilation, co-production, folk theories, identity, identity strainer theory, marginalization, marginalized identity, social media}
}

@article{10.1145/3477054,
author = {Papaphilippou, Philippos and Meng, Jiuxi and Gebara, Nadeen and Luk, Wayne},
title = {Hipernetch: High-Performance FPGA Network Switch},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3477054},
doi = {10.1145/3477054},
abstract = {We present Hipernetch, a novel FPGA-based design for performing high-bandwidth network switching. FPGAs have recently become more popular in data centers due to their promising capabilities for a wide range of applications. With the recent surge in transceiver bandwidth, they could further benefit the implementation and refinement of network switches used in data centers. Hipernetch replaces the crossbar with a “combined parallel round-robin arbiter”. Unlike a crossbar, the combined parallel round-robin arbiter is easy to pipeline, and does not require centralised iterative scheduling algorithms that try to fit too many steps in a single or a few FPGA cycles. The result is a network switch implementation on FPGAs operating at a high frequency and with a low port-to-port latency. Our proposed Hipernetch architecture additionally provides a competitive switching performance approaching output-queued crossbar switches. Our implemented Hipernetch designs exhibit a throughput that exceeds 100 Gbps per port for switches of up to 16 ports, reaching an aggregate throughput of around 1.7 Tbps.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = nov,
articleno = {3},
numpages = {31},
keywords = {stream processing, sorting network applications, scheduling algorithms, arbiter, round-robin, FPGA, Network switch}
}

@article{10.1145/3477929,
author = {Agarwal, Anshul and Ramamritham, Krithi},
title = {A Novel Approach for Deploying Minimum Sensors in Smart Buildings},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3477929},
doi = {10.1145/3477929},
abstract = {Buildings, viewed as cyber-physical systems, become smart by deploying Building Management Systems (BMS). They should be aware about the state and environment of the building. This is achieved by developing a sensing system that senses different interesting factors of the building, called as “facets of sensing.” Depending on the application, different facets need to be sensed at various locations. Existing approaches for sensing these facets consist of deploying sensors at all the places so they can be sensed directly. But installing numerous sensors often aggravate the issues of user inconvenience, cost of installation and maintenance, and generation of e-waste. This article proposes how intelligently using the existing information can help to estimate the facets in cyber-physical systems like buildings, thereby reducing the sensors to be deployed. In this article, an optimization framework has been developed, which optimally deploys sensors in a building such that it satisfies BMS requirements with minimum number of sensors. The proposed solution is applied to real-world scenarios with cyber-physical systems. The results indicate that the proposed optimization framework is able to reduce the number of sensors by 59\% and 49\% when compared to the baseline and heuristic approach, respectively.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = nov,
articleno = {2},
numpages = {29},
keywords = {smart buildings, building management system (BMS), Sensor placement}
}

@article{10.1145/3478289,
author = {Wang, Zhaoguo and Chen, Haibo and Wang, Youyun and Tang, Chuzhe and Wang, Huan},
title = {The Concurrent Learned Indexes for Multicore Data Storage},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/3478289},
doi = {10.1145/3478289},
abstract = {We present XIndex, which is a concurrent index library and designed for fast queries. It includes a concurrent ordered index (XIndex-R) and a concurrent hash index (XIndex-H). Similar to a recent proposal of the learned index, the indexes in XIndex use learned models to optimize index efficiency. Compared with the learned index, for the ordered index, XIndex-R is able to handle concurrent writes effectively and adapts its structure according to runtime workload characteristics. For the hash index, XIndex-H is able to avoid the resize operation blocking concurrent writes. Furthermore, the indexes in XIndex can index string keys much more efficiently than the learned index. We demonstrate the advantages of XIndex with YCSB, TPC-C (KV), which is a TPC-C-inspired benchmark for key-value stores, and micro-benchmarks. Compared with ordered indexes of Masstree and Wormhole, XIndex-R achieves up to 3.2\texttimes{} and 4.4\texttimes{} performance improvement on a 24-core machine. Compared with hash indexes of Intel TBB HashMap, XIndex-H achieves up to 3.1\texttimes{} speedup. The performance further improves by 91\% after adding the optimizations on indexing string keys. The library is open-sourced.1},
journal = {ACM Trans. Storage},
month = jan,
articleno = {8},
numpages = {35},
keywords = {indexing, concurrent algorithms, Learned indexes}
}

@article{10.1145/3485503,
author = {Malewski, Stefan and Greenberg, Michael and Tanter, \'{E}ric},
title = {Gradually structured data},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485503},
doi = {10.1145/3485503},
abstract = {Dynamically-typed languages offer easy interaction with ad hoc data such as JSON and S-expressions; statically-typed languages offer powerful tools for working with structured data, notably algebraic datatypes, which are a core feature of typed languages both functional and otherwise. Gradual typing aims to reconcile dynamic and static typing smoothly. The gradual typing literature has extensively focused on the computational aspect of types, such as type safety, effects, noninterference, or parametricity, but the application of graduality to data structuring mechanisms has been much less explored. While row polymorphism and set-theoretic types have been studied in the context of gradual typing, algebraic datatypes in particular have not, which is surprising considering their wide use in practice. We develop, formalize, and prototype a novel approach to gradually structured data with algebraic datatypes. Gradually structured data bridges the gap between traditional algebraic datatypes and flexible data management mechanisms such as tagged data in dynamic languages, or polymorphic variants in OCaml. We illustrate the key ideas of gradual algebraic datatypes through the evolution of a small server application from dynamic to progressively more static checking, formalize a core functional language with gradually structured data, and establish its metatheory, including the gradual guarantees.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {126},
numpages = {29},
keywords = {semi-structured data, gradual typing, algebraic datatypes}
}

@article{10.1145/3488058,
author = {Gan, Wensheng and Chen, Guoting and Yin, Hongzhi and Fournier-Viger, Philippe and Chen, Chien-Ming and Yu, Philip S.},
title = {Towards Revenue Maximization with Popular and Profitable Products},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3488058},
doi = {10.1145/3488058},
abstract = {Economic-wise, a common goal for companies conducting marketing is to maximize the return revenue/profit by utilizing the various effective marketing strategies. Consumer behavior is crucially important in economy and targeted marketing, in which behavioral economics can provide valuable insights to identify the biases and profit from customers. Finding credible and reliable information on products’ profitability is, however, quite difficult since most products tend to peak at certain times w.r.t. seasonal sales cycles in a year. On-Shelf Availability (OSA) plays a key factor for performance evaluation. Besides, staying ahead of hot product trends means we can increase marketing efforts without selling out the inventory. To fulfill this gap, in this paper, we first propose a general profit-oriented framework to address the problem of revenue maximization based on economic behavior, and compute the On-shelf Popular and most Profitable Products (OPPPs) for the targeted marketing. To tackle the revenue maximization problem, we model the k-satisfiable product concept and propose an algorithmic framework for searching OPPP and its variants. Extensive experiments are conducted on several real-world datasets to evaluate the effectiveness and efficiency of the proposed algorithm.},
journal = {ACM/IMS Trans. Data Sci.},
month = may,
articleno = {42},
numpages = {21},
keywords = {revenue maximization, on-shelf availability, consumer behavior, Economy}
}

@article{10.1145/3490396,
author = {Miksa, Tomasz and Oblasser, Simon and Rauber, Andreas},
title = {Automating Research Data Management Using Machine-Actionable Data Management Plans},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3490396},
doi = {10.1145/3490396},
abstract = {Many research funders mandate researchers to create and maintain data management plans (DMPs) for research projects that describe how research data is managed to ensure its reusability. A DMP, being a static textual document, is difficult to act upon and can quickly become obsolete and impractical to maintain. A new generation of machine-actionable DMPs (maDMPs) was therefore proposed by the Research Data Alliance to enable automated integration of information and updates. maDMPs open up a variety of use cases enabling interoperability of research systems and automation of data management tasks.In this article, we describe a system for machine-actionable data management planning in an institutional context. We identify common use cases within research that can be automated to benefit from machine-actionability of DMPs. We propose a reference architecture of an maDMP support system that can be embedded into an institutional research data management infrastructure. The system semi-automates creation and maintenance of DMPs, and thus eases the burden for the stakeholders responsible for various DMP elements. We evaluate the proposed system in a case study conducted at the largest technical university in Austria and quantify to what extent the DMP templates provided by the European Commission and a national funding body can be pre-filled. The proof-of-concept implementation shows that maDMP workflows can be semi-automated, thus workload on involved parties can be reduced and quality of information increased. The results are especially relevant to decision makers and infrastructure operators who want to design information systems in a systematic way that can utilize the full potential of maDMPs.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {22},
keywords = {FAIR, RDA, RDM, automation, requirements engineering, funder template, enterprise architecture, business processes, machine-actionable, Data management plan}
}

@article{10.1145/3494566,
author = {Sharma, Ms Promila and Meena, Uma and Sharma, Girish Kumar},
title = {Intelligent Data Analysis using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3494566},
doi = {10.1145/3494566},
abstract = {Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {94},
numpages = {20},
keywords = {tourism industry, Intelligent data}
}

@article{10.1145/3495254,
author = {Liao, Jianwei and Li, Jun and Zhao, Mingwang and Sha, Zhibing and Cai, Zhigang},
title = {Read Refresh Scheduling and Data Reallocation against Read Disturb in SSDs},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3495254},
doi = {10.1145/3495254},
abstract = {Read disturb is a circuit-level noise in flash-based Solid-State Drives (SSDs), induced by intensive read requests, which may result in unexpected read errors. The approach of read refresh (RR) is commonly adopted to mitigate its negative effects by unconditionally migrating all valid data pages in the RR block to another new block. However, routine RR operations greatly impact the I/O responsiveness of SSDs, because the processing on normal I/O requests must be blocked at the same time. To further reduce the negative effects of read refresh, this article proposes a read refresh scheduling and data reallocation method to deal with two primary issues with respect to an RR operation, including where to place data pages and when to trigger page migrations. Specifically, we first construct a data reallocation model to match the data pages in the RR block and the destination blocks for addressing the issue of where to place the data. The model considers not only the read hotness of pages in the RR block, but also the accumulated read counts of the destination blocks. Moreover, for addressing the issue of when to trigger data migrations, we build a timing decision model to determine the time points for completing page migrations by considering the factors of the intensity of I/Os and the disturb situation on the RR block. Through a series of simulation experiments based on several realistic disk traces, we illustrate that the proposed RR scheduling and data reallocation mechanism can noticeably reduce the read errors by more than 10.3\%, on average, and the long-tail latency by between 43.9\% and 64.0\%
at the 99.99th percentile, in contrast to state-of-the-art methods.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = feb,
articleno = {18},
numpages = {27},
keywords = {reliability, modeling, read refresh, read disturb, Solid-state drivers}
}

@article{10.1145/3498332,
author = {Pokhriyal, Neeti and Valentino, Benjamin and Vosoughi, Soroush},
title = {An Interpretable Model for Real-time Tracking of Economic Indicators Using Social Media Data},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3498332},
doi = {10.1145/3498332},
abstract = {Measures of public opinion on economic matters are vital in creating accurate official statistics needed to design appropriate policy interventions and shape private investment decisions. This is traditionally done using public opinion polls and representative surveys. However, this process is time and money intensive and currently suffers from reduced public participation. As a result, official statistics are usually delayed and are not frequently available at the resolution needed for better policy interventions. Hence, researchers have looked into anonymized digital data to continuously sense information about public behaviors, especially those related to consumer sentiment index and unemployment insurance claims. However, past studies relied on linear models with simplistic assumptions and thus provided limited extrapolatory power and no insights as to why these predictive methods work. Worryingly, the strong correlations reported in these studies disappeared when the original models were tested with newer social media data.We propose a novel interpretable machine learning model, called Group Additive Gaussian Processes, to provide accurate and near real-time estimates of economic indicators about public behaviors using social media data, along with insights into the model behavior. Our model exploits the underlying structure in data and encodes interpretability in the modeling framework. It is based on Gaussian Process regression, which provides a robust non-parametric Bayesian learning framework that produces calibrated uncertainty measures along with its predictions. A key challenge in the learning task is learning from limited training data. We demonstrate how our model not only learns but also generalizes well in these scenarios. Through extensive evaluation we show how our model performs on two important indicators of economic health-consumer confidence index and unemployment insurance claims data. Further, we demonstrate how our model can reduce the need to conduct surveys by producing highly accurate and frequent estimates in between the surveying periods.},
journal = {ACM/IMS Trans. Data Sci.},
month = mar,
articleno = {36},
numpages = {32},
keywords = {Uncertainty estimations, Bayesian modeling, interpretability, Google purchase history data, Reddit data, Unemployment Insurance Claims, Consumer sentiment index}
}

@article{10.1145/3498664,
author = {Lim, Jay P. and Nagarakatte, Santosh},
title = {One polynomial approximation to produce correctly rounded results of an elementary function for multiple representations and rounding modes},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498664},
doi = {10.1145/3498664},
abstract = {Mainstream math libraries for floating point (FP) do not produce correctly rounded results for all inputs. In contrast, CR-LIBM and RLIBM provide correctly rounded implementations for a specific FP representation with one rounding mode. Using such libraries for a representation with a new rounding mode or with different precision will result in wrong results due to double rounding. This paper proposes a novel method to generate a single polynomial approximation that produces correctly rounded results for all inputs for multiple rounding modes and multiple precision configurations. To generate a correctly rounded library for n-bits, our key idea is to generate a polynomial approximation for a representation with n+2-bits using the round-to-odd mode. We prove that the resulting polynomial approximation will produce correctly rounded results for all five rounding modes in the standard and for multiple representations with k-bits such that |E| +1 &lt; k ≤ n, where |E| is the number of exponent bits in the representation. Similar to our prior work in the RLIBM project, we approximate the correctly rounded result when we generate the library with n+2-bits using the round-to-odd mode. We also generate polynomial approximations by structuring it as a linear programming problem but propose enhancements to polynomial generation to handle the round-to-odd mode. Our prototype is the first 32-bit float library that produces correctly rounded results with all rounding modes in the IEEE standard for all inputs with a single polynomial approximation. It also produces correctly rounded results for any FP configuration ranging from 10-bits to 32-bits while also being faster than mainstream libraries.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {3},
numpages = {28},
keywords = {round-to-odd, floating point, correctly rounded math libraries}
}

@article{10.1145/3503465,
author = {Roorda, Esther and Rasoulinezhad, Seyedramin and Leong, Philip H. W. and Wilton, Steven J. E.},
title = {FPGA Architecture Exploration for DNN Acceleration},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3503465},
doi = {10.1145/3503465},
abstract = {Recent years have seen an explosion of machine learning applications implemented on Field-Programmable Gate Arrays (FPGAs). FPGA vendors and researchers have responded by updating their fabrics to more efficiently implement machine learning accelerators, including innovations such as enhanced Digital Signal Processing (DSP) blocks and hardened systolic arrays. Evaluating architectural proposals is difficult, however, due to the lack of publicly available benchmark circuits.This paper addresses this problem by presenting an open-source benchmark circuit generator that creates realistic DNN-oriented circuits for use in FPGA architecture studies. Unlike previous generators, which create circuits that are agnostic of the underlying FPGA, our circuits explicitly instantiate embedded blocks, allowing for meaningful comparison of recent architectural proposals without the need for a complete inference computer-aided design (CAD) flow. Our circuits are compatible with the VTR CAD suite, allowing for architecture studies that investigate routing congestion and other low-level architectural implications.In addition to addressing the lack of machine learning benchmark circuits, the architecture exploration flow that we propose allows for a more comprehensive evaluation of FPGA architectures than traditional static benchmark suites. We demonstrate this through three case studies which illustrate how realistic benchmark circuits can be generated to target different heterogeneous FPGAs.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = may,
articleno = {33},
numpages = {37},
keywords = {hardware acceleration, benchmarking, neural networks, FPGA architecture}
}

@article{10.1145/3505637,
author = {Higgott, Oscar},
title = {PyMatching: A Python Package for Decoding Quantum Codes with Minimum-Weight Perfect Matching},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3505637},
doi = {10.1145/3505637},
abstract = {This article introduces PyMatching, a fast open-source Python package for decoding quantum error-correcting codes with the minimum-weight perfect matching (MWPM) algorithm. PyMatching includes the standard MWPM decoder as well as a variant, which we call local matching, that restricts each syndrome defect to be matched to another defect within a local neighborhood. The decoding performance of local matching is almost identical to that of the standard MWPM decoder in practice, while reducing the computational complexity. We benchmark the performance of PyMatching, showing that local matching is several orders of magnitude faster than implementations of the full MWPM algorithm using NetworkX or Blossom V for problem sizes typically considered in error correction simulations. PyMatching and its dependencies are open-source, and it can be used to decode any quantum code for which syndrome defects come in pairs using a simple Python interface. PyMatching supports the use of weighted edges, hook errors, boundaries and measurement errors, enabling fast decoding, and simulation of fault-tolerant quantum computing.},
journal = {ACM Transactions on Quantum Computing},
month = jun,
articleno = {16},
numpages = {16},
keywords = {surface code quantum computing, Quantum error correction}
}

@article{10.1145/3506704,
author = {Kumar, Rakesh and Alipour, Mehdi and Black-Schaffer, David},
title = {Dependence-aware Slice Execution to Boost MLP in Slice-out-of-order Cores},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3506704},
doi = {10.1145/3506704},
abstract = {Exploiting memory-level parallelism (MLP) is crucial to hide long memory and last-level cache access latencies. While out-of-order (OoO) cores, and techniques building on them, are effective at exploiting MLP, they deliver poor energy efficiency due to their complex and energy-hungry hardware. This work revisits slice-out-of-order (sOoO) cores as an energy-efficient alternative for MLP exploitation. sOoO cores achieve energy efficiency by constructing and executing slices of MLP-generating instructions out-of-order only with respect to the rest of instructions; the slices and the remaining instructions, by themselves, execute in-order. However, we observe that existing sOoO cores miss significant MLP opportunities due to their dependence-oblivious in-order slice execution, which causes dependent slices to frequently block MLP generation. To boost MLP generation, we introduce Freeway, a sOoO core based on a new dependence-aware slice execution policy that tracks dependent slices and keeps them from blocking subsequent independent slices and MLP extraction. The proposed core incurs minimal area and power overheads, yet approaches the MLP benefits of fully OoO cores. Our evaluation shows that Freeway delivers 12\% better performance than the state-of-the-art sOoO core and is within 7\% of the MLP limits of full OoO execution.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {25},
numpages = {28},
keywords = {instruction scheduling, memory level parallelism, Microarchitecture}
}

@article{10.1145/3507904,
author = {Alam, Md Mahbub and Torgo, Luis and Bifet, Albert},
title = {A Survey on Spatio-temporal Data Analytics Systems},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3507904},
doi = {10.1145/3507904},
abstract = {Due to the surge of spatio-temporal data volume, the popularity of location-based services and applications, and the importance of extracted knowledge from spatio-temporal data to solve a wide range of real-world problems, a plethora of research and development work has been done in the area of spatial and spatio-temporal data analytics in the past decade. The main goal of existing works was to develop algorithms and technologies to capture, store, manage, analyze, and visualize spatial or spatio-temporal data. The researchers have contributed either by adding spatio-temporal support with existing systems, by developing a new system from scratch, or by implementing algorithms for processing spatio-temporal data. The existing ecosystem of spatial and spatio-temporal data analytics systems can be categorized into three groups, (1) spatial databases (SQL and NoSQL), (2) big spatial data processing infrastructures, and (3) programming languages and GIS software. Since existing surveys mostly investigated infrastructures for processing big spatial data, this survey has explored the whole ecosystem of spatial and spatio-temporal analytics. This survey also portrays the importance and future of spatial and spatio-temporal data analytics.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {219},
numpages = {38},
keywords = {spatial stream, trajectory, spatio-temporal, spatial, spatial libraries, GIS software, big spatial infrastructures, Spatial databases}
}

@article{10.1145/3508027,
author = {Khadirsharbiyani, Soheil and Kotra, Jagadish and Rao, Karthik and Kandemir, Mahmut},
title = {Data Convection: A GPU-Driven Case Study for Thermal-Aware Data Placement in 3D DRAMs},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3508027},
doi = {10.1145/3508027},
abstract = {Stacked DRAMs have been studied, evaluated in multiple scenarios, and even productized in the last decade. The large available bandwidth they offer make them an attractive choice, particularly, in high-performance computing (HPC) environments. Consequently, many prior research efforts have studied and evaluated 3D stacked DRAM-based designs. Despite offering high bandwidth, stacked DRAMs are severely constrained by the overall memory capacity offered. In this paper, we study and evaluate integrating stacked DRAM on top of a GPU in a 3D manner which in tandem with the 2.5D stacked DRAM increases the capacity and the bandwidth without increasing the package size. This integration of 3D stacked DRAMs aids in satisfying the capacity requirements of emerging workloads like deep learning. Though this vertical 3D integration of stacked DRAMs also increases the total available bandwidth, we observe that the bandwidth offered by these 3D stacked DRAMs is severely limited by the heat generated on the GPU. Based on our experiments on a cycle-level simulator, we make a key observation that the sections of the 3D stacked DRAM that are closer to the GPU have lower retention-times compared to the farther layers of stacked DRAM. This thermal-induced variable retention-times causes certain sections of 3D stacked DRAM to be refreshed more frequently compared to the others, thereby resulting in thermal-induced NUMA paradigms. To alleviate such thermal-induced NUMA behavior, we propose and experimentally evaluate three different incarnations of Data Convection, i.e., Intra-layer, Inter-layer, and Intra + Inter-layer, that aim at placing the most-frequently accessed data in a thermal-induced retention-aware fashion, taking into account both bank-level and channel-level parallelism. Our evaluations on a cycle-level GPU simulator indicate that, in a multi-application scenario, our Intra-layer, Inter-layer and Intra + Inter-layer algorithms improve the overall performance by 1.8\%, 11.7\%, and 14.4\%, respectively, over a baseline that already encompasses 3D+2.5D stacked DRAMs.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = feb,
articleno = {7},
numpages = {25},
keywords = {thermal issues, gpu, data placement, 3d stacked dram}
}

@article{10.1145/3510026,
author = {Pan, Hongyi and Badawi, Diaa and Cetin, Ahmet Enis},
title = {Block Walsh–Hadamard Transform-based Binary Layers in Deep Neural Networks},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3510026},
doi = {10.1145/3510026},
abstract = {Convolution has been the core operation of modern deep neural networks. It is well known that convolutions can be implemented in the Fourier Transform domain. In this article, we propose to use binary block Walsh–Hadamard transform (WHT) instead of the Fourier transform. We use WHT-based binary layers to replace some of the regular convolution layers in deep neural networks. We utilize both one-dimensional (1D) and 2D binary WHTs in this article. In both 1D and 2D layers, we compute the binary WHT of the input feature map and denoise the WHT domain coefficients using a nonlinearity that is obtained by combining soft-thresholding with the tanh function. After denoising, we compute the inverse WHT. We use 1D-WHT to replace the 1 \texttimes{} 1 convolutional layers, and 2D-WHT layers can replace the 3 \texttimes{} 3 convolution layers and Squeeze-and-Excite layers. 2D-WHT layers with trainable weights can be also inserted before the Global Average Pooling layers to assist the dense layers. In this way, we can reduce the number of trainable parameters significantly with a slight decrease in trainable parameters. In this article, we implement the WHT layers into MobileNet-V2, MobileNet-V3-Large, and ResNet to reduce the number of parameters significantly with negligible accuracy loss. Moreover, according to our speed test, the 2D-FWHT layer runs about 24 times as fast as the regular 3 \texttimes{} 3 convolution with 19.51\% less RAM usage in an NVIDIA Jetson Nano experiment.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {69},
numpages = {25},
keywords = {image classification, smooth-thresholding, block division, Fast Walsh–Hadamard transform}
}

@article{10.1145/3510853,
author = {Volkel, Kevin and Tomek, Kyle J. and Keung, Albert J. and Tuck, James M.},
title = {DINOS: Data INspired Oligo Synthesis for DNA Data Storage},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1550-4832},
url = {https://doi.org/10.1145/3510853},
doi = {10.1145/3510853},
abstract = {As interest in DNA-based information storage grows, the costs of synthesis have been identified as a key bottleneck. A potential direction is to tune synthesis for data. Data strands tend to be composed of a small set of recurring code word sequences, and they contain longer sequences of repeated data. To exploit these properties, we propose a new framework called DINOS. DINOS consists of three key parts: (i) The first is a hierarchical strand assembly algorithm, inspired by gene assembly techniques that can assemble arbitrary data strands from a small set of primitive blocks. (ii) The assembly algorithm relies on our novel formulation for how to construct primitive blocks, spanning a variety of useful configurations from a set of code words and overhangs. Each primitive block is a code word flanked by a pair of overhangs that are created by a cyclic pairing process that keeps the number of primitive blocks small. Using these primitive blocks, any data strand of arbitrary length can be assembled, theoretically. We show a minimal system for a binary code with as few as six primitive blocks, and we generalize our processes to support an arbitrary set of overhangs and code words. (iii) We exploit our hierarchical assembly approach to identify redundant sequences and coalesce the reactions that create them to make assembly more efficient.We evaluate DINOS and describe its key characteristics. For example, the number of reactions needed to make a strand can be reduced by increasing the number of overhangs or the number of code words, but increasing the number of overhangs offers a small advantage over increasing code words while requiring substantially fewer primitive blocks. However, density is improved more by increasing the number of code words. We also find that a simple redundancy coalescing technique is able to reduce reactions by 90.6\% and 41.2\% on average for decompressed and compressed data, respectively, even when the smallest data fragments being assembled are 16 bits. With a simple padding heuristic that finds even more redundancy, we can further decrease reactions for the same operating point up to 91.1\% and 59\% for decompressed and compressed data, respectively, on average. Our approach offers greater density by up to 80\% over a prior general purpose gene assembly technique. Finally, in an analysis of synthesis costs in which we make 1 GB volume using de novo synthesis versus making only the primitive blocks with de novo synthesis and otherwise assembling using DINOS, we estimate DINOS as 105\texttimes{} cheaper than de novo synthesis.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = jun,
articleno = {53},
numpages = {35},
keywords = {DNA assembly algorithms, DNA synthesis, DNA information storage}
}

@article{10.1145/3511902,
author = {Wang, Jin and Chen, Jiahao and Xiong, Neal and Alfarraj, Osama and Tolba, Amr and Ren, Yongjun},
title = {S-BDS: An Effective Blockchain-based Data Storage Scheme in Zero-Trust IoT},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3511902},
doi = {10.1145/3511902},
abstract = {With the development of the Internet of Things (IoT), a large-scale, heterogeneous, and dynamic distributed network has been formed among IoT devices. There is an extreme need to establish a trust mechanism between devices, and blockchain can provide a zero-trust security framework for IoT. However, the efficiency of the blockchain is far from meeting the application requirements of the IoT, which has become the biggest resistance to the application of the blockchain in the IoT. Therefore, this paper combines sharding to build an effective Blockchain-based IoT data storage scheme (S-BDS). Sharding can solve the problem of blockchain capacity and scalability. While the blockchain provides data immutability and traceability for the IoT, it also brings huge demands for data credibility verification. The communication delay in the IoT system seriously affects the security of the system, while the Merkle proof of traditional blockchain occupies a lot of communication resources. This paper constructs Insertable Vector Commitment (IVC) in the bilinear group and replaces the Merkle tree with IVC to store IoT data in the blockchain. The construct has small-sized proof. It also has the ability to record the number of updates, which can prevent replay-attacks. Experiments show that each block processes 1,000 transactions, the proof size of a single data piece is 30\% of the original scheme, and proofs from different shards can be aggregated. IVC can effectively reduce communication congestion and improve the stability and security of the IoT system.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {42},
numpages = {23},
keywords = {cryptographic commitment, date storage, Internet of Things, zero-trust, Blockchain}
}

@article{10.1145/3517805,
author = {Yinying, Cai and Li, Juan and Wang, Bo},
title = {Data Mining Techniques and Machine Learning Algorithms in the Multimedia System to Enhance Engineering Education},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517805},
doi = {10.1145/3517805},
abstract = {In the current digital era, engineering education worldwide faces a massive challenge in education and career development. By authorizing educators and administrators to migrate to the actions, cloud services technology has transformed into the educational environment. A Multimedia assisted smart learning system (MSLS) has been suggested in this paper where universities/colleges will advocate future development and begin skill-set enhancement courses by e-learning. To classify their employment prospects at the early stage of graduation, this proposed system measures learners' academic/skill data. Machine learning and Data mining are advanced research fields whose accelerated advancement is attributable to developments in data processing research, database industry growth, and business requirements for methods capable of extracting useful information from massive data stores. In addition, for skill set evaluation, a practical algorithm is suggested to find different groups of students that lack the appropriate skill set. The anticipated student groups can be provided with opportunities by e-learning to enhance their required skill set. The findings suggest that more critical choices can boost employment prospects and overall educational development by implementing the new engineering education system.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {112},
numpages = {21},
keywords = {engineering education, multimedia system, data mining, Machine learning}
}

@article{10.1145/3519419,
author = {Mazilu, Lacramioara and Paton, Norman W. and Konstantinou, Nikolaos and Fernandes, Alvaro A. A.},
title = {Fairness-aware Data Integration},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3519419},
doi = {10.1145/3519419},
abstract = {Machine learning can be applied in applications that take decisions that impact people’s lives. Such techniques have the potential to make decision making more objective, but there also is a risk that the decisions can discriminate against certain groups as a result of bias in the underlying data. Reducing bias, or promoting fairness, has been a focus of significant investigation in machine learning, for example, based on pre-processing the training data, changing the learning algorithm, or post-processing the results of the learning. However, prior to these activities, data integration discovers and integrates the data that is used for training, and data integration processes have the potential to produce data that leads to biased conclusions. In this article, we propose an approach that generates schema mappings in ways that take into account: (i) properties that are intrinsic to mapping results that may give rise to bias in analyses; and (ii) bias observed in classifiers trained on the results of different sets of mappings. The approach explores a space of different ways of integrating the data, using a Tabu search algorithm, guided by bias-aware objective functions that represent different types of bias.The resulting approach is evaluated using Adult Census and German Credit datasets to explore the extent to which and the circumstances in which the approach can increase the fairness of the results of the data integration process.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {28},
numpages = {26},
keywords = {bias, fairness, data preparation, Data integration}
}

@article{10.1145/3524129,
author = {Jahanshahi, Ali and Yu, Nanpeng and Wong, Daniel},
title = {PowerMorph: QoS-Aware Server Power Reshaping for Data Center Regulation Service},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3524129},
doi = {10.1145/3524129},
abstract = {Adoption of renewable energy in power grids introduces stability challenges in regulating the operation frequency of the electricity grid. Thus, electrical grid operators call for provisioning of frequency regulation services from end-user customers, such as data centers, to help balance the power grid’s stability by dynamically adjusting their energy consumption based on the power grid’s need. As renewable energy adoption grows, the average reward price of frequency regulation services has become much higher than that of the electricity cost. Therefore, there is a great cost incentive for data centers to provide frequency regulation service.Many existing techniques modulating data center power result in significant performance slowdown or provide a low amount of frequency regulation provision. We present PowerMorph, a tight QoS-aware data center power-reshaping framework, which enables commodity servers to provide practical frequency regulation service. The key behind PowerMorph&nbsp;is using “complementary workload” as an additional knob to modulate server power, which provides high provision capacity while satisfying tight QoS constraints of latency-critical workloads. We achieve up to 58\% improvement to TCO under common conditions, and in certain cases can even completely eliminate the data center electricity bill and provide a net profit.},
journal = {ACM Trans. Archit. Code Optim.},
month = aug,
articleno = {36},
numpages = {27},
keywords = {co-location, quality of service, regulation service, power management, Data center}
}

@article{10.1145/3529395,
author = {Lv, Zhihan and Chen, Dongliang and Lv, Haibin},
title = {Smart City Construction and Management by Digital Twins and BIM Big Data in COVID-19 Scenario},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3529395},
doi = {10.1145/3529395},
abstract = {With the rapid development of information technology and the spread of Corona Virus Disease 2019 (COVID-19), the government and urban managers are looking for ways to use technology to make the city smarter and safer. Intelligent transportation can play a very important role in the joint prevention. This work expects to explore the building information modeling (BIM) big data (BD) processing method of digital twins (DTs) of Smart City, thus speeding up the construction of Smart City and improve the accuracy of data processing. During construction, DTs build the same digital copy of the smart city. On this basis, BIM designs the building's keel and structure, optimizing various resources and configurations of the building. Regarding the fast data growth in smart cities, a complex data fusion and efficient learning algorithm, namely Multi-Graphics Processing Unit (GPU), is proposed to process the multi-dimensional and complex BD based on the compositive rough set model. The Bayesian network solves the multi-label classification. Each label is regarded as a Bayesian network node. Then, the structural learning approach is adopted to learn the label Bayesian network's structure from data. On the P53-old and the P53-new datasets, the running time of Multi-GPU decreases as the number of GPUs increases, approaching the ideal linear speedup ratio. With the continuous increase of K value, the deterministic information input into the tag BN will be reduced, thus reducing the classification accuracy. When K = 3, MLBN can provide the best data analysis performance. On genbase dataset, the accuracy of MLBN is 0.982 ± 0.013. Through experiments, the BIM BD processing algorithm based on Bayesian Network Structural Learning (BNSL) helps decision-makers use complex data in smart cities efficiently.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {117},
numpages = {21},
keywords = {Bayesian Network Structural Learning Algorithm, multimedia big data, BIM, digital twins, Smart City}
}

@article{10.1145/3529753,
author = {Yates, Darren and Islam, Md Zahidul},
title = {Data Mining on Smartphones: An Introduction and Survey},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3529753},
doi = {10.1145/3529753},
abstract = {Data mining is the science of extracting information or “knowledge” from data. It is a task commonly executed on cloud computing resources, personal computers and laptops. However, what about smartphones? Despite the fact that these ubiquitous mobile devices now offer levels of hardware and performance approaching that of laptops, locally executed model-training using data mining methods on smartphones is still notably rare. On-device model-training offers a number of advantages. It largely mitigates issues of data security and privacy, since no data is required to leave the device. It also ensures a self-contained, fully portable data mining solution requiring no cloud computing or network resources and able to operate in any location. In this article, we focus on the intersection of smartphones and data mining. We investigate the growth in smartphone performance, survey smartphone usage models in previous research, and look at recent developments in locally executed data mining on smartphones.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {101},
numpages = {38},
keywords = {augmented reality, deep learning, decision trees, data mining, smartphones, Survey}
}

@article{10.1145/3532189,
author = {Hu, Yue and Qu, Ao and Wang, Yanbing and Work, Daniel B.},
title = {Streaming Data Preprocessing via Online Tensor Recovery for Large Environmental Sensor Networks},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3532189},
doi = {10.1145/3532189},
abstract = {Measuring the built and natural environment at a fine-grained scale is now possible with low-cost urban environmental sensor networks. However, fine-grained city-scale data analysis is complicated by tedious data cleaning including removing outliers and imputing missing data. While many methods exist to automatically correct anomalies and impute missing entries, challenges still exist on data with large spatial-temporal scales and shifting patterns. To address these challenges, we propose an online robust tensor recovery (OLRTR) method to preprocess streaming high-dimensional urban environmental datasets. A small-sized dictionary that captures the underlying patterns of the data is computed and constantly updated with new data. OLRTR enables online recovery for large-scale sensor networks that provide continuous data streams, with a lower computational memory usage compared to offline batch counterparts. In addition, we formulate the objective function so that OLRTR can detect structured outliers, such as faulty readings over a long period of time. We validate OLRTR on a synthetically degraded National Oceanic and Atmospheric Administration temperature dataset, and apply it to the Array of Things city-scale sensor network in Chicago, IL, showing superior results compared with several established online and batch-based low-rank decomposition methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {122},
numpages = {24},
keywords = {urban computing, internet of things, outlier detection, multilinear analysis, tensor factorization, Robust tensor recovery}
}

@article{10.1145/3533381,
author = {Adhikari, Deepak and Jiang, Wei and Zhan, Jinyu and He, Zhiyuan and Rawat, Danda B. and Aickelin, Uwe and Khorshidi, Hadi A.},
title = {A Comprehensive Survey on Imputation of Missing Data in Internet of Things},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533381},
doi = {10.1145/3533381},
abstract = {The Internet of Things (IoT) is enabled by the latest developments in smart sensors, communication technologies, and Internet protocols with broad applications. Collecting data from IoT and generating information from these data become tedious tasks in real-life applications when missing data are encountered in datasets. It is of critical importance to deal with the missing data timely for intelligent decision-making. Hence, this survey attempts to provide a structured and comprehensive overview of the research on the imputation of incomplete data in IoT. The article starts by providing an overview of incomplete data based on the architecture of IoT. Then, it discusses the various strategies to handle the missing data, the assumptions used, the computing platform, and the issues related to them. The article also explores the application of imputation in the area of IoT. We encourage researchers and data analysts to use known imputation techniques and discuss various issues and challenges. Finally, potential future directions regarding the method are suggested. We believe this survey will provide a better understanding of the research of incomplete data and serve as a guide for future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {133},
numpages = {38},
keywords = {Internet of Things, computing platform for incomplete data, deep learning, machine learning, multiple imputations, Imputation of missing data}
}

@article{10.1145/3533382,
author = {Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and Bohlke-Schneider, Michael and Salinas, David and Stella, Lorenzo and Aubet, Fran\c{c}ois-Xavier and Callot, Laurent and Januschowski, Tim},
title = {Deep Learning for Time Series Forecasting: Tutorial and Literature Survey},
year = {2022},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533382},
doi = {10.1145/3533382},
abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {121},
numpages = {36},
keywords = {neural networks, forecasting, Time series}
}

@article{10.1145/3535457,
author = {Young, May and Hu, Alan J. and Lemieux, Guy G. F.},
title = {Cache Abstraction for Data Race Detection in Heterogeneous Systems with Non-coherent Accelerators},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3535457},
doi = {10.1145/3535457},
abstract = {Embedded systems are becoming increasingly complex and heterogeneous, featuring multiple processor cores (which might themselves be heterogeneous) as well as specialized hardware accelerators, all accessing shared memory. Many accelerators are non-coherent (i.e., do not support hardware cache coherence) because it reduces hardware complexity, cost, and power consumption, while potentially offering superior performance. However, the disadvantage of non-coherence is that the software must explicitly synchronize between accelerators and processors, and this synchronization is notoriously error-prone.We propose an analysis technique to find data races in software for heterogeneous systems that include non-coherent accelerators. Our approach builds on classical results for data race detection, but the challenge turns out to be analyzing cache behavior rather than the behavior of the non-coherent accelerators. Accordingly, our central contribution is a novel, sound (data-race-preserving) abstraction of cache behavior. We prove our abstraction sound, and then to demonstrate the precision of our abstraction, we implement it in a simple dynamic race detector for a system with a processor and a massively parallel accelerator provided by a commercial FPGA-based accelerator vendor. On eleven software examples provided by the vendor, the tool had zero false positives and was able to detect previously unknown data races in two of the 11 examples.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
articleno = {6},
numpages = {25},
keywords = {caching, memory coherence, hardware accelerator, Data race}
}

@article{10.1145/3538647,
author = {Apriansyah, M. Ridwan and Yokota, Rio},
title = {Parallel QR Factorization of Block Low-rank Matrices},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3538647},
doi = {10.1145/3538647},
abstract = {We present two new algorithms for Householder QR factorization of Block Low-Rank (BLR) matrices: one that performs block-column-wise QR and another that is based on tiled QR. We show how the block-column-wise algorithm exploits BLR structure to achieve arithmetic complexity of 𝒪(mn), while the tiled BLR-QR exhibits 𝒪(mn1.5 complexity. However, the tiled BLR-QR has finer task granularity that allows parallel task-based execution on shared memory systems. We compare the block-column-wise BLR-QR using fork-join parallelism with tiled BLR-QR using task-based parallelism. We also compare these two implementations of Householder BLR-QR with a block-column-wise Modified Gram–Schmidt (MGS) BLR-QR using fork-join parallelism and a state-of-the-art vendor-optimized dense Householder QR in Intel MKL. For a matrix of size 131k \texttimes{} 65k, all BLR methods are more than an order of magnitude faster than the dense QR in MKL. Our methods are also robust to ill conditioning and produce better orthogonal factors than the existing MGS-based method. On a CPU with 64 cores, our parallel tiled Householder and block-column-wise Householder algorithms show a speedup of 50 and 37 times, respectively.},
journal = {ACM Trans. Math. Softw.},
month = sep,
articleno = {27},
numpages = {28},
keywords = {task-based execution, householder reflections, QR factorization, Block low-rank matrix}
}

@article{10.1145/3543069,
author = {Ioannou, Lenos and Fahmy, Suhaib A.},
title = {Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3543069},
doi = {10.1145/3543069},
abstract = {Long-Short Term Memory (LSTM) networks, and Recurrent Neural Networks (RNNs) in general, have demonstrated their suitability in many time series data applications, especially in Natural Language Processing (NLP). Computationally, LSTMs introduce dependencies on previous outputs in each layer that complicate their computation and the design of custom computing architectures, compared to traditional feed-forward networks. Most neural network acceleration work has focused on optimising the core matrix-vector operations on highly capable FPGAs in server environments. Research that considers the embedded domain has often been unsuitable for streaming inference, relying heavily on batch processing to achieve high throughput. Moreover, many existing accelerator architectures have not focused on fully exploiting the underlying FPGA architecture, resulting in designs that achieve lower operating frequencies than the theoretical maximum. This paper presents a flexible overlay architecture for LSTMs on FPGA SoCs that is built around a streaming dataflow arrangement, uses DSP block capabilities directly, and is tailored to keep parameters within the architecture while moving input data serially to mitigate external memory access overheads. The architecture is designed as an overlay that can be configured to implement alternative models or update model parameters at runtime. It achieves higher operating frequency and demonstrates higher performance than other lightweight LSTM accelerators, as demonstrated in an FPGA SoC implementation.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = dec,
articleno = {8},
numpages = {26},
keywords = {machine learning, overlay, neural networks, LSTM}
}

@article{10.1145/3546913,
author = {Accinelli, Chiara and Catania, Barbara and Guerrini, Giovanna and Minisi, Simone},
title = {A Coverage-based Approach to Nondiscrimination-aware Data Transformation},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3546913},
doi = {10.1145/3546913},
abstract = {The development of technological solutions satisfying nondiscriminatory requirements is one of the main current challenges for data processing. Back-end operators for preparing, i.e., extracting and transforming, data play a relevant role w.r.t. nondiscrimination, since they can introduce bias with an impact on the entire data life-cycle. In this article, we focus on back-end transformations, defined in terms of Select-Project-Join queries, and on coverage. Coverage aims at guaranteeing that the input, or training, dataset includes enough examples for each (protected) category of interest, thus increasing diversity with the aim of limiting the introduction of bias during the next analytical steps. The article proposes an approach to automatically rewrite a transformation with a result that violates coverage constraints, into the “closest” query satisfying the constraints. The approach is approximate and relies on a sample-based cardinality estimation, thus it introduces a trade-off between accuracy and efficiency. The efficiency and the effectiveness of the approach are experimentally validated on synthetic and real data.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {26},
numpages = {26},
keywords = {query rewriting, coverage, nondiscrimination, Data transformation}
}

@article{10.1145/3546942,
author = {Costea, Andreea and Tiwari, Abhishek and Chianasta, Sigmund and R, Kishore and Roychoudhury, Abhik and Sergey, Ilya},
title = {Hippodrome: Data Race Repair Using Static Analysis Summaries},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3546942},
doi = {10.1145/3546942},
abstract = {Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyze and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study, conducted on popular open-source projects, has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {41},
numpages = {33},
keywords = {static analysis, program repair, Concurrency}
}

@article{10.1145/3547141,
author = {Alam, Syed Asad and Gregg, David and Gambardella, Giulio and Preusser, Thomas and Blott, Michaela},
title = {On the RTL Implementation of FINN Matrix Vector Unit},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3547141},
doi = {10.1145/3547141},
abstract = {Field-programmable gate array (FPGA)–based accelerators are becoming increasingly popular for deep neural network (DNN) inference due to their ability to scale performance with increasing degrees of specialization with dataflow architectures or custom data type precision. In order to reduce the barrier for software engineers and data scientists to adopt FPGAs, C++- and OpenCL-based design entries with high-level synthesis (HLS) have been introduced. They provide higher abstraction compared with register-transfer level (RTL)–based design. HLS offers faster development time, better maintainability, and more flexibility in code exploration when evaluating several options for multi-dimension tensors, convolutional layers, or different degrees of parallelism. For this reason, HLS has been adopted by DNN accelerator generation frameworks such as FINN and hls4ml.In this article, we present an alternative backend library for FINN, leveraging RTL. We investigate and evaluate, across a spectrum of design dimensions, the pros and cons of an RTL-based implementation versus the original HLS variant. We show that for smaller design parameters, RTL produces significantly smaller circuits as compared with HLS. For larger circuits, however, the look-up table (LUT) count of RTL-based design is slightly higher, up to around 15\%. On the other hand, HLS consistently requires more flip-flops (FFs; with an orders-of-magnitude difference for smaller designs) and block RAMs (BRAMs; 2\texttimes{} more). This also impacts the critical path delay, with RTL producing significantly faster circuits, up to around 80\%. RTL also benefits from at least a 10\texttimes{} reduction in synthesis time. Finally, the results were validated in practice using two real-world use cases, one of a multi-layer perceptron (MLP) used in network intrusion detection and the other a convolution network called ResNet, used in image recognition. Overall, since HLS frameworks code-generate the hardware design, the benefits of the ease in the design entry is less important. As such, the gained benefits in synthesis time together with some design-dependent resource benefits make the RTL abstraction an attractive alternative.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
articleno = {94},
numpages = {27},
keywords = {FPGA, RTL, HLS, convolutional neural network, FINN}
}

@article{10.1145/3551497,
author = {Banerjee, Anindya and Nagasamudram, Ramana and Naumann, David A. and Nikouei, Mohammad},
title = {A Relational Program Logic with Data Abstraction and Dynamic Framing},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3551497},
doi = {10.1145/3551497},
abstract = {Dedicated to Tony Hoare.In a paper published in 1972, Hoare articulated the fundamental notions of hiding invariants and simulations. Hiding: invariants on encapsulated data representations need not be mentioned in specifications that comprise the API of a module. Simulation: correctness of a new data representation and implementation can be established by proving simulation between the old and new implementations using a coupling relation defined on the encapsulated state. These results were formalized semantically and for a simple model of state, though the paper claimed this could be extended to encompass dynamically allocated objects. In recent years, progress has been made toward formalizing the claim, for simulation, though mainly in semantic developments. In this article, hiding and simulation are combined with the idea in Hoare’s 1969 paper: a logic of programs. For an object-based language with dynamic allocation, we introduce a relational Hoare logic with stateful frame conditions that formalizes encapsulation, hiding of invariants, and couplings that relate two implementations. Relations and other assertions are expressed in first-order logic. Specifications can express a wide range of relational properties such as conditional equivalence and noninterference with declassification. The proof rules facilitate relational reasoning by means of convenient alignments and are shown sound with respect to a conventional operational semantics. A derived proof rule for equivalence of linked programs directly embodies representation independence. Applicability to representative examples is demonstrated using an SMT-based implementation.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jan,
articleno = {25},
numpages = {136},
keywords = {automated verification, product programs, representation independence, data abstraction, logics of programs, relational verification, Relational properties}
}

@article{10.1145/3555091,
author = {Hoffmann, Sven and Pinatti de Carvalho, Aparecido Fabiano and Schweitzer, Marcus and Abele, Nils Darwin and Wulf, Volker},
title = {Producing and Consuming Instructional Material in Manufacturing Contexts: Evaluation of an AR-based Cyber-Physical Production System for Supporting Knowledge and Expertise Sharing},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555091},
doi = {10.1145/3555091},
abstract = {Fast-paced knowledge and expertise sharing (KES) is a typical demand in contemporary workplaces due to dynamic markets and ever-changing work practices. Past and current computer supported cooperative work (CSCW) research has long been investigating how computer technologies can support people with KES. Recent claims have asserted that augmented reality- (AR-)based cyber-physical production systems (CPPS) are poised to bring significant changes in the ways that KES unfolds in manufacturing contexts. This paper scrutinises such claims by implementing a short-term evaluation of an AR-based CPPS and assessing how it can potentially support (1) the generation of AR content by experienced production workers and (2) the visualisation and processing of such content by novice workers. We, therefore, contribute a user study to the CSCW community that sheds light on the use of a particular type of AR-based CPPS for KES in industrial contexts.?},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {366},
numpages = {36},
keywords = {manufacturing contexts, machine set-up, knowledge and expertise sharing, evaluation, cyber-physical production systems, augmented reality}
}

@article{10.1145/3555139,
author = {Thomer, Andrea K. and Akmon, Dharma and York, Jeremy J. and Tyler, Allison R. B. and Polasek, Faye and Lafia, Sara and Hemphill, Libby and Yakel, Elizabeth},
title = {The Craft and Coordination of Data Curation: Complicating Workflow Views of Data Science},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555139},
doi = {10.1145/3555139},
abstract = {Data curation is the process of making a dataset fit-for-use and archivable. It is critical to data-intensive science because it makes complex data pipelines possible, studies reproducible, and data reusable. Yet the complexities of the hands-on, technical, and intellectual work of data curation is frequently overlooked or downplayed. Obscuring the work of data curation not only renders the labor and contributions of data curators invisible but also hides the impact that curators' work has on the later usability, reliability, and reproducibility of data. To better understand the work and impact of data curation, we conducted a close examination of data curation at a large social science data repository, the Inter-university Consortium for Political and Social Research (ICPSR). We asked: What does curatorial work entail at ICPSR, and what work is more or less visible to different stakeholders and in different contexts? And, how is that curatorial work coordinated across the organization? We triangulated accounts of data curation from interviews and records of curation in Jira tickets to develop a rich and detailed account of curatorial work. While we identified numerous curatorial actions performed by ICPSR curators, we also found that curators rely on a number of craft practices to perform their jobs. The reality of their work practices defies the rote sequence of events implied by many life cycle or workflow models. Further, we show that craft practices are needed to enact data curation best practices and standards. The craft that goes into data curation is often invisible to end users, but it is well recognized by ICPSR curators and their supervisors. Explicitly acknowledging and supporting data curators as craftspeople is important in creating sustainable and successful curatorial infrastructures.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {414},
numpages = {29},
keywords = {workflows, social science data, knowledge infrastructure, data curation, craft, coordination}
}

@article{10.1145/3555370,
author = {Sobczyk, Aleksandros and Gallopoulos, Efstratios},
title = {pylspack: Parallel Algorithms and Data Structures for Sketching, Column Subset Selection, Regression, and Leverage Scores},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3555370},
doi = {10.1145/3555370},
abstract = {We present parallel algorithms and data structures for three fundamental operations in Numerical Linear Algebra: (i) Gaussian and CountSketch random projections and their combination, (ii) computation of the Gram matrix, and (iii) computation of the squared row norms of the product of two matrices, with a special focus on “tall-and-skinny” matrices, which arise in many applications. We provide a detailed analysis of the ubiquitous CountSketch transform and its combination with Gaussian random projections, accounting for memory requirements, computational complexity and workload balancing. We also demonstrate how these results can be applied to column subset selection, least squares regression and leverage scores computation. These tools have been implemented in pylspack, a publicly available Python package1 whose core is written in C++ and parallelized with OpenMP and that is compatible with standard matrix data structures of SciPy and NumPy. Extensive numerical experiments indicate that the proposed algorithms scale well and significantly outperform existing libraries for tall-and-skinny matrices.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {27},
keywords = {statistical leverage scores, preconditioning, regression, column subset selection, sketching, sparse data structures, Parallel algorithms}
}

@article{10.1145/3555537,
author = {Chalhoub, George and Flechais, Ivan},
title = {Data Protection at a Discount: Investigating the UX of Data Protection from User, Designer, and Business Leader Perspectives},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555537},
doi = {10.1145/3555537},
abstract = {Smart homes are dangerous - a sentiment arising from prior research exploring the user experience (UX) of data protection for smart home devices. While this research has explored data protection shortcomings for users, UX is a designed encounter reconciling development, economic, compliance and strategic business priorities. And so, in addition to studying user perspectives, there is a gap in understanding how designers and business leaders influence the UX of data protection. To address this gap, we study smart home users, designers and business leaders, exploring how they experience data protection interactions, regulation, and processes. Our findings confirm that users have poor data protection interactions (e.g., consent and data access requests). We also find that business leaders and designers experience difficulties in identifying, applying, and tailoring suitable processes and practices for data protection for which some have developed "discount data protection": shortcuts, heuristics, and common sense practices to overcome these challenges.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {436},
numpages = {36},
keywords = {user experience, smart home, data protection, consent}
}

@article{10.1145/3555561,
author = {Miceli, Milagros and Posada, Julian},
title = {The Data-Production Dispositif},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555561},
doi = {10.1145/3555561},
abstract = {Machine learning (ML) depends on data to train and verify models. Very often, organizations outsource processes related to data work (i.e., generating and annotating data and evaluating outputs) through business process outsourcing (BPO) companies and crowdsourcing platforms. This paper investigates outsourced ML data work in Latin America by studying three platforms in Venezuela and a BPO in Argentina. We lean on the Foucauldian notion of dispositif to define the data-production dispositif as an ensemble of discourses, actions, and objects strategically disposed to (re)produce power/knowledge relations in data and labor. Our dispositif analysis comprises the examination of 210 data work instruction documents, 55 interviews with data workers, managers, and requesters, and participant observation. Our findings show that discourses encoded in instructions reproduce and normalize the worldviews of requesters. Precarious working conditions and economic dependency alienate workers, making them obedient to instructions. Furthermore, discourses and social contexts materialize in artifacts, such as interfaces and performance metrics, limiting workers' agency and normalizing specific ways of interpreting data. We conclude by stressing the importance of counteracting the data-production dispositif by fighting alienation and precarization, and empowering data workers to become assets in the quest for high-quality data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {460},
numpages = {37},
keywords = {platform labor, machine learning, data work, data production, data labeling, crowdsourcing}
}

@article{10.1145/3555617,
author = {Gould, Sandy J. J. and Wiseman, Sarah},
title = {Dealing with Digital Service Closure},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555617},
doi = {10.1145/3555617},
abstract = {People integrate digital services into their day-to-day lives, often with the assumption that they will always be available. What happens when these services close down? The introduction of services might be carefully planned, but their closure may not benefit from the same degree of consideration. A more developed understanding of the effects of closures might make it possible to minimize negative consequences for users. This paper builds on sustainability, digital memories, and collaborative-work research through an empirical investigation of service closure. Fifty-five participants completed a questionnaire that solicited experiences of service closure and attitudes toward prospective closure. Through a qualitative analysis of participant responses, we synthesized six themes that reflected the practical and emotional effects of service closure on people: disempowerment, disconnection, loss of capability, trust, time and effort, and notice periods. We make suggestions for ways that service features related to these themes might be managed during closure, but also identify less tractable challenges: as part of this investigation, we introduce and develop the concept of service patinas to describe the important but entirely service-bound data that contextualize digital artefacts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {504},
numpages = {25},
keywords = {sustainability, social media, sharing, service patinas, service design, service closure, obsolescence, digital memories, digital consumption objects, data rights, collaboration}
}

@article{10.1145/3557727,
author = {Luinaud, Thomas and Langlois, J. M. Pierre and Savaria, Yvon},
title = {Symbolic Analysis for Data Plane Programs Specialization},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3557727},
doi = {10.1145/3557727},
abstract = {Programmable network data planes have extended the capabilities of packet processing in network devices by allowing custom processing pipelines and agnostic packet processing. While a variety of applications can be implemented on current programmable data planes, there are significant constraints due to hardware limitations. One way to meet these constraints is by optimizing data plane programs. Program optimization can be achieved by specializing code that leverages architectural specificity or by compilation passes. In the case of programmable data planes, to respond to the varying requirements of a large set of applications, data plane programs can target different architectures. This leads to difficulties when developers want to reuse the code. One solution to that is to use compiler optimization techniques. We propose performing data plane program specialization to reduce the generated program size. To this end, we propose to specialize in programs written in P4, a Domain Specific Language (DSL) designed for specifying network data planes. The proposed method takes advantage of key aspects of the P4 language to perform a symbolic analysis on a P4 program and then partially evaluate the program to specialize it. The approach we propose is independent of the target architecture. We evaluate the specialization technique by implementing a packet deparser on an FPGA. The results demonstrate that program specialization can reduce the resource usage by a factor of 2 for various packet deparsers.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {1},
numpages = {21},
keywords = {packet deparsers, FPGA, network data plane, P4 language, program specialization, Compiler optimization}
}

@article{10.1145/3561303,
author = {Zhu, Xiaojun and Han, Zhouqing and Tang, Shaojie and Xu, Lijie and Dong, Chao},
title = {Deploying the Minimum Number of Rechargeable UAVs for a Quarantine Barrier},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3561303},
doi = {10.1145/3561303},
abstract = {To control the rapid spread of COVID-19, we consider deploying a set of Unmanned Aerial Vehicles (UAVs) to form a quarantine barrier such that anyone crossing the barrier can be detected. We use a charging pile to recharge UAVs. The problem is scheduling UAVs to cover the barrier, and, for any scheduling strategy, estimating the minimum number of UAVs needed to cover the barrier forever. We propose breaking the barrier into subsegments so that each subsegment can be monitored by a single UAV. We then analyze two scheduling strategies, where the first one is simple to implement and the second one requires fewer UAVs. The first strategy divides UAVs into groups with each group covering a subsegment. For this strategy, we derive a closed-form formula for the minimum number of UAVs. In the case of insufficient UAVs, we give a recursive function to compute the exact coverage time and give a dynamic-programming algorithm to allocate UAVs to subsegments to maximize the overall coverage time. The second strategy schedules all UAVs dynamically. We prove a lower and an upper bound on the minimum number of UAVs. We implement a prototype system to verify the proposed coverage model and perform simulations to investigate the performance.},
journal = {ACM Trans. Sen. Netw.},
month = dec,
articleno = {40},
numpages = {28},
keywords = {energy limitation, monitoring, scheduling, barrier coverage, quarantine, COVID-19, Rechargeable UAVs}
}

@article{10.1145/3563394,
author = {Ganewattha, Chanaka and Khan, Zaheer and Lehtom\"{a}ki, Janne and Latva-Aho, Matti},
title = {Hardware-accelerated Real-time Drift-awareness for Robust Deep Learning on Wireless RF Data},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3563394},
doi = {10.1145/3563394},
abstract = {Proactive and intelligent management of network resource utilization (RU) using deep learning (DL) can significantly improve the efficiency and performance of the next generation of wireless networks. However, variations in wireless RU are often affected by uncertain events and change points due to the deviations of real data distribution from that of the original training data. Such deviations, which are known as dataset drifts, can subsequently lead to a shift in the corresponding decision boundary degrading the DL model prediction performance. To address these challenges, we present hardware-accelerated real-time radio frequency (RF) analytics and drift-awareness modules for robust DL predictions. We have prototyped the proposed design on a Zynq-7000 System-on-Chip that contains an FPGA and an embedded ARM processor. We have used Xilinx Vivado design suite for synthesis and analysis of the HDL design for the proposed solution. To detect dataset drifts, the proposed solution adopts a distance-based technique on FPGA to quantify in real-time the change between the prediction distribution obtained from DL predictions and data distribution of input streaming samples. Using various performance metrics, we have extensively evaluated the performance of the proposed solution and shown that it can significantly improve the DL model robustness in the presence of dataset drifts.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = mar,
articleno = {19},
numpages = {29},
keywords = {HDL design, FPGA, HLS, Xilinx, robustness, deep learning, dataset drift detection, resource utilization, Proactive resource allocation}
}

@article{10.1145/3563456,
author = {Shu, Jiwu and Fang, Kedong and Chen, Youmin and Wang, Shuo},
title = {TH-iSSD: Design and Implementation of a Generic and Reconfigurable Near-Data Processing Framework},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3563456},
doi = {10.1145/3563456},
abstract = {We present the design and implementation of TH-iSSD, a near-data processing framework to address the data movement problem. TH-iSSD does not pose any restriction to the hardware selection and is highly reconfigurable—its core components, such as the on-device compute unit (e.g., FPGA, embedded CPUs) and data collectors (e.g., camera, sensors), can be easily replaced to adapt to different use cases. TH-iSSD achieves this goal by incorporating highly flexible computation and data paths. In the data path, TH-iSSD adopts an efficient device-level data switch that exchanges data with both host CPUs and peripheral sensors; it also enables direct accesses between the sensing, computation, and storage hardware components, which completely eliminates the redundant data movement overhead, and thus delivers both high performance and energy efficiency. In the computation path, TH-iSSD provides an abstraction of filestream for developers, which abstracts a collection of data along with the related computation task as a file. Since existing applications are familiar with POSIX-like interfaces, they can be ported on top of our platform with minimal code modification. Moreover, TH-iSSD also introduces mechanisms including pipelined near-data processing and priority-aware I/O scheduling to make TH-iSSD perform more effectively. We deploy TH-iSSD to accelerate two types of applications: the content-based information retrieval system and the edge zero-streaming system. Our experimental results show that TH-iSSD achieves up to 1.6\texttimes{} higher throughput and 36\% lower latency than compute-centric designs.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
articleno = {96},
numpages = {23},
keywords = {storage architecture, deep learning, information retrieval, Near data processing}
}

@article{10.1145/3564663,
author = {Chen, Jou-An and Niu, Wei and Ren, Bin and Wang, Yanzhi and Shen, Xipeng},
title = {Survey: Exploiting Data Redundancy for Optimization of Deep Learning},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3564663},
doi = {10.1145/3564663},
abstract = {Data redundancy is ubiquitous in the inputs and intermediate results of Deep Neural Networks (DNN). It offers many significant opportunities for improving DNN performance and efficiency and has been explored in a large body of work. These studies have scattered in many venues across several years. The targets they focus on range from images to videos and texts, and the techniques they use to detect and exploit data redundancy also vary in many aspects. There is not yet a systematic examination and summary of the many efforts, making it difficult for researchers to get a comprehensive view of the prior work, the state of the art, differences and shared principles, and the areas and directions yet to explore. This article tries to fill the void. It surveys hundreds of recent papers on the topic, introduces a novel taxonomy to put the various techniques into a single categorization framework, offers a comprehensive description of the main methods used for exploiting data redundancy in improving multiple kinds of DNNs on data, and points out a set of research opportunities for future exploration.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {212},
numpages = {38},
keywords = {transformer, convolutional neural network, deep neural network, representation redundancy, Data redundancy}
}

@article{10.1145/3565557,
author = {Anderson, Daniel and Blelloch, Guy E.},
title = {Parallel Minimum Cuts in O(m log2 n) Work and Low Depth},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3565557},
doi = {10.1145/3565557},
abstract = {We present a randomized O(m log2 n) work, O(polylog n) depth parallel algorithm for minimum cut. This algorithm matches the work bounds of a recent sequential algorithm by Gawrychowski, Mozes, and Weimann [ICALP’20], and improves on the previously best parallel algorithm by Geissmann and Gianinazzi [SPAA’18], which performs O(m log4 n) work in O(polylog n) depth.Our algorithm makes use of three components that might be of independent interest. First, we design a parallel data structure that efficiently supports batched mixed queries and updates on trees. It generalizes and improves the work bounds of a previous data structure of Geissmann and Gianinazzi and is work efficient with respect to the best sequential algorithm. Second, we design a parallel algorithm for approximate minimum cut that improves on previous results by Karger and Motwani. We use this algorithm to give a work-efficient procedure to produce a tree packing, as in Karger’s sequential algorithm for minimum cuts. Last, we design an efficient parallel algorithm for solving the minimum 2-respecting cut problem.},
journal = {ACM Trans. Parallel Comput.},
month = dec,
articleno = {18},
numpages = {28},
keywords = {dynamic trees, graph algorithms, parallel algorithms, Minimum cut}
}

@article{10.1145/3567444,
author = {Yuan, Gongsheng and Lu, Jiaheng and Yan, Zhengtong and Wu, Sai},
title = {A Survey on Mapping Semi-Structured Data and Graph Data to Relational Data},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3567444},
doi = {10.1145/3567444},
abstract = {The data produced by various services should be stored and managed in an appropriate format for gaining valuable knowledge conveniently. This leads to the emergence of various data models, including relational, semi-structured, and graph models, and so on. Considering the fact that the mature relational databases established on relational data models are still predominant in today’s market, it has fueled interest in storing and processing semi-structured data and graph data in relational databases so that mature and powerful relational databases’ capabilities can all be applied to these various data. In this survey, we review existing methods on mapping semi-structured data and graph data into relational tables, analyze their major features, and give a detailed classification of those methods. We also summarize the merits and demerits of each method, introduce open research challenges, and present future research directions. With this comprehensive investigation of existing methods and open problems, we hope this survey can motivate new mapping approaches through drawing lessons from each model’s mapping strategies, as well as a new research topic - mapping multi-model data into relational tables.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {218},
numpages = {38},
keywords = {model mapping, property graph, RDF, graph data, XML, JSON, semi-structured data, relational storage, Relational schema}
}

@article{10.1145/3568421,
author = {Bertrand, Jules and Dufoss\'{e}, Fanny and Singh, Somesh and U\c{c}ar, Bora},
title = {Algorithms and Data Structures for Hyperedge Queries},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
issn = {1084-6654},
url = {https://doi.org/10.1145/3568421},
doi = {10.1145/3568421},
abstract = {We consider the problem of querying the existence of hyperedges in hypergraphs. More formally, given a hypergraph, we need to answer queries of the form: “Does the following set of vertices form a hyperedge in the given hypergraph?” Our aim is to set up data structures based on hashing to answer these queries as fast as possible. We propose an adaptation of a well-known perfect hashing approach for the problem at hand. We analyze the space and runtime complexity of the proposed approach and experimentally compare it with the state-of-the-art hashing-based solutions. Experiments demonstrate the efficiency of the proposed approach with respect to the state-of-the-art.},
journal = {ACM J. Exp. Algorithmics},
month = dec,
articleno = {1.13},
numpages = {23},
keywords = {hypergraphs, perfect hashing, Hashing}
}

@article{10.1145/3568675,
author = {Zhang, Yuanpeng and Jiang, Yizhang and Alireza, Jolfaei},
title = {Mutual Supervised Fusion \&amp; Transfer Learning with Interpretable Linguistic Meaning for Social Data Analytics},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3568675},
doi = {10.1145/3568675},
abstract = {Social data analytics is often taken as the most commonly used method for community discovery, product recommendations, knowledge graph, and so on. In this study, social data are firstly represented in different feature spaces by using various feature extraction algorithms. Then we build a transfer learning model to leverage knowledge from multiple feature spaces. During modeling, since the assumption that the training and the testing data have the same distribution is always true, we give a theorem and its proof which asserts the necessary and sufficient condition for achieving a minimum testing error. We also theoretically demonstrate that maximizing the classification error consistency across different feature spaces can improve the classification performance. Additionally, the cluster assumption derived from semi-supervised learning is introduced to enhance knowledge transfer. Finally, a Tagaki-Sugeno-Kang (TSK) fuzzy system-based learning algorithm is proposed, which can generate interpretable fuzzy rules. Experimental results not only demonstrate the promising social data classification performance of our proposed approach but also show its interpretability which is missing in many other models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {152},
numpages = {20},
keywords = {cluster assumption, classification error consensus, mutual supervised fusion, social data analytics, TSK fuzzy system}
}

@article{10.1145/3569085,
author = {Li, Yang and Purcell, Michael and Rakotoarivelo, Thierry and Smith, David and Ranbaduge, Thilina and Ng, Kee Siong},
title = {Private Graph Data Release: A Survey},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3569085},
doi = {10.1145/3569085},
abstract = {The application of graph analytics to various domains has yielded tremendous societal and economical benefits in recent years. However, the increasingly widespread adoption of graph analytics comes with a commensurate increase in the need to protect private information in graph data, especially in light of the many privacy breaches in real-world graph data that were supposed to preserve sensitive information. This article provides a comprehensive survey of private graph data release algorithms that seek to achieve the fine balance between privacy and utility, with a specific focus on provably private mechanisms. Many of these mechanisms are natural extensions of the Differential Privacy framework to graph data, but we also investigate more general privacy formulations like Pufferfish Privacy that address some of the limitations of Differential Privacy. We also provide a wide-ranging survey of the applications of private graph data release mechanisms to social networks, finance, supply chain, and health care. This article should benefit practitioners and researchers alike in the increasingly important area of private analytics and data release.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {226},
numpages = {39},
keywords = {healthcare data, supply chains, financial services, social networks, privacy-preserving protocols, graph differential privacy, Private graph analytics}
}

@article{10.1145/3569422,
author = {Shen, Ziyu and Liu, Binghui and Zhou, Qing and Liu, Zheng and Xia, Bin and Li, Yun},
title = {Cost-sensitive Tensor-based Dual-stage Attention LSTM with Feature Selection for Data Center Server Power Forecasting},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3569422},
doi = {10.1145/3569422},
abstract = {Power forecasting has a guiding effect on power-aware scheduling strategies to reduce unnecessary power consumption in data centers. Many metrics related to power consumption can be collected in physical servers, such as the status of CPU, memory, and other components. However, most existing methods empirically exploit a small number of metrics to forecast power consumption. To this end, this article uses feature selection based on causality to explore the metrics that strongly influence the power consumption of different tasks. Moreover, we propose a tensor-based dual-stage attention LSTM to forecast the non-linear and non-periodic power consumption. In the proposed model, a multi-way delay embedding transform is utilized to convert the time series into tensors along the temporal direction. The LSTM combines with the tensor technique and the attention mechanism to capture the temporal pattern effectively. In addition, we adopt the cost-sensitive loss function to optimize the specific power forecasting problem in data centers. The experimental results demonstrate that our method can achieve up to 1.4\% to 4.3\% forecasting accuracy improvement compared with the state-of-the-art models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {20},
keywords = {feature selection, LSTM, tensor, time series, data center, Power forecasting}
}

@article{10.1145/3571240,
author = {Chen, Zilin and Lafont, Ambroise and O'Connor, Liam and Keller, Gabriele and McLaughlin, Craig and Jackson, Vincent and Rizkallah, Christine},
title = {Dargent: A Silver Bullet for Verified Data Layout Refinement},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571240},
doi = {10.1145/3571240},
abstract = {Systems programmers need fine-grained control over the memory layout of data structures, both to produce performant code and to comply with well-defined interfaces imposed by existing code, standardised protocols or hardware. Code that manipulates these low-level representations in memory is hard to get right. Traditionally, this problem is addressed by the implementation of tedious marshalling code to convert between compiler-selected data representations and the desired compact data formats. Such marshalling code is error-prone and can lead to a significant runtime overhead due to excessive copying. While there are many languages and systems that address the correctness issue, by automating the generation and, in some cases, the verification of the marshalling code, the performance overhead introduced by the marshalling code remains. In particular for systems code, this overhead can be prohibitive. In this work, we address both the correctness and the performance problems. We present a data layout description language and data refinement framework, called Dargent, which allows programmers to declaratively specify how algebraic data types are laid out in memory. Our solution is applied to the Cogent language, but the general ideas behind our solution are applicable to other settings. The Dargent framework generates C code that manipulates data directly with the desired memory layout, while retaining the formal proof that this generated C code is correct with respect to the functional semantics. This added expressivity removes the need for implementing and verifying marshalling code, which eliminates copying, smoothens interoperability with surrounding systems, and increases the trustworthiness of the overall system.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {47},
numpages = {27},
keywords = {systems programming, data refinement, certifying compiler}
}

@article{10.1145/3571820,
author = {Gebauer, Richard and Karcher, Nick and G\"{u}ler, Mehmed and Sander, Oliver},
title = {QiCells: A Modular RFSoC-based Approach to Interface Superconducting Quantum Bits},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3571820},
doi = {10.1145/3571820},
abstract = {Quantum computers will be a revolutionary extension of the heterogeneous computing world. They consist of many quantum bits (qubits) and require a careful design of the interface between the classical computer architecture and the quantum processor. For example, even single nanosecond variations of the interaction may have an influence on the quantum state. Designing a tailored interface electronics is therefore a major challenge, both in terms of signal integrity with respect to single channels, as well as the scaling of the signal count. We developed such an interface electronics, an RFSoC-based qubit control system called QiController. In this article, we present the modular FPGA firmware design of our system. It features so-called digital unit cells, or QiCells. Each cell contains all the logic necessary to interact with a single superconducting qubit, including a custom-built RISC-V-based sequencer. Synchronization and data exchange between the cells is facilitated using a special star-point structure. Versatile routing and frequency-division multiplexing of generated signals between QiCells and converters are also supported. High-level programmability is provided using a custom Python-based description language and an associated compiler. We furthermore provide the resource utilization of our design and demonstrate its correct operation using an actual superconducting five-qubit chip.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = may,
articleno = {32},
numpages = {23},
keywords = {RISC-V, RFSoC, quantum computing, quantum-classical interface, quantum bits, pulse generation, FPGA, Data acquisition}
}

@article{10.1145/3573383,
author = {Quintana-Ort\'{\i}, Gregorio and Hernando, Fernando and Igual, Francisco D.},
title = {Algorithm&nbsp;1033: Parallel Implementations for Computing the Minimum Distance of a Random Linear Code on Distributed-memory Architectures},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3573383},
doi = {10.1145/3573383},
abstract = {The minimum distance of a linear code is a key concept in information theory. Therefore, the time required by its computation is very important to many problems in this area. In this article, we introduce a family of implementations of the Brouwer–Zimmermann algorithm for distributed-memory architectures for computing the minimum distance of a random linear code over 𝔽2. Both current commercial and public-domain software only work on either unicore architectures or shared-memory architectures, which are limited in the number of cores/processors employed in the computation. Our implementations focus on distributed-memory architectures, thus being able to employ hundreds or even thousands of cores in the computation of the minimum distance. Our experimental results show that our implementations are much faster, even up to several orders of magnitude, than current implementations widely used nowadays.},
journal = {ACM Trans. Math. Softw.},
month = mar,
articleno = {8},
numpages = {24},
keywords = {distributed-memory, linear codes, minimum distance, Information theory}
}

@article{10.1145/3575809,
author = {Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas},
title = {Experience: Differentiating Between Isolated and Sequence Missing Data},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3575809},
doi = {10.1145/3575809},
abstract = {Missing data is one of the most persistent problems found in data that hinders information and value extraction. Handling missing data is a preprocessing task that has been extensively studied by the research community and remains an active research topic due to its impact and pervasiveness. Many surveys have been conducted to evaluate traditional and state-of-the-art techniques, however, the accuracy of missing data imputation techniques is evaluated without differentiating between isolated and sequence missing instances. In this article, we highlight the presence of both of these types of missing data at different percentages in real-world time-series datasets. We demonstrate that existing imputation techniques have different estimation accuracies for isolated and sequence missing instances. We then propose using a hybrid approach that differentiate between the two types of missing data to yield improved overall imputation accuracy.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {18},
numpages = {15},
keywords = {missing data, data cleaning, Data preprocessing}
}

@article{10.1145/3577017,
author = {Nie, Qi and Malik, Sharad},
title = {CNNFlow: Memory-driven Data Flow Optimization for Convolutional Neural Networks},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/3577017},
doi = {10.1145/3577017},
abstract = {Convolution Neural Networks (CNNs) are widely deployed in computer vision applications. The datasets are large, and the data reuse across different parts is heavily interleaved. Given that memory access (SRAM and especially DRAM) is more expensive in both performance and energy than computation, maximizing data reuse to reduce data movement across the memory hierarchy is critical to improving execution efficiency. This is even more important for the common use case of CNNs on mobile devices where computing/memory resources are limited. We propose CNNFlow, a memory-driven dataflow optimization framework to automatically schedule CNN computation on a given CNN architecture to maximize data reuse at each level of the memory hierarchy. We provide a mathematical calculation for data reuses in terms of parameters including loop ordering, blocking, and memory-bank allocation for tensors in CNN. We then present a series of techniques that help prune the large search space and reduce the cost of the exploration. This provides, for the first time, an exact and practical search algorithm for optimal solutions to minimize memory access cost for CNN. The efficacy is demonstrated for two widely used CNN algorithms: AlexNet and VGG16 with 5 and 13 convolution layers, respectively. CNNFlow finds the optimal solution for each layer within tens of minutes of compute time. Its solution requires about 20\% fewer DRAM accesses and 40\%–80\% fewer SRAM accesses compared to state-of-the-art algorithms in the literature.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = mar,
articleno = {40},
numpages = {36},
keywords = {data scheduling, memory utilization, software-hardware co-design, Convolutional Neural Network, Accelerator}
}

@article{10.1145/3578935,
author = {Yang, Taocun and Huang, Yaping and Xie, Yanlin and Liu, Junbo and Wang, Shengchun},
title = {MixOOD: Improving Out-of-distribution Detection with Enhanced Data Mixup},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1551-6857},
url = {https://doi.org/10.1145/3578935},
doi = {10.1145/3578935},
abstract = {Detecting out-of-distribution (OOD) inputs for deep learning models is a critical task when models are deployed in real-world environments. Recently, a large number of works have been dedicated to tackling the OOD detection problem. One of the most straightforward and effective ways is OOD training, which adds heterogeneous auxiliary data in the training stage. However, the extra auxiliary data cannot be involved arbitrarily. A high-quality and powerful auxiliary dataset must contain samples that belong to OOD but are close to in-distribution (ID), which can teach the model to learn more information about OOD samples, furthermore, distinguish OOD from ID. The key issue for this problem is how to simply acquire such distinctive OOD samples. In this article, we propose an enhanced Mixup-based OOD (MixOOD) detection strategy that can be attached to any threshold-based OOD detecting method. Different from the traditional Mixup designed for ID data augmentation, our proposed MixOOD generates augmented images with deliberately modified Mixup and then uses them as auxiliary OOD data to leverage the OOD detection. We test our method with classical OOD detecting approaches like Maximum Softmax Probability, Energy Score, and Out-of-distribution detector for Neural networks. Experiments show that models with MixOOD can better distinguish in- and out-of-distribution samples than the original version of each approach.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {155},
numpages = {18},
keywords = {machine learning, anomaly detection, data augmentation, mixup, Out-of-distribution detection}
}

@article{10.1145/3579031,
author = {Jentner, Wolfgang and Lindholz, Giuliana and Hauptmann, Hanna and El-Assady, Mennatallah and Ma, Kwan-Liu and Keim, Daniel},
title = {Visual Analytics of Co-Occurrences to Discover Subspaces in Structured Data},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3579031},
doi = {10.1145/3579031},
abstract = {We present an approach that shows all relevant subspaces of categorical data condensed in a single picture. We model the categorical values of the attributes as co-occurrences with data partitions generated from structured data using pattern mining. We show that these co-occurrences are a-priori, allowing us to greatly reduce the search space, effectively generating the condensed picture where conventional approaches filter out several subspaces as these are deemed insignificant. The task of identifying interesting subspaces is common but difficult due to exponential search spaces and the curse of dimensionality. One application of such a task might be identifying a cohort of patients defined by attributes such as gender, age, and diabetes type that share a common patient history, which is modeled as event sequences. Filtering the data by these attributes is common but cumbersome and often does not allow a comparison of subspaces. We contribute a powerful multi-dimensional pattern exploration approach (MDPE-approach) agnostic to the structured data type that models multiple attributes and their characteristics as co-occurrences, allowing the user to identify and compare thousands of subspaces of interest in a single picture. In our MDPE-approach, we introduce two methods to dramatically reduce the search space, outputting only the boundaries of the search space in the form of two tables. We implement the MDPE-approach in an interactive visual interface (MDPE-vis) that provides a scalable, pixel-based visualization design allowing the identification, comparison, and sense-making of subspaces in structured data. Our case studies using a gold-standard dataset and external domain experts confirm our approach’s and implementation’s applicability. A third use case sheds light on the scalability of our approach and a user study with 15 participants underlines its usefulness and power.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {10},
numpages = {49},
keywords = {subspace search, pattern mining, Structured data mining}
}

@article{10.1145/3579467,
author = {Ehsan, Upol and Saha, Koustuv and De Choudhury, Munmun and Riedl, Mark O.},
title = {Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579467},
doi = {10.1145/3579467},
abstract = {Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap-divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {34},
numpages = {32},
keywords = {AI ethics, AI governance, explainable ai, fate, framework, human-AI interaction, human-centered explainable ai, organizational dynamics, participatory design, responsible ai, sociotechnical gap, user study}
}

@article{10.1145/3579480,
author = {Darian, Shiva and Chauhan, Aarjav and Marton, Ricky and Ruppert, Janet and Anderson, Kathleen and Clune, Ryan and Cupchak, Madeline and Gannett, Max and Holton, Joel and Kamas, Elizabeth and Kibozi-Yocka, Jason and Mauro-Gallegos, Devin and Naylor, Simon and O'Malley, Meghan and Patel, Mehul and Sandberg, Jack and Siegler, Troy and Tate, Ryan and Temtim, Abigil and Whaley, Samantha and Voida, Amy},
title = {Enacting Data Feminism in Advocacy Data Work},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579480},
doi = {10.1145/3579480},
abstract = {In this paper, we present the results of a study that examines the role of data in nonprofit advocacy work. We conducted semi-structured interviews with 25 individuals who play critical roles in the data work of 18 different advocacy organizations. Our analysis reveals five key stakeholders in advocacy data work-beneficiaries, policymakers, funding and partner organizations, gatekeepers, and local publics. It also contributes a framework of four functions of data work in nonprofit organizations-data as amplifier, activator, legitimizer, and incubator. We characterize the challenges in data work that exist, particularly in widespread attempts to reappropriate data work across functions. These challenges in reappropriation are often rooted in participants' effects to enact data feminist principles from the margins of the data economy. Finally, we discuss how nonprofit institutions operate outside of the dominant data work goals known as the three Ss (surveillance, selling, and science) and propose a fourth S, social good, that is working to challenge the norms of the data economy and should be considered in research regarding the data economy moving forward.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {47},
numpages = {28},
keywords = {advocacy, data economy, data feminism, data work, nonprofit, social sector, third sector}
}

@article{10.1145/3579511,
author = {Dove, Graham and Shanley, Jack and Matuk, Camillia and Nov, Oded},
title = {Open Data Intermediaries: Motivations, Barriers and Facilitators to Engagement},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579511},
doi = {10.1145/3579511},
abstract = {Open data programs have become increasingly established at national and local levels of government. While the degree of success these programs have had in achieving their objectives remains open to question, one factor that has been identified as important to any success is the role of open data intermediaries, individuals and organizations that help others to make use of open data. In this paper we investigate how people become engaged with open data, what their motivations are, and the barriers and facilitators program participants perceive with regard to using open data effectively. We interview participants from a variety of backgrounds with differing levels of experience and engagement with open data. Participants include students learning how to train others in open data techniques and tools; people who attend open data events and use open data for commercial or social benefit; and representatives from local government, municipal agencies and a civic tech non-profit. We identify pathways to successfully developing and nurturing a community of open data intermediaries, and make five recommendations for organizations planning and managing open data programs.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {78},
numpages = {22},
keywords = {civic tech, open data, open data intermediaries}
}

@article{10.1145/3580367,
author = {Khokhar, Rashid Hussain and Fung, Benjamin C. M. and Iqbal, Farkhund and Al-Hussaeni, Khalil and Hussain, Mohammed},
title = {Differentially Private Release of Heterogeneous Network for Managing Healthcare Data},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3580367},
doi = {10.1145/3580367},
abstract = {With the increasing adoption of digital health platforms through mobile apps and online services, people have greater flexibility connecting with medical practitioners, pharmacists, and laboratories and accessing resources to manage their own health-related concerns. Many healthcare institutions are connecting with each other to facilitate the exchange of healthcare data, with the goal of effective healthcare data management. The contents generated over these platforms are often shared with third parties for a variety of purposes. However, sharing healthcare data comes with the potential risk of exposing patients’ sensitive information to privacy threats. In this article, we address the challenge of sharing healthcare data while protecting patients’ privacy. We first model a complex healthcare dataset using a heterogeneous information network that consists of multi-type entities and their relationships. We then propose DiffHetNet, an edge-based differentially private algorithm, to protect the sensitive links of patients from inbound and outbound attacks in the heterogeneous health network. We evaluate the performance of our proposed method in terms of information utility and efficiency on different types of real-life datasets that can be modeled as networks. Experimental results suggest that DiffHetNet generally yields less information loss and is significantly more efficient in terms of runtime in comparison with existing network anonymization methods. Furthermore, DiffHetNet is scalable to large network datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {90},
numpages = {30},
keywords = {information utility, healthcare data management, differential privacy, Heterogeneous information network}
}

@article{10.1145/3580487,
author = {Smith, Duncan and Elliot, Mark and Sakshaug, Joseph W.},
title = {To Link or Synthesize? An Approach to Data Quality Comparison},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3580487},
doi = {10.1145/3580487},
abstract = {Linking administrative data to produce more informative data for subsequent analysis has become an increasingly common practice. However, there might be concomitant risks of disclosing sensitive information about individuals. One practice that reduces these risks is data synthesis. In data synthesis the data are used to fit a model from which synthetic data are then generated. The synthetic data are then released to end users. There are some scenarios where an end user might have the option of using linked data or accepting synthesized data. However, linkage and synthesis are susceptible to errors that could limit their usefulness. Here, we investigate the problem of comparing the quality of linked data to synthesized data and demonstrate through simulations how the problem might be approached. These comparisons are important when considering how an end user can be supplied with the highest-quality data and in situations where one must consider risk/utility tradeoffs.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {14},
numpages = {20},
keywords = {administrative data, imputation, synthesis, Record linkage}
}

@article{10.1145/3581789,
author = {Zhang, Mimi and Parnell, Andrew},
title = {Review of Clustering Methods for Functional Data},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3581789},
doi = {10.1145/3581789},
abstract = {Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {91},
numpages = {34},
keywords = {shape analysis, multivariate functional data, dependent functional data, Curve registration}
}

@article{10.1145/3582577,
author = {Wang, Ruihang and Cao, Zhiwei and Zhou, Xin and Wen, Yonggang and Tan, Rui},
title = {Green Data Center Cooling Control via Physics-guided Safe Reinforcement Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3582577},
doi = {10.1145/3582577},
abstract = {Deep reinforcement learning (DRL) has shown good performance in tackling Markov decision process (MDP) problems. As DRL optimizes a long-term reward, it is a promising approach to improving the energy efficiency of data-center cooling. However, enforcement of thermal safety constraints during DRL’s state exploration is a main challenge. The widely adopted reward-shaping approach adds negative reward when the exploratory action results in unsafety. Thus, it needs to experience sufficient unsafe states before it learns how to prevent unsafety. In this article, we propose a safety-aware DRL framework for data-center cooling control. It applies offline imitation learning and online post-hoc rectification to holistically prevent thermal unsafety during online DRL. In particular, the post-hoc rectification searches for the minimum modification to the DRL-recommended action such that the rectified action will not result in unsafety. The rectification is designed based on a thermal state transition model that is fitted using historical safe operation traces and able to extrapolate the transitions to unsafe states explored by DRL. Extensive evaluation for chilled water and direct expansion-cooled data centers in two climate conditions show that our approach saves 18\% to 26.6\% of total data-center power compared with conventional control and reduces safety violations by 94.5\% to 99\% compared with reward shaping. We also extend the proposed framework to address data centers with non-uniform temperature distributions for detailed safety considerations. The evaluation shows that our approach saves 14\% power usage compared with the PID control while addressing safety compliance during the training.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = may,
articleno = {19},
numpages = {26},
keywords = {Data center, safe reinforcement learning, energy efficiency, computational fluid dynamics, proper orthogonal decomposition}
}

@article{10.1145/3585005,
author = {Qi, Hua and Wang, Zhijie and Guo, Qing and Chen, Jianlang and Juefei-Xu, Felix and Zhang, Fuyuan and Ma, Lei and Zhao, Jianjun},
title = {ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585005},
doi = {10.1145/3585005},
abstract = {Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons’ status and weights’ gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {129},
numpages = {31},
keywords = {neural architecture search, DNN repair, Deep learning}
}

@article{10.1145/3585514,
author = {Baert, Wouter and Vannieuwenhoven, Nick},
title = {Algorithm&nbsp;1036: ATC, An Advanced Tucker Compression Library for Multidimensional Data},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3585514},
doi = {10.1145/3585514},
abstract = {We present ATC, a C++ library for advanced Tucker-based lossy compression of dense multidimensional numerical data in a shared-memory parallel setting, based on the sequentially truncated higher-order singular value decomposition (ST-HOSVD) and bit plane truncation. Several techniques are proposed to improve speed, memory usage, error control and compression rate. First, a hybrid truncation scheme is described which combines Tucker rank truncation and TTHRESH quantization. We derive a novel expression to approximate the error of truncated Tucker decompositions in the case of core and factor perturbations. We parallelize the quantization and encoding scheme and adjust this phase to improve error control. Implementation aspects are described, such as an ST-HOSVD procedure using only a single transposition. We also discuss several usability features of ATC, including the presence of multiple interfaces, extensive data type support, and integrated downsampling of the decompressed data. Numerical results show that ATC maintains state-of-the-art Tucker compression rates while providing average speed-up factors of 2.2 to 3.5 and halving memory usage. Our compressor provides precise error control, deviating only 1.4\% from the requested error on average. Finally, ATC often achieves higher compression than non-Tucker-based compressors in the high-error domain.},
journal = {ACM Trans. Math. Softw.},
month = jun,
articleno = {21},
numpages = {25},
keywords = {bit plane truncation, ST-HOSVD, Tucker decomposition, tensors, Data compression}
}

@article{10.1145/3585521,
author = {Bucknall, Alex R. and Fahmy, Suhaib A.},
title = {ZyPR: End-to-end Build Tool and Runtime Manager for Partial Reconfiguration of FPGA SoCs at the Edge},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3585521},
doi = {10.1145/3585521},
abstract = {Partial reconfiguration (PR) is a key enabler to the design and development of adaptive systems on modern Field Programmable Gate Array (FPGA) Systems-on-Chip (SoCs), allowing hardware to be adapted dynamically at runtime. Vendor-supported PR infrastructure is performance-limited and blocking, drivers entail complex memory management, and software/hardware design requires bespoke knowledge of the underlying hardware. This article presents ZyPR: a complete end-to-end framework that provides high-performance reconfiguration of hardware from within a software abstraction in the Linux userspace, automating the process of building PR applications with support for the Xilinx Zynq and Zynq UltraScale+ architectures, aimed at enabling non-expert application designers to leverage PR for edge applications. We compare ZyPR against traditional vendor tooling for PR management as well as recent open source tools that support PR under Linux. The framework provides a high-performance runtime along with low overhead for its provided abstractions. We introduce improvements to our previous work, increasing the provisioning throughput for PR bitstreams on the Zynq Ultrascale+ by &nbsp;2\texttimes{} and &nbsp;5.4\texttimes{} compared to Xilinx’s FPGA Manager.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = jun,
articleno = {34},
numpages = {33},
keywords = {adaptive systems, partial reconfiguration, Field programmable gate arrays}
}

@article{10.1145/3588433,
author = {Shahbazi, Nima and Lin, Yin and Asudeh, Abolfazl and Jagadish, H. V.},
title = {Representation Bias in Data: A Survey on Identification and Resolution Techniques},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3588433},
doi = {10.1145/3588433},
abstract = {Data-driven algorithms are only as good as the data they work with, while datasets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons, ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that “bias in, bias out,” one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This article reviews the literature on identifying and resolving representation bias as a feature of a dataset, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple design dimensions and provides a side-by-side comparison of their properties.There is still a long way to fully address representation bias issues in data. The authors hope that this survey motivates researchers to approach these challenges in the future by observing existing work within their respective domains.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {293},
numpages = {39},
keywords = {AI-ready data, data-centric AI, data equity systems, fairness in machine learning, Responsible data science}
}

@article{10.1145/3588434,
author = {Fakas, Georgios J. and Kalamatianos, Georgios},
title = {Proportionality on Spatial Data with Context},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3588434},
doi = {10.1145/3588434},
abstract = {More often than not, spatial objects are associated with some context, in the form of text, descriptive tags (e.g., points of interest, flickr photos), or linked entities in semantic graphs (e.g., Yago2, DBpedia). Hence, location-based retrieval should be extended to consider not only the locations but also the context of the objects, especially when the retrieved objects are too many and the query result is overwhelming. In this article, we study the problem of selecting a subset of the query result, which is the most representative. We argue that objects with similar context and nearby locations should proportionally be represented in the selection. Proportionality dictates the pairwise comparison of all retrieved objects and hence bears a high cost. We propose novel algorithms which greatly reduce the cost of proportional object selection in practice. In addition, we propose pre-processing, pruning, and approximate computation techniques that their combination reduces the computational cost of the algorithms even further. We theoretically analyze the approximation quality of our approaches. Extensive empirical studies on real datasets show that our algorithms are effective and efficient. A user evaluation verifies that proportional selection is more preferable than random selection and selection based on object diversification.},
journal = {ACM Trans. Database Syst.},
month = may,
articleno = {4},
numpages = {37},
keywords = {ranking, spatial data, Ptolemy’s spatial diversity, keyword search, fairness, diversity, Proportionality}
}

@article{10.1145/3588573,
author = {van Rensburg, Bianca Jansen and Puteaux, Pauline and Puech, William and Pedeboy, Jean-Pierre},
title = {3D Object Watermarking from Data Hiding in the Homomorphic Encrypted Domain},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3588573},
doi = {10.1145/3588573},
abstract = {For over a decade, 3D objects are an increasingly popular form of media. It has become necessary and urgent to secure them during their transmission or archiving. In this article, we propose a new method to obtain a watermarked 3D object from high-capacity data hiding in the encrypted domain. Based on the homomorphic properties of the Paillier cryptosystem, our proposed method allows us to embed several secret messages in the encrypted domain with a high-capacity. These messages can be extracted in the plain-text domain after the 3D object decryption. To the best of our knowledge, we are the first to propose a data hiding method in the encrypted domain where the high-capacity watermark is conserved in the plain-text domain after the 3D object is decrypted. The encryption and the data hiding in the encrypted domain are format compliant and without size expansion, despite the use of the Paillier cryptosystem. Each time a new message is embedded in the encrypted domain, flags are added in order to indicate which blocks are still available for the embedding of additional messages. After the decryption of a watermarked encrypted 3D object, our method produces a watermarked 3D object which is visually very similar to the original 3D object. From the decrypted watermarked 3D object, we can then extract all the embedded messages directly in the plain-text domain, without the need for an auxiliary file. Moreover, large keys are used, rending our method secure for real-life applications.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jun,
articleno = {175},
numpages = {20},
keywords = {format compliant, signal processing in the encrypted domain, Paillier homomorphic encryption, 3D object security, high-capacity data hiding, Multimedia security}
}

@article{10.1145/3588685,
author = {Huang, Qiang and Luo, Pingyi and Tung, Anthony K. H.},
title = {A New Sparse Data Clustering Method Based On Frequent Items},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588685},
doi = {10.1145/3588685},
abstract = {Large, sparse categorical data is a natural way to represent complex data like sequences, trees, and graphs. Such data is prevalent in many applications, e.g., Criteo released a terabyte size click log data of 4 billion records with millions of dimensions. While most existing clustering algorithms like k-Means work well on dense, numerical data, there exist relatively few algorithms that can cluster sets of sparse categorical features.In this paper, we propose a new method called k-FreqItems that performs scalable clustering over high-dimensional, sparse data. To make clustering results easily interpretable, k-FreqItems is built upon a novel sparse center representation called FreqItem which will choose a set of high-frequency, non-zero dimensions to represent the cluster. Unlike most existing clustering algorithms, which adopt Euclidean distance as the similarity measure, k-FreqItems uses the popular Jaccard distance for comparing sets.Since the efficiency and effectiveness of k-FreqItems are highly dependent on an initial set of representative seeds, we introduce a new randomized initialization method, SILK, to deal with the seeding problem of k-FreqItems. SILK uses locality-sensitive hash (LSH) functions for oversampling and identifies frequently co-occurred data in LSH buckets to determine a set of promising seeds, allowing k-FreqItems to converge swiftly in an iterative process. Experimental results over seven real-world sparse data sets show that the SILK seeding is around 1.1sim3.2\texttimes{} faster yet more effective than the state-of-the-art seeding methods. Notably, SILK scales up well to a billion data objects on a commodity machine with 4 GPUs. The code is available at https://github.com/HuangQiang/k-FreqItems.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {5},
numpages = {28},
keywords = {Jaccard distance, frequent items, locality-sensitive hashing, minhash, seeding, sparse data clustering}
}

@article{10.1145/3588701,
author = {Ahuja, Ritesh and Zeighami, Sepanta and Ghinita, Gabriel and Shahabi, Cyrus},
title = {A Neural Approach to Spatio-Temporal Data Release with User-Level Differential Privacy},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588701},
doi = {10.1145/3588701},
abstract = {Several "data-for-good" projects [1, 5, 12] initiated by major companies (e.g., Meta, Google) release to the public spatio-temporal datasets to benefit COVID-19 spread modeling [17, 47, 64] and understand human mobility [14, 24]. Most often, spatio-temporal data are provided in the form of snapshot high resolution population density information, where the released statistics capture population counts in small areas for short time periods. Since high resolution is required for utility (e.g., in modeling COVID hotspots) privacy risks are elevated. To prevent malicious actors from using the data to infer sensitive details about individuals, the released datasets must be first sanitized. Typically, [1, 5, 7, 12], differential privacy (DP) is employed as protection model, due to its formal protection guarantees that prevent an adversary to learn whether a particular individual's data has been included in the release or not.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {21},
numpages = {25},
keywords = {differential privacy, neural networks, spatio-temporal data}
}

@article{10.1145/3588919,
author = {Bornemann, Leon and Bleifu\ss{}, Tobias and Kalashnikov, Dmitri V. and Nargesian, Fatemeh and Naumann, Felix and Srivastava, Divesh},
title = {Matching Roles from Temporal Data: Why Joe Biden is not only President, but also Commander-in-Chief},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588919},
doi = {10.1145/3588919},
abstract = {We present role matching, a novel, fine-grained integrity constraint on temporal fact data, i.e., (subject, predicate, object, timestamp)-quadruples. A role is a combination of subject and predicate and can be associated with different objects as the real world evolves and the data changes over time. A role matching states that the associated object of two or more roles should always match across time. Once discovered, role matchings can serve as integrity constraints to improve data quality, for instance of structured data in Wikipedia[3]. If violated, role matchings can alert data owners or editors and thus allow them to correct the error. Finding all role matchings is challenging due both to the inherent quadratic complexity of the matching problem and the need to identify true matches based on the possibly short history of the facts observed so far.To address the first challenge, we introduce several blocking methods both for clean and dirty input data. For the second challenge, the matching stage, we show how the entity resolution method Ditto[27] can be adapted to achieve satisfactory performance for the role matching task. We evaluate our method on datasets from Wikipedia infoboxes, showing that our blocking approaches can achieve 95\% recall, while maintaining a reduction ratio of more than 99.99\%, even in the presence of dirty data. In the matching stage, we achieve a macro F1-score of 89\% on our datasets, using automatically generated labels.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {65},
numpages = {26},
keywords = {blocking, change exploration, data quality, entity matching}
}

@article{10.1145/3588921,
author = {Zhu, Yiwen and Sen, Rathijit and Horton, Robert and Agosta, John Mark},
title = {Runtime Variation in Big Data Analytics},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588921},
doi = {10.1145/3588921},
abstract = {The dynamic nature of resource allocation and runtime conditions on Cloud can result in high variability in a job's runtime across multiple iterations, leading to a poor experience. Identifying the sources of such variation and being able to predict and adjust for them is crucial to cloud service providers to design reliable data processing pipelines, provision and allocate resources, adjust pricing services, meet SLOs and debug performance hazards.In this paper, we analyze the runtime variation of millions of production Scope jobs on Cosmos, an exabyte-scale internal analytics platform at Microsoft. We propose an innovative 2-step approach to predict job runtime distribution by characterizing typical distribution shapes combined with a classification model with an average accuracy of &gt;96\%, using an innovative interpretable machine-learning algorithm out-performing traditional regression models and better capturing long tails. We examine factors such as job plan characteristics and inputs, resource allocation, physical cluster heterogeneity and utilization, and scheduling policies.To the best of our knowledge, this is the first study on predicting categories of runtime distributions for enterprise analytics workloads at scale. Furthermore, we examine how our methods can be used to analyze what-if scenarios, focusing on the impact of resource allocation, scheduling, and physical cluster provisioning decisions on a job's runtime consistency and predictability.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {67},
numpages = {20},
keywords = {big data, clustering, interpretability, predictions, variation}
}

@article{10.1145/3588936,
author = {Luo, Zhaojing and Cai, Shaofeng and Wang, Yatong and Ooi, Beng Chin},
title = {Regularized Pairwise Relationship based Analytics for Structured Data},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588936},
doi = {10.1145/3588936},
abstract = {In line with the increasing machine learning model inference accuracy, deep learning (DL) models have been increasingly applied to structured data for a wide spectrum of real-world applications, including product recommendations, online advertisement, healthcare analytics and risk analysis. However, unlike unstructured data, structured data is high-dimensional and sparse and therefore engenders a large number of parameters in DL, making DL models more prone to overfitting. To alleviate the overfitting problem, various regularization methods have been designed to constrain the model parameters as a means to control the model complexity. Unfortunately, these methods are often restricted to regularizing the parameter values directly without considering the intrinsic correlations and dependencies between attribute fields of structured data which is however key to effective structured data modeling.In this paper, we re-examine DL for structured data from a new perspective of attribute interactions. In particular, we seek to explicitly model and regularize the pairwise relationships between attribute fields of structured data, in a field-adaptive manner, via a proposed attentive and interpretable framework called ATT-Reg. Specifically, in this framework, a set of attentive weight matrices are introduced to each attribute field for modeling obviously different relationships with its neighboring attribute fields. Further, we derive from the Bayesian viewpoint a novel Attentive Regularization method for imposing adaptive regularization strengths on different pairs of attribute fields, based on the informativeness of their relationship, which is calculated using both data-driven information and functional dependency (FD) knowledge. Such adaptive regularization facilitates each attribute field to learn discriminative and diversified representations for more effective predictive analytics. We also develop a feature attribution method for supporting more interpretable predictions We validate the effectiveness of our ATT-Reg on six real-world datasets. Extensive experimental results show that ATT-Reg achieves significant improvement over state-of-the-art graph models, attentive models as well as regularization methods and supports an excellent degree of interpretation.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {82},
numpages = {27},
keywords = {attentive regularization, feature importance, interpretability, pairwise relationship, structured data}
}

@article{10.1145/3588943,
author = {Dar, Chen and Hershcovitch, Moshik and Morrison, Adam},
title = {RLS Side Channels: Investigating Leakage of Row-Level Security Protected Data Through Query Execution Time},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588943},
doi = {10.1145/3588943},
abstract = {Many modern use cases of relational databases involve multi-tenancy. To allow a tenant to only access its data, relational database systems (RDBMSs) introduced row-level security (RLS). RLS enables specifying per-row access controls, which the database enforces by rewriting tenant queries to add an RLS policy filter that filters out rows the tenant is not allowed to view. Unfortunately, while RLS blocks queries from returning unauthorized data, side-effects of query execution can form a side-channel that leaks information about such secret data.This paper investigates how RLS query execution time can leak information about rows that the querying tenant is restricted from viewing. We show that in PostgreSQL and SQL Server, an attacker can craft index-using queries to learn whether a value they are not authorized to view exists in an RLS-protected table, and in some cases, how many times such a value exists in the table. Our attack succeeds in a realistic cloud setting: we successfully attack managed PostgreSQL and SQL Server database instances on AWS from virtual machines in the same and different data centers.To block the RLS time side-channel, we design a data-oblivious query scheme for the case of unique keys. We also analyze the trade-offs created by the data-oblivious approach for non-unique keys.To facilitate the evaluation of RLS attacks and defenses, we introduce a benchmark that supports multi-tenancy and RLS, which are not supported by established benchmarks such as YCSB. We implement our solution in PostgreSQL and show that it achieves security with minimal performance impact.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {89},
numpages = {25},
keywords = {row-level security, time side-channel}
}

@article{10.1145/3588945,
author = {Chen, Sibei and Tang, Nan and Fan, Ju and Yan, Xuemi and Chai, Chengliang and Li, Guoliang and Du, Xiaoyong},
title = {HAIPipe: Combining Human-generated and Machine-generated Pipelines for Data Preparation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588945},
doi = {10.1145/3588945},
abstract = {Data preparation is crucial in achieving optimized results for machine learning (ML). However, having a good data preparation pipeline is highly non-trivial for ML practitioners, which is not only domain-specific, but also dataset-specific. There are two common practices. Human-generated pipelines (HI-pipelines) typically use a wide range of any operations or libraries but are highly experience- and heuristic-based. In contrast, machine-generated pipelines (AI-pipelines), a.k.a. AutoML, often adopt a predefined set of sophisticated operations and are search-based and optimized. These two common practices are mutually complementary. In this paper, we study a new problem that, given an HI-pipeline and an AI-pipeline for the same ML task, can we combine them to get a new pipeline (HAI-pipeline) that is better than the provided HI-pipeline and AI-pipeline? We propose HAIPipe, a framework to address the problem, which adopts an enumeration-sampling strategy to carefully select the best performing combined pipeline. We also introduce a reinforcement learning (RL) based approach to search an optimized AI-pipeline. Extensive experiments using 1400+ real-world HI-pipelines (Jupyter notebooks from Kaggle) verify that HAIPipe can significantly outperform the approaches using either HI-pipelines or AI-pipelines alone.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {91},
numpages = {26},
keywords = {data preparation, pipeline generation, reinforcement learning}
}

@article{10.1145/3588965,
author = {Wang, Zeyu and Wang, Qitong and Wang, Peng and Palpanas, Themis and Wang, Wei},
title = {Dumpy: A Compact and Adaptive Index for Large Data Series Collections},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588965},
doi = {10.1145/3588965},
abstract = {Data series indexes are necessary for managing and analyzing the increasing amounts of data series collections that are nowadays available. These indexes support both exact and approximate similarity search, with approximate search providing high-quality results within milliseconds, which makes it very attractive for certain modern applications. Reducing the pre-processing (i.e., index building) time and improving the accuracy of search results are two major challenges. DSTree and the iSAX index family are state-of-the-art solutions for this problem. However, DSTree suffers from long index building times, while iSAX suffers from low search accuracy. In this paper, we identify two problems of the iSAX index family that adversely affect the overall performance. First, we observe the presence of a proximity-compactness trade-off related to the index structure design (i.e., the node fanout degree), significantly limiting the efficiency and accuracy of the resulting index. Second, a skewed data distribution will negatively affect the performance of iSAX. To overcome these problems, we propose Dumpy, an index that employs a novel multi-ary data structure with an adaptive node splitting algorithm and an efficient building workflow. Furthermore, we devise Dumpy-Fuzzy as a variant of Dumpy which further improves search accuracy by proper duplication of series. Experiments with a variety of large, real datasets demonstrate that the Dumpy solutions achieve considerably better efficiency, scalability and search accuracy than its competitors.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {111},
numpages = {27},
keywords = {data series indexing, similarity search}
}

@article{10.1145/3589186,
author = {Kumar, Puneet and Bhatt, Gaurav and Ingle, Omkar and Goyal, Daksh and Raman, Balasubramanian},
title = {Affective Feedback Synthesis Towards Multimodal Text and Image Data},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3589186},
doi = {10.1145/3589186},
abstract = {In this article, we have defined a novel task of affective feedback synthesis that generates feedback for input text and corresponding images in a way similar to humans responding to multimodal data. A feedback synthesis system has been proposed and trained using ground-truth human comments along with image–text input. We have also constructed a large-scale dataset consisting of images, text, Twitter user comments, and the number of likes for the comments by crawling news articles through Twitter feeds. The proposed system extracts textual features using a transformer-based textual encoder. The visual features have been extracted using a Faster region-based convolutional neural networks model. The textual and visual features have been concatenated to construct multimodal features that the decoder uses to synthesize the feedback. We have compared the results of the proposed system with baseline models using quantitative and qualitative measures. The synthesized feedbacks have been analyzed using automatic and human evaluation. They have been found to be semantically similar to the ground-truth comments and relevant to the given text–image input.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {190},
numpages = {23},
keywords = {context vector, dataset construction, multimodal input, feedback synthesis, Affective computing}
}

@article{10.1145/3589263,
author = {Kuschewski, Maximilian and Sauerwein, David and Alhomssi, Adnan and Leis, Viktor},
title = {BtrBlocks: Efficient Columnar Compression for Data Lakes},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589263},
doi = {10.1145/3589263},
abstract = {Analytics is moving to the cloud and data is moving into data lakes. These reside on object storage services like S3 and enable seamless data sharing and system interoperability. To support this, many systems build on open storage formats like Apache Parquet. However, these formats are not optimized for remotely-accessed data lakes and today's high-throughput networks. Inefficient decompression makes scans CPU-bound and thus increases query time and cost. With this work we present BtrBlocks, an open columnar storage format designed for data lakes. BtrBlocks uses a set of lightweight encoding schemes, achieving fast and efficient decompression and high compression ratios.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {118},
numpages = {26},
keywords = {columnar storage, compression, data lake, query processing}
}

@article{10.1145/3589265,
author = {Fathollahzadeh, Saeed and Boehm, Matthias},
title = {GIO: Generating Efficient Matrix and Frame Readers for Custom Data Formats by Example},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589265},
doi = {10.1145/3589265},
abstract = {Data Scientists deal with a wide variety of file data formats and data representations. Probably the most difficult to handle are custom data formats that liberally define their own particular flat or nested structure with multiple custom delimiters, multi-line records, or undocumented semantics of attribute sequences, co-appearances, and repetitions. As a prerequisite for exploratory ML model training, data scientists need to map these data representations into regular frames or matrices. Unfortunately, existing tools and frameworks provide only limited support for aiding this process, which causes redundant manual efforts and unnecessary data quality issues. In this paper, we initiate work on automatic matrix and frame reader generation by example. A user provides a sample of raw text data and its mapped matrix or frame representation. Our GIO framework then first identifies the mapping rules from raw to structured data, and subsequently generates source code of an efficient, multi-threaded reader for reading full raw datasets of this format. In order to facilitate manual improvements, both the mapping rules, and generated reader can be modified as needed. Our experiments show that GIO is able to correctly identify the mapping rules for basic text formats like CSV, LibSVM, MatrixMarket; custom text formats from publishing, automotive, and health care; as well as various nested formats such as JSON and XML. Additionally, the automatically generated readers yield competitive performance compared to hand-coded readers and tuned libraries like RapidJSON.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {120},
numpages = {26},
keywords = {custom data format, data loading, efficient readers, raw data}
}

@article{10.1145/3589273,
author = {Grafberger, Stefan and Groth, Paul and Schelter, Sebastian},
title = {Automating and Optimizing Data-Centric What-If Analyses on Native Machine Learning Pipelines},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589273},
doi = {10.1145/3589273},
abstract = {Software systems that learn from data with machine learning (ML) are used in critical decision-making processes. Unfortunately, real-world experience shows that the pipelines for data preparation, feature encoding and model training in ML systems are often brittle with respect to their input data. As a consequence, data scientists have to run different kinds of data centric what-if analyses to evaluate the robustness and reliability of such pipelines, e.g., with respect to data errors or preprocessing techniques. These what-if analyses follow a common pattern: they take an existing ML pipeline, create a pipeline variant by introducing a small change, and execute this pipeline variant to see how the change impacts the pipeline's output score. The application of existing analysis techniques to ML pipelines is technically challenging as they are hard to integrate into existing pipeline code and their execution introduces large overheads due to repeated work.We propose mlwhatif to address these integration and efficiency challenges for data-centric what-if analyses on ML pipelines. mlwhatif enables data scientists to declaratively specify what-if analyses for an ML pipeline, and to automatically generate, optimize and execute the required pipeline variants. Our approach employs pipeline patches to specify changes to the data, operators and models of a pipeline. Based on these patches, we define a multi-query optimizer for efficiently executing the resulting pipeline variants jointly, with four subsumption-based optimization rules. Subsequently, we detail how to implement the pipeline variant generation and optimizer of mlwhatif. For that, we instrument native ML pipelines written in Python to extract dataflow plans with re-executable operators.We experimentally evaluate mlwhatif, and find that its speedup scales linearly with the number of pipeline variants in applicable cases, and is invariant to the input data size. In end-to-end experiments with four analyses on more than 60 pipelines, we show speedups of up to 13x compared to sequential execution, and find that the speedup is invariant to the model and featurization in the pipeline. Furthermore, we confirm the low instrumentation overhead of mlwhatif.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {128},
numpages = {26},
keywords = {data preparation for machine learning, data-centric ai, machine learning pipelines}
}

@article{10.1145/3589287,
author = {Cai, Kuntai and Xiao, Xiaokui and Cormode, Graham},
title = {PrivLava: Synthesizing Relational Data with Foreign Keys under Differential Privacy},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589287},
doi = {10.1145/3589287},
abstract = {Answering database queries while preserving privacy is an important problem that has attracted considerable research attention in recent years. A canonical approach to this problem is to use synthetic data. That is, we replace the input database R with a synthetic database R* that preserves the characteristics of R, and use R* to answer queries. Existing solutions for relational data synthesis, however, either fail to provide strong privacy protection, or assume that R contains a single relation. In addition, it is challenging to extend the existing single-relation solutions to the case of multiple relations, because they are unable to model the complex correlations induced by the foreign keys. Therefore, multi-relational data synthesis with strong privacy guarantees is an open problem.In this paper, we address the above open problem by proposing PrivLava, the first solution for synthesizing relational data with foreign keys under differential privacy, a rigorous privacy framework widely adopted in both academia and industry. The key idea of PrivLava is to model the data distribution in R using graphical models, with latent variables included to capture the inter-relational correlations caused by foreign keys. We show that PrivLava supports arbitrary foreign key references that form a directed acyclic graph, and is able to tackle the common case when R contains a mixture of public and private relations. Extensive experiments on census data sets and the TPC-H benchmark demonstrate that PrivLava significantly outperforms its competitors in terms of the accuracy of aggregate queries processed on the synthetic data.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {142},
numpages = {25},
keywords = {data synthesis, differential privacy}
}

@article{10.1145/3589301,
author = {Ma, Pingchuan and Ding, Rui and Wang, Shuai and Han, Shi and Zhang, Dongmei},
title = {XInsight: eXplainable Data Analysis Through The Lens of Causality},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589301},
doi = {10.1145/3589301},
abstract = {In light of the growing popularity of Exploratory Data Analysis (EDA), understanding the underlying causes of the knowledge acquired by EDA is crucial. However, it remains under-researched. This study promotes a transparent and explicable perspective on data analysis, called eXplainable Data Analysis (XDA). For this reason, we present XInsight, a general framework for XDA. XInsight provides data analysis with qualitative and quantitative explanations of causal and non-causal semantics. This way, it will significantly improve human understanding and confidence in the outcomes of data analysis, facilitating accurate data interpretation and decision making in the real world. XInsight is a three-module, end-to-end pipeline designed to extract causal graphs, translate causal primitives into XDA semantics, and quantify the quantitative contribution of each explanation to a data fact. XInsight uses a set of design concepts and optimizations to address the inherent difficulties associated with integrating causality into XDA. Experiments on synthetic and real-world datasets as well as a user study demonstrate the highly promising capabilities of XInsight.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {156},
numpages = {27},
keywords = {bayesian network, data analysis, data management}
}

@article{10.1145/3589306,
author = {Bian, Haoqiong and Sha, Tiannan and Ailamaki, Anastasia},
title = {Using Cloud Functions as Accelerator for Elastic Data Analytics},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589306},
doi = {10.1145/3589306},
abstract = {Cloud function (CF) services, such as AWS Lambda, have been applied as the new computing infrastructure in implementing analytical query engines. For bursty and sparse workloads, CF-based query engine is more elastic than the traditional query engines running in servers, i.e., virtual machines (VMs), and might provide a higher performance/price ratio. However, it is still controversial whether CF services are good suites for general analytical workloads, in respect of the limitations of CFs in storage, network, and lifetime, as well as the much higher resource unit prices than VMs.In this paper, we first present micro-benchmark evaluations of the features of CF and VM. We reveal that for query processing, though CF is more elastic than VM, it is less scalable and is more expensive for continuous workloads. Then, to get the best of both worlds, we propose Pixels-Turbo - a hybrid query engine that processes queries in a scalable VM cluster by default and invokes CFs to accelerate the processing of unpredictable workload spikes. In the query engine, we propose several optimizations to improve the performance and scalability of the CF-based operators and a cost-based optimizer to select the appropriate algorithm and parallelism for the physical query plan. Evaluations on TPC-H and real-world workload show that our query engine has a 1-2 orders of magnitude higher performance/price ratio than state-of-the-art serverless query engines for sustained workloads while not compromising the elasticity for workload spikes.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {161},
numpages = {27},
keywords = {FAAS, OLAP, QAAS, cloud databases, cloud function, cloud storage, column store, cost efficiency, data lake, data warehouse, elasticity, query optimization, query processing, serverless}
}

@article{10.1145/3589307,
author = {Wang, Xin and Wang, Zhengru and Wu, Zhenyu and Zhang, Shuhao and Shi, Xuanhua and Lu, Li},
title = {Data Stream Clustering: An In-depth Empirical Study},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589307},
doi = {10.1145/3589307},
abstract = {Data Stream Clustering (DSC) plays an important role in mining continuous and unlabeled data streams in real-world applications. Over the last decades, numerous DSC algorithms have been proposed with promising clustering accuracy and efficiency. Despite the significant differences among existing DSC algorithms, they are commonly built around four key design aspects: summarizing data structure, window model, outlier detection mechanism, and offline refinement strategy. However, there is a lack of empirical studies on these key design aspects in the same codebase using real-world workloads with distinct characteristics. As a result, it is difficult for researchers to improve upon the state-of-the-art. In this paper, we conduct such a study of DSC on its four key design aspects. We implemented state-of-the-art variants of all of these design choices in an open-sourced platform from scratch and evaluated them using both real-world and synthetic workloads. Our analysis identifies the fundamental issues and trade-offs of each design choice in terms of both accuracy and efficiency. We even find that combining flexible design choices led to the development of a new algorithm called Benne, which can be tuned to achieve either better accuracy or better efficiency compared to the state-of-the-art.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {162},
numpages = {26},
keywords = {clustering algorithm, data stream, empirical study}
}

@article{10.1145/3589317,
author = {Castro Fernandez, Raul},
title = {Data-Sharing Markets: Model, Protocol, and Algorithms to Incentivize the Formation of Data-Sharing Consortia},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589317},
doi = {10.1145/3589317},
abstract = {Organizations that would mutually benefit from pooling their data are otherwise wary of sharing. This is because sharing data is costly-in time and effort-and, at the same time, the benefits of sharing are not clear. Without a clear cost-benefit analysis, participants default in not sharing. As a consequence, many opportunities to create valuable data-sharing consortia never materialize, and the value of data remains locked.We introduce a new sharing model, market protocol, and algorithms to incentivize the creation of data-sharing markets. The combined contributions of this paper, which we call DSC, incentivize the creation of data-sharing markets that unleash the value of data for its participants. The sharing model introduces two incentives; one that guarantees that participating is better than not doing so and another that compensates participants according to how valuable their data is. Because operating the consortia is costly, we are also concerned with ensuring its operation is sustainable: we design a protocol that ensures that a valuable data-sharing consortium forms when it is sustainable.We introduce algorithms to elicit the value of data from the participants, which is used first to cover the costs of operating the consortia and second to compensate for data contributions. For the latter, we challenge using the Shapley value to allocate revenue. We offer analytical and empirical evidence for this and introduce an alternative method that compensates participants better and leads to the formation of data-sharing consortia.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {172},
numpages = {25},
keywords = {data markets, data sharing, incentives, machine learning sharing}
}

@article{10.1145/3589770,
author = {Xu, Zhichen and Gao, Ying and Davidson, Andrew},
title = {Keep Your Distributed Data Warehouse Consistent at a Minimal Cost},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589770},
doi = {10.1145/3589770},
abstract = {Large data warehouses store interdependent tables that are updated independently in response to business logic changes or late arrival of critical data. To keep the warehouse consistent, changes to upstream tables need to be propagated to downstream tables in a timely fashion. However, a naive change propagation algorithm can cause many unnecessary updates or recalculations of downstream tables, which drives up the cost of data warehouse management.In this paper, we describe our solution that can ensure the eventual consistency of the data warehouse while avoiding unnecessary table updates. We also show that the optimal trade-off between computational cost reduction and meeting data freshness constraints can be found by solving a dynamic programming problem.The proposed solution is currently in production to manage the YouTube Data Warehouse and has reduced update requests by 25\% by eliminating non-trivial duplicates. These requests would have been carried out by large batch jobs over big data. Eliminating them has led to a proportionate reduction in computing resources.One key advantage of our approach is that it can be used in a heterogeneous, distributed data warehouse environment where the operator software may not have complete control over the query processors. This is because our approach only relies on having dependency information for tables and can operate on the post-state of data sources.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {190},
numpages = {25},
keywords = {consistency management, data warehouse}
}

@article{10.1145/3589772,
author = {Dong, Siying and P, Shiva Shankar and Pan, Satadru and Ananthabhotla, Anand and Ekambaram, Dhanabal and Sharma, Abhinav and Dayal, Shobhit and Parikh, Nishant Vinaybhai and Jin, Yanqin and Kim, Albert and Patil, Sushil and Zhuang, Jay and Dunster, Sam and Mahajan, Akanksha and Chelluri, Anirudh and Datye, Chaitanya and Santana, Lucas Vasconcelos and Garg, Nitin and Gawde, Omkar},
title = {Disaggregating RocksDB: A Production Experience},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589772},
doi = {10.1145/3589772},
abstract = {As in the general industry, there is a trend in Meta's data centers to migrate data from locally attached SSDs to cloud storage. We extended RocksDB [26], a widely used open-source storage engine designed and built for local SSDs, to leverage disaggregated storage. RocksDB's design, such as its data and log files' access patterns, makes an append-only distributed file system a desirable underlying storage. At Meta, we built disaggregated RocksDB using Tectonic File System [35], which so far had mainly been used for our data warehouse and blob storage stacks. We identified that metadata overhead and tail latencies were Tectonic's major performance gaps and addressed them accordingly. We improved the reliability, performance and other requirements with both general and customized optimizations to the core engine in RocksDB. We also took the time to deeply understand the common challenges presented by applications running on RocksDB and implemented enhancements to address them. This architecture enabled RocksDB to adapt to a more distributed architecture for performance enhancements.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {192},
numpages = {24},
keywords = {RocksDB, disaggregated storage, distributed file system, log-structured merge-tree}
}

@article{10.1145/3589773,
author = {Zhao, Hanyu and Yang, Zhi and Cheng, Yu and Tian, Chao and Ren, Shiru and Xiao, Wencong and Yuan, Man and Chen, Langshi and Liu, Kaibo and Zhang, Yang and Li, Yong and Lin, Wei},
title = {GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines for Deep Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589773},
doi = {10.1145/3589773},
abstract = {Training data pre-processing pipelines are essential to deep learning (DL). As the performance of model training keeps increasing with both hardware advancements (e.g., faster GPUs) and various software optimizations, the data pre-processing on CPUs is becoming more resource-intensive and a severe bottleneck of the pipeline. This problem is even worse in the cloud, where training jobs exhibit diverse CPU-GPU demands that usually result in mismatches with fixed hardware configurations and resource fragmentation, degrading both training performance and cluster utilization.We introduce GoldMiner, an input data processing service for stateless operations used in pre-processing data for DL model training. GoldMiner decouples data pre-processing from model training into a new role called the data worker. Data workers facilitate scaling of data pre-processing to anywhere in a cluster, effectively pooling the resources across the cluster to satisfy the diverse requirements of training jobs. GoldMiner achieves this decoupling in a fully automatic and elastic manner. The key insight is that data pre-processing is inherently stateless, thus can be executed independently and elastically. This insight guides GoldMiner to automatically extract stateless computation out of a monolithic training program, efficiently disaggregate it across data workers, and elastically scale data workers to tune the resource allocations across jobs to optimize cluster efficiency. We have applied GoldMiner to industrial workloads, and our evaluation shows that GoldMiner can transform unmodified training programs to use data workers, accelerating individual training jobs by up to 12.1x. GoldMiner also improves average job completion time and aggregate GPU utilization by up to 2.5x and 2.1x in a 64-GPU cluster, respectively, by scheduling data workers with elasticity.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {193},
numpages = {25},
keywords = {data pre-processing, deep learning, disaggregation, scheduling}
}

@article{10.1145/3589786,
author = {Langenecker, Sven and Sturm, Christoph and Schalles, Christian Schalles and Binnig, Carsten},
title = {Steered Training Data Generation for Learned Semantic Type Detection},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589786},
doi = {10.1145/3589786},
abstract = {In this paper, we introduce STEER to adapt learned semantic type extraction approaches to a new, unseen data lake. STEER provides a data programming framework for semantic labeling which is used to generate new labeled training data with minimal overhead. At its core, STEER comes with a novel training data generation procedure called Steered-Labeling that can generate high quality training data not only for non-numeric but also for numerical columns. With this generated training data STEER is able to fine-tune existing learned semantic type extraction models. We evaluate our approach on four different data lakes and show that we can significantly improve the performance of two different types of learned models across all data lakes.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {201},
numpages = {25},
keywords = {data discovery, data lakes, semantic type detection}
}

@article{10.1145/3589958,
author = {Mayer, Peter and Zou, Yixin and Lowens, Byron M. and Dyer, Hunter A. and Le, Khue and Schaub, Florian and Aviv, Adam J.},
title = {Awareness, Intention, (In)Action: Individuals’ Reactions to Data Breaches},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3589958},
doi = {10.1145/3589958},
abstract = {Data breaches are prevalent. We provide novel insights into individuals’ awareness, perception, and responses to breaches that affect them through two online surveys: a main survey (n = 413) in which we presented participants with up to three breaches that affected them, and a follow-up survey (n = 108) in which we investigated whether the main study participants followed through with their intentions to act. Overall, 73\% of participants were affected by at least one breach, but participants were unaware of 74\% of breaches affecting them. Although some reported intention to take action, most participants believed the breach would not impact them. We also found a sizable intention-behavior gap. Participants did not follow through with their intention when they were apathetic about breaches, considered potential costs, forgot, or felt resigned about taking action. Our findings suggest that breached organizations should be held accountable for more proactively informing and protecting affected consumers.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {77},
numpages = {53},
keywords = {behavior, emotion, awareness, perception, security, privacy, Data breach}
}

@article{10.1145/3591359,
author = {Ao, Jing and Cheng, Zehui and Chirkova, Rada and Kolaitis, Phokion G.},
title = {Theory and Practice of Relational-to-RDF Temporal Data Exchange and Query Answering},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3591359},
doi = {10.1145/3591359},
abstract = {We consider the problem of answering temporal queries on RDF stores, in presence of atemporal RDFS domain ontologies, of relational data sources that include temporal information, and of rules that map the domain information in the source schemas into the target ontology. Our proposed practice-oriented solution consists of two rule-based domain-independent algorithms. The first algorithm materializes target RDF data via a version of data exchange that enriches both the data and the ontology with temporal information from the relational sources. The second algorithm accepts as inputs temporal queries expressed in terms of the domain ontology using a lightweight temporal extension of SPARQL, and ensures successful evaluation of the queries on the materialized temporally-enriched RDF data. To study the quality of the information generated by the algorithms, we develop a general framework that formalizes the relational-to-RDF temporal data-exchange problem. The framework includes a chase formalism and a formal solution for the problem of answering temporal queries in the context of relational-to-RDF temporal data exchange. In this article, we present the algorithms and the formal framework that proves correctness of the information output by the algorithms, and also report on the algorithm implementation and experimental results for two application domains.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {15},
numpages = {27},
keywords = {RDF / RDFS / SPARQL, information quality in temporal data exchange, theory of temporal databases, Data-intensive sciences and databases}
}

@article{10.1145/3592534,
author = {Simard, Vanessa and R\"{o}nnqvist, Mikael and Lebel, Luc and Lehoux, Nadia},
title = {A Method to Classify Data Quality for Decision Making Under Uncertainty},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3592534},
doi = {10.1145/3592534},
abstract = {Every decision-making process is subject to a certain degree of uncertainty. In sectors where the outcomes of the operations planned are uncertain and difficult to control such as in forestry, data describing the available resources can have a large impact on productivity. When planning activities, it is often assumed that such data are accurate, which causes a need for more replanning efforts. Data verification is kept to a minimum even though using erroneous information increases the level of uncertainty. In this context, it is relevant to develop a process to evaluate whether the data used for planning decisions are appropriate, so as to ensure the decision validity and provide information for better understanding and actions. However, the level of data quality alone can sometimes be difficult to interpret and needs to be put into perspective. This article proposes an extension to most data quality assessment techniques by comparing data to past quality levels. A classification method is proposed to evaluate the level of data quality in order to support decision making. Such classification provides insights into the level of uncertainty associated with the data. The method developed is then exploited using a theoretical case based on the literature and a practical case based on the forest sector. An example of how classified data quality can improve decisions in a transportation problem is finally shown.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {17},
numpages = {27},
keywords = {forest industry, decision-making process, uncertainty, Data quality}
}

@article{10.1145/3592615,
author = {Iqbal, Farkhund and Abbasi, Ahmed and Javed, Abdul Rehman and Almadhor, Ahmad and Jalil, Zunera and Anwar, Sajid and Rida, Imad},
title = {Data Augmentation-based Novel Deep Learning Method for Deepfaked Images Detection},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {11},
issn = {1551-6857},
url = {https://doi.org/10.1145/3592615},
doi = {10.1145/3592615},
abstract = {Recent advances in artificial intelligence have led to deepfake images, enabling users to replace a real face with a genuine one. deepfake images have recently been used to malign public figures, politicians, and even average citizens. deepfake but realistic images have been used to stir political dissatisfaction, blackmail, propagate false news, and even carry out bogus terrorist attacks. Thus, identifying real images from fakes has got more challenging. To avoid these issues, this study employs transfer learning and data augmentation technique to classify deepfake images. For experimentation, 190,335 RGB-resolution deepfake and real images and image augmentation methods are used to prepare the dataset. The experiments use the deep learning models: convolutional neural network (CNN), Inception V3, visual geometry group (VGG19), and VGG16 with a transfer learning approach. Essential evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix, and AUC-ROC curve score) are used to test the efficacy of the proposed approach. Results revealed that the proposed approach achieves an accuracy, recall, F1-score and AUC-ROC score of 90\% and 91\% precision, with our fine-tuned VGG16 model outperforming other DL models in recognizing real and deepfakes.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {339},
numpages = {15},
keywords = {Deepfake detection, data augmentation, image processing, deep learning, artificial intelligence, transfer learning}
}

@article{10.1145/3592616,
author = {Priestley, Maria and O’donnell, Fionnt\'{a}n and Simperl, Elena},
title = {A Survey of Data Quality Requirements That Matter in ML Development Pipelines},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3592616},
doi = {10.1145/3592616},
abstract = {The fitness of the systems in which Machine Learning (ML) is used depends greatly on good-quality data. Specifications on what makes a good-quality dataset have traditionally been defined by the needs of the data users—typically analysts and engineers. Our article critically examines the extent to which established data quality frameworks are applicable to contemporary use cases in ML. Using a review of recent literature at the intersection of ML, data management, and human-computer interaction, we find that the classical “fitness-for-use” view of data quality can benefit from a more stage-specific approach that is sensitive to where in the ML lifecycle the data are encountered. This helps practitioners to plan their data quality tasks in a manner that meets the needs of the stakeholders who will encounter the dataset, whether it be data subjects, software developers or organisations. We therefore propose a new treatment of traditional data quality criteria by structuring them according to two dimensions: (1) the stage of the ML lifecycle where the use case occurs vs. (2) the main categories of data quality that can be pursued (intrinsic, contextual, representational and accessibility). To illustrate how this works in practice, we contribute a temporal mapping of the various data quality requirements that are important at different stages of the ML data pipeline. We also share some implications for data practitioners and organisations that wish to enhance their data management routines in preparation for ML.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {11},
numpages = {39},
keywords = {data innovation, data management, data ecosystems, machine learning, Data quality}
}

@article{10.1145/3593043,
author = {Goknil, Arda and Nguyen, Phu and Sen, Sagar and Politaki, Dimitra and Niavis, Harris and Pedersen, Karl John and Suyuthi, Abdillah and Anand, Abhilash and Ziegenbein, Amina},
title = {A Systematic Review of Data Quality in CPS and IoT for Industry 4.0},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3593043},
doi = {10.1145/3593043},
abstract = {The Internet of Things (IoT) and Cyber-Physical Systems (CPS) are the backbones of Industry 4.0, where data quality is crucial for decision support. Data quality in these systems can deteriorate due to sensor failures or uncertain operating environments. Our objective is to summarize and assess the research efforts that address data quality in data-centric CPS/IoT industrial applications. We systematically review the state-of-the-art data quality techniques for CPS and IoT in Industry 4.0 through a systematic literature review (SLR) study. We pose three research questions, define selection and exclusion criteria for primary studies, and extract and synthesize data from these studies to answer our research questions. Our most significant results are (i) the list of data quality issues, their sources, and application domains, (ii) the best practices and metrics for managing data quality, (iii) the software engineering solutions employed to manage data quality, and (iv) the state of the data quality techniques (data repair, cleaning, and monitoring) in the application domains. The results of our SLR can help researchers obtain an overview of existing data quality issues, techniques, metrics, and best practices. We suggest research directions that require attention from the research community for follow-up work.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {327},
numpages = {38},
keywords = {systematic review, Industry 4.0, CPS, IoT, Data quality}
}

@article{10.1145/3593294,
author = {Pan, Bofeng and Stakhanova, Natalia and Ray, Suprio},
title = {Data Provenance in Security and Privacy},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3593294},
doi = {10.1145/3593294},
abstract = {Provenance information corresponds to essential metadata that describes the entities, users, and processes involved in the history and evolution of a data object. The benefits of tracking provenance information have been widely understood in a variety of domains; however, only recently have provenance solutions gained interest in the security community. Indeed, on the one hand, provenance allows for a reliable historical analysis enabling security-related applications such as forensic analysis and attribution of malicious activity. On the other hand, the unprecedented changes in the threat landscape place demands for securing provenance information to facilitate its trustworthiness. With the recent growth of provenance studies in security, in this work we examine the role of data provenance in security and privacy. To set this work in context, we outline fundamental principles and models of data provenance and explore how the existing studies achieve security principles. We further review the existing schemes for securing data provenance collection and manipulation known as secure provenance and the role of data provenance for security and privacy, which we refer to as threat provenance.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {323},
numpages = {35},
keywords = {threat provenance, secure provenance, privacy, security, Data provenance}
}

@article{10.1145/3594723,
author = {Koho, Mikko and Coladangelo, L. P. and Ransom, Lynn and Emery, Doug},
title = {Wikibase Model for Premodern Manuscript Metadata Harmonization, Linked Data Integration, and Discovery},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594723},
doi = {10.1145/3594723},
abstract = {To facilitate discovery of premodern manuscripts in U.S. memory institutions, Digital Scriptorium, a growing consortium of over 35 institutional members representing American libraries, museums, and other cultural heritage institutions, has developed a digital platform for an online national union catalog. The platform will allow low-barrier and efficient collection, aggregation, and enrichment of member metadata and sustainably publish it as Linked Open Data. This article describes the methods and principles behind the data model development and the decision to use Wikibase. The results of the prototype implementation and testing phase demonstrate the practicality and sustainability of Digital Scriptorium’s approach to building an online national union catalog based on Linked Open Data technologies and practices.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {56},
numpages = {25},
keywords = {semantic web, cultural heritage, digital humanities, premodern manuscripts, data interoperability, Wikibase, data modeling, Linked Open Data}
}

@article{10.1145/3595286,
author = {Albers, Susanne and Quedenfeld, Jens},
title = {Algorithms for Right-sizing Heterogeneous Data Centers},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/3595286},
doi = {10.1145/3595286},
abstract = {Power consumption is a dominant and still growing cost factor in data centers. In time periods with low load, the energy consumption can be reduced by powering down unused servers. We resort to a model introduced by Lin, Wierman, Andrew, and Thereska [23, 24] that considers data centers with identical machines and generalize it to heterogeneous data centers with d different server types. The operating cost of a server depends on its load and is modeled by an increasing, convex function for each server type. In contrast to earlier work, we consider the discrete setting, where the number of active servers must be integral. Thereby, we seek truly feasible solutions. For homogeneous data centers (d=1), both the offline and the online problem were solved optimally in References [3, 4].In this article, we study heterogeneous data centers with general time-dependent operating cost functions. We develop an online algorithm based on a work function approach that achieves a competitive ratio of 2d + 1 + ε for any ε &gt; 0. For time-independent operating cost functions, the competitive ratio can be reduced to 2d + 1. There is a lower bound of 2d shown in Reference&nbsp;[5], so our algorithm is nearly optimal. For the offline version, we give a graph-based (1+ε)-approximation algorithm. Additionally, our offline algorithm is able to handle time-variable data-center sizes.},
journal = {ACM Trans. Parallel Comput.},
month = dec,
articleno = {20},
numpages = {28},
keywords = {discrete setting, approximation algorithm, competitive analysis, online algorithm, energy conservation, Heterogeneous machines}
}

@article{10.1145/3596261,
author = {Santhalingam, Panneer Selvam and Pathak, Parth and Rangwala, Huzefa and Kosecka, Jana},
title = {Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596261},
doi = {10.1145/3596261},
abstract = {The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we "translate" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {74},
numpages = {34},
keywords = {Accessible computing, Data collection, IMU, Sign language recognition, Wearables}
}

@article{10.1145/3597305,
author = {Bono, Carlo A. and Cappiello, Cinzia and Pernici, Barbara and Ramalli, Edoardo and Vitali, Monica},
title = {Pipeline Design for Data Preparation for Social Media Analysis},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597305},
doi = {10.1145/3597305},
abstract = {In a data-driven culture, in which analytics applications are the main resources for supporting decision-making, the use of high-quality datasets is mandatory to minimize errors and risks. For this reason, data analysis tasks need to be preceded by a data preparation pipeline. The design of such a pipeline is not trivial: the data analyst must carefully choose the appropriate operations considering several aspects. This is often performed by adopting a trial-and-error approach that does not always lead to the most effective solution. In addition, extracting information from social media poses specific problems due to the need to consider only posts relevant for the analysis, for its dependence from the context being considered, for its multimedia contents, and for the risk of filtering out informative posts with automatic filters. In this article, we propose a systematic approach to support the design of pipelines that are able to effectively extract a relevant dataset for the goal of the analysis of data from social media. We provide a conceptual model for designing and annotating the data preparation pipeline with quality and performance information, thus providing the data analyst preliminary information on the expected quality of the resulting dataset in a context-aware manner. The generation of metadata related to the processing tasks has been recognized as essential for enabling data sharing and reusability. To this aim, the dataset resulting from the pipeline application is automatically annotated with provenance metadata to get a detailed description of all the activities performed by the pipeline on them. As a case study, we consider the design of a pipeline for creating datasets of images extracted from social media in order to analyze behavioural aspects during COVID-19.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {42},
numpages = {25},
keywords = {data provenance, human-in-the-loop processes, Data preparation pipeline}
}

@article{10.1145/3597310,
author = {Irrera, Ornella and Mannocci, Andrea and Manghi, Paolo and Silvello, Gianmaria},
title = {A Novel Curated Scholarly Graph Connecting Textual and Data Publications},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597310},
doi = {10.1145/3597310},
abstract = {In the last decade, scholarly graphs became fundamental to storing and managing scholarly knowledge in a structured and machine-readable way. Methods and tools for discovery and impact assessment of science rely on such graphs and their quality to serve scientists, policymakers, and publishers. Since research data became very important in scholarly communication, scholarly graphs started including dataset metadata and their relationships to publications. Such graphs are the foundations for Open Science investigations, data-article publishing workflows, discovery, and assessment indicators. However, due to the heterogeneity of practices (FAIRness is indeed in the making), they often lack the complete and reliable metadata necessary to perform accurate data analysis; e.g., dataset metadata is inaccurate, author names are not uniform, and the semantics of the relationships is unknown, ambiguous or incomplete.This work describes an open and curated scholarly graph we built and published as a training and test set for data discovery, data connection, author disambiguation, and link prediction tasks. Overall the graph contains 4,047 publications, 5,488 datasets, 22 software, 21,561 authors; 9,692 edges interconnect publications to datasets and software and are labeled with semantics that outline whether a publication is citing, referencing, documenting, supplementing another product.To ensure high-quality metadata and semantics, we relied on the information extracted from PDFs of the publications and the datasets and software webpages to curate and enrich nodes metadata and edges semantics. To the best of our knowledge, this is the first ever published resource, including publications and datasets with manually validated and curated metadata.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {26},
numpages = {24},
keywords = {open science, datasets, data enrichment, data curation, Scholarly knowledge graphs}
}

@article{10.1145/3600097,
author = {Rendle, Steffen and Zhang, Li},
title = {On Reducing User Interaction Data for Personalization},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3600097},
doi = {10.1145/3600097},
abstract = {Most recommender systems rely on user interaction data for personalization. Usually, the recommendation quality improves with more data. In this work, we study the quality implications when limiting user interaction data for personalization purposes. We formalize this problem and provide algorithms for selecting a smaller subset of user interaction data. We propose a selection method that picks the subset of a user’s history items that maximizes the expected recommendation quality. We show on well-studied benchmarks that it is possible to achieve high-quality results with small subsets of less than 10 items per user.},
journal = {ACM Trans. Recomm. Syst.},
month = aug,
articleno = {14},
numpages = {28},
keywords = {data efficiency, Item recommendation}
}

@article{10.1145/3603708,
author = {Krasikov, Pavel and Legner, Christine},
title = {A Method to Screen, Assess, and Prepare Open Data for Use},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603708},
doi = {10.1145/3603708},
abstract = {Open data's value-creating capabilities and innovation potential are widely recognized, resulting in a notable increase in the number of published open data sources. A crucial challenge for companies intending to leverage open data is to identify suitable open datasets that support specific business scenarios and prepare these datasets for use. Researchers have developed several open data assessment techniques, but those are restricted in scope, do not consider the use context, and are not embedded in the complete set of activities required for open data consumption in enterprises. Therefore, our research aims to develop prescriptive knowledge in the form of a meaningful method to screen, assess, and prepare open data for use in an enterprise setting. Our findings complement existing open data assessment techniques by providing methodological guidance to prepare open data of uncertain quality for use in a value-adding and demand-oriented manner, enabled by knowledge graphs and linked data concepts. From an academic perspective, our research conceptualizes open data preparation as a purposeful and value-creating process.},
journal = {J. Data and Information Quality},
month = nov,
articleno = {43},
numpages = {25},
keywords = {Open data, Data preparation, Data quality, Action Design Research, Knowledge Graph, Data sourcing, Enterprise data integration}
}

@article{10.1145/3603710,
author = {Wenz, Viola and Kesper, Arno and Taentzer, Gabriele},
title = {Clustering Heterogeneous Data Values for Data Quality Analysis},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603710},
doi = {10.1145/3603710},
abstract = {Data is of high quality if it is fit for its intended purpose. Data heterogeneity can be a major quality problem, as quality aspects such as understandability and consistency can be compromised. Heterogeneity of data values is particularly common when data is manually entered by different people using inadequate control rules. In this case, syntactic and semantic heterogeneity often go hand in hand. Heterogeneity of data values may be a direct result of problems in the acquisition process, quality problems of the underlying data model, or possibly erroneous data transformations. For example, in the cultural heritage domain, it is common to analyze data fields by manually searching lists of data values sorted alphabetically or by number of occurrences. Additionally, search functions such as regular expression matching are used to detect specific patterns. However, this requires a priori knowledge and technical skills that domain experts often do not have. Since such datasets often contain thousands of values, the entire process is very time-consuming. Outliers or subtle differences between values that may be critical to data quality can be easily overlooked. To improve this process of analyzing the quality of data values, we propose a bottom-up human-in-the-loop approach that clusters values of a data field according to syntactic similarity. The clustering is intended to help domain experts explore the heterogeneity of values in a data field and can be configured by domain experts according to their domain knowledge. The overview of the syntactic diversity of the data values gives an impression of the rules and practices of data acquisition as well as their violations. From this, experts can infer potential quality issues with the data acquisition process and system, as well as the data model and data transformations. We outline a proof-of-concept implementation of the approach. Our evaluation found that clustering adds value to data quality analysis, especially for detecting quality problems in data models.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {28},
numpages = {33},
keywords = {semi-structured data, value abstraction, data analysis, data heterogeneity, clustering, Data quality}
}

@article{10.1145/3604283,
author = {Wang, Ruihang and Xia, Deneng and Cao, Zhiwei and Wen, Yonggang and Tan, Rui and Zhou, Xin},
title = {Toward Data Center Digital Twins via Knowledge-based Model Calibration and Reduction},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3604283},
doi = {10.1145/3604283},
abstract = {Computational fluid dynamics (CFD) models have been widely used for prototyping data centers. Evolving them into high-fidelity and real-time digital twins is desirable for the online operations of data centers. However, CFD models often have unsatisfactory accuracy and high computation overhead. Manually calibrating the CFD model parameters is tedious and labor-intensive. Existing automatic calibration approaches apply heuristics to search the model configurations. However, each search step requires a long-lasting process of repeatedly solving the CFD model, rendering them impractical, especially for complex CFD models. This article presents Kalibre, a knowledge-based neural surrogate approach that calibrates a CFD model by iterating four steps of (i) training a neural surrogate model, (ii) finding the optimal parameters through neural surrogate retraining, (iii) configuring the found parameters back to the CFD model, and (iv) validating the CFD model using sensor-measured data. Thus, the parameter search is offloaded to the lightweight neural surrogate. To speed up Kalibre’s convergence, we incorporate prior knowledge in training data initialization and surrogate architecture design. With about ten hours of computation on a 64-core processor, Kalibre achieves mean absolute errors (MAEs) of 0.57°C and 0.88°C in calibrating the CFD models of two production data halls hosting thousands of servers. To accelerate CFD-based simulation, we further propose Kalibreduce that incorporates the energy balance principle to reduce the order of the calibrated CFD model. Evaluation shows the model reduction only introduces 0.1°C to 0.27°C extra errors while accelerating the CFD-based simulations by thousand times.},
journal = {ACM Trans. Model. Comput. Simul.},
month = oct,
articleno = {11},
numpages = {24},
keywords = {proper orthogonal decomposition, knowledge-based neural network, surrogate model, computational fluid dynamics, Data center}
}

@article{10.1145/3604907,
author = {Fekete, S\'{a}ndor P. and Keldenich, Phillip and Krupke, Dominik and Schirra, Stefan},
title = {Minimum Partition into Plane Subgraphs: The CG:SHOP Challenge 2022},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
issn = {1084-6654},
url = {https://doi.org/10.1145/3604907},
doi = {10.1145/3604907},
abstract = {We give an overview of the 2022 Computational Geometry Challenge targeting the problem Minimum Partition into Plane Subsets, which consists of partitioning a given set of line segments into a minimum number of non-crossing subsets.},
journal = {ACM J. Exp. Algorithmics},
month = aug,
articleno = {1.9},
numpages = {13},
keywords = {CG:SHOP, intersection graphs, plane subgraphs, graph coloring, algorithm engineering, geometric optimization, Computational geometry}
}

@article{10.1145/3604932,
author = {Besta, Maciej and Gerstenberger, Robert and Peter, Emanuel and Fischer, Marc and Podstawski, Micha\l{} and Barthels, Claude and Alonso, Gustavo and Hoefler, Torsten},
title = {Demystifying Graph Databases: Analysis and Taxonomy of Data Organization, System Designs, and Graph Queries},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604932},
doi = {10.1145/3604932},
abstract = {Numerous irregular graph datasets, for example social networks or web graphs, may contain even trillions of edges. Often, their structure changes over time and they have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size and irregularity of such datasets, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., document stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., Resource Description Framework or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and Atomicity, Consistency, Isolation, Durability). Fifty-one graph database systems are presented and compared, including Neo4j, OrientDB, and Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we outline future research and engineering challenges related to graph databases.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {31},
numpages = {40},
keywords = {Document Stores, Wide-Column Stores, RDBMS, Key-Value Stores, Triple Stores, Labeled Property Graph, RDF, Graph Representations, Graph Transactions, Graph Queries, Data Layout, Graph Models, Graph Database Management Systems, NoSQL Stores, Graph Databases, Graphs}
}

@article{10.1145/3605910,
author = {Koch, In\^{e}s and Teixeira Lopes, Carla and Ribeiro, Cristina},
title = {Moving from ISAD(G) to a CIDOC CRM-based Linked Data Model in the Portuguese Archives},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3605910},
doi = {10.1145/3605910},
abstract = {Archives are facing numerous challenges. On the one hand, archival assets are evolving to encompass digitized documents and increasing quantities of born-digital information in diverse formats. On the other hand, the audience is changing along with how it wishes to access archival material. Moreover, the interoperability requirements of cultural heritage repositories are growing. In this context, the Portuguese Archives started an ambitious program aiming to evolve its data model, migrate existing records, and build a new archival management system appropriate to both archival tasks and public access. The overall goal is to have a fine-grained and flexible description, more machine-actionable than the current one. This work describes ArchOnto, a linked open data model for archives, and rules for its automatic population from existing records. ArchOnto adopts a semantic web approach and encompasses the CIDOC Conceptual Reference Model and additional ontologies, envisioning interoperability with datasets curated by multiple communities of practice. Existing ISAD(G)-conforming descriptions are being migrated to the new model using the direct mappings provided here. We used a sample of 25 records associated with different description levels to validate the completeness and conformity of ArchOnto to existing data. This work is in progress and is original in several respects: (1) it is one of the first approaches to use CIDOC CRM in the context of archives, identifying problems and questions that emerged during the process and pinpointing possible solutions; (2) it addresses the balance in the model between the migration of existing records and the construction of new ones by archive professionals; and (3) it adopts an open world view on linking archival data to global information sources.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {71},
numpages = {21},
keywords = {data migration, semantic web, linked open data, archival description, archives, Cultural heritage}
}

@article{10.1145/3605944,
author = {Chen, Zhida and Cong, Gao and Aref, Walid G.},
title = {STAR: A Cache-based Stream Warehouse System for Spatial Data},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/3605944},
doi = {10.1145/3605944},
abstract = {The proliferation of mobile phones and location-based services has given rise to an explosive growth in spatial data. To enable spatial data analytics, spatial data needs to be streamed into a data stream warehouse system that can provide real-time analytical results over the most recent and historical spatial data in the warehouse. Existing data stream warehouse systems are not tailored for spatial data. In this article, we introduce the STAR system. STAR is a distributed in-memory data stream warehouse system that provides low-latency and up-to-date analytical results over a fast-arriving spatial data stream. STAR supports both snapshot and continuous queries that are composed of aggregate functions and ad hoc query constraints over spatial, textual, and temporal data attributes. STAR implements a cache-based mechanism to facilitate the processing of snapshot queries that collectively utilizes the techniques of query-based caching (i.e., view materialization) and object-based caching. Moreover, to speed up processing continuous queries, STAR proposes a novel index structure that achieves high efficiency in both object checking and result updating. Extensive experiments over real datasets demonstrate the superior performance of STAR over existing systems.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = nov,
articleno = {28},
numpages = {27},
keywords = {distributed system, warehouse system, data stream, Spatial data}
}

@article{10.1145/3606263,
author = {Yu, Yuning and Hsu, Shanglin and Chen, Andre and Chen, Yutian and Tang, Bin},
title = {Truthful and Optimal Data Preservation in Base Station-less Sensor Networks: An Integrated Game Theory and Network Flow Approach},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3606263},
doi = {10.1145/3606263},
abstract = {We aim to preserve a large amount of data generated inside base station-less sensor networks (BSNs) while considering that sensor nodes are selfish. BSNs refer to emerging sensing applications deployed in challenging and inhospitable environments (e.g., underwater exploration); as such, there do not exist data-collecting base stations in the BSN to collect the data. Consequently, the generated data has to be stored inside the BSN before uploading opportunities become available. Our goal is to preserve the data inside the BSN with minimum energy cost by incentivizing the storage- and energy-constrained sensor nodes to participate in the data preservation process. We refer to the problem as DPP: data preservation problem in the BSN. Previous research assumes that all the sensor nodes are cooperative and that sensors have infinite battery power and design a minimum-cost flow-based data preservation solution. However, in a distributed setting and under different control, the resource-constrained sensor nodes could behave selfishly only to conserve their resources and maximize their benefit.In this article, we first solve DPP by designing an integer linear programming (ILP)-based optimal solution without considering selfishness. We then establish a game-theoretical framework that achieves provably truthful and optimal data preservation in BSNs. For a special case of DPP wherein nodes are not energy-constrained, referred to as DPP-W, we design a data preservation game DPG-1 that integrates algorithmic mechanism design (AMD) and a more efficient minimum cost flow-based data preservation solution. We show that DPG-1 yields dominant strategies for sensor nodes and delivers truthful and optimal data preservation. For the general case of DPP (wherein nodes are energy-constrained), however, DPG-1 fails to achieve truthful and optimal data preservation. Utilizing packet-level flow observation of sensor node behaviors computed by minimum cost flow and ILP, we uncover the cause of the failure of the DPG-1. It is due to the packet dropping by the selfish nodes that manipulate the AMD technique. We then design a data preservation game DPG-2 for DPP that traces and punishes manipulative nodes in the BSN. We show that DPG-2 delivers dominant strategies for truth-telling nodes and achieves provably optimal data preservation with cheat-proof guarantees. Via extensive simulations under different network parameters and dynamics, we show that our games achieve system-wide data preservation solutions with optimal energy cost while enforcing truth-telling of sensor nodes about their private cost types. One salient feature of our work is its integrated game theory and network flows approach. With the observation of flow level sensor node behaviors provided by the network flows, our proposed games can synthesize “microscopic” (i.e., selfish and local) behaviors of sensor nodes and yield targeted “macroscopic” (i.e., optimal and global) network performance of data preservation in the BSN.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {5},
numpages = {40},
keywords = {algorithmic mechanism design, game theory, integer linear program, network flow (minimum cost flow and maximum flow), energy-efficiency, data preservation, Base station-less sensor networks}
}

@article{10.1145/3606950,
author = {Bruun, Simone Borg and Lioma, Christina and Maistro, Maria},
title = {Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3606950},
doi = {10.1145/3606950},
abstract = {Providing personalized recommendations for insurance products is particularly challenging due to the intrinsic and distinctive features of the insurance domain. First, unlike more traditional domains like retail, movie and so on, a large amount of user feedback is not available and the item catalog is smaller. Second, due to the higher complexity of products, the majority of users still prefer to complete their purchases over the phone instead of online. We present different recommender models to address such data scarcity in the insurance domain. We use recurrent neural networks with three different types of loss functions and architectures (cross-entropy, censored Weibull, and attention). Our models cope with data scarcity by learning from multiple sessions and different types of user actions. Moreover, differently from previous session-based models, our models learn to predict a target action that does not happen within the session. Our models outperform state-of-the-art baselines on a real-world insurance dataset, with ca. 44K users, 16 items, 54K purchases, and 117K sessions. Moreover, combining our models with demographic data boosts the performance. Analysis shows that considering multiple sessions and several types of actions are both beneficial for the models, and that our models are not unfair with respect to age, gender, and income.},
journal = {ACM Trans. Recomm. Syst.},
month = aug,
articleno = {2},
numpages = {24},
keywords = {Insurance recommendation, session-based recommender system, recurrent neural network}
}

@article{10.1145/3607534,
author = {Sheff, Isaac and Wang, Xinwen and Babel, Kushal and Ni, Haobin and van Renesse, Robbert and Myers, Andrew C.},
title = {Charlotte: Reformulating Blockchains into a Web of Composable Attested Data Structures for Cross-Domain Applications},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3607534},
doi = {10.1145/3607534},
abstract = {Cross-domain applications are rapidly adopting blockchain techniques for immutability, availability, integrity, and interoperability. However, for most applications, global consensus is unnecessary and may not even provide sufficient guarantees. We propose a new distributed data structure: Attested Data Structures &nbsp;(ADS), which generalize not only blockchains but also many other structures used by distributed applications. As in blockchains, data in ADSs is immutable and self-authenticating. ADSs go further by supporting application-defined proofs (attestations). Attestations enable applications to plug in their own mechanisms to ensure availability and integrity. We present Charlotte, a framework for composable ADSs. Charlotte deconstructs conventional blockchains into more primitive mechanisms. Charlotte can be used to construct blockchains but does not impose the usual global-ordering overhead. Charlotte offers a flexible foundation for interacting applications that define their own policies for availability and integrity. Unlike traditional distributed systems, Charlotte supports heterogeneous trust: different observers have their own beliefs about who might fail, and how. Nevertheless, each observer has a consistent, available view of data.Charlotte’s data structures are interoperable and composable: applications and data structures can operate fully independently or can share data when desired. Charlotte defines a language-independent format for data blocks and a network API for servers.To demonstrate Charlotte’s flexibility, we implement several integrity mechanisms, including consensus and proof of work. We explore the power of disentangling availability and integrity mechanisms in prototype applications. The results suggest that Charlotte can be used to build flexible, fast, composable applications with strong guarantees.},
journal = {ACM Trans. Comput. Syst.},
month = dec,
articleno = {2},
numpages = {52},
keywords = {distributed systems, authenticated data structure, DAG, Blockchain}
}

@article{10.1145/3607843,
author = {Hubers, Alex and Morris, J. Garrett},
title = {Generic Programming with Extensible Data Types: Or, Making Ad Hoc Extensible Data Types Less Ad Hoc},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607843},
doi = {10.1145/3607843},
abstract = {We present a novel approach to generic programming over extensible data types. Row types capture the  
structure of records and variants, and can be used to express record and variant subtyping, record extension, and modular composition of case branches. We extend row typing to capture generic programming over rows themselves, capturing patterns including lifting operations to records and variations from their component types, and the duality between cases blocks over variants and records of labeled functions, without placing specific requirements on the fields or constructors present in the records and variants. We formalize our approach in System R𝜔, an extension of F𝜔 with row types, and give a denotational semantics for (stratified) R𝜔 in Agda.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {201},
numpages = {29},
keywords = {extensible data types, generic programming, qualified types, row polymorphism, row types}
}

@article{10.1145/3607858,
author = {Baudon, Tha\"{\i}s and Radanne, Gabriel and Gonnord, Laure},
title = {Bit-Stealing Made Legal: Compilation for Custom Memory Representations of Algebraic Data Types},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607858},
doi = {10.1145/3607858},
abstract = {Initially present only in functional languages such as OCaml and Haskell, Algebraic Data Types (ADTs) have now become pervasive in mainstream languages, providing nice data abstractions and an elegant way to express functions through pattern matching. Unfortunately, ADTs remain seldom used in low-level programming. One reason is that their increased convenience comes at the cost of abstracting away the exact memory layout of values. Even Rust, which tries to optimize data layout, severely limits control over memory representation. In this article, we present a new approach to specify the data layout of rich data types based on a dual view: a source type, providing a high-level description available in the rest of the code, along with a memory type, providing full control over the memory layout. This dual view allows for better reasoning about memory layout, both for correctness, with dedicated validity criteria linking the two views, and for optimizations that manipulate the memory view. We then provide algorithms to compile constructors and destructors, including pattern matching, to their low-level memory representation. We prove our compilation algorithms correct, implement them in a tool called ribbit that compiles to LLVM IR, and show some early experimental results.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {216},
numpages = {34},
keywords = {Algebraic Data Types, Compilation, Data Layouts, Pattern Matching}
}

@article{10.1145/3608115,
author = {Bremer, Christina and Gujral, Harshit and Lin, Michelle and Hinkers, Lily and Becker, Christoph and Coroam\u{a}, Vlad C.},
title = {How Viable are Energy Savings in Smart Homes? A Call to Embrace Rebound Effects in Sustainable HCI},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3608115},
doi = {10.1145/3608115},
abstract = {As part of global climate action, digital technologies are seen as a key enabler of energy efficiency savings. A popular application domain for this work is smart homes. There is a risk, however, that these efficiency gains result in rebound effects, which reduce or even overcompensate the savings. Rebound effects are well-established in economics, but it is less clear whether they also inform smart energy research in other disciplines. In this paper, we ask: to what extent have rebound effects and their underlying mechanisms been considered in computing, HCI and smart home research? To answer this, we conducted a literature mapping drawing on four scientific databases and a SIGCHI corpus. Our results reveal limited consideration of rebound effects and significant opportunities for HCI to advance this topic. We conclude with a taxonomy of actions for HCI to address rebound effects and help determine the viability of energy efficiency projects.},
journal = {ACM J. Comput. Sustain. Soc.},
month = sep,
articleno = {4},
numpages = {24},
keywords = {rebound effect, energy savings, energy efficiency, smart home, Sustainable HCI}
}

@article{10.1145/3609106,
author = {Mishra, Vishesh and Mittal, Sparsh and Hassan, Neelofar and Singhal, Rekha and Chatterjee, Urbi},
title = {VADF: Versatile Approximate Data Formats for Energy-Efficient Computing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609106},
doi = {10.1145/3609106},
abstract = {Approximate computing (AC) techniques provide overall performance gains in terms of power and energy savings at the cost of minor loss in application accuracy. For this reason, AC has emerged as a viable method for efficiently supporting several compute-intensive applications, e.g., machine learning, deep learning, and image processing, that can tolerate bounded errors in computations. However, most prior techniques do not consider the possibility of soft errors or malicious bit-flips in AC systems. These errors may interact with approximation-introduced errors in unforeseen ways, leading to disastrous consequences, such as the failure of computing systems. A recent research effort, FTApprox (DATE’21) proposes an error-resilient approximate data format. FTApprox stores two blocks, starting from the one containing the most significant valid (MSV) bit. It also stores location of the MSV block and protects them using error-correcting bits (ECBs). However, FTApprox has crucial limitations such as lack of flexibility, redundantly storing zeros in the MSV, etc.In this paper, we propose a novel storage format named Versatile Approximate Data Format (VADF) for storing approximate integer numbers while providing resilience to soft errors. VADF prescribes rules for storing, for example, a 32-bit number in either 8-bit, 12-bit or 16-bit numbers. VADF identifies the MSV bit and stores a certain number of bits following the MSV bit. It also stores the location of the MSV bit and protects it by ECBs. VADF does not explicitly store the MSB bit itself and this prevents VADF from accruing significant errors. VADF incurs lower error than both truncation methodologies and FTApprox. We further evaluate five image-processing and machine-learning applications and confirm that VADF provides higher application quality than FTApprox in the presence and absence of soft errors. Finally, VADF allows the use of narrow arithmetic units. For example, instead of using a 32-bit multiplier/adder, one can first use VADF (or FTApprox) to compress the data and then use a 8-bit multiplier/adder. Through this approach, VADF facilitates 95.97\% and 79.3\% energy savings in multiplication and addition, respectively. However, the subsequent re-conversion of the 8-bit output data to 32-bit data using Inv-VADF(16,3,32) diminishes the energy savings by 9.6\% for addition and 0.56\% for multiplication operation, respectively. The code is available at .},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {111},
numpages = {21},
keywords = {soft-error resilience, approximate data formats, Approximate computing}
}

@article{10.1145/3609131,
author = {Cola\c{c}o, Jean-Louis and Mendler, Michael and Pauget, Baptiste and Pouzet, Marc},
title = {A Constructive State-based Semantics and Interpreter for a Synchronous Data-flow Language with State Machines},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609131},
doi = {10.1145/3609131},
abstract = {Scade is a domain-specific synchronous functional language used to implement safety-critical real-time software for more than twenty years. Two main approaches have been considered for its semantics: (i) an indirect collapsing semantics based on a source-to-source translation of high-level constructs into a data-flow core language whose semantics is precisely specified and is the entry for code generation; a relational synchronous semantics, akin to Esterel, that applies directly to the source. It defines what is a valid synchronous reaction but hides, on purpose, if a semantics exists, is unique and can be computed; hence, it is not executable.This paper presents, for the first time, an executable, state-based semantics for a language that has the key constructs of Scade all together, in particular the arbitrary combination of data-flow equations and hierarchical state machines. It can apply directly to the source language before static checks and compilation steps. It is constructive in the sense that the language in which the semantics is defined is a statically typed functional language with call-by-value and strong normalization, e.g., it is expressible in a proof-assistant where all functions terminate. It leads to a reference, purely functional, interpreter. This semantics is modular and can account for possible errors, allowing to establish what property is ensured by each static verification performed by the compiler. It also clarifies how causality is treated in Scade compared with Esterel.This semantics can serve as an oracle for compiler testing and validation; to prototype novel language constructs before they are implemented, to execute possibly unfinished models or that are correct but rejected by the compiler; to prove the correctness of compilation steps.The semantics given in the paper is implemented as an interpreter in a purely functional style, in OCaml.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {152},
numpages = {26},
keywords = {embedded software, synchronous programming, dynamic semantics, Programming language}
}

@article{10.1145/3609235,
author = {Oralbayeva, Nurziya and Aly, Amir and Sandygulova, Anara and Belpaeme, Tony},
title = {Data-driven Communicative Behaviour Generation: A Survey},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
url = {https://doi.org/10.1145/3609235},
doi = {10.1145/3609235},
abstract = {The development of data-driven behaviour generating systems has recently become the focus of considerable attention in the fields of human–agent interaction and human–robot interaction. Although rule-based approaches were dominant for years, these proved inflexible and expensive to develop. The difficulty of developing production rules, as well as the need for manual configuration to generate artificial behaviours, places a limit on how complex and diverse rule-based behaviours can be. In contrast, actual human–human interaction data collected using tracking and recording devices makes humanlike multimodal co-speech behaviour generation possible using machine learning and specifically, in recent years, deep learning. This survey provides an overview of the state of the art of deep learning-based co-speech behaviour generation models and offers an outlook for future research in this area.},
journal = {J. Hum.-Robot Interact.},
month = jan,
articleno = {2},
numpages = {39},
keywords = {Datasets, neural networks, data-driven behaviour generation}
}

@article{10.1145/3609336,
author = {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and Rana, Omer and Perera, Charith},
title = {Query Interface for Smart City Internet of Things Data Marketplaces: A Case Study},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3609336},
doi = {10.1145/3609336},
abstract = {Cities are increasingly becoming augmented with sensors through public, private, and academic sector initiatives. Most of the time, these sensors are deployed with a primary purpose (objective) in mind (e.g., deploy sensors to understand noise pollution) by a sensor owner (i.e., the organization that invests in sensing hardware, e.g., a city council). Over the past few years, communities undertaking smart city development projects have understood the importance of making the sensor data available to a wider community—beyond their primary usage. Different business models have been proposed to achieve this, including creating data marketplaces. The vision is to encourage new startups and small and medium-scale businesses to create novel products and services using sensor data to generate additional economic value. Currently, data are sold as pre-defined independent datasets (e.g., noise level and parking status data may be sold separately). This approach creates several challenges, such as (i) difficulties in pricing, which leads to higher prices (per dataset); (ii) higher network communication and bandwidth requirements; and (iii) information overload for data consumers (i.e., those who purchase data). We investigate the benefit of semantic representation and its reasoning capabilities toward creating a business model that offers data on demand within smart city Internet of Things data marketplaces. The objective is to help data consumers (i.e., small and medium enterprises) acquire the most relevant data they need. We demonstrate the utility of our approach by integrating it into a real-world IoT data marketplace (developed by the synchronicity-iot.eu project). We discuss design decisions and their consequences (i.e., tradeoffs) on the choice and selection of datasets. Subsequently, we present a series of data modeling principles and recommendations for implementing IoT data marketplaces.},
journal = {ACM Trans. Internet Things},
month = sep,
articleno = {19},
numpages = {39},
keywords = {knowledge management, linked data, multi-dimensional querying, data discovery, semantic interoperability, Internet of Things}
}

@article{10.1145/3609425,
author = {Hussein, Dina and Bhat, Ganapati},
title = {SensorGAN: A Novel Data Recovery Approach for Wearable Human Activity Recognition},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3609425},
doi = {10.1145/3609425},
abstract = {Human activity recognition&nbsp;(HAR) and, more broadly, activities of daily life recognition using wearable devices have the potential to transform a number of applications, including mobile healthcare, smart homes, and fitness monitoring. Recent approaches for HAR use multiple sensors on various locations on the body to achieve higher accuracy for complex activities. While multiple sensors increase the accuracy, they are also susceptible to reliability issues when one or more sensors are unable to provide data to the application due to sensor malfunction, user error, or energy limitations. Training multiple activity classifiers that use a subset of sensors is not desirable, since it may lead to reduced accuracy for applications. To handle these limitations, we propose a novel generative approach that recovers the missing data of sensors using data available from other sensors. The recovered data are then used to seamlessly classify activities. Experiments using three publicly available activity datasets show that with data missing from one sensor, the proposed approach achieves accuracy that is within 10\% of the accuracy with no missing data. Moreover, implementation on a wearable device prototype shows that the proposed approach takes about 1.5&nbsp;ms for recovering data in the w-HAR dataset, which results in an energy consumption of 606&nbsp;μJ. The low-energy consumption ensures that SensorGAN is suitable for effectively recovering data in tinyML applications on energy-constrained devices.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = may,
articleno = {53},
numpages = {28},
keywords = {Human activity recognition, wearable electronics, missing data detection, data imputation, clustering, health monitoring}
}

@article{10.1145/3609486,
author = {Seneviratne, Oshani and Adams, Kacy and McGuinness, Deborah L.},
title = {Accountable Bench-to-Bedside Data-Sharing Mechanism for Researchers},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3–4},
url = {https://doi.org/10.1145/3609486},
doi = {10.1145/3609486},
abstract = {We present a trustworthy mechanism for sharing, reusing, and repurposing data to address the challenge of the costly and time-consuming effort needed to bring an innovative idea from the bench (basic research) to the bedside (clinical level). Even though researchers may generate a solution on their own, other aspects of research, including peer review and dissemination of data/results, have an inherent social component. Compared with the centralized mechanisms of data-sharing (and the subsequent reuse and repurposing), many, if not all, aspects of these processes can be decentralized by using blockchain (for full decentralized and autonomous control), coupled with provenance (to ascertain how and where the resources have been leveraged) and incentive semantics (for characterizing how researchers would be rewarded for their contributions). By capturing metadata details at each step of the workflow, data will be easier to audit, verify, and merge with related datasets. It is common in settings where data is either sensitive or valuable (or both) to have formal data use agreements or sometimes less formal rules for reuse, which we have captured in smart contracts. A key innovative aspect of this work is the departure from the traditional natural language–based data use agreements to make these agreements more computable, resulting in enhanced usability and interoperability by a broader community. We have developed the Data Sharing Ontology, a structured vocabulary to guide various incentive mechanisms and criteria used in the decentralized protocol we introduced with smart contracts. Our solution can track data reuse, provide peer reviews on accountable data reuse, and report any violations, thus providing metrics for measuring data producers’ impact on reward structures and research measures. We introduce the SCIENCE-index designed to incentivize data-sharing in scientific research, which builds upon prior indices used in academic research, such as the h-index and the data-index. The SCIENCE-index is publicly available and automatically calculated by a smart contract based on an individual’s data sharing, reuse, and responsible stewardship activities. By incentivizing fair and honest data-related activities, the SCIENCE-index can help improve the speed, cost, and quality of scientific research. As an example application of this decentralized data-sharing framework, we demonstrate how this approach could radically improve the quality and the efficiency of scientific output in the setting of COVID-19 research data-sharing from the National COVID Cohort Collaborative (N3C).},
journal = {Trans. Soc. Comput.},
month = oct,
articleno = {5},
numpages = {23},
keywords = {researcher’s dilemma, incentives, blockchain, Data-sharing}
}

@article{10.1145/3610107,
author = {Wilcox, Lauren and Brewer, Robin and Diaz, Fernando},
title = {AI Consent Futures: A Case Study on Voice Data Collection with Clinicians},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610107},
doi = {10.1145/3610107},
abstract = {As new forms of data capture emerge to power new AI applications, questions abound about the ethical implications of these data collection practices. In this paper, we present clinicians' perspectives on the prospective benefits and harms of voice data collection during health consultations. Such data collection is being proposed as a means to power models to assist clinicians with medical data entry, administrative tasks, and consultation analysis. Yet, clinicians' attitudes and concerns are largely absent from the AI narratives surrounding these use cases, and the academic literature investigating them. Our qualitative interview study used the concept of an informed consent process as a type of design fiction, to support elicitation of clinicians' perspectives on voice data collection and use associated with a fictional, near-term AI assistant. Through reflexive thematic analysis of in-depth sessions with physicians, we distilled eight classes of potential risks that clinicians are concerned about, including workflow disruptions, self-censorship, and errors that could impact patient eligibility for services. We conclude with an in-depth discussion of these prospective risks, reflect on the use of the speculative processes that illuminated them, and reconsider evaluation criteria for AI-assisted clinical documentation technologies in light of our findings.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {316},
numpages = {30},
keywords = {AI, audio, data, design fiction, ethics, health, speculative design, voice}
}

@article{10.1145/3610224,
author = {Kapp, Alexandra and Hansmeyer, Julia and Mihaljevi\'{c}, Helena},
title = {Generative Models for Synthetic Urban Mobility Data: A Systematic Literature Review},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3610224},
doi = {10.1145/3610224},
abstract = {Although highly valuable for a variety of applications, urban mobility data are rarely made openly available, as it contains sensitive personal information. Synthetic data aims to solve this issue by generating artificial data that resembles an original dataset in structural and statistical characteristics, but omits sensitive information. For mobility data, a large number of corresponding models have been proposed in the past decade. This systematic review provides a structured comparative overview of the current state of this heterogeneous, active field of research. A special focus is put on the applicability of the reviewed models in practice.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {93},
numpages = {37},
keywords = {literature review, privacy, mobility data generation, data synthesis, synthetic data generation, mobility traces, trip data, trajectories, location sequences, Mobility data}
}

@article{10.1145/3610402,
author = {Amini, Maryam and Stanica, Razvan and Rosenberg, Catherine},
title = {Where Are the (Cellular) Data?},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3610402},
doi = {10.1145/3610402},
abstract = {New generations of cellular networks are data oriented, targeting the integration of machine learning and artificial intelligence solutions. Data availability, required to train and compare machine learning based networking solutions, is therefore becoming an important topic and a significant concern. Operators do collect data, but they rarely share it because of privacy concerns. This article starts by reviewing the few publicly available cellular datasets, which created bursts of innovation with their release. The scarcity of such data is so acute that researchers are collecting network data using their own tools, developed in-house and covered in the second part of this survey.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {48},
numpages = {25},
keywords = {measurement tools, datasets, Cellular networks}
}

@article{10.1145/3610904,
author = {Tabatabaie, Mahan and He, Suining and Shin, Kang G.},
title = {Cross-Modality Graph-based Language and Sensor Data Co-Learning of Human-Mobility Interaction},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610904},
doi = {10.1145/3610904},
abstract = {Learning the human--mobility interaction (HMI) on interactive scenes (e.g., how a vehicle turns at an intersection in response to traffic lights and other oncoming vehicles) can enhance the safety, efficiency, and resilience of smart mobility systems (e.g., autonomous vehicles) and many other ubiquitous computing applications. Towards the ubiquitous and understandable HMI learning, this paper considers both "spoken language" (e.g., human textual annotations) and "unspoken language" (e.g., visual and sensor-based behavioral mobility information related to the HMI scenes) in terms of information modalities from the real-world HMI scenarios. We aim to extract the important but possibly implicit HMI concepts (as the named entities) from the textual annotations (provided by human annotators) through a novel human language and sensor data co-learning design.To this end, we propose CG-HMI, a novel Cross-modality Graph fusion approach for extracting important Human-Mobility Interaction concepts from co-learning of textual annotations as well as the visual and behavioral sensor data. In order to fuse both unspoken and spoken "languages", we have designed a unified representation called the human--mobility interaction graph (HMIG) for each modality related to the HMI scenes, i.e., textual annotations, visual video frames, and behavioral sensor time-series (e.g., from the on-board or smartphone inertial measurement units). The nodes of the HMIG in these modalities correspond to the textual words (tokenized for ease of processing) related to HMI concepts, the detected traffic participant/environment categories, and the vehicle maneuver behavior types determined from the behavioral sensor time-series. To extract the inter- and intra-modality semantic correspondences and interactions in the HMIG, we have designed a novel graph interaction fusion approach with differentiable pooling-based graph attention. The resulting graph embeddings are then processed to identify and retrieve the HMI concepts within the annotations, which can benefit the downstream human-computer interaction and ubiquitous computing applications. We have developed and implemented CG-HMI into a system prototype, and performed extensive studies upon three real-world HMI datasets (two on car driving and the third one on e-scooter riding). We have corroborated the excellent performance (on average 13.11\% higher accuracy than the other baselines in terms of precision, recall, and F1 measure) and effectiveness of CG-HMI in recognizing and extracting the important HMI concepts through cross-modality learning. Our CG-HMI studies also provide real-world implications (e.g., road safety and driving behaviors) about the interactions between the drivers and other traffic participants.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {125},
numpages = {25},
keywords = {Human-mobility interaction, cross-modality graph interaction fusion, human-mobility interaction concept extraction, language and sensor data co-learning, named entity recognition}
}

@article{10.1145/3610932,
author = {Shende, Chinmaey and Sahoo, Soumyashree and Sam, Stephen and Patel, Parit and Morillo, Reynaldo and Wang, Xinyu and Ware, Shweta and Bi, Jinbo and Kamath, Jayesh and Russell, Alexander and Song, Dongjin and Wang, Bing},
title = {Predicting Symptom Improvement During Depression Treatment Using Sleep Sensory Data},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610932},
doi = {10.1145/3610932},
abstract = {Depression is a serious mental illness. The current best guideline in depression treatment is closely monitoring patients and adjusting treatment as needed. Close monitoring of patients through physician-administered follow-ups or self-administered questionnaires, however, is difficult in clinical settings due to high cost, lack of trained professionals, and burden to the patients. Sensory data collected from mobile devices has been shown to provide a promising direction for long-term monitoring of depression symptoms. Most existing studies in this direction, however, focus on depression detection; the few studies that are on predicting changes in depression are not in clinical settings. In this paper, we investigate using one type of sensory data, sleep data, collected from wearables to predict improvement of depression symptoms over time after a patient initiates a new pharmacological treatment. We apply sleep trend filtering to noisy sleep sensory data to extract high-level sleep characteristics and develop a family of machine learning models that use simple sleep features (mean and variation of sleep duration) to predict symptom improvement. Our results show that using such simple sleep features can already lead to validation F1 score up to 0.68, indicating that using sensory data for predicting depression improvement during treatment is a promising direction.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {121},
numpages = {21},
keywords = {Depression, Machine Learning, Mental Health, Sensory Data, Wearables}
}

@article{10.1145/3611093,
author = {Kang, Daniel and Guibas, John and Bailis, Peter and Hashimoto, Tatsunori and Sun, Yi and Zaharia, Matei},
title = {Data Management for ML-Based Analytics and Beyond},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3611093},
doi = {10.1145/3611093},
abstract = {The increasing capabilities of machine learning (ML) has enabled the deployment of ML methods in a variety of applications, ranging from unstructured data analytics to autonomous vehicles. Due to the volumes of data over which ML is deployed, it is infeasible for humans to monitor deployments: the Tesla fleet of vehicles produces exabytes of data and millions of hours of video per day. As a result, ML deployments can fail in unexpected and catastrophic ways.In this work, we highlight three important but underlooked aspects of ML deployment pipelines: (1) managing high-quality training data, (2) monitoring ML errors at deployment time, and (3) connecting end use to deployment algorithms. We first demonstrate that training labels are often erroneous, contrary to standard practice, even when labeled by leading vendors. We then demonstrate that standard methods of deploying ML methods can lead to downstream errors. As a first step toward addressing these issues, we review and contextualize two abstractions for finding errors in training data and deployments. We further describe how to improve algorithms for analytics queries as a case study for optimizing ML pipelines end to end.Problem statementThis paper considers the problem of end-to-end machine learning (ML) deployments, from the collection of training data all the way to answering queries using ML models. We focus on errors in ML models and the data used to train them, along with algorithms for end-to-end uses of these models.MethodsWe provide simple abstractions, model assertions and learned observation assertions (LOA) to find errors that are pervasive in ML model deployments and the data used to train these ML models. We further implemented our abstractions in open-source APIs. Our abstractions and APIs are easy for those who are not experts in ML to use and deploy.ResultsWe show that model assertions and LOA can be deployed in as few as 10 lines of code per assertion. They can find errors with up to 100\% precision across domains ranging from video analytics, tabular data analytics, and translation.SignificanceIt is standard in the literature to assume that training data is "gold" (i.e., 100\% accurate) and that bulk ML model metrics such as accuracy properly reflect model performance. We show that these are not the case, even in widely studied settings. Our simple abstractions point towards methods of checking end-to-end deployments. We hope that future work builds on our APIs.},
journal = {ACM / IMS J. Data Sci.},
month = jan,
articleno = {4},
numpages = {23},
keywords = {ML Analytics, Errors in ML data, Errors in ML model outputs}
}

@article{10.1145/3612918,
author = {Sun, Danfeng and Hu, Junjie and Wu, Huifeng and Wu, Jia and Yang, Jian and Sheng, Quan Z. and Dustdar, Schahram},
title = {A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3612918},
doi = {10.1145/3612918},
abstract = {The scope of the Industrial Internet of Things (IIoT) has stretched beyond manufacturing to include energy, healthcare, transportation, and all that tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors, actuators, programmable logic controllers, distributed control systems (DCS), embedded devices, supervisory control, and data acquisition systems—all produced by manufacturers for different purposes and with different data structures and formats; designed according to different standards and made to follow different protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous data, and how can we uniformly structure them to suit thousands of different applications? In this article, we survey the four pillars of information science that enable collaborative data access in an IIoT—standardization, data acquisition, data fusion, and scalable architecture—to provide an up-to-date audit of current research in the field. Here, standardization in IIoT relies on standards and technologies to make things communicative; data acquisition attempts to transparently collect data through plug-and-play architectures, reconfigurable schemes, or hardware expansion; data fusion refers to the techniques and strategies for overcoming heterogeneity in data formats and sources; and scalable architecture provides basic techniques to support heterogeneous requirements. The article also concludes with an overview of the frontier researches and emerging technologies for supporting or challenging data access from the aspects of 5G, machine learning, blockchain, and semantic web.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {50},
numpages = {37}
}

@article{10.1145/3612919,
author = {Leventidis, Aristotelis and Di Rocco, Laura and Gatterbauer, Wolfgang and Miller, Ren\'{e}e J. and Riedewald, Mirek},
title = {DomainNet: Homograph Detection and Understanding in Data Lake Disambiguation},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3612919},
doi = {10.1145/3612919},
abstract = {Modern data lakes are heterogeneous in the vocabulary that is used to describe data. We study a problem of disambiguation in data lakes: How can we determine if a data value occurring more than once in the lake has different meanings and is therefore a homograph?
While word and entity disambiguation have been well studied in computational linguistics, data management, and data science, we show that data lakes provide a new opportunity for disambiguation of data values, because tables implicitly define a massive network of interconnected values. We introduce DomainNet, which efficiently represents this network, and investigate to what extent it can be used to disambiguate values without requiring any supervision.DomainNet leverages network-centrality measures on a bipartite graph whose nodes represent data values and attributes to determine if a value is a homograph. A thorough experimental evaluation demonstrates that state-of-the-art techniques in domain discovery cannot be re-purposed to compete with our method. Specifically, using a domain discovery method to identify homographs achieves an F1-score of 0.38 versus 0.69 for DomainNet, which separates homographs well from data values that have a unique meaning. On a real data lake, our top-100 precision is 93\%. Given a homograph, we also present a novel method for determining the number of meanings of the homograph and for assigning its data lake attributes to a meaning. We show the influence of homographs on two downstream tasks: entity-matching and domain discovery.},
journal = {ACM Trans. Database Syst.},
month = sep,
articleno = {9},
numpages = {40},
keywords = {network-centrality measures, homograph detection, Data Discovery}
}

@article{10.1145/3612932,
author = {Sun, Yan and Han, Yi and Fan, Jicong},
title = {Laplacian-based Cluster-Contractive t-SNE for High-Dimensional Data Visualization},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3612932},
doi = {10.1145/3612932},
abstract = {Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this article, we propose LaptSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, LaptSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of LaptSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also provide out-of-sample extension, large-scale extension, and mini-batch extension for our LaptSNE to facilitate dimensionality reduction in various scenarios.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {19},
numpages = {22},
keywords = {graph Laplacian, t-SNE, data visualization, Dimensionality reduction}
}

@article{10.1145/3613245,
author = {Shen, Zhihao and Du, Wan and Zhao, Xi and Zou, Jianhua},
title = {Retrieving Similar Trajectories from Cellular Data of Multiple Carriers at City Scale},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3613245},
doi = {10.1145/3613245},
abstract = {Retrieving similar trajectories aims to search for the trajectories that are close to a query trajectory in spatio-temporal domain from a large trajectory dataset. This is critical for a variety of applications, like transportation planning and mobility analysis. Unlike previous studies that perform similar trajectory retrieval on fine-grained GPS data or single cellular carrier, we investigate the feasibility of finding similar trajectories from cellular data of multiple carriers, which provide more comprehensive coverage of population and space. To handle the issues of spatial bias of cellular data from multiple carriers, coarse spatial granularity, and irregular sparse temporal sampling, we develop a holistic system cellSim. Specifically, to avoid the issue of spatial bias, we first propose a novel map matching approach, which transforms the cell tower sequences from multiple carriers to routes on a unified road map. Then, to address the issue of temporal sparse sampling, we generate multiple routes with different confidences to increase the probability of finding truly similar trajectories. Finally, a new trajectory similarity measure is developed for similar trajectory search by calculating the similarities between the irregularly-sampled trajectories. Extensive experiments on a large-scale cellular dataset from two carriers and real-world 1,701 km query trajectories reveal that cellSim provides state-of-the-art performance for similar trajectory retrieval.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {47},
numpages = {28},
keywords = {Trajectory similarity, map matching, cellular data, human mobility}
}

@article{10.1145/3614425,
author = {Xing, Xiaodan and Wu, Huanjun and Wang, Lichao and Stenson, Iain and Yong, May and Del Ser, Javier and Walsh, Simon and Yang, Guang},
title = {Non-imaging Medical Data Synthesis for Trustworthy AI: A Comprehensive Survey},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3614425},
doi = {10.1145/3614425},
abstract = {Data quality is a key factor in the development of trustworthy AI in healthcare. A large volume of curated datasets with controlled confounding factors can improve the accuracy, robustness, and privacy of downstream AI algorithms. However, access to high-quality datasets is limited by the technical difficulties of data acquisition, and large-scale sharing of healthcare data is hindered by strict ethical restrictions. Data synthesis algorithms, which generate data with distributions similar to real clinical data, can serve as a potential solution to address the scarcity of good quality data during the development of trustworthy AI. However, state-of-the-art data synthesis algorithms, especially deep learning algorithms, focus more on imaging data while neglecting the synthesis of non-imaging healthcare data, including clinical measurements, medical signals and waveforms, and electronic healthcare records (EHRs). Therefore, in this article, we will review synthesis algorithms, particularly for non-imaging medical data, with the aim of providing trustworthy AI in this domain. This tutorial-style review article will provide comprehensive descriptions of non-imaging medical data synthesis, covering aspects such as algorithms, evaluations, limitations, and future research directions.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {165},
numpages = {35},
keywords = {Medical data synthesis, electronic healthcare records}
}

@article{10.1145/3615666,
author = {Connolly, Kate and Klempay, Anna and McCann, Mary and Brenner, Paul},
title = {Dark Web Marketplaces: Data for Collaborative Threat Intelligence},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3615666},
doi = {10.1145/3615666},
abstract = {The dark web has become an increasingly important landscape for the sale of illicit cyber goods. Given the prevalence of malware and tools that are used to steal data from individuals on these markets, it is crucial that every company, governing body, and cyber professional be aware of what information is sold on these marketplaces. Knowing this information will allow these entities to protect themselves against cyber attacks and from information breaches. In this article, we announce the public release of a data set on dark web marketplaces’ cybersecurity-related listings. We spent multiple years seeking out websites that sold illicit digital goods and collected data on the available products. Due to the marketplaces’ varied and complex layers of security, we leveraged the flexible Selenium WebDriver with Python to navigate the web pages and collect data. We present analysis of categories of malicious cyber goods sold on marketplaces, prices, persistent vendors, ratings, and other basic information on marketplace storefronts. Additionally, we share the tools and techniques we’ve compiled, enabling others to scrape dark web marketplaces at a significantly lower risk. We invite professionals who opt to gather data from the dark web to contribute to the publicly shared threat intelligence resource.},
journal = {Digital Threats},
month = oct,
articleno = {49},
numpages = {12},
keywords = {malware, threat intelligence, markets, Dark web}
}

@article{10.1145/3617326,
author = {Zhu, Rui and Wang, Bin and Yang, Xiaochun and Zheng, Baihua},
title = {Closest Pairs Search Over Data Stream},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617326},
doi = {10.1145/3617326},
abstract = {k-closest pair (KCP for short) search is a fundamental problem in database research. Given a set of d-dimensional streaming data S, KCP search aims to retrieve k pairs with the shortest distances between them. While existing works have studied continuous 1-closest pair query (i.e., k=1) over dynamic data environments, which allow for object insertions/deletions, they require high computational costs and cannot easily support KCP search with k&gt;1. This paper investigates the problem of KCP search over data stream, aiming to incrementally maintain as few pairs as possible to support KCP search with arbitrarily k. To achieve this, we introduce the concept of NNS (short for Nearest Neighbour pair-Set), which consists of all the nearest neighbour pairs and allows us to support KCP search via only accessing O(k) objects. We further observe that in most cases, we only need to use a small portion of NNS to answer KCP search as typically k\l{}l n. Based on this observation, we propose TNNS (short for Threshold-based NNpair Set), which contains a small number of high-quality NN pairs, and a partition named τ-DLBP (short for τ-Distance Lower-Bound based Partition) to organize objects, with τ being an integer significantly smaller than n. τ-DLBP organizes objects using up to O(\l{}og n / τ) partitions and is able to support the construction and update of TNNS efficiently.},
journal = {Proc. ACM Manag. Data},
month = nov,
articleno = {205},
numpages = {26},
keywords = {cube, k-closest pair search, partition, streaming data}
}

@article{10.1145/3617338,
author = {Siddiqi, Shafaq and Kern, Roman and Boehm, Matthias},
title = {SAGA: A Scalable Framework for Optimizing Data Cleaning Pipelines for Machine Learning Applications},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617338},
doi = {10.1145/3617338},
abstract = {In the exploratory data science lifecycle, data scientists often spent the majority of their time finding, integrating, validating and cleaning relevant datasets. Despite recent work on data validation, and numerous error detection and correction algorithms, in practice, data cleaning for ML remains largely a manual, unpleasant, and labor-intensive trial and error process, especially in large-scale, distributed computation. The target ML application---such as classification or regression models---can be used as a signal of valuable feedback though, for selecting effective data cleaning strategies. In this paper, we introduce SAGA, a framework for automatically generating the top-K most effective data cleaning pipelines. SAGA adopts ideas from Auto-ML, feature selection, and hyper-parameter tuning. Our framework is extensible for user-provided constraints, new data cleaning primitives, and ML applications; automatically generates hybrid runtime plans of local and distributed operations; and performs pruning by interesting properties (e.g., monotonicity). Instead of full automation---which is rather unrealistic---SAGA simplifies the mechanical aspects of data cleaning. Our experiments show that SAGA yields robust accuracy improvements over state-of-the-art, and good scalability regarding increasing data sizes and number of evaluated pipelines.},
journal = {Proc. ACM Manag. Data},
month = nov,
articleno = {218},
numpages = {26},
keywords = {data cleaning for ML, data cleaning pipelines, data- and task-parallel execution, evolutionary algorithms, hyper-parameter tuning, linear-algebra-based primitives}
}

@article{10.1145/3617340,
author = {Sioulas, Panagiotis and Mytilinis, Ioannis and Ailamaki, Anastasia},
title = {SH2O: Efficient Data Access for Work-Sharing Databases},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617340},
doi = {10.1145/3617340},
abstract = {Interactive applications require processing tens to hundreds of concurrent analytical queries within tight time constraints. In such setups, where high concurrency causes contention, work-sharing databases are critical for improving scalability and for bounding the increase in response time. However, as such databases share data access using full scans and expensive shared filters, they suffer from a data-access bottleneck that jeopardizes interactivity.We present SH2O: a novel data-access operator that addresses the data-access bottleneck of work-sharing databases. SH2O is based on the idea that an access pattern based on judiciously selected multidimensional ranges can replace a set of shared filters. To exploit the idea in an efficient and scalable manner, SH2O uses a three-tier approach: i) it uses spatial indices to efficiently access the ranges without overfetching, ii) it uses an optimizer to choose which filters to replace such that it maximizes cost-benefit for index accesses, and iii) it exploits partitioning schemes and independently accesses each data partition to reduce the number of filters in the access pattern. Furthermore, we propose a tuning strategy that chooses a partitioning and indexing scheme that minimizes SH2O's cost for a target workload. Our evaluation shows a speedup of 1.8-22.2 for batches of hundreds of data-access-bound queries.},
journal = {Proc. ACM Manag. Data},
month = nov,
articleno = {220},
numpages = {26},
keywords = {analytical query processing, databases, indexing, work sharing}
}

@article{10.1145/3617366,
author = {Hornecker, Eva and Hogan, Trevor and Hinrichs, Uta and Van Koningsbruggen, Rosa},
title = {A Design Vocabulary for Data Physicalization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3617366},
doi = {10.1145/3617366},
abstract = {Although physical artifacts that represent data have been used for centuries, the research field—known as data physicalization—has only recently gained traction. Compared to data visualization, there is no established vocabulary for analyzing and discussing the properties of physicalizations. Through a grounded analysis of examples and literature, we propose a comprehensive design vocabulary, which consist of three separate, but connected parts: explicit variables, implicit properties, and consequential aspects. Explicit variables build on visual variables known from visualization and extend it to account for physicalization’s multi-modal nature. Implicit properties concern elements which are central to the design intention and user experience of physicalizations, yet are not a result of “explicit” encoding strategies. Finally, consequential aspects refer to unintentional effects of design decisions, that influence how a physicalization is experienced. Our work illustrates how physicalizations incorporate opportunities and challenges that are not afforded in other data representations, such as embodiment and imagined touch. With this, we contribute to generating theory on physicalization. Our design vocabulary can support (1) creators through informing their design processes and highlighting design strategies, (2) educators, and (3) academics and practitioners to analyse existing physicalizations, and reflect on the impact of design decisions on interpretation and experience.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {2},
numpages = {62},
keywords = {design language, embodiment, material turn, materiality, data narrative, user experience, InfoVis, Visualization}
}

@article{10.1145/3617377,
author = {Gonz\'{a}lez-Zelaya, Vladimiro and Salas, Juli\'{a}n and Meg\'{\i}as, David and Missier, Paolo},
title = {Fair and Private Data Preprocessing through Microaggregation},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3617377},
doi = {10.1145/3617377},
abstract = {Privacy protection for personal data and fairness in automated decisions are fundamental requirements for responsible Machine Learning. Both may be enforced through data preprocessing and share a common target: data should remain useful for a task, while becoming uninformative of the sensitive information. The intrinsic connection between privacy and fairness implies that modifications performed to guarantee one of these goals, may have an effect on the other, e.g., hiding a sensitive attribute from a classification algorithm might prevent a biased decision rule having such attribute as a criterion. This work resides at the intersection of algorithmic fairness and privacy. We show how the two goals are compatible, and may be simultaneously achieved, with a small loss in predictive performance. Our results are competitive with both state-of-the-art fairness correcting algorithms and hybrid privacy-fairness methods. Experiments were performed on three widely used benchmark datasets: Adult Income, COMPAS, and German Credit.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {49},
numpages = {24},
keywords = {ethical AI, algorithmic fairness, privacy preserving data mining, fair classification, Responsible machine learning}
}

@article{10.1145/3617595,
author = {He, Linyun and Shanbhag, Uday V. and Song, Eunhye},
title = {Stochastic Approximation for Multi-period Simulation Optimization with Streaming Input Data},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-3301},
url = {https://doi.org/10.1145/3617595},
doi = {10.1145/3617595},
abstract = {We consider a continuous-valued simulation optimization (SO) problem, where a simulator is built to optimize an expected performance measure of a real-world system while parameters of the simulator are estimated from streaming data collected periodically from the system. At each period, a new batch of data is combined with the cumulative data and the parameters are re-estimated with higher precision. The system requires the decision variable to be selected in all periods. Therefore, it is sensible for the decision-maker to update the decision variable at each period by solving a more precise SO problem with the updated parameter estimate to reduce the performance loss with respect to the target system. We define this decision-making process as the multi-period SO problem and introduce a multi-period stochastic approximation (SA) framework that generates a sequence of solutions. Two algorithms are proposed: Re-start SA (ReSA) reinitializes the stepsize sequence in each period, whereas Warm-start SA (WaSA) carefully tunes the stepsizes, taking both fewer and shorter gradient-descent steps in later periods as parameter estimates become increasingly more precise. We show that under suitable strong convexity and regularity conditions, ReSA and WaSA achieve the best possible convergence rate in expected sub-optimality either when an unbiased or a simultaneous perturbation gradient estimator is employed, while WaSA accrues significantly lower computational cost as the number of periods increases. In addition, we present the regularized ReSA, which obviates the need to know the strong convexity constant and achieves the same convergence rate at the expense of additional computation.},
journal = {ACM Trans. Model. Comput. Simul.},
month = apr,
articleno = {6},
numpages = {27},
keywords = {Multi-period simulation optimization, multi-period stochastic approximation, simulation optimization under input model risk}
}

@article{10.1145/3622801,
author = {Paulino, Herv\'{e} and Almeida Matos, Ana and Cederquist, Jan and Giunti, Marco and Matos, Jo\~{a}o and Ravara, Ant\'{o}nio},
title = {AtomiS: Data-Centric Synchronization Made Practical},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622801},
doi = {10.1145/3622801},
abstract = {Data-Centric Synchronization (DCS) shifts the reasoning about concurrency restrictions from control structures to data declaration. It is a high-level declarative approach that abstracts away from the actual concurrency control mechanism(s) in use. Despite its advantages, the practical use of DCS is hindered by the fact that it may require many annotations and/or multiple implementations of the same method to cope with differently qualified parameters.  

To overcome these limitations, in this paper we present AtomiS, a new DCS approach that requires only qualifying types of parameters and return values in interface definitions, and of fields in class definitions. The latter may also be abstracted away in type parameters, rendering class implementations virtually annotation-free. From this high level specification, a static analysis infers the atomicity constraints that are local to each method, considering valid only the method variants that are consistent with the specification, and performs code generation for all valid variants of each method. The generated code is then the target for automatic injection of concurrency control primitives that are responsible for ensuring the absence of data-races,  
atomicity-violations and deadlocks.  

We provide a Java implementation and showcase the applicability of AtomiS in real-life code. For the  
benchmarks analysed, AtomiS requires fewer annotations than the original number of regions requiring locks, as well as fewer annotations than Atomic Sets (a reference DCS proposal).},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {226},
numpages = {30},
keywords = {Programming Model, Inference and Synthesis, Data-Centric, Concurrency}
}

@article{10.1145/3622840,
author = {Sahebolamri, Arash and Barrett, Langston and Moore, Scott and Micinski, Kristopher},
title = {Bring Your Own Data Structures to Datalog},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622840},
doi = {10.1145/3622840},
abstract = {The restricted logic programming language Datalog has become a popular implementation target for deductive-analytic workloads including social-media analytics and program analysis. Modern Datalog engines compile Datalog rules to joins over explicit representations of relations—often B-trees or hash maps. While these modern engines have enabled high scalability in many application domains, they have a crucial weakness: achieving the desired algorithmic complexity may be impossible due to representation-imposed overhead of the engine’s data structures. In this paper, we present the "Bring Your Own Data Structures" (Byods) approach, in the form of a DSL embedded in Rust. Using Byods, an engineer writes logical rules which are implicitly parametric on the concrete data structure representation; our implementation provides an interface to enable "bringing their own" data structures to represent relations, which harmoniously interact with code generated by our compiler (implemented as Rust procedural macros). We formalize the semantics of Byods as an extension of Datalog’s; our formalization captures the key properties demanded of data structures compatible with Byods, including properties required for incrementalized (semi-na\"{\i}ve) evaluation. We detail many applications of the Byods approach, implementing analyses requiring specialized data structures for transitive and equivalence relations to scale, including an optimized version of the Rust borrow checker Polonius; highly-parallel PageRank made possible by lattices; and a large-scale analysis of LLVM utilizing index-sharing to scale. Our results show that Byods offers both improved algorithmic scalability (reduced time and/or space complexity) and runtimes competitive with state-of-the-art parallelizing Datalog solvers.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {264},
numpages = {26},
keywords = {Static Analysis, Program Analysis, Logic Programming, Datalog}
}

@article{10.1145/3623271,
author = {Baswana, Surender and Bhanja, Koustav and Pandey, Abhyuday},
title = {Minimum+1 (s, t)-cuts and Dual-edge Sensitivity Oracle},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1549-6325},
url = {https://doi.org/10.1145/3623271},
doi = {10.1145/3623271},
abstract = {Let G be a directed multi-graph on n vertices and m edges with a designated source vertex s and a designated sink vertex t. We study the (s,t)-cuts of capacity minimum+1 and as an important application of them, we give a solution to the dual-edge sensitivity for (s,t)-mincuts—reporting an (s,t)-mincut upon failure or insertion of any pair of edges. Picard and Queyranne [Mathematical Programming Studies, 13(1): 8–16 (1980)] showed that there exists a directed acyclic graph (DAG) that compactly stores all minimum (s,t)-cuts of G. This structure also acts as an oracle for the single-edge sensitivity of minimum (s,t)-cut. For undirected multi-graphs, Dinitz and Nutov [STOC, 509–518 (1995)] showed that there exists an 𝒪(n) size 2-level Cactus model that stores all global cuts of capacity minimum+1. However, for minimum+1 (s,t)-cuts, no such compact structure exists till date. We present the following structural and algorithmic results on minimum+1 (s,t)-cuts.
(1)Structure: There is an 𝒪(m) size 2-level DAG structure that stores all minimum+1 (s,t)-cuts of G such that each minimum+1 (s,t)-cut appears as 3-transversal cut—it intersects any path in this structure at most thrice. We also show that there is an 𝒪(mn) size structure for storing and characterizing all minimum+1 (s,t)-cuts in terms of 1-transversal cuts. (2)Data structure: There exists an 𝒪(n2) size data structure that, given a pair of vertices {u,v} that are not separated by an (s,t)-mincut, can determine in 𝒪(1) time if there exists a minimum+1 (s,t)-cut, say (A,B), such that s,u ∊ A and v,t∊ B; the corresponding cut can be reported in 𝒪(|B|) time.(3)Sensitivity oracle: There exists an 𝒪(n2) size data structure that solves the dual-edge sensitivity problem for (s,t)-mincuts. It takes 𝒪(1) time to report the capacity of a resulting (s,t)-mincut (A,B) and 𝒪(|B|) time to report the cut.(4)Lower bounds: For the data structure problems addressed in results (2) and (3) above, we also provide a matching conditional lower bound. We establish a close relationship among three seemingly unrelated problems—all-pairs directed reachability problem, the dual-edge sensitivity problem for (s,t)-mincuts, and the problem of reporting the capacity of ({x,y}, {u,v})-mincut for any four vertices x,y,u,v in G. Assuming the Directed Reachability Hypothesis by Patrascu [SIAM J. Computing, 827–847 (2011)] and Goldstein et&nbsp;al. [WADS, 421–436 (2017)], this leads to  (tilde{Omega }(n^2))  lower bounds on the space for the latter two problems.},
journal = {ACM Trans. Algorithms},
month = oct,
articleno = {38},
numpages = {41},
keywords = {characterization of cuts, graph structures, t)-cut, (s, sensitivity oracle, fault tolerant, maximum flow, minimum cuts, Minimum+1 cuts}
}

@article{10.1145/3623404,
author = {Li, Chu-Chen and Li, Cheng-Te and Lin, Shou-De},
title = {Learning Privacy-Preserving Embeddings for Image Data to Be Published},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3623404},
doi = {10.1145/3623404},
abstract = {Deep learning shows superiority in learning feature representations that offer promising performance in various application domains. Recent advances have shown that privacy attributes of users and patients (e.g., identity, gender, and race) can be accurately inferred from image data. To avoid the risk of privacy leaking, data owners can resort to releasing the embeddings rather than the original images. In this article, we aim at learning to generate privacy-preserving embeddings from image data. The obtained embeddings are required to maintain the data utility (e.g., keeping the performance of the main task, such as disease prediction) and to simultaneously prevent the private attributes of data instances from being accurately inferred. We also want the hard embeddings to be successfully used to reconstruct the original images. We propose a hybrid method based on multi-task learning to reach the goal. The key idea is twofold. One is to learn the feature encoder that can benefit the main task and fool the sensitive task at the same time via iterative training and feature disentanglement. The other is to incorporate the learning of adversarial examples to mislead the sensitive attribute classification’s performance. Experiments conducted on Multi-Attribute Facial Landmark (MAFL) and NIH Chest X-ray datasets exhibit the effectiveness of our hybrid method. A set of advanced studies also shows the usefulness of each model component, the difficulty in data reconstruction, and the performance impact of task correlation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {105},
numpages = {26},
keywords = {adversarial examples, feature disentanglement, adversarial learning, medical images, Privacy preservation}
}

@article{10.1145/3623640,
author = {Breuer, Timo and Fuhr, Norbert and Schaer, Philipp},
title = {Validating Synthetic Usage Data in Living Lab Environments},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3623640},
doi = {10.1145/3623640},
abstract = {Evaluating retrieval performance without editorial relevance judgments is challenging, but instead, user interactions can be used as relevance signals. Living labs offer a way for small-scale platforms to validate information retrieval systems with real users. If enough user interaction data are available, then click models can be parameterized from historical sessions to evaluate systems before exposing users to experimental rankings. However, interaction data are sparse in living labs, and little is studied about how click models can be validated for reliable user simulations when click data are available in moderate amounts.This work introduces an evaluation approach for validating synthetic usage data generated by click models in data-sparse human-in-the-loop environments like living labs. We ground our methodology on the click model’s estimates about a system ranking compared to a reference ranking for which the relative performance is known. Our experiments compare different click models and their reliability and robustness as more session log data become available. In our setup, simple click models can reliably determine the relative system performance with already 20 logged sessions for 50 queries. In contrast, more complex click models require more session data for reliable estimates, but they are a better choice in simulated interleaving experiments when enough session data are available. While it is easier for click models to distinguish between more diverse systems, it is harder to reproduce the system ranking based on the same retrieval algorithm with different interpolation weights. Our setup is entirely open, and we share the code to reproduce the experiments.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {5},
numpages = {33},
keywords = {Synthetic usage data, click signals, system evaluation, living labs}
}

@article{10.1145/3625101,
author = {Ye, Xiaoqing and Sun, Yang and Liu, Dun and Li, Tianrui},
title = {A Multisource Data Fusion-based Heterogeneous Graph Attention Network for Competitor Prediction},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3625101},
doi = {10.1145/3625101},
abstract = {Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {39},
numpages = {20},
keywords = {graph neural network, business data, business intelligence, Competitor identification}
}

@article{10.1145/3625264,
author = {Ponton, Jose Luis and Yun, Haoran and Aristidou, Andreas and Andujar, Carlos and Pelechano, Nuria},
title = {SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3625264},
doi = {10.1145/3625264},
abstract = {Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions though, it is important to minimize the number of sensors attached to the subject’s body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {5},
numpages = {14},
keywords = {Motion tracking, character animation, wearable devices, sparse data}
}

@article{10.1145/3625389,
author = {Herodotou, Herodotos and Kakoulli, Elena},
title = {Cost-based Data Prefetching and Scheduling in Big Data Platforms over Tiered Storage Systems},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3625389},
doi = {10.1145/3625389},
abstract = {The use of storage tiering is becoming popular in data-intensive compute clusters due to the recent advancements in storage technologies. The Hadoop Distributed File System, for example, now supports storing data in memory, SSDs, and HDDs, while OctopusFS and hatS offer fine-grained storage tiering solutions. However, current big data platforms (such as Hadoop and Spark) are not exploiting the presence of storage tiers and the opportunities they present for performance optimizations. Specifically, schedulers and prefetchers will make decisions only based on data locality information and completely ignore the fact that local data are now stored on a variety of storage media with different performance characteristics. This article presents Trident, a scheduling and prefetching framework that is designed to make task assignment, resource scheduling, and prefetching decisions based on both locality and storage tier information. Trident formulates task scheduling as a minimum cost maximum matching problem in a bipartite graph and utilizes two novel pruning algorithms for bounding the size of the graph, while still guaranteeing optimality. In addition, Trident extends YARN’s resource request model and proposes a new storage-tier-aware resource scheduling algorithm. Finally, Trident includes a cost-based data prefetching approach that coordinates with the schedulers for optimizing prefetching operations. Trident is implemented in both Spark and Hadoop and evaluated extensively using a realistic workload derived from Facebook traces as well as an industry-validated benchmark, demonstrating significant benefits in terms of application performance and cluster efficiency.},
journal = {ACM Trans. Database Syst.},
month = nov,
articleno = {11},
numpages = {40},
keywords = {task scheduling, data prefetching, tiered storage, Distributed file systems}
}

@article{10.1145/3626096,
author = {Saundefinedlican, Enes and Afacan, Engin},
title = {MOEA/D vs. NSGA-II: A Comprehensive Comparison for Multi/Many Objective Analog/RF Circuit Optimization through a Generic Benchmark},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3626096},
doi = {10.1145/3626096},
abstract = {Thanks to the enhanced computational capacity of modern computers, even sophisticated analog/radio frequency (RF) circuit sizing problems can be solved via electronic design automation (EDA) tools. Recently, several analog/RF circuit optimization algorithms have been successfully applied to automatize the analog/RF circuit design process. Conventionally, metaheuristic algorithms are widely used in optimization process. Among various nature-inspired algorithms, evolutionary algorithms (EAs) have been more preferred due to their superiorities (robustness, efficiency, accuracy etc.) over the other algorithms. Furthermore, EAs have been diversified and several distinguished analog/RF circuit optimization approaches for single-, multi-, and many-objective problems have been reported in the literature. However, there are conflicting claims on the performance of these algorithms and no objective performance comparison has been revealed yet. In the previous work, only a few case study circuits have been under test to demonstrate the superiority of the utilized algorithm, so a limited comparison has been made for only these specific circuits. The underlying reason is that the literature lacks a generic benchmark for analog/RF circuit sizing problem. To address these issues, we propose a comprehensive comparison of the most popular two evolutionary computation algorithms, namely Non-Sorting Genetic Algorithm-II and Multi-Objective Evolutionary Algorithm&nbsp;based Decomposition, in this article. For that purpose, we introduce two ad hoc testbenches for analog and RF circuits including the common building blocks. The comparison has been made at both multi- and many-objective domains and the performances of algorithms have been quantitatively revealed through the well-known Pareto-optimal front quality metrics.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {15},
numpages = {23},
keywords = {testbench, many-objective, optimization, design automation, MOEA/D, NSGA-II, CAD, EDA, evolutionary algorithms, metaheuristics, RF, Analog}
}

@article{10.1145/3626520,
author = {Zhou, Dawei and He, Jingrui},
title = {Rare Category Analysis for Complex Data: A Review},
year = {2023},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626520},
doi = {10.1145/3626520},
abstract = {Though the sheer volume of data that is collected is immense, it is the rare categories that are often the most important in many high-impact domains, ranging from financial fraud detection in online transaction networks to emerging trend detection in social networks, from spam image detection on social media platforms to rare disease diagnosis in medical decision support systems. The unique challenges of rare category analysis include (1) the highly skewed class distribution; (2) the non-separable nature of the rare categories from the majority classes; (3) data and task heterogeneity; and (4) the time-evolving property of the input data sources. This survey reviews state-of-the-art techniques used in complex rare category analysis, where the majority classes have a smooth distribution while the minority classes exhibit the compactness property in the feature space or subspace. Rare category analysis aims to identify, characterize, represent, and interpret anomalies that not only show statistical significance but also exhibit interesting patterns (e.g., compactness, high-order structures, showing in a burst). We introduce our study, define the problem setting, and describe the unique challenges of complex rare category analysis. We then present a comprehensive review of recent advances that are designed for this problem setting, from rare category exploration without any label information to rare category exposition that characterizes rare examples with a compact representation, from the representation of rare patterns in a salient embedding space to the interpretation the prediction results and providing relevant clues for the end-users’ interpretation. Finally we discuss potential challenges and shed light on the future directions for complex rare category analysis.1},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {123},
numpages = {35},
keywords = {anomaly detection, imbalanced data distribution, Rare category analysis}
}

@article{10.1145/3626712,
author = {Huang, Yicong and Wang, Zuozhi and Li, Chen},
title = {Udon: Efficient Debugging of User-Defined Functions in Big Data Systems with Line-by-Line Control},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626712},
doi = {10.1145/3626712},
abstract = {Many big data systems are written in languages such as C, C++, Java, and Scala to process large amounts of data efficiently, while data analysts often use Python to conduct data wrangling, statistical analysis, and machine learning. User-defined functions (UDFs) are commonly used in these systems to bridge the gap between the two ecosystems. In this paper, we propose Udon, a novel debugger to support fine-grained debugging of UDFs. Udon encapsulates the modern line-by-line debugging primitives, such as the ability to set breakpoints, perform code inspections, and make code modifications while executing a UDF on a single tuple. It includes a novel debug-aware UDF execution model to ensure the responsiveness of the operator during debugging. It utilizes advanced state-transfer techniques to satisfy breakpoint conditions that span across multiple UDFs. It incorporates various optimization techniques to reduce the runtime overhead. We conduct experiments with multiple UDF workloads on various datasets and show its high efficiency and scalability.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {225},
numpages = {26},
keywords = {big data systems, debugging, user-defined functions (UDFs)}
}

@article{10.1145/3626727,
author = {Islam, Md Mouinul and Asadi, Mahsa and Basu Roy, Senjuti},
title = {Equitable Top-k Results for Long Tail Data},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626727},
doi = {10.1145/3626727},
abstract = {For datasets exhibiting long tail phenomenon, we identify a fairness concern in existing top-k algorithms, that return a "fixed" set of k results for a given query. This causes a handful of popular records (products, items, etc) getting overexposed and always be returned to the user query, whereas, there exists a long tail of niche records that may be equally desirable (have similar utility). To alleviate this, we propose θ-Equiv-top-k-MMSP inside existing top-k algorithms - instead of returning a fixed top-k set, it generates all (or many) top-k sets that are equivalent in utility and creates a probability distribution over those sets. The end user will be returned one of these sets during the query time proportional to its associated probability, such that, after many draws from many end users, each record will have as equal exposure as possible (governed by uniform selection probability). θ-Equiv-top-k-MMSP is formalized with two sub-problems. (a) θ-Equiv-top-k-Sets to produce a set S of sets, each set has k records, where the sets are equivalent in utility with the top-k set; (b) MaxMinFair to produce a probability distribution over S, that is, PDF(S), such that the records in S have uniform selection probability. We formally study the hardness of θ-Equiv-top-k-MMSP. We present multiple algorithmic results - (a) An exact solution for θ-Equiv-top-k-Sets, and MaxMinFair. (b) We design highly scalable algorithms that solve θ-Equiv-top-k-Sets through a random walk and is backed by probability theory, as well as a greedy solution designed for MaxMinFair. (c) We finally present an adaptive random walk based algorithm that solves θ-Equiv-top-k-Sets and MaxMinFair at the same time. We empirically study how θ-Equiv-top-k-MMSP can alleviate a equitable exposure concerns that group fairness suffers from. We run extensive experiments using 6 datasets and design intuitive baseline algorithms that corroborate our theoretical analysis.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {240},
numpages = {24},
keywords = {equal exposure, fairness, long tail data, optimization algorithm, top-k retrieval}
}

@article{10.1145/3626732,
author = {Zhang, Jiujing and Shen, Zhitao and Yang, Shiyu and Meng, Lingkai and Xiao, Chuan and Jia, Wei and Li, Yue and Sun, Qinhui and Zhang, Wenjie and Lin, Xuemin},
title = {High-Ratio Compression for Machine-Generated Data},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626732},
doi = {10.1145/3626732},
abstract = {Machine-generated data is rapidly growing and poses challenges for data-intensive systems, especially as the growth of data outpaces the growth of storage space. To cope with the storage issue, compression plays a critical role in storage engines, particularly for data-intensive applications, where a high compression ratio and efficient random access are essential. However, existing compression techniques tend to focus on general-purpose and data block approaches, but overlook the inherent structure of machine-generated data and hence result in low compression ratios or limited lookup efficiency. To address these limitations, we introduce the Pattern-Based Compression (PBC) algorithm, which specifically targets patterns in machine-generated data to achieve Pareto-optimality in most cases. Unlike traditional data block-based methods, PBC compresses data on a per-record basis, facilitating rapid random access. Our experimental evaluation demonstrates that PBC, on average, achieves a compression ratio twice as high as the state-of-the-art techniques while maintaining competitive compression and decompression speeds. We also integrate PBC to a production database system and achieve improvements on both comparison ratio and throughput.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {245},
numpages = {27},
keywords = {data compression, machine-generated data}
}

@article{10.1145/3626737,
author = {Yang, Zehai and Chen, Shimin},
title = {MOST: Model-Based Compression with Outlier Storage for Time Series Data},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626737},
doi = {10.1145/3626737},
abstract = {Time series data are used in a wide variety of applications. The explosive growth of the amount of time series data poses a significant challenge in efficient data storage and query processing. Unfortunately, existing compression techniques either show only low to medium compression ratio on time series data, or incur significant decompression overhead during query processing.We propose a novel compression technique, MOST (Model-based compression with Outlier STorage) for time series data. As measurement values often change smoothly in a period of time, we divide a time series into segments of smooth changes, then compute a linear model for each segment. Since tiny errors are often acceptable in analysis tasks, we omit data points whose computed values are within a pre-specified error threshold from the actual values, thereby effectively reducing the data size. Outliers are rare but important for many applications, and therefore we store outliers explicitly. Moreover, for processing MOST compressed data, we propose a segment-outlier dual-mode query engine that computes segments as a whole as much as possible, and build a prototype MostDB. Experimental results on real-world data sets show that MOST achieves 9.45-15.04x compression ratios. Compared to existing time series databases, MostDB achieves up to 11.68x speedups for common queries from the IoTDB Benchmark.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {250},
numpages = {29},
keywords = {dual-mode query engine, model-based compression, outlier}
}

@article{10.1145/3626756,
author = {Wang, Qiming and Castro Fernandez, Raul},
title = {Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626756},
doi = {10.1145/3626756},
abstract = {Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility.In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform state-of-the-art approaches on well-known benchmarks. All in all, the technique is a stepping stone towards building learned discovery systems.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {262},
numpages = {27},
keywords = {data discovery, natural language questions, self-supervised}
}

@article{10.1145/3626757,
author = {Sha, Mo and Li, Jialin and Wang, Sheng and Li, Feifei and Tan, Kian-Lee},
title = {TEE-based General-purpose Computational Backend for Secure Delegated Data Processing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626757},
doi = {10.1145/3626757},
abstract = {The increasing prevalence of data breaches necessitates robust data protection measures in computational tasks. Secure computation outsourcing (SCO) presents a viable solution by safeguarding the confidentiality of inputs and outputs in data processing without disclosure. Nonetheless, this approach assumes the existence of a trustworthy coordinator to orchestrate and oversee the process, typically implying that data owners must fulfill this role themselves. In this paper, we consider secure delegated data processing (SDDP), an expanded data processing scenario wherein data owners simply delegate their data to SDDP providers for subsequent value mining or other downstream applications, eliminating the necessary involvement of data owners or trusted entities to dive into data processing deeply. However, general-purpose SDDP poses significant challenges in permitting the discretionary execution of computational tasks by SDDP providers on sensitive data while ensuring confidentiality. Existing approaches are insufficient to support SDDP in either efficiency or universality. To tackle this issue, we propose TGCB, a TEE-based General-purpose Computational Backend, designed to endow general-purpose computation with SDDP capabilities from an engineering perspective, powered by TEE-based code integrity and data confidentiality. Central to TGCB is the Encryption Programming Language (EPL) that defines computational tasks in SDDP. Specifically, SDDP providers can express arbitrary computable functions as EPL scripts, processed by TGCB's interfaces, securely interpreted and executed in TEE, ensuring data confidentiality throughout the process. As a universal computational backend, TGCB extensively bolsters data security in existing general-purpose computational tasks, allowing data owners to leverage SDDP without privacy concerns.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {263},
numpages = {28},
keywords = {data confidentiality, data processing, programming language and interpreter, secure delegated computing, trust execution environment}
}

@article{10.1145/3626758,
author = {Tian, Yao and Yan, Tingyun and Zhang, Ruiyuan and Huang, Kai and Zheng, Bolong and Zhou, Xiaofang},
title = {A Learned Cuckoo Filter for Approximate Membership Queries over Variable-sized Sliding Windows on Data Streams},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626758},
doi = {10.1145/3626758},
abstract = {Designing a space-efficient data structure to answer membership queries while ensuring high accuracy and real-time response is a challenging task in the field of stream processing. Many techniques have been developed to answer these queries in a sliding windows manner. However, assuming the user will conduct the query with the presupposed window size is not always practical. In this paper, we introduce a novel data structure called Learned Cuckoo Filter (LCF). It can provide satisfactory results for the approximate membership query on data streams, regardless of the user-defined query windows. LCF operates by adaptively maintaining cuckoo filters with the assistance of a well-trained oracle that learned the frequency feature of the data within the stream. To further enhance memory utilization, we develop a compact version of LCF (denoted by LCF_C), which selectively removes redundant information to reduce space consumption without compromising query accuracy. Furthermore, we conduct a thorough theoretical analysis of query accuracy and provide detailed guidelines for optimal parameter selection (denoted by LCF_O). Extensive experimental studies on synthetic and real-world datasets demonstrate the superiority of the proposed methods in terms of both space consumption and accuracy. Compared to the state-of-the-art algorithms, LCF_O can reduce up to 61\% of space cost at the same error level, and achieve up to 12\texttimes{} improved accuracy with the same space cost.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {264},
numpages = {26},
keywords = {approximate membership query, cuckoo filter, data stream, machine learning, variable-sized sliding windows}
}

@article{10.1145/3626775,
author = {Monterubbiano, Andrea and Langlet, Jonatan and Walzer, Stefan and Antichi, Gianni and Reviriego, Pedro and Pontarelli, Salvatore},
title = {Lightweight Acquisition and Ranging of Flows in the Data Plane},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3626775},
doi = {10.1145/3626775},
abstract = {As networks get more complex, the ability to track almost all the flows is becoming of paramount importance. This is because we can then detect transient events impacting only a subset of the traffic. Solutions for flow monitoring exist, but it is getting very difficult to produce accurate estimations for every &lt;flowID,counter&gt; tuple given the memory constraints of commodity programmable switches. Indeed, as networks grow in size, more flows have to be tracked, increasing the number of tuples to be recorded. At the same time, end-host virtualization requires more specific flowIDs, enlarging the memory cost for every single entry. Finally, the available memory resources have to be shared with other important functions as well (e.g., load balancing, forwarding, ACL).To address those issues, we present FlowLiDAR (Flow Lightweight Detection and Ranging), a new solution that is capable of tracking almost all the flows in the network while requiring only a modest amount of data plane memory which is not dependent on the size of flowIDs. We implemented the scheme in P4, tested it using real traffic from ISPs and compared it against four state-of-the-art solutions: FlowRadar, NZE, PR-sketch, and Elastic Sketch. While those can only reconstruct up to 60\% of the tuples, FlowLiDAR can track 98.7\% of them with the same amount of memory.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {44},
numpages = {24},
keywords = {programmable data plane, high-speed networking, flow measurement}
}

@article{10.1145/3627160,
author = {Zang, Andi and Xu, Runsheng and Trajcevski, Goce and Zhou, Fan},
title = {Data Issues in High-Definition Maps Furniture – A Survey},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3627160},
doi = {10.1145/3627160},
abstract = {The rapid advancements in sensing techniques, networking, and artificial intelligence (AI) algorithms in recent years have brought autonomous driving vehicles closer to common use in vehicular transportation. One of the fundamental components to enable autonomous driving functionalities are High-Definition (HD) maps – a type of map that carries highly accurate and much richer information than conventional maps. The creation and use of HD maps rely on advances in multiple disciplines, such as computer vision/object perception, geographic information systems, sensing, simultaneous localization and mapping, machine learning, etc. To date, several survey papers have been published describing the literature related to HD maps and their use in specialized contexts. In this survey, we aim to provide (1) a comprehensive overview of the issues and solutions related to HD maps and their use without attachment to a particular context; (2) a detailed coverage of the important domain knowledge of HD map furniture, from acquisition techniques and extraction approaches, through HD map–related datasets, to furniture quality assessment metrics, for the purpose of providing a comprehensive understanding of the entire workflow of HD map furniture generation, as well as its use.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jan,
articleno = {5},
numpages = {37},
keywords = {high-definition maps, heterogeneous datasets, autonomous driving}
}

@article{10.1145/3627169,
author = {Gooch, Megan and Strange, Damon},
title = {Consolidating Research Data Management Infrastructure: Towards Sustainable Digital Scholarship},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3627169},
doi = {10.1145/3627169},
abstract = {The sustainability of digital research outputs, particularly in the humanities where these frequently comprise archives of digital cultural heritage material, has always offered a challenge to the researchers and institutions who have responsibility for them. The amount of upfront care, effort, and funding that goes into developing a research project during the active (and funded) research phase is rarely replicated within the post-project maintenance and curation of the delivered digital assets or archives. What often defines the sustainability of a research project and its archive is a combination of research method and expected life span for the digital collection. Innovation in research data design is often at the expense of its longevity. But this does not need to be so. The tradeoff between longevity and functionality is a false dichotomy. Yet what is clear is that care and consideration in planning the research data storage or archive for a project can make a big difference. A data management plan that meets grant funder requirements is asked for for many research projects, but it is more than simply a funding document. Good research data management ensures that outputs are available online for years to come and available for future research and innovation. This article offers a practical insight to the methods being employed at the University of Oxford to support Digital Humanities scholars (and beyond) safeguard their digital legacy for future generations.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {82},
numpages = {16},
keywords = {digital humanities, digital sustainability, Research data management}
}

@article{10.1145/3627817,
author = {Xiong, Peiyu and Tegegn, Michael and Sarin, Jaskeerat Singh and Pal, Shubhraneel and Rubin, Julia},
title = {It Is All about Data: A Survey on the Effects of Data on Adversarial Robustness},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3627817},
doi = {10.1145/3627817},
abstract = {Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews a particular subset of this literature that focuses on investigating properties of training data in the context of model robustness under evasion attacks. It first summarizes the main properties of data leading to adversarial vulnerability. It then discusses guidelines and techniques for improving adversarial robustness by enhancing the data representation and learning procedures, as well as techniques for estimating robustness guarantees given particular data. Finally, it discusses gaps of knowledge and promising future research directions in this area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {174},
numpages = {41},
keywords = {Machine learning, adversarial robustness, evasion attack, data properties}
}

@article{10.1145/3628449,
author = {Huang, Qiang and Ma, Jing and Li, Jundong and Guo, Ruocheng and Sun, Huiyan and Chang, Yi},
title = {Modeling Interference for Individual Treatment Effect Estimation from Networked Observational Data},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3628449},
doi = {10.1145/3628449},
abstract = {Estimating individual treatment effect (ITE) from observational data has attracted great interest in recent years, which plays a crucial role in decision-making across many high-impact domains such as economics, medicine, and e-commerce. Most existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of other units connected to it in the network, which is referred to as the interference or spillover effect. In this article, we study an important problem of ITE estimation from networked observational data by modeling the interference between different units and provide a principled framework to support such study. Methodologically, we propose a novel framework, SPNet, that first captures the influence of hidden confounders with the aid of graph convolutional network and then models the interference by introducing an environment summary variable and developing a masked attention mechanism. Experimental evaluations on several semi-synthetic datasets based on real-world networks corroborate the superiority of our proposed framework over state-of-the-art individual treatment effect estimation methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {48},
numpages = {21},
keywords = {network interference, ITE estimation, Causal inference}
}

@article{10.1145/3629521,
author = {Zhang, Shiqing and Naderan-Tahan, Mahmood and Jahre, Magnus and Eeckhout, Lieven},
title = {Characterizing Multi-Chip GPU Data Sharing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3629521},
doi = {10.1145/3629521},
abstract = {Multi-chip Graphics Processing Unit (GPU) systems are critical to scale performance beyond a single GPU chip for a wide variety of important emerging applications. A key challenge for multi-chip GPUs, though, is how to overcome the bandwidth gap between inter-chip and intra-chip communication. Accesses to shared data, i.e., data accessed by multiple chips, pose a major performance challenge as they incur remote memory accesses possibly congesting the inter-chip links and degrading overall system performance. This article characterizes the shared dataset in multi-chip GPUs in terms of (1) truly versus falsely shared data, (2) how the shared dataset scales with input size, (3) along which dimensions the shared dataset scales, and (4) how sensitive the shared dataset is with respect to the input’s characteristics, i.e., node degree and connectivity in graph workloads. We observe significant variety in scaling behavior across workloads: some workloads feature a shared dataset that scales linearly with input size, whereas others feature sublinear scaling (following a  (sqrt {2})  or  (sqrt [3]{2})  relationship). We further demonstrate how the shared dataset affects the optimum last-level cache organization (memory-side versus SM-side) in multi-chip GPUs, as well as optimum memory page allocation and thread scheduling policy. Sensitivity analyses demonstrate the insights across the broad design space.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {56},
numpages = {24},
keywords = {data sharing, multi-GPU systems, Graphics processing unit (GPU)}
}

@article{10.1145/3631128,
author = {Henley, Amanda and Bruckner, Lorin and Jacobs, Hannah and Jansen, Matthew and Nunez, Brianna and Rodriguez, Rolando and Wilson, Morgan},
title = {On the Books: Jim Crow and Algorithms of Resistance, a Collections as Data Case Study},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3631128},
doi = {10.1145/3631128},
abstract = {On the Books: Jim Crow and Algorithms of Resistance is a collections as data and machine learning project from the University Libraries at the University of North Carolina at Chapel Hill. This project has created a plain text corpus of North Carolina legal volumes (1866–1967) and used machine learning to identify likely Jim Crow laws. The project has been well received and is now being expanded to two additional states, while assessing the use of On the Books products in research and instruction. State partners at the University of South Carolina and the University of Virginia are adapting the On the Books methodology to create corpora for their own states. Three teaching fellows created learning modules that use products from On the Books and taught the modules to college-level courses. Research fellows are making use of the products on research projects of their own design. This article will provide background for the On the Books project and will assess its use for multiple purposes: as a workflow to be reproduced by others, as content for use in teaching and learning, and as a resource for researchers. To demonstrate the utility of On the Books as a research tool, the article is co-authored by one of the research fellows. The project “Mental Health, Disability, and Jim Crow Laws in North Carolina, 1866–1967,” makes use of the legal corpus as a primary source for researching the intersections of information, mental health, nutrition, and shifts from agricultural to industrial economics in the history of North Carolina. By assessing the experiences of those making use of On the Books products, this article contributes to the understanding of best practices for those interested in creating and supporting collections as data so they may be used successfully for reproducibility, research, and teaching.},
journal = {J. Comput. Cult. Herit.},
month = jan,
articleno = {85},
numpages = {20},
keywords = {legal corpora, social justice, digital humanities, user experience, Collections as data}
}

@article{10.1145/3631448,
author = {Loerakker, Meagan B. and Niess, Jasmin and Bentvelzen, Marit and Wo\'{z}niak, Pawe\l{} W.},
title = {Designing Data Visualisations for Self-Compassion in Personal Informatics},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3631448},
doi = {10.1145/3631448},
abstract = {Wearable personal trackers offer exciting opportunities to contribute to one's well-being, but they also can foster negative experiences. It remains a challenge to understand how we can design personal informatics experiences that help users frame their data in a positive manner and foster self-compassion. To explore this, we conducted a study where we compared different visualisations for user-generated screen time data. We examined positive, neutral and negative framings of the data and whether or not a point of reference was provided in a visualisation. The results show that framing techniques have a significant effect on reflection, rumination and self-compassion. We contribute insights into what design features of data representations can support positive experiences in personal informatics.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {169},
numpages = {22},
keywords = {Personal informatics, data visualisation, reflection, rumination, self-compassion}
}

@article{10.1145/3631528,
author = {Zhang, Dunbo and Lang, Qingjie and Wang, Ruoxi and Shen, Li},
title = {Extension VM: Interleaved Data Layout in Vector Memory},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3631528},
doi = {10.1145/3631528},
abstract = {While vector architecture is widely employed in processors for neural networks, signal processing, and high-performance computing; however, its performance is limited by inefficient column-major memory access. The column-major access limitation originates from the unsuitable mapping of multidimensional data structures to two-dimensional vector memory spaces. In addition, the traditional data layout mapping method creates an irreconcilable conflict between row- and column-major accesses. Ideally, both row- and column-major accesses can take advantage of the bank parallelism of vector memory.To this end, we propose the Interleaved Data Layout (IDL) method in vector memory, which can distribute vector elements into different banks regardless of whether they are in the row- or column-major category, so that any vector memory access can benefit from bank parallelism. Additionally, we propose an Extension Vector Memory (EVM) architecture to achieve IDL in vector memory. EVM can support two data layout methods and vector memory access modes simultaneously. The key idea is to continuously distribute the data that needs to be accessed from the main memory to different banks during the loading period. Thus, EVM can provide a larger spatial locality level through careful programming and the extension ISA support.The experimental results showed a 1.43-fold improvement of state-of-the-art vector processors by the proposed architecture, with an area cost of only 1.73\%. Furthermore, the energy consumption was reduced by 50.1\%.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {18},
numpages = {23},
keywords = {Vector architecture, vector memory, processing-in-memory, data layout}
}

@article{10.1145/3631529,
author = {Luo, Longfei and Yu, Dingcui and Lv, Yina and Shi, Liang},
title = {Critical Data Backup with Hybrid Flash-Based Consumer Devices},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3631529},
doi = {10.1145/3631529},
abstract = {Hybrid flash-based storage constructed with high-density and low-cost flash memory has become increasingly popular in consumer devices in the last decade due to its low cost. However, its poor reliability is one of the major concerns. To protect critical data for guaranteeing user experience, some methods are proposed to improve the reliability of consumer devices with non-hybrid flash storage. However, with the widespread use of hybrid storage, these methods will result in severe problems, including significant performance and endurance degradation. This is caused by the fact that the different characteristics of flash memory in hybrid storage are not considered, e.g., performance, endurance, and access granularity. To address these problems, a critical data backup (CDB) design is proposed to ensure critical data reliability at a low cost. The basic idea is to accumulate two copies of critical data in the fast memory first to make full use of its performance and endurance. Then, one copy will be migrated to the slow memory in the stripe to avoid the write amplification caused by different access granularity between them. By respecting the different characteristics of flash memory in hybrid storage, CDB can achieve encouraging performance and endurance improvement compared with the state-of-the-art. Furthermore, to avoid performance and lifetime degradation caused by the backup data occupying too much space of fast memory, CDB Pro is designed. Two advanced schemes are integrated. One is making use of the pseudo-single-level-cell (pSLC) technique to make a part of slow memory become high-performance. By supplying some high-performance space, data will be fully updated before being evicted to slow memory. More invalid data are generated which reduces eviction costs. Another is to categorize data into three types according to their different life cycles. By putting the same type of data in a block, the eviction efficiency is improved. Therefore, both can improve device performance and lifetime based on CDB. Experiments are conducted to prove the efficiency of CDB and CDB Pro. Experimental results show that compared with the state-of-the-arts, CDB can ensure critical data reliability with lower device performance and lifetime loss whereas CDB Pro can diminish the loss further.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {1},
numpages = {23},
keywords = {critical data backup, hybrid SSDs, high-density NAND flash memory, RAID-1}
}

@article{10.1145/3632407,
author = {Le Deunf, Julian and Khannoussi, Arwa and Lecornu, Laurent and Meyer, Patrick and Puentes, John},
title = {Data Quality Assessment through a Preference Model},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3632407},
doi = {10.1145/3632407},
abstract = {Evaluating the quality of data is a problem of a multi-dimensional nature and quite frequently depends on the perspective of an expected use or final purpose of the data. Numerous works have explored the well-known specification of data quality dimensions in various application domains, without addressing the inter-dependencies and aggregation of quality attributes for decision support. In this work we therefore propose a context-dependent formal process to evaluate the quality of data which integrates a preference model from the field of Multi-Criteria Decision Aiding. The parameters of this preference model are determined through interviews with work-domain experts. We show the interest of the proposal on a case study related to the evaluation of the quality of hydrographical survey data.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {7},
numpages = {21},
keywords = {Data quality assessment, multi-criteria decision aiding, preference modelling, quality parameters, assessment explanations}
}

@article{10.1145/3632866,
author = {Sieczkowski, Filip and Stepanenko, Sergei and Sterling, Jonathan and Birkedal, Lars},
title = {The Essence of Generalized Algebraic Data Types},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632866},
doi = {10.1145/3632866},
abstract = {This paper considers direct encodings of generalized algebraic data types (GADTs) in a minimal suitable lambda-calculus. To this end, we develop an extension of System Fω with recursive types and internalized type equalities with injective constant type constructors. We show how GADTs and associated pattern-matching constructs can be directly expressed in the calculus, thus showing that it may be treated as a highly idealized modern functional programming language. We prove that the internalized type equalities in conjunction with injectivity rules increase the expressive power of the calculus by establishing a non-macro-expressibility result in Fω, and prove the system type-sound via a syntactic argument. Finally, we build two relational models of our calculus: a simple, unary model that illustrates a novel, two-stage interpretation technique, necessary to account for the equational constraints; and a more sophisticated, binary model that relaxes the construction to allow, for the first time, formal reasoning about data-abstraction in a calculus equipped with GADTs.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {24},
numpages = {29},
keywords = {Generalized Algebraic Data Types, Logical Relations}
}

@article{10.1145/3632870,
author = {Pailoor, Shankara and Wang, Yuepeng and Dillig, I\c{s}\i{}l},
title = {Semantic Code Refactoring for Abstract Data Types},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632870},
doi = {10.1145/3632870},
abstract = {Modifications to the data representation of an abstract data type (ADT) can require significant semantic refactoring of the code. Motivated by this observation, this paper presents a new method to automate semantic code refactoring tasks. Our method takes as input the original ADT implementation, a new data representation, and a so-called relational representation invariant (relating the old and new data representations), and automatically generates a new ADT implementation that is semantically equivalent to the original version. Our method is based on counterexample-guided inductive synthesis (CEGIS) but leverages three key ideas that allow it to handle real-world refactoring tasks. First, our approach reduces the underlying relational synthesis problem to a set of (simpler) programming-by-example problems, one for each method in the ADT. Second, it leverages symbolic reasoning techniques, based on logical abduction, to deduce code snippets that should occur in the refactored version. Finally, it utilizes a notion of partial equivalence to make inductive synthesis much more effective in this setting. We have implemented the proposed approach in a new tool called Revamp ‍ for automatically refactoring Java classes and evaluated it on 30 Java class mined from Github. Our evaluation shows that Revamp can correctly refactor the entire ADT in 97\% of the cases and that it can successfully re-implement 144 out of the 146 methods that require modifications.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {28},
numpages = {32},
keywords = {Abstract Data Types, Program Synthesis, Refactoring}
}

@article{10.1145/3632871,
author = {Sun, Pu and Song, Fu and Chen, Yuqi and Chen, Taolue},
title = {EasyBC: A Cryptography-Specific Language for Security Analysis of Block Ciphers against Differential Cryptanalysis},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632871},
doi = {10.1145/3632871},
abstract = {Differential cryptanalysis is a powerful algorithmic-level attack, playing a central role in evaluating the security of symmetric cryptographic primitives. In general, the resistance against differential cryptanalysis can be characterized by the maximum expected differential characteristic probability. In this paper, we present generic and extensible approaches based on mixed integer linear programming (MILP) to bound such probability. We design a high-level cryptography-specific language EasyBC tailored for block ciphers and provide various rigorous procedures, as differential denotational semantics, to automate the generation of MILP from block ciphers written in EasyBC. We implement an open-sourced tool that provides support for fully automated resistance evaluation of block ciphers against differential cryptanalysis. The tool is extensively evaluated on 23 real-life cryptographic primitives including all the 10 finalists of the NIST lightweight cryptography standardization process. The experiments confirm the expressivity of EasyBC and show that the tool can effectively prove the resistance against differential cryptanalysis for all block ciphers under consideration. EasyBC makes resistance evaluation against differential cryptanalysis easily accessible to cryptographers.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {29},
numpages = {34},
keywords = {Block Ciphers, Cryptography-Specific Language, Differential Cryptanalysis}
}

@article{10.1145/3632893,
author = {Chataing, Nicolas and Dolan, Stephen and Scherer, Gabriel and Yallop, Jeremy},
title = {Unboxed Data Constructors: Or, How cpp Decides a Halting Problem},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632893},
doi = {10.1145/3632893},
abstract = {We propose a new language feature for ML-family languages, the ability to selectively unbox certain data constructors, so that their runtime representation gets compiled away to just the identity on their argument. Unboxing must be statically rejected when it could introduce confusion, that is, distinct values with the same representation. We discuss the use-case of big numbers, where unboxing allows to write code that is both efficient and safe, replacing either a safe but slow version or a fast but unsafe version. We explain the static analysis necessary to reject incorrect unboxing requests. We present our prototype implementation of this feature for the OCaml programming language, discuss several design choices and the interaction with advanced features such as Guarded Algebraic Datatypes. Our static analysis requires expanding type definitions in type expressions, which is not necessarily normalizing in presence of recursive type definitions. In other words, we must decide normalization of terms in the first-order λ-calculus with recursion. We provide an algorithm to detect non-termination on-the-fly during reduction, with proofs of correctness and completeness. Our algorithm turns out to be closely related to the normalization strategy for macro expansion in the cpp preprocessor.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {51},
numpages = {31},
keywords = {boxing, data representation, recursive definitions, sum types, tagging, termination}
}

@article{10.1145/3633462,
author = {Ahmad, Khalid and Cecka, Cris and Garland, Michael and Hall, Mary},
title = {Exploring Data Layout for Sparse Tensor Times Dense Matrix on GPUs},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3633462},
doi = {10.1145/3633462},
abstract = {An important sparse tensor computation is sparse-tensor-dense-matrix multiplication (SpTM), which is used in tensor decomposition and applications. SpTM is a multi-dimensional analog to sparse-matrix-dense-matrix multiplication (SpMM). In this article, we employ a hierarchical tensor data layout that can unfold a multidimensional tensor to derive a 2D matrix, making it possible to compute SpTM using SpMM kernel implementations for GPUs. We compare two SpMM implementations to the state-of-the-art PASTA sparse tensor contraction implementation using: (1) SpMM with hierarchical tensor data layout; and, (2) unfolding followed by an invocation of cuSPARSE’s SpMM. Results show that SpMM can outperform PASTA 70.9\% of the time, but none of the three approaches is best overall. Therefore, we use a decision tree classifier to identify the best performing sparse tensor contraction kernel based on precomputed properties of the sparse tensor.},
journal = {ACM Trans. Archit. Code Optim.},
month = feb,
articleno = {20},
numpages = {20},
keywords = {Sparse tensors, SpMM, data layout}
}

@article{10.1145/3634751,
author = {Yang, Ming-Chuan and Wong, Guo-Wei and Chen, Meng Chang},
title = {Sparse Grid Imputation Using Unpaired Imprecise Auxiliary Data: Theory and Application to PM2.5 Estimation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3634751},
doi = {10.1145/3634751},
abstract = {Sparse grid imputation (SGI) is a challenging problem, as its goal is to infer the values of the entire grid from a limited number of cells with values. Traditionally, the problem is solved using regression methods such as KNN and kriging, whereas in the real world, there is often extra information—usually imprecise—that can aid inference and yield better performance. In the SGI problem, in addition to the limited number of fixed grid cells with precise target domain values, there are contextual data and imprecise observations over the whole grid. To solve this problem, we propose a distribution estimation theory for the whole grid and realize the theory via the composition architecture of the Target-Embedding and the Contextual CycleGAN trained with contextual information and imprecise observations. Contextual CycleGAN is structured as two generator–discriminator pairs and uses different types of contextual loss to guide the training. We consider the real-world problem of fine-grained PM2.5 inference with realistic settings: a few (less than 1\%) grid cells with precise PM2.5 data and all grid cells with contextual information concerning weather and imprecise observations from satellites and microsensors. The task is to infer reasonable values for all grid cells. As there is no ground truth for empty cells, out-of-sample mean squared error and Jensen–Shannon divergence measurements are used in the empirical study. The results show that Contextual CycleGAN supports the proposed theory and outperforms the methods used for comparison.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {65},
numpages = {26},
keywords = {distribution transformation, imprecise observations, contextual CycleGAN, Sparse grid imputation}
}

@article{10.1145/3634911,
author = {Anand, Abhijit and Leonhardt, Jurek and Singh, Jaspreet and Rudra, Koustav and Anand, Avishek},
title = {Data Augmentation for Sample Efficient and Robust Document Ranking},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {1046-8188},
url = {https://doi.org/10.1145/3634911},
doi = {10.1145/3634911},
abstract = {Contextual ranking models have delivered impressive performance improvements over classical models in the document ranking task. However, these highly over-parameterized models tend to be data-hungry and require large amounts of data even for fine-tuning. In this article, we propose data-augmentation methods for effective and robust ranking performance. One of the key benefits of using data augmentation is in achieving sample efficiency or learning effectively when we have only a small amount of training data. We propose supervised and unsupervised data augmentation schemes by creating training data using parts of the relevant documents in the query-document pairs. We then adapt a family of contrastive losses for the document ranking task that can exploit the augmented data to learn an effective ranking model. Our extensive experiments on subsets of the MS MARCO and TREC-DL test sets show that data augmentation, along with the ranking-adapted contrastive losses, results in performance improvements under most dataset sizes. Apart from sample efficiency, we conclusively show that data augmentation results in robust models when transferred to out-of-domain benchmarks. Our performance improvements in in-domain and more prominently in out-of-domain benchmarks show that augmentation regularizes the ranking model and improves its robustness and generalization capability.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {119},
numpages = {29},
keywords = {Information retrieval, IR, ranking, document ranking, contrastive loss, data augmentation, interpolation, ranking performance}
}

@article{10.1145/3634913,
author = {Wang, Tianyi and Chen, Shu-Ching},
title = {Adaptive Joint Spatio-Temporal Graph Learning Network for Traffic Data Forecasting},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {2374-0353},
url = {https://doi.org/10.1145/3634913},
doi = {10.1145/3634913},
abstract = {Traffic data forecasting has become an integral part of the intelligent traffic system. Great efforts are spent developing tools and techniques to estimate traffic flow patterns. Many existing approaches lack the ability to model the complex and dynamic spatio-temporal relations in the traffic data, which are crucial in capturing the traffic dynamic. In this work, we propose AJSTGL, a novel adaptive joint spatio-temporal graph learning network for traffic data forecasting. The proposed model utilizes static and adaptive graph learning modules to capture the static and dynamic spatial traffic patterns and optimize the graph learning process. A sequence-to-sequence fusion model is proposed to learn the temporal correlation and combine the output of multiple parallelized encoders. We also develop a spatio-temporal graph transformer module to complement the sequence-to-sequence fusion module by dynamically capturing the time-evolving node relations in long-term intervals. Experiments on three large-scale traffic flow datasets demonstrate that our model could outperform other state-of-the-art baseline methods.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = oct,
articleno = {26},
numpages = {20},
keywords = {Traffic forecasting, graph convolutional network, spatio-temporal data, transformer}
}

@article{10.1145/3635309,
author = {Clement, Nathan and Schoen, Alan and Boedihardjo, Arnold and Jenkins, Andrew},
title = {Synthetic Data and Hierarchical Object Detection in Overhead&nbsp;Imagery},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3635309},
doi = {10.1145/3635309},
abstract = {The performance of neural network models is often limited by the availability of big datasets. To treat this problem, we survey and develop novel synthetic data generation and augmentation techniques for enhancing low/zero-sample learning in satellite imagery. In addition to extending synthetic data generation approaches, we propose a hierarchical detection architecture to improve the utility of synthetic training samples. We consider existing techniques for producing synthetic imagery–3D models and neural style transfer–as well as introducing our own adversarially trained reskinning network, the GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic data in a two-stage, hierarchical detection/classification model of our own construction. To test the effectiveness of synthetic imagery, we employ it in the training of detection models and our two stage model, and evaluate the resulting models on real satellite images. All modalities of synthetic data are tested extensively on practical, geospatial analysis problems. Our experiments show that synthetic data developed using our approach can often enhance detection performance, particularly when combined with some real training images. When the only source of data is synthetic, our GAN-Reskinner often boosts performance over conventionally rendered 3D models and in all cases, the hierarchical model outperforms the baseline end-to-end detection architecture.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {117},
numpages = {20},
keywords = {low-shot learning, synthetic data, remote sensing, object detection, Deep learning}
}

@article{10.1145/3636423,
author = {Cheng, Debo and Li, Jiuyong and Liu, Lin and Liu, Jixue and Le, Thuc Duy},
title = {Data-Driven Causal Effect Estimation Based on Graphical Causal Modelling: A Survey},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3636423},
doi = {10.1145/3636423},
abstract = {In many fields of scientific research and real-world applications, unbiased estimation of causal effects from non-experimental data is crucial for understanding the mechanism underlying the data and for decision-making on effective responses or interventions. A great deal of research has been conducted to address this challenging problem from different angles. For estimating causal effect in observational data, assumptions such as Markov condition, faithfulness, and causal sufficiency are always made. Under the assumptions, full knowledge, such as a set of covariates or an underlying causal graph, is typically required. A practical challenge is that in many applications, no such full knowledge or only some partial knowledge is available. In recent years, research has emerged to use search strategies based on graphical causal modelling to discover useful knowledge from data for causal effect estimation, with some mild assumptions, and has shown promise in tackling the practical challenge. In this survey, we review these data-driven methods on causal effect estimation for a single treatment with a single outcome of interest and focus on the challenges faced by data-driven causal effect estimation. We concisely summarise the basic concepts and theories that are essential for data-driven causal effect estimation using graphical causal modelling but are scattered around the literature. We identify and discuss the challenges faced by data-driven causal effect estimation and characterise the existing methods by their assumptions and the approaches to tackling the challenges. We analyse the strengths and limitations of the different types of methods and present an empirical evaluation to support the discussions. We hope this review will motivate more researchers to design better data-driven methods based on graphical causal modelling for the challenging problem of causal effect estimation.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {127},
numpages = {37},
keywords = {instrumental variable, latent confounders, causal effect estimation, graphical causal model, causality, Causal inference}
}

@article{10.1145/3637315,
author = {Shelby, Renee and Srinivasan, Ramya and Burgdorf, Katharina and Lena, Jennifer C. and Rostamzadeh, Negar},
title = {Creative ML Assemblages: The Interactive Politics of People, Processes, and Products},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637315},
doi = {10.1145/3637315},
abstract = {Creative ML tools are collaborative systems that afford artistic creativity through their myriad interactive relationships. We propose using "assemblage thinking" to support analyses of creative ML by approaching it as a system in which the elements of people, organizations, culture, practices, and technology constantly influence each other. We model these interactions as "coordinating elements" that give rise to the social and political characteristics of a particular creative ML context, and call attention to three dynamic elements of creative ML whose interactions provide unique context for the social impact a particular system has: people, creative processes, and products. As creative assemblages are highly contextual, we present these as analytical concepts that computing researchers can adapt to better understand the functioning of a particular system or phenomena and identify intervention points to foster desired change. This paper contributes to theorizing interactions with AI in the context of art, and how these interactions shape the production of algorithmic art.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {38},
numpages = {30},
keywords = {art \&amp; technology, assemblage, creative ai, cultural studies, machine learning}
}

@article{10.1145/3637316,
author = {Tseng, Emily and Bellini, Rosanna and Lee, Yeuk-Yu and Ramjit, Alana and Ristenpart, Thomas and Dell, Nicola},
title = {Data Stewardship in Clinical Computer Security: Balancing Benefit and Burden in Participatory Systems},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637316},
doi = {10.1145/3637316},
abstract = {The mass collection and reuse of social data requires a reimagining of privacy and consent, with particular attention to the (in)equitable distribution of benefits and burdens between researchers and subjects. Instrumenting frontline clinical services to collect and steward data might mitigate the exploitation inherent to data collection---with attention to how subjects can meaningfully participate in stewardship. We explore participatory data stewardship in the context of clinical computer security for survivors of intimate partner violence (IPV). Via semi-structured interviews with IPV support workers, we explore how data are produced within the IPV care ecosystem at the Clinic to End Tech Abuse (CETA). We then conduct design provocations with clients of IPV services and their support workers, exploring possibilities for participatory data mechanisms like open records and dynamic consent. We find participation in data stewardship may benefit clients through improved agency, self-reflection, and control of self-narrative, and that incurred burdens may be alleviated by enlisting trusted stewards. We close with future work for CSCW interrogating how knowledge of digital-safety harms can and should be produced from clinical encounters, towards more equitable ways of knowing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {39},
numpages = {29},
keywords = {data stewardship, digital safety, intimate privacy, open science, participation}
}

@article{10.1145/3637367,
author = {Chandhiramowuli, Srravya and Taylor, Alex S. and Heitlinger, Sara and Wang, Ding},
title = {Making Data Work Count},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637367},
doi = {10.1145/3637367},
abstract = {In this paper, we examine the work of data annotation. Specifically, we focus on the role of counting or quantification in organising annotation work. Based on an ethnographic study of data annotation in two outsourcing centres in India, we observe that counting practices and its associated logics are an integral part of day-to-day annotation activities. In particular, we call attention to the presumption of total countability observed in annotation - the notion that everything, from tasks, datasets and deliverables, to workers, work time, quality and performance, can be managed by applying the logics of counting. To examine this, we draw on sociological and socio-technical scholarship on quantification and develop the lens of a 'regime of counting' that makes explicit the specific counts, practices, actors and structures that underpin the pervasive counting in annotation. We find that within the AI supply chain and data work, counting regimes aid the assertion of authority by the AI clients (also called requesters) over annotation processes, constituting them as reductive, standardised, and homogenous. We illustrate how this has implications for i) how annotation work and workers get valued, ii) the role human discretion plays in annotation, and iii) broader efforts to introduce accountable and more just practices in AI. Through these implications, we illustrate the limits of operating within the logic of total countability. Instead, we argue for a view of counting as partial - located in distinct geographies, shaped by specific interests and accountable in only limited ways. This, we propose, sets the stage for a fundamentally different orientation to counting and what counts in data annotation.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {90},
numpages = {26},
keywords = {accountability, artificial intelligence, counting, data annotation, data work, global south, quantification, workplace ethnography}
}

@article{10.1145/3637840,
author = {Balcan, Maria-Florina and Dick, Travis and Sandholm, Tuomas and Vitercik, Ellen},
title = {Learning to Branch: Generalization Guarantees and Limits of Data-Independent Discretization},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {71},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/3637840},
doi = {10.1145/3637840},
abstract = {Tree search algorithms, such as branch-and-bound, are the most widely used tools for solving combinatorial and non-convex problems. For example, they are the foremost method for solving (mixed) integer programs and constraint satisfaction problems. Tree search algorithms come with a variety of tunable parameters that are notoriously challenging to tune by hand. A growing body of research has demonstrated the power of using a data-driven approach to automatically optimize the parameters of tree search algorithms. These techniques use a training set of integer programs sampled from an application-specific instance distribution to find a parameter setting that has strong average performance over the training set. However, with too few samples, a parameter setting may have strong average performance on the training set but poor expected performance on future integer programs from the same application. Our main contribution is to provide the first sample complexity guarantees for tree search parameter tuning. These guarantees bound the number of samples sufficient to ensure that the average performance of tree search over the samples nearly matches its future expected performance on the unknown instance distribution. In particular, the parameters we analyze weight scoring rules used for variable selection. Proving these guarantees is challenging because tree size is a volatile function of these parameters: we prove that, for any discretization (uniform or not) of the parameter space, there exists a distribution over integer programs such that every parameter setting in the discretization results in a tree with exponential expected size, yet there exist parameter settings between the discretized points that result in trees of constant size. In addition, we provide data-dependent guarantees that depend on the volatility of these tree-size functions: our guarantees improve if the tree-size functions can be well approximated by simpler functions. Finally, via experiments, we illustrate that learning an optimal weighting of scoring rules reduces tree size.},
journal = {J. ACM},
month = apr,
articleno = {13},
numpages = {73},
keywords = {Parameter tuning, integer programming, tree search, branch-and-bound, learning theory, constraint satisfaction problems}
}

@article{10.1145/3638771,
author = {Liu, Xingyu and Hua, Zhongyun and Yi, Shuang and Zhang, Yushu and Zhou, Yicong},
title = {Bi-directional Block Encoding for Reversible Data Hiding over Encrypted Images},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {1551-6857},
url = {https://doi.org/10.1145/3638771},
doi = {10.1145/3638771},
abstract = {Reversible data hiding over encrypted images (RDH-EI) technology is a viable solution for privacy-preserving cloud storage, as it enables the reversible embedding of additional data into images while maintaining image confidentiality. Since the data hiders, e.g., cloud servers, are willing to embed as much data as possible for storage, management, or other processing purposes, a large embedding capacity is desirable in an RDH-EI scheme. In this article, we introduce a novel bi-directional block encoding (BDBE) method, which, for the first time, encodes the distances of values in a binary sequence from both ends. This approach allows for encoding images with smaller sizes compared to traditional and state-of-the-art encoding methods. Leveraging the BDBE technique, we propose a high-capacity RDH-EI scheme. In this scheme, the content owner initially predicts the image pixels and then employs BDBE to encode the prediction errors, creating space for data embedding. The resulting encoded data are subsequently encrypted using a secure stream cipher, such as the Advanced Encryption Standard, before being transmitted to a data hider. The data hider can embed confidential information within the encrypted image for the purposes of storage, management, or other processing. Upon receiving the data, an authorized receiver can accurately recover the original image and the embedded data without any loss. Experimental results demonstrate that our RDH-EI scheme achieves a significantly larger embedding capacity compared to several state-of-the-art schemes.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {149},
numpages = {23},
keywords = {Image encoding, reversible data hiding, encrypted image}
}

@article{10.1145/3638781,
author = {Liu, Gang and Inae, Eric and Luo, Tengfei and Jiang, Meng},
title = {Rationalizing Graph Neural Networks with Data Augmentation},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3638781},
doi = {10.1145/3638781},
abstract = {Graph rationales are representative subgraph structures that best explain and support the graph neural network (GNN) predictions. Graph rationalization involves the joint identification of these subgraphs during GNN training, resulting in improved interpretability and generalization. GNN is widely used for node-level tasks such as paper classification and graph-level tasks such as molecular property prediction. However, on both levels, little attention has been given to GNN rationalization and the lack of training examples makes it difficult to identify the optimal graph rationales. In this work, we address the problem by proposing a unified data augmentation framework with two novel operations on environment subgraphs to rationalize GNN prediction. We define the environment subgraph as the remaining subgraph after rationale identification and separation. The framework efficiently performs rationale–environment separation in the representation space for a node’s neighborhood graph or a graph’s complete structure to avoid the high complexity of explicit graph decoding and encoding. We conduct experiments on 17 datasets spanning node classification, graph classification, and graph regression. Results demonstrate that our framework is effective and efficient in rationalizing and enhancing GNNs for different levels of tasks on graphs.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {86},
numpages = {23},
keywords = {Graph neural network, node classification, graph property prediction, data augmentation, rationalization}
}

@article{10.1145/3639037,
author = {Jiang, Xi and Liu, Shinan and Gember-Jacobson, Aaron and Bhagoji, Arjun Nitin and Schmitt, Paul and Bronzino, Francesco and Feamster, Nick},
title = {NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3639037},
doi = {10.1145/3639037},
abstract = {Datasets of labeled network traces are essential for a multitude of machine learning (ML) tasks in networking, yet their availability is hindered by privacy and maintenance concerns, such as data staleness. To overcome this limitation, synthetic network traces can often augment existing datasets. Unfortunately, current synthetic trace generation methods, which typically produce only aggregated flow statistics or a few selected packet attributes, do not always suffice, especially when model training relies on having features that are only available from packet traces. This shortfall manifests in both insufficient statistical resemblance to real traces and suboptimal performance on ML tasks when employed for data augmentation. In this paper, we apply diffusion models to generate high-resolution synthetic network traffic traces. We present NetDiffusion1, a tool that uses a finely-tuned, controlled variant of a Stable Diffusion model to generate synthetic network traffic that is high fidelity and conforms to protocol specifications. Our evaluation demonstrates that packet captures generated from NetDiffusion can achieve higher statistical similarity to real data and improved ML model performance than current state-of-the-art approaches (e.g., GAN-based approaches). Furthermore, our synthetic traces are compatible with common network analysis tools and support a myriad of network tasks, suggesting that NetDiffusion can serve a broader spectrum of network analysis and testing tasks, extending beyond ML-centric applications.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = feb,
articleno = {11},
numpages = {32},
keywords = {diffusion model}, keywords{network traffic, synthesis}
}

@article{10.1145/3639057,
author = {Sagor, Mohammad and Haroon, Amran and Stoleru, Radu and Bhunia, Suman and Altaweel, Ala and Chao, Mengyuan and Jin, Liuyi and Maurice, Maxwell and Blalock, Roger},
title = {DistressNet-NG: A Resilient Data Storage and Sharing Framework for Mobile Edge Computing in Cyber-Physical Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3639057},
doi = {10.1145/3639057},
abstract = {Mobile Edge Computing (MEC) has been gaining a major interest for use in Cyber-Physical Systems (CPS) for Disaster Response and Tactical applications. These CPS generate a very large amount of mission-critical and personal data that require resilient and secure storage and sharing. In this article, we present the design, implementation, and evaluation of a framework for resilient data storage and sharing for MEC in CPS targeting the aforementioned applications. Our framework is built on the resiliency of three main components: EdgeKeeper, which ensures resilient coordination of the framework’s components; RSock, which provides resilient communication among CPS’s nodes; and R-Drive/R-Share which, leveraging EdgeKeeper and RSock, provides resilient data storage and sharing. EdgeKeeper employs a set of replicas and a consensus protocol for storing critical meta-data and ensuring fast reorganization of the CPS; RSock decides an optimal degree for replicating data that is communicated over lossy links. R-Drive employs an adaptive erasure-coded and encrypted resilient data storage; R-Share, leveraging RSock provides resilient peer-to-peer data sharing. We implemented our proposed framework on rapidly deployable systems (e.g., manpacks, testMobile Edge Clouds) and on Android devices, and integrated it with existing MEC applications. Performance evaluation results from three real-world deployments show that our framework provides resilient data storage and sharing in MEC for CPS.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = jul,
articleno = {37},
numpages = {31},
keywords = {Mobile Edge Cloud (MEC), disaster response, resilient storage, secure sharing, resilient coordination, adaptive erasure coding}
}

@article{10.1145/3639063,
author = {Chen, Zefeng and Gan, Wensheng and Wu, Jiayang and Hu, Kaixia and Lin, Hong},
title = {Data Scarcity in Recommendation Systems: A Survey},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639063},
doi = {10.1145/3639063},
abstract = {The prevalence of online content has led to the widespread adoption of recommendation systems (RSs), which serve diverse purposes such as news, advertisements, and e-commerce recommendations. Despite their significance, data scarcity issues have significantly impaired the effectiveness of existing RS models and hindered their progress. To address this challenge, the concept of knowledge transfer, particularly from external sources like pre-trained language models, emerges as a potential solution to alleviate data scarcity and enhance RS development. However, the practice of knowledge transfer in RSs is intricate. Transferring knowledge between domains introduces data disparities, and the application of knowledge transfer in complex RS scenarios can yield negative consequences if not carefully designed. Therefore, this article contributes to this discourse by addressing the implications of data scarcity on RSs and introducing various strategies, such as data augmentation, self-supervised learning, transfer learning, broad learning, and knowledge graph utilization, to mitigate this challenge. Furthermore, it delves into the challenges and future direction within the RS domain, offering insights that are poised to facilitate the development and implementation of robust RSs, particularly when confronted with data scarcity. We aim to provide valuable guidance and inspiration for researchers and practitioners, ultimately driving advancements in the field of RS.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = jan,
keywords = {Recommendation systems, data scarcity, transferring knowledge, large model, fusion}
}

@article{10.1145/3639268,
author = {Zeng, Xianzhi and Zhang, Shuhao and Zhong, Hongbin and Zhang, Hao and Lu, Mian and Zheng, Zhao and Chen, Yuqiang},
title = {PECJ: Stream Window Join on Disorder Data Streams with Proactive Error Compensation},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639268},
doi = {10.1145/3639268},
abstract = {Stream Window Join (SWJ), a vital operation in stream analytics, struggles with achieving a balance between accuracy and latency due to out-of-order data arrivals. Existing methods predominantly rely on adaptive buffering, but often fall short in performance, thereby constraining practical applications. We introduce PECJ, a solution that proactively incorporates unobserved data to enhance accuracy while reducing latency, thus requiring robust predictive modeling of stream oscillation. At the heart of PECJ lies a mathematical formulation of the posterior distribution approximation (PDA) problem using variational inference (VI). This approach circumvents error propagation while meeting the low-latency demands of SWJ. We detail the implementation of PECJ, striking a balance between complexity and generality, and discuss both analytical and learning-based approaches. Experimental evaluations reveal PECJ's superior performance. The successful integration of PECJ into a multi-threaded SWJ benchmark testbed further establishes its practical value, demonstrating promising advancements in enhancing data stream processing capabilities amidst out-of-order data.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {13},
numpages = {24},
keywords = {data stream, error compensation, out-of-order arrival, variational methods}
}

@article{10.1145/3639283,
author = {Liu, Tongyu and Fan, Ju and Tang, Nan and Li, Guoliang and Du, Xiaoyong},
title = {Controllable Tabular Data Synthesis Using Diffusion Models},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639283},
doi = {10.1145/3639283},
abstract = {Controllable tabular data synthesis plays a crucial role in numerous applications by allowing users to generate synthetic data with specific conditions. These conditions can include synthesizing tuples with predefined attribute values or creating tuples that exhibit a particular correlation with an external table. However, existing approaches lack the flexibility to support new conditions and can be time-consuming when dealing with multiple conditions. To overcome these limitations, we propose a novel approach that leverages diffusion models to first learn an unconditional generative model. Subsequently, we introduce lightweight controllers to guide the unconditional generative model in generating synthetic data that satisfies different conditions. The primary research challenge lies in effectively supporting controllability using lightweight solutions while ensuring the realism of the synthetic data. To address this challenge, we design an unconditional diffusion model tailored specifically for tabular data. Additionally, we propose a new sampling method that enables correlation-aware controls throughout the data generation process. We conducted extensive experiments across various applications for controllable tabular data synthesis, which show that our approach outperforms the state-of-the-art methods.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {28},
numpages = {29},
keywords = {controllable data synthesis, diffusion model, tabular data synthesis}
}

@article{10.1145/3639291,
author = {Jasny, Matthias and Thostrup, Lasse and Tamimi, Sajjad and Koch, Andreas and Istv\'{a}n, Zsolt and Binnig, Carsten},
title = {Zero-sided RDMA: Network-driven Data Shuffling for Disaggregated Heterogeneous Cloud DBMSs},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639291},
doi = {10.1145/3639291},
abstract = {In this paper, we present a novel communication scheme called zero-sided RDMA, enabling data exchange as a native network service using a programmable switch. In contrast to one- or two-sided RDMA, in zero-sided RDMA, neither the sender nor the receiver is actively involved in data exchange. Zero-sided RDMA thus enables efficient RDMA-based data shuffling between heterogeneous hardware devices in a disaggregated setup without the need to implement a complete RDMA stack on each heterogeneous device or the need for a CPU that is co-located with the accelerator to coordinate the data transfer. As such, we think that zero-sided RDMA is a major building block to make efficient use of heterogeneous accelerators in future cloud DBMSs. In our evaluation, we show that zero-sided RDMA can outperform existing one-sided RDMA-based schemes for accelerator-to-accelerator communication and thus speed up typical distributed database operations such as joins.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {36},
numpages = {28},
keywords = {FPGA, GPU, RDMA, communication scheme, heterogeneous compute}
}

@article{10.1145/3639311,
author = {Luo, Xuan and Pei, Jian and Xu, Cheng and Zhang, Wenjie and Xu, Jianliang},
title = {Fast Shapley Value Computation in Data Assemblage Tasks as Cooperative Simple Games},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639311},
doi = {10.1145/3639311},
abstract = {In this paper, we tackle the challenging problem of Shapley value computation in data markets in a novel setting of data assemblage tasks with binary utility functions among data owners. By modeling these scenarios as cooperative simple games, we leverage pivotal probabilities to transform the computation into a problem of counting beneficiaries. Moreover, we make an insightful observation that the Shapley values can be computed using subsets of minimal syntheses within the inclusion-exclusion framework in combinatorics. Based on this insight, we develop a game decomposition approach and utilize techniques in Boolean function decomposition into disjunctive normal form. One interesting property of our method is that the time complexity depends only on the data owners participating in those minimal syntheses, rather than all the data owners. Extensive experiments with real data sets demonstrate a significant efficiency improvement for computing the Shapley values in data assemblage tasks modeled as simple games.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {56},
numpages = {28},
keywords = {data assemblage, data market, shapley value, simple game}
}

@article{10.1145/3639405,
author = {Zhou, Zhiyuan and Zhou, Xiaolei and Guo, Baoshen and Wang, Shuai and He, Tian},
title = {Multi-sensor Data-driven Route Prediction in Instant Delivery with a 3-Conversion Network},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3639405},
doi = {10.1145/3639405},
abstract = {Route prediction in instant delivery is still challenging due to the unique characteristics compared with conventional delivery services, such as strict deadlines, overlapped delivery time of multiple orders, and diverse individual preferences on delivery routes. Recently, development in the mobile Internet of Things (IoT) offers the opportunity to collect multi-sensor data with rich real-time information. Therefore, this study proposes a route prediction model called Roupid, which leverages multi-sensor data to improve the accuracy of route prediction in instant delivery. Specifically, we design a 3-Conversion Network-based route prediction framework to take full advantage of various information provided by multi-sensor data, including the encounter data sensed by Bluetooth low energy (BLE) beacons, active site data reported by smart handheld devices, and trajectory data detected by GPS. The 3-Conversion Network we propose is based on a deep neural network framework, which integrates an improved relational graph attention network with edge features (RGATE) to encode global information that couriers typically consider when planning routes. We evaluate our Roupid with real-world data collected from one of the largest instant delivery companies in the world, i.e., Eleme. Experimental results show that our Roupid outperforms other state-of-the-art baselines and offers up to 85.51\% of the route prediction precision.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {50},
numpages = {21},
keywords = {Multi-sensor data, route prediction, instant delivery}
}

@article{10.1145/3639411,
author = {Li, Xinjiao and Wu, Guowei and Yao, Lin and Zheng, Zhaolong and Geng, Shisong},
title = {Utility-aware Privacy Perturbation for Training Data},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3639411},
doi = {10.1145/3639411},
abstract = {Data perturbation under differential privacy constraint is an important approach of protecting data privacy. However, as the data dimensions increase, the privacy budget&nbsp;allocated to each dimension decreases and thus the amount of noise added increases, which eventually leads to lower data utility in training tasks. To protect the privacy of training data while enhancing data utility, we propose a Utility-aware training data Privacy Perturbation scheme based on attribute Partition and budget Allocation (UPPPA). UPPPA includes three procedures: the quantification of attribute privacy and attribute importance, attribute partition, and budget&nbsp;allocation. The quantification of attribute privacy and attribute importance based on information entropy and attribute correlation provide an arithmetic basis for attribute partition and budget&nbsp;allocation. During the attribute partition, all attributes of training data are classified into high and low classes to achieve privacy amplification and utility enhancement. During the budget&nbsp;allocation, a γ-privacy model is proposed to balance data privacy and data utility so as to provide privacy constraint and guide budget&nbsp;allocation. Three comprehensive sets of real-world data are applied to evaluate the performance of UPPPA. Experiments and privacy analysis show that our scheme can achieve the tradeoff between privacy and utility.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {103},
numpages = {21},
keywords = {Training data privacy, Data perturbation, Data utility}
}

@article{10.1145/3639826,
author = {Wu, Jiaojiao and Cai, Zhigang and Yang, Fan and Li, Jun and Trahay, Francois and Yang, Zheng and Wang, Chao and Liao, Jianwei},
title = {Polling Sanitization to Balance I/O Latency and Data Security of High-density SSDs},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3639826},
doi = {10.1145/3639826},
abstract = {Sanitization is an effective approach for ensuring data security through scrubbing invalid but sensitive data pages, with the cost of impacts on storage performance due to moving out valid pages from the sanitization-required wordline, which is a logical read/write unit and consists of multiple pages in high-density SSDs. To minimize the impacts on I/O latency and data security, this article proposes a polling-based scheduling approach for data sanitization in high-density SSDs. Our method polls a specific SSD channel for completing data sanitization at the block granularity, meanwhile other channels can still service I/O requests. Furthermore, our method assigns a low priority to the blocks that are more likely to have future adjacent page invalidations inside sanitization-required wordlines, while selecting the sanitization block, to minimize the negative impacts of moving valid pages. Through a series of emulation experiments on several disk traces of real-world applications, we show that our proposal can decrease the negative effects of data sanitization in terms of the risk-performance index, which is a united time metric of I/O responsiveness and the unsafe time interval, by 16.34\%, on average, compared to related sanitization methods.},
journal = {ACM Trans. Storage},
month = feb,
articleno = {13},
numpages = {23},
keywords = {High-density SSDs, I/O latency, data security, sanitization, polling, scheduling}
}

@article{10.1145/3641028,
author = {Kolovson, Samantha and So, Samuel and Munson, Sean A.},
title = {Using Speculative Design to Understand Preferred Futures for the Design and Use of Tracking Data in U.S. College Sport Teams},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3641028},
doi = {10.1145/3641028},
abstract = {US college sports teams are increasingly adopting personal data technologies, such as wearable sensors, with a goal of improving individual and team performance as well as individual safety. These tools can also reinforce the power that coaches hold over student-athletes and compromise student-athletes' needs for privacy and agency. To investigate preferred, and anti-preferred, approaches for navigating this complex sociotechnical challenge, we used a speculative design approach in which student athletes and technology design students developed three videos that portray tensions between student-athletes and coaches around the use of sports tracking technologies. We then shared these videos with 15 participants including student-athletes, coaches, and designers. Drawing on the perspectives of student-athletes, team staff, and designers embedded in the videos and expressed in reaction to the videos, we describe preferences for boundaries on tracking and sharing, how tracking data represent athletes, and for data practices. We also propose design requirements and recommendations for use to better align tracking technologies with these preferences.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {189},
numpages = {35},
keywords = {data-supported collaboration, personal data, power, research through design, speculative design, sports}
}

@article{10.1145/3643541,
author = {Alchieri, Leonardo and Abdalazim, Nouran and Alecci, Lidia and Gashi, Shkurta and Gjoreski, Martin and Santini, Silvia},
title = {Lateralization Effects in Electrodermal Activity Data Collected Using Wearable Devices},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643541},
doi = {10.1145/3643541},
abstract = {Electrodermal activity (EDA) is a physiological signal that can be used to infer humans' affective states and stress levels. EDA can nowadays be monitored using unobtrusive wearable devices, such as smartwatches, and leveraged in personal informatics systems. A still largely uncharted issue concerning EDA is the impact on real applications of potential differences observable on signals measured concurrently on the left and right side of the human body. This phenomenon, called lateralization, originates from the distinct functions that the brain's left and right hemispheres exert on EDA. In this work, we address this issue by examining the impact of EDA lateralization in two classification tasks: a cognitive load recognition task executed in the lab and a sleep monitoring task in a real-world setting. We implement a machine learning pipeline to compare the performance obtained on both classification tasks using EDA data collected from the left and right sides of the body. Our results show that using EDA from the side that is not associated with the specific hemisphere activation leads to a significant decline in performance for the considered classification tasks. This finding highlights that researchers and practitioners relying on EDA data should consider possible EDA lateralization effects when deciding on sensor placement.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {2},
numpages = {30},
keywords = {Electrodermal Activity, cognitive load, lateralization, machine learning, sleep monitoring, wearable sensing}
}

@article{10.1145/3643641,
author = {Wei, Junyu and Zhang, Guangyan and Chen, Junchao and Wang, Yang and Zheng, Weimin and Sun, Tingtao and Wu, Jiesheng and Jiang, Jiangwei},
title = {Exploiting Data-pattern-aware Vertical Partitioning to Achieve Fast and Low-cost Cloud Log Storage},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3643641},
doi = {10.1145/3643641},
abstract = {Cloud logs can be categorized into on-line, off-line, and near-line logs based on the access frequency. Among them, near-line logs are mainly used for debugging, which means they prefer a low query latency for better user experience. Besides, the storage system for near-line logs prefers a low overall cost including the storage cost to store compressed logs, and the computation cost to compress logs and execute queries. These requirements pose challenges to achieving fast and cheap cloud log storage.This article proposes LogGrep, the first log compression and query tool that exploits both static and runtime patterns to properly structurize and organize log data in fine-grained units. The key idea of LogGrep is “vertical partitioning”: it stores each log entry into multiple partitions by first parsing logs into variable vectors according to static patterns and then extracting runtime pattern(s) automatically within each variable vector. Based on such runtime patterns, LogGrep further decomposes the variable vectors into fine-grained units called “Capsules” and stamps each Capsule with a summary of its values. During the query process, LogGrep can avoid decompressing and scanning Capsules that cannot match the keywords, with the help of the extracted runtime patterns and the Capsule stamps. We further show that the interactive debugging can well utilize the advantages of the vertical-partitioning-based method and mitigate its weaknesses as well. To this end, LogGrep integrates incremental locating and partial reconstruction to mitigate the read amplification incurred by vertical-partitioning-based method.We evaluate LogGrep on 37 cloud logs from the production environment of Alibaba Cloud and the public datasets. The results show that LogGrep can reduce the query latency and the overall cost by an order of magnitude compared with state-of-the-art works. Such results have confirmed that it is worthwhile applying a more sophisticated vertical-partitioning-based method to accelerate queries on compressed cloud logs.},
journal = {ACM Trans. Storage},
month = feb,
articleno = {12},
numpages = {35},
keywords = {Cloud log, data compression, full-text query, runtime pattern, static pattern}
}

@article{10.1145/3643812,
author = {Li, Wen and Bao, Lingfeng and Chen, Jiachi and Grundy, John and Xia, Xin and Yang, Xiaohu},
title = {Market Manipulation of Cryptocurrencies: Evidence from Social Media and Transaction Data},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3643812},
doi = {10.1145/3643812},
abstract = {The cryptocurrency market cap has experienced a great increase in recent years. However, large price fluctuations demonstrate the need for governance structures and identify whether there are market manipulations. In this article, we conduct three analyses—social media data analysis, blockchain data analysis, and price bubble analysis—to investigate whether market manipulation exists on Bitcoin, Ethereum, and Dogecoin platforms. Social media data analysis aims to find the reasons for price fluctuations. Blockchain data analysis is used to find detailed behavior of the manipulators. Price bubble analysis is used to investigate the relation between price fluctuation and manipulators’ behavior. By using the three analyses, we show that market manipulation exists on Bitcoin, Ethereum, and Dogecoin. However, market manipulation of Bitcoin is limited, and for most of Bitcoin’s price fluctuations, we found other explanations. The price for Ethereum is the most sensitive to technical updates. Technical companies/teams usually hype some new concepts (e.g., ICO, DeFi), which causes a price spike. The price of Dogecoin has a high correlation with Elon Musk’s X (formerly known as Twitter) activity, showing that influential individuals have the ability to manipulate its prices. In addition, the poor monetary liquidity of Dogecoin allows some users to manipulate its price.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {8},
numpages = {26},
keywords = {Blockchain, cryptocurrencies, market manipulation, empirical study}
}

@article{10.1145/3645097,
author = {Ho\ss{}feld, Konstantin and Damsgaard, Hans Jakob and Nurmi, Jar and Blott, Michaela and Preu\ss{}er, Thomas B.},
title = {High-efficiency Compressor Trees for Latest AMD FPGAs},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1936-7406},
url = {https://doi.org/10.1145/3645097},
doi = {10.1145/3645097},
abstract = {High-fan-in dot product computations are ubiquitous in highly relevant application domains, such as signal processing and machine learning. Particularly, the diverse set of data formats used in machine learning poses a challenge for flexible efficient design solutions. Ideally, a dot product summation is composed from a carry-free compressor tree followed by a terminal carry-propagate addition. On FPGA, these compressor trees are constructed from generalized parallel counters whose architecture is closely tied to the underlying reconfigurable fabric. This work reviews known counter designs and proposes new ones in the context of the new AMD Versal™ fabric. On this basis, we develop a compressor generator featuring variable-sized counters, novel counter composition heuristics, explicit clustering strategies, and case-specific optimizations like logic gate absorption. In comparison to the Vivado™ default implementation, the combination of such a compressor with a novel, highly efficient quaternary adder reduces the LUT footprint across different bit matrix input shapes by 45\% for a plain summation and by 46\% for a terminal accumulation at a slight cost in critical path delay still allowing an operation well above 500&nbsp;MHz. We demonstrate the aptness of our solution at examples of low-precision integer dot product accumulation units.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = apr,
articleno = {30},
numpages = {32},
keywords = {Compressor tree, matrix compression, parallel counters}
}

@article{10.1145/3648105,
author = {Yao, Zhenjie and Chen, Yixin and Wang, Jinwei and Li, Junjuan and Chen, Shuohua and Wu, Shouling and Tu, Yanhui and Zhao, Ming-Hui and Zhang, Luxia},
title = {Interpretable Trend Analysis Neural Networks for Longitudinal Data Analysis},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3648105},
doi = {10.1145/3648105},
abstract = {Cohort study is one of the most commonly used study methods in medical and public health researches, which result in longitudinal data. Conventional statistical models and machine learning methods are not capable of modeling the evolution trend of the variables in longitudinal data. In this article, we propose a Trend Analysis Neural Networks (TANN), which models the evolution trend of the variables by adaptive feature learning. TANN was tested on dataset of Kaiuan research. The task was to predict occurrence of cardiovascular events within 2 and 5 years, with three repeated medical examinations during 2008 and 2013. For 2-year prediction, The AUC of the TANN is 0.7378, which is a significant improvement than that of conventional methods, while that of TRNS, RNN, DNN, GBDT, RF, and LR are 0.7222, 0.7034, 0.7054, 0.7136, 0.7160, and 0.7024, respectively. For 5-year prediction, TANN also shows improvement. The experimental results show that the proposed TANN achieves better prediction performance on cardiovascular events prediction than conventional models. Furthermore, by analyzing the weights of TANN, we could find out important trends of the indicators, which are ignored by conventional machine learning models. The trend discovery mechanism interprets the model well. TANN is an appropriate balance between high performance and interpretability.},
journal = {ACM Trans. Comput. Healthcare},
month = apr,
articleno = {8},
numpages = {13},
keywords = {Neural networks, longitudinal data, trend analysis, interpretability}
}

@article{10.1145/3648356,
author = {Jung, Gyuwon and Park, Sangjun and Ma, Eun-Yeol and Kim, Heeyoung and Lee, Uichin},
title = {Tutorial on Matching-based Causal Analysis of Human Behaviors Using Smartphone Sensor Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3648356},
doi = {10.1145/3648356},
abstract = {Smartphones can unobtrusively capture human behavior and contextual data such as user interaction and mobility. Thus far, smartphone sensor data have primarily been used to gain behavioral insights through correlation analysis. This article provides a tutorial on the causal analysis of human behavior using smartphone sensor data by reviewing well-known matching methods. The key steps of the causal inference pipeline employing matching methods are illustrated using a concrete scenario involving the identification of a causal relationship between phone usage and physical activity. Several practical considerations for conducting causal inferences about human behaviors using smartphone sensor data are also discussed.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {236},
numpages = {33},
keywords = {Smartphone sensor data, causal inference, human behavior, observational study}
}

@article{10.1145/3648476,
author = {Pereira, Jo\~{a}o L. M. and Fonseca, Manuel J. and Lopes, Ant\'{o}nia and Galhardas, Helena},
title = {Cleenex: Support for User Involvement during an Iterative Data Cleaning Process},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3648476},
doi = {10.1145/3648476},
abstract = {The existence of large amounts of data increases the probability of occurring data quality problems. A data cleaning process that corrects these problems is usually an iterative process, because it may need to be re-executed and refined to produce high-quality data. Moreover, due to the specificity of some data quality problems and the limitation of data cleaning programs to cover all problems, often a user has to be involved during the program executions by manually repairing data. However, there is no data cleaning framework that appropriately supports this involvement in such an iterative process, a form of human-in-the-loop, to clean structured data. Moreover, data preparation tools that somehow involve the user in data cleaning processes have not been evaluated with real users to assess their effort.Therefore, we propose Cleenex, a data cleaning framework with support for user involvement during an iterative data cleaning process, and conduct two data cleaning experimental evaluations: an assessment of the Cleenex components that support the user when manually repairing data with a simulated user; and a comparison, in terms of user involvement, of data preparation tools with real users.Results show that Cleenex components reduce the user effort when manually cleaning data during a data cleaning process, for example, the number of tuples visualized is reduced in 99\%. Moreover, when performing data cleaning tasks with Cleenex, real users need less time/effort (e.g., half the clicks) and, based on questionnaires, prefer it to the other tools used for comparison, OpenRefine and Pentaho Data Integration.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {6},
numpages = {26},
keywords = {Data quality, data curation, user involvement, human-in-the-loop}
}

@article{10.1145/3648609,
author = {Wang, Shiyu and Du, Yuanqi and Guo, Xiaojie and Pan, Bo and Qin, Zhaohui and Zhao, Liang},
title = {Controllable Data Generation by Deep Learning: A Review},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3648609},
doi = {10.1145/3648609},
abstract = {Designing and generating new data under targeted properties has been attracting various critical applications such as molecule design, image editing and speech synthesis. Traditional hand-crafted approaches heavily rely on expertise experience and intensive human efforts, yet still suffer from the insufficiency of scientific knowledge and low throughput to support effective and efficient data generation. Recently, the advancement of deep learning has created the opportunity for expressive methods to learn the underlying representation and properties of data. Such capability provides new ways of determining the mutual relationship between the structural patterns and functional properties of the data and leveraging such relationships to generate structural data, given the desired properties. This article is a systematic review that explains this promising research area, commonly known as controllable deep data generation. First, the article raises the potential challenges and provides preliminaries. Then the article formally defines controllable deep data generation, proposes a taxonomy on various techniques and summarizes the evaluation metrics in this specific domain. After that, the article introduces exciting applications of controllable deep data generation, experimentally analyzes and compares existing works. Finally, this article highlights the promising future directions of controllable deep data generation and identifies five potential challenges.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {228},
numpages = {38},
keywords = {Data generation, deep learning, deep generative models, property controllable generation}
}

@article{10.1145/3648613,
author = {Perelman, Gary and Serrano, Marcos and Dubois, Emmanuel},
title = {Exploiting Physical Referent Features as Input for Multidimensional Data Selection in Augmented Reality},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3648613},
doi = {10.1145/3648613},
abstract = {Embedding data into the physical environment using augmented reality (AR) is a practical approach for data visualization as it offers a large and flexible display space on or around the physical referent, i.e., the physical object to which the data is related. Yet, current interaction in such context is often performed using cumbersome dedicated devices, tiring mid-air gestures, or awkward on-body input. In this article, we investigate the use of the physical referent itself as a support for input interaction with an embedded space-time cube (STC) representation. Hence, we first identify the most promising mappings between the physical features of the referent (edges, faces, corners) and the STC dimensions. Then, we design three data selection techniques using the physical referent and compare them to mid-air gestures when performing selection tasks on the STC. Overall, our work demonstrates that using the physical referent to support input interaction with embedded data representations is an efficient and comfortable approach for data selection in standing and sitting situations.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {50},
numpages = {40},
keywords = {Augmented reality, physical referent, interaction techniques}
}

@article{10.1145/3649134,
author = {Bugedo, Sebasti\'{a}n and Riveros, Cristian and Salas, Jorge},
title = {A Family of Centrality Measures for Graph Data Based on Subgraphs},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3649134},
doi = {10.1145/3649134},
abstract = {We present the theoretical foundations and first experimental study of a new approach in centrality measures for graph data. The main principle is straightforward: the more relevant subgraphs around a vertex, the more central it is in the network. We formalize the notion of “relevant subgraphs” by choosing a family of subgraphs that, given a graph G and a vertex v, assigns a subset of connected subgraphs of G that contains v. Any of such families defines a measure of centrality by counting the number of subgraphs assigned to the vertex, i.e., a vertex will be more important for the network if it belongs to more subgraphs in the family. We show several examples of this approach. In particular, we propose the All-Subgraphs (All-Trees) centrality, a centrality measure that considers every subgraph (tree). We study fundamental properties over families of subgraphs that guarantee desirable properties over the centrality measure. Interestingly, All-Subgraphs and All-Trees satisfy all these properties, showing their robustness as centrality notions. To conclude the theoretical analysis, we study the computational complexity of counting certain families of subgraphs and show a linear time algorithm to compute the All-Subgraphs and All-Trees centrality for graphs with bounded treewidth. Finally, we implemented these algorithms and computed these measures over more than one hundred real-world networks. With this data, we present an empirical comparison between well-known centrality measures and those proposed in this work.},
journal = {ACM Trans. Database Syst.},
month = may,
articleno = {10},
numpages = {45},
keywords = {Graph data, graph centrality, centrality measures}
}

@article{10.1145/3649319,
author = {Quattrocchi, Giovanni and Heuvel, Willem-Jan van den and Tamburri, Damian Andrew},
title = {The Data Product-service Composition Frontier: A Hybrid Learning Approach},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3649319},
doi = {10.1145/3649319},
abstract = {The service dominant logic is a base concept behind modern economies and software products, with service composition being a well-known practice for companies to gain a competitive edge over others by joining differentiated services together, typically assembled according to a number of features. At the other end of the spectrum, product compositions are a marketing device to sell products together in bundles that often augment the value for the customer, e.g., with suggested product interactions, sharing, and so on. Unfortunately, currently each of these two streams—namely, product and service composition—are carried out and delivered individually in splendid isolation: anything is being offered as a product and as a service, disjointly. We argue that the next wave of services computing features more and more service fusion with physical counterparts as well as data around them. Therefore a need emerges to investigate the interactive engagement of both (data) products and services. This manuscript offers a real-life implementation in support of this argument, using (1) genetic algorithms (GA) to shape product-service clusters, (2) end-user feedback to make the GAs interactive with a data-driven fashion, and (3) a hybridized approach which factors into our solution an ensemble machine-learning method considering additional features. All this research was conducted in an industrial environment. With such a cross-fertilized, data-driven, and multi-disciplinary approach, practitioners from both fields may benefit from their mutual state of the art as well as learn new strategies for product, service, and data product-service placement for increased value to the customer as well as the service provider. Results show promise but also highlight plenty of avenues for further research.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {6},
numpages = {22},
keywords = {Anomaly detection, time series, unsupervised, literature review}
}

@article{10.1145/3649447,
author = {Zhao, Fei and Zhang, Chengcui and Geng, Baocheng},
title = {Deep Multimodal Data Fusion},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3649447},
doi = {10.1145/3649447},
abstract = {Multimodal Artificial Intelligence (Multimodal AI), in general, involves various types of data (e.g., images, texts, or data collected from different sensors), feature engineering (e.g., extraction, combination/fusion), and decision-making (e.g., majority vote). As architectures become more and more sophisticated, multimodal neural networks can integrate feature extraction, feature fusion, and decision-making processes into one single model. The boundaries between those processes are increasingly blurred. The conventional multimodal data fusion taxonomy (e.g., early/late fusion), based on which the fusion occurs in, is no longer suitable for the modern deep learning era. Therefore, based on the main-stream techniques used, we propose a new fine-grained taxonomy grouping the state-of-the-art (SOTA) models into five classes: Encoder-Decoder methods, Attention Mechanism methods, Graph Neural Network methods, Generative Neural Network methods, and other Constraint-based methods. Most existing surveys on multimodal data fusion are only focused on one specific task with a combination of two specific modalities. Unlike those, this survey covers a broader combination of modalities, including Vision + Language (e.g., videos, texts), Vision + Sensors (e.g., images, LiDAR), and so on, and their corresponding tasks (e.g., video captioning, object detection). Moreover, a comparison among these methods is provided, as well as challenges and future directions in this area.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {216},
numpages = {36},
keywords = {Data fusion, neural networks, multimodal deep learning}
}

@article{10.1145/3649473,
author = {Azorin, Raphael and Monterubbiano, Andrea and Castellano, Gabriele and Gallo, Massimo and Pontarelli, Salvatore and Rossi, Dario},
title = {Taming the Elephants: Affordable Flow Length Prediction in the Data Plane},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT1},
url = {https://doi.org/10.1145/3649473},
doi = {10.1145/3649473},
abstract = {Machine Learning (ML) shows promising potential for enhancing networking tasks by providing early traffic predictions. However, implementing an ML-enabled system is a challenging task due to network devices limited resources. While previous works have shown the feasibility of running simple ML models in the data plane, integrating them into a practical end-to-end system is not an easy task. It requires addressing issues related to resource management and model maintenance to ensure that the performance improvement justifies the system overhead. In this work, we propose DUMBO, a versatile end-to-end system to generate and exploit early flow size predictions at line rate. Our system seamlessly integrates and maintains a simple ML model that offers early coarse-grain flow size prediction in the data plane. We evaluate the proposed system on flow scheduling, per-flow packet inter-arrival time distribution, and flow size estimation using real traffic traces, and perform experiments using an FPGA prototype running on an AMD(R)-Xilinx(R) Alveo U280 SmartNIC. Our results show that DUMBO outperforms traditional state-of-the-art approaches by equipping network devices data planes with a lightweight ML model. Code is available at https://github.com/cpt-harlock/DUMBO.},
journal = {Proc. ACM Netw.},
month = mar,
articleno = {5},
numpages = {24},
keywords = {data plane, in-network machine learning, per-flow monitoring}
}

@article{10.1145/3649596,
author = {Wan, Xiaohui and Zheng, Zheng and Qin, Fangyun and Lu, Xuhui},
title = {Data Complexity: A New Perspective for Analyzing the Difficulty of Defect Prediction Tasks},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3649596},
doi = {10.1145/3649596},
abstract = {Defect prediction is crucial for software quality assurance and has been extensively researched over recent decades. However, prior studies rarely focus on data complexity in defect prediction tasks, and even less on understanding the difficulties of these tasks from the perspective of data complexity. In this article, we conduct an empirical study to estimate the hardness of over 33,000 instances, employing a set of measures to characterize the inherent difficulty of instances and the characteristics of defect datasets. Our findings indicate that: (1) instance hardness in both classes displays a right-skewed distribution, with the defective class exhibiting a more scattered distribution; (2) class overlap is the primary factor influencing instance hardness and can be characterized through feature, structural, and instance-level overlap; (3) no universal preprocessing technique is applicable to all datasets, and it may not consistently reduce data complexity, fortunately, dataset complexity measures can help identify suitable techniques for specific datasets; (4)&nbsp;integrating data complexity information into the learning process can enhance an algorithm’s learning capacity. In summary, this empirical study highlights the crucial role of data complexity in defect prediction tasks, and provides a novel perspective for advancing research in defect prediction techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {141},
numpages = {45},
keywords = {Defect prediction, machine learning, data complexity, instance hardness}
}

@article{10.1145/3649838,
author = {Sato, Shigeyuki and Nakamaru, Tomoki},
title = {Multiverse Notebook: Shifting Data Scientists to Time Travelers},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649838},
doi = {10.1145/3649838},
abstract = {Computational notebook environments are popular and de facto standard  
tools for programming in data science, whereas computational notebooks are notorious in  
software engineering. The criticism there stems from the characteristic  
of facilitating unrestricted dynamic patching of running programs, which  
makes exploratory coding quick but the resultant code messy and  
inconsistent. In this work, we first reveal that dynamic patching is a  
natural demand rather than a mere bad practice in data science  
programming on Kaggle. We then develop Multiverse Notebook, a  
computational notebook engine for time-traveling exploration. It  
enables users to time-travel to any past state and restart with new code  
from there under state isolation. We present an approach to efficiently  
implementing time-traveling exploration. We empirically evaluate  
Multiverse Notebook on ten real-world tasks from Kaggle. Our experiments  
show that time-traveling exploration on Multiverse Notebook is  
reasonably efficient.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {121},
numpages = {30},
keywords = {Computational notebook, Exploratory programming, Memory management}
}

@article{10.1145/3649840,
author = {Ryan, Gabriel and Cetin, Burcu and Lim, Yongwhan and Jana, Suman},
title = {Accurate Data Race Prediction in the Linux Kernel through Sparse Fourier Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649840},
doi = {10.1145/3649840},
abstract = {Testing for data races in the Linux OS kernel is challenging because there is an exponentially large space of system calls and thread interleavings that can potentially lead to concurrent executions with races. In this work, we introduce a new approach for modeling execution trace feasibility and apply it to Linux OS Kernel race prediction. To address the fundamental scalability challenge posed by the exponentially large domain of possible execution traces, we decompose the task of predicting trace feasibility into independent prediction subtasks encoded as learning Boolean indicator functions for specific memory accesses, and apply a sparse fourier learning approach to learning each feasibility subtask.  

Boolean functions that are sparse in their fourier domain can be efficiently learned by estimating the coefficients of their fourier expansion. Since the feasibility of each memory access depends on only a few other relevant memory accesses or system calls (e.g., relevant inter-thread communications), we observe that trace feasibility functions often have this sparsity property and can be learned efficiently. We use learned trace feasibility functions in conjunction with conservative alias analysis to implement a kernel race-testing system, HBFourier, that uses sparse fourier learning to efficiently model feasibility when making predictions. We evaluate our approach on a recent Linux development kernel and show it finds 44 more races with 15.7\% more accurate race predictions than the next best performing system in our evaluation, in addition to identifying 5 new race bugs confirmed by kernel developers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {123},
numpages = {23},
keywords = {Data Race Prediction, Linux Kernel Testing, Sparse Fourier Learning}
}

@article{10.1145/3649894,
author = {Wang, Pengfei and Jiao, Dian and Yang, Leyou and Wang, Bin and Yu, Ruiyun},
title = {Hypergraph-based Truth Discovery for Sparse Data in Mobile Crowdsensing},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3649894},
doi = {10.1145/3649894},
abstract = {Mobile crowdsensing leverages the power of a vast group of participants to collect sensory data, thus presenting an economical solution for data collection. However, due to the variability among participants, the quality of sensory data varies significantly, making it crucial to extract truthful information from sensory data of differing quality. Additionally, given the fixed time and monetary costs for the participants, they typically only perform a subset of tasks. As a result, the datasets collected in real-world scenarios are usually sparse. Current truth discovery methods struggle to adapt to datasets with varying sparsity, especially when dealing with sparse datasets. In this article, we propose an adaptive Hypergraph-based EM truth discovery method, HGEM. The HGEM algorithm leverages the topological characteristics of hypergraphs to model sparse datasets, thereby improving its performance in evaluating the reliability of participants and the true value of the event to be observed. Experiments based on simulated and real-world scenarios demonstrate that HGEM consistently achieves higher predictive accuracy.},
journal = {ACM Trans. Sen. Netw.},
month = apr,
articleno = {69},
numpages = {23},
keywords = {Mobile crowdsensing, truth discovery, hypergraph, sparse data}
}

@article{10.1145/3651147,
author = {Lindner, Peter and Basil John, Sachin and Koch, Christoph and Suciu, Dan},
title = {The Moments Method for Approximate Data Cube Queries},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3651147},
doi = {10.1145/3651147},
abstract = {We investigate an approximation algorithm for various aggregate queries on partially materialized data cubes. Data cubes are interpreted as probability distributions, and cuboids from a partial materialization populate the terms of a series expansion of the target query distribution. Unknown terms in the expansion are just assumed to be 0 in order to recover an approximate query result. We identify this method as a variant of related approaches from other fields of science, that is, the Bahadur representation and, more generally, (biased) Fourier expansions of Boolean functions. Existing literature indicates a rich but intricate theoretical landscape. Focusing on the data cube application, we start by investigating worst-case error bounds. We build upon prior work to obtain provably optimal materialization strategies with respect to query workloads. In addition, we propose a new heuristic method governing materialization decisions. Finally, we show that well-approximated queries are guaranteed to have well-approximated roll-ups.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {84},
numpages = {23},
keywords = {aggregate queries, approximate query answering, bahadur expansion, data cubes, fourier expansion}
}

@article{10.1145/3651167,
author = {Rahmani, Hossein A. and Naghiaei, Mohammadmehdi and Deldjoo, Yashar},
title = {A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3651167},
doi = {10.1145/3651167},
abstract = {In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these ML systems that aid users in making decisions. The majority of past literature research on recommender systems fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this article, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their&nbsp;activity level and&nbsp;main-streamness, while producer groups are defined according to their popularity level. For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases. Our results on different group segmentation also indicate that the amount of improvement can vary and is dependent on group segmentation, indicating that the amount of bias produced and how much the algorithm can improve it depend on the protected group definition, a factor that, to our knowledge, has not been examined in great depth in previous studies but rather is highlighted by the results discovered in this study.},
journal = {ACM Trans. Recomm. Syst.},
month = jun,
articleno = {19},
numpages = {24},
keywords = {Responsible IR, recommender systems, fairness, ranking, bias mitigation, consumer and provider, multi-stakeholder}
}

@article{10.1145/3651609,
author = {Bender, Michael A. and Farach-Colton, Mart\'{\i}n and Goodrich, Michael T. and Koml\'{o}s, Hanna},
title = {History-Independent Dynamic Partitioning: Operation-Order Privacy in Ordered Data Structures},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3651609},
doi = {10.1145/3651609},
abstract = {A data structure is history independent if its internal representation reveals nothing about the history of operations beyond what can be determined from the current contents of the data structure. History independence is typically viewed as a security or privacy guarantee, with the intent being to minimize risks incurred by a security breach or audit. Despite widespread advances in history independence, there is an important data-structural primitive that previous work has been unable to replace with an equivalent history-independent alternative---dynamic partitioning. In dynamic partitioning, we are given a dynamic set S of ordered elements and a size-parameter B, and the objective is to maintain a partition of S into ordered groups, each of size Θ(B). Dynamic partitioning is important throughout computer science, with applications to B-tree rebalancing, write-optimized dictionaries, log-structured merge trees, other external-memory indexes, geometric and spatial data structures, cache-oblivious data structures, and order-maintenance data structures. The lack of a history-independent dynamic-partitioning primitive has meant that designers of history-independent data structures have had to resort to complex alternatives. In this paper, we achieve history-independent dynamic partitioning. Our algorithm runs asymptotically optimally against an oblivious adversary, processing each insert/delete with O(1) operations in expectation and O(B log N/loglog N) with high probability in set size N.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {108},
numpages = {27},
keywords = {algorithms, data structures, external memory, history independence, online algorithms, randomized algorithms}
}

@article{10.1145/3651858,
author = {Li, Ziyang and Li, Dongsheng and Chen, Yingwen and Chen, Kai and Zhang, Yiming},
title = {Decentralized Scheduling for Data-Parallel Tasks in the Cloud},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/3651858},
doi = {10.1145/3651858},
abstract = {For latency-sensitive data processing applications in the cloud, concurrent data-parallel tasks need to be scheduled and processed quickly. A data-parallel task usually consists of a set of sub-tasks, generating a set of flows that are collectively referred to as coflows. The state-of-the-art schedulers collect coflow information in the cloud to optimize coflow-level performance. However, most of the coflows, classified as small coflows because they consist of only short flows, have been largely overlooked. This article presents OptaX, a decentralized network scheduling service that collaboratively schedules data-parallel tasks’ small coflows. OptaX adopts a cross-layer, commercial off-the-shelf switch-compatible design that leverages the sendbuffer information in the kernel to adaptively optimize flow scheduling in the network. Specifically, OptaX (i) monitors the system calls (syscalls) in the hosts to obtain their sendbuffer footprints, and (ii) recognizes small coflows and assigns high priorities to their flows. OptaX transfers these flows in a FIFO manner by adjusting TCP’s two attributes: window size and round-trip time. We have implemented OptaX as a Linux kernel module. The evaluation shows that OptaX is at least 2.2\texttimes{} faster than fair sharing and 1.2\texttimes{} faster than only assigning small coflows with the highest priority. We further apply OptaX to improve the small I/O performance of Ursa, a distributed block storage system that provides virtual disks where small I/O is dominant. Ursa with OptaX achieves significant improvement compared to the original Ursa for small I/O latency.},
journal = {ACM Trans. Parallel Comput.},
month = jun,
articleno = {10},
numpages = {23},
keywords = {Decentralized scheduling, data-parallel tasks, coflows, cross-layer scheduling}
}

@article{10.1145/3652162,
author = {Bernab\'{e}-Rodr\'{\i}guez, Julen and Garreta, Albert and Lage, Oscar},
title = {A Decentralized Private Data Marketplace using Blockchain and Secure Multi-Party Computation},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3652162},
doi = {10.1145/3652162},
abstract = {Big data has proven to be a very useful tool for companies and users, but companies with larger datasets have ended being more competitive than the others thanks to machine learning or artificial intelligence. Secure multi-party computation (SMPC) allows the smaller companies to jointly train arbitrary models on their private data while assuring privacy, and thus gives data owners the ability to perform what are currently known as federated learning algorithms. Besides, with a blockchain it is possible to coordinate and audit those computations in a decentralized way.In this document, we consider a private data marketplace as a space where researchers and data owners meet to agree the use of private data for statistics or more complex model trainings. This document presents a candidate architecure for a private data marketplace by combining SMPC and a public, general-purpose blockchain. Such a marketplace is proposed as a smart contract deployed in the blockchain, while the privacy preserving computation is held by SMPC.},
journal = {ACM Trans. Priv. Secur.},
month = jun,
articleno = {19},
numpages = {29},
keywords = {Multi-party computation, blockchain, edge computing, distributed computation, data economy}
}

@article{10.1145/3652596,
author = {Jiang, Weiwei and Goncalves, Jorge and Kostakos, Vassilis},
title = {Mobile Near-infrared Sensing—A Systematic Review on Devices, Data, Modeling, and Applications},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3652596},
doi = {10.1145/3652596},
abstract = {Mobile near-infrared sensing is becoming an increasingly important method in many research and industrial areas. To help consolidate progress in this area, we use the PRISMA guidelines to conduct a systematic review of mobile near-infrared sensing, including (1) existing prototypes and commercial products, (2) data collection techniques, (3) machine learning methods, and (4) relevant application areas. Our work measures historical and current trends and identifies current challenges and future directions for this emerging topic.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {201},
numpages = {36},
keywords = {Mobile computing, near-infrared, mobile sensing, data, machine learning}
}

@article{10.1145/3652950,
author = {Gordon, Eric and Harlow, John and Whitman, Samantha A. and Lee, Myeong},
title = {Data Discretion: Screen-Level Bureaucrats and Municipal Decision-Making},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3652950},
doi = {10.1145/3652950},
abstract = {Public servants tasked with implementing rules or policies on the street level often make discretionary decisions based on local context. Lipsky has labeled them street-level bureaucrats. During the COVID-19 pandemic, as most face-to-face interactions facilitated by local government moved online, many street-level decisions were moved to screens, representing the actions of whom Bouvins and Zouridis refer to as screen-level bureaucrats. Discretionary decision making among public servants continued, but much of it centered on the collection, analysis, and use of data. This article reports on research conducted in 2021 with municipal employees in a large northeastern city in the United States, and examines their changing relationship with data in determining how and to whom they deliver services. In the context of a public health crisis, public servants exercised “data discretion” to rapidly alter services and to internally advocate for underserved populations. The specific practices of discretionary decision making are explored through candid interviews with public servants. The discussion explores how these emergent practices might persist post crisis, how discretion is counterintuitively used to build trust in government, and how data discretion is not limited only to the progressive values manifested in the example of a northeastern city.},
journal = {Digit. Gov.: Res. Pract.},
month = jun,
articleno = {11},
numpages = {14},
keywords = {Data discretion, screen-level bureaucracy, decision making, COVID-19}
}

@article{10.1145/3653307,
author = {Ai, Ziyan and Chiu, Dickson K. W. and Ho, Kevin K. W.},
title = {Social Media Analytics of User Evaluation for Innovative Digital Cultural and Creative Products: Experiences regarding Dunhuang Cultural Heritage},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3653307},
doi = {10.1145/3653307},
abstract = {Social media platforms play an increasingly important role in cultural communication as society develops, attracting promotions and discussions about digital cultural and creative products (CACPs). This research investigates the cultural collaboration between Tencent and Dunhuang Research Academy (Dunhuang Academy) and analyzes user evaluation of integrating cultural heritage education into CACPs. We obtained data through Weibo and compared user evaluations and semantic social network analysis of digital CACPs, including interactive products, games, and music. Results indicated that users were more interested in landscapes, dubbing, and user-generated content (UGC) for interactive products, character versions, posters and skills for games, and singers and songs for concerts. Semantic social network analysis was also used to explore the Dunhuang CACP Circle. Scant studies evaluate the usefulness of integrating cultural heritage into different digital CACPs, especially in Asia. Our suggestions help promoters understand user needs for digital CACPs and better user experience and value.},
journal = {J. Comput. Cult. Herit.},
month = may,
articleno = {42},
numpages = {25},
keywords = {User evaluation, cultural heritage, digital cultural and creative products, Neo-culture creativity, Web crawling, Weibo declaration}
}

@article{10.1145/3653317,
author = {Masmoudi, Maroua and Ben Abdallah Ben Lamine, Sana and Karray, Mohamed Hedi and Archimede, Bernard and Baazaoui Zghal, Hajer},
title = {Semantic Data Integration and Querying: A Survey and Challenges},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3653317},
doi = {10.1145/3653317},
abstract = {Digital revolution produces massive, heterogeneous and isolated data. These latter remain underutilized, unsuitable for integrated querying and knowledge discovering. Hence the importance of this survey on data integration which identifies challenging issues and trends. First, an overview of the different generations and basics of data integration is given. Then, semantic data integration is focused, since it semantically links data allowing wider insights and decision-making. More than thirty works are reviewed. The goal is to help analysts to identify relevant criteria to compare then choose among semantic data integration approaches, focusing on the category (materialized, virtual or hybrid) and querying techniques.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {209},
numpages = {35},
keywords = {Data integration, ontology, query processing, ETL, OBDA, semantic mapping}
}

@article{10.1145/3653673,
author = {Li, Hanzhe and Gu, Jingjing and Lu, Xinjiang and Shen, Dazhong and Liu, Yuting and Deng, YaNan and Shi, Guoliang and Xiong, Hui},
title = {Beyond Relevance: Factor-level Causal Explanation for User Travel Decisions with Counterfactual Data Augmentation},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {1046-8188},
url = {https://doi.org/10.1145/3653673},
doi = {10.1145/3653673},
abstract = {Point-of-Interest (POI) recommendation, an important research hotspot in the field of urban computing, plays a crucial role in urban construction. While understanding the process of users’ travel decisions and exploring the causality of POI choosing is not easy due to the complex and diverse influencing factors in urban travel scenarios. Moreover, the spurious explanations caused by severe data sparsity, i.e., misrepresenting universal relevance as causality, may also hinder us from understanding users’ travel decisions. To this end, in this article, we propose a factor-level causal explanation generation framework based on counterfactual data augmentation for user travel decisions, named Factor-level Causal Explanation for User Travel Decisions (FCE-UTD), which can distinguish between true and false causal factors and generate true causal explanations. Specifically, we first assume that a user decision is composed of a set of several different factors. Then, by preserving the user decision structure with a joint counterfactual contrastive learning paradigm, we learn the representation of factors and detect the relevant factors. Next, we further identify true causal factors by constructing counterfactual decisions with a counterfactual representation generator, in particular, it can not only augment the dataset and mitigate the sparsity but also contribute to clarifying the causal factors from other false causal factors that may cause spurious explanations. Besides, a causal dependency learner is proposed to identify causal factors for each decision by learning causal dependency scores. Extensive experiments conducted on three real-world datasets demonstrate the superiority of our approach in terms of check-in rate, fidelity, and downstream tasks under different behavior scenarios. The extra case studies also demonstrate the ability of FCE-UTD to generate causal explanations in POI choosing.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {128},
numpages = {31},
keywords = {Causal explanation generation, urban travel decisions, counterfactual data augmentation, contrastive learning}
}

@article{10.1145/3653697,
author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
title = {"We Have No Idea How Models will Behave in Production until Production": How Engineers Operationalize Machine Learning},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3653697},
doi = {10.1145/3653697},
abstract = {Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production. Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering. When considered holistically, the job seems staggering---how do MLEs do MLOps, and what are their unaddressed challenges? To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance. We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response. Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems. We introduce the 3Vs of MLOps: velocity, visibility, and versioning --- three virtues of successful ML deployments that MLEs learn to balance and grow as they mature. Finally, we discuss design implications and opportunities for future work.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {206},
numpages = {34},
keywords = {interview study, mlops}
}

@article{10.1145/3653982,
author = {DeSmet, Chance and Cook, Diane},
title = {HydraGAN: A Cooperative Agent Model for Multi-Objective Data Generation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3653982},
doi = {10.1145/3653982},
abstract = {Generative adversarial networks have become a de facto approach to generate synthetic data points that resemble their real counterparts. We tackle the situation where the realism of individual samples is not the sole criterion for synthetic data generation. Additional constraints such as privacy preservation, distribution realism, and diversity promotion may also be essential to optimize. To address this challenge, we introduce HydraGAN, a multi-agent network that performs multi-objective synthetic data generation. We theoretically verify that training the HydraGAN system, containing a single generator and an arbitrary number of discriminators, leads to a Nash equilibrium. Experimental results for six datasets indicate that HydraGAN consistently outperforms prior methods in maximizing the Area under the Radar Chart, balancing a combination of cooperative or competitive data generation goals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {60},
numpages = {21},
keywords = {Synthetic data generation, multi-agent GAN, contrasting objectives, privacy-preserving data mining}
}

@article{10.1145/3654664,
author = {Sarafraz, Gita and Behnamnia, Armin and Hosseinzadeh, Mehran and Balapour, Ali and Meghrazi, Amin and Rabiee, Hamid R.},
title = {Domain Adaptation and Generalization of Functional Medical Data: A Systematic Survey of Brain Data},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3654664},
doi = {10.1145/3654664},
abstract = {Despite the excellent capabilities of machine learning algorithms, their performance deteriorates when the distribution of test data differs from the distribution of training data. In medical data research, this problem is exacerbated by its connection to human health, expensive equipment, and meticulous setups. Consequently, achieving domain generalizations and domain adaptations under distribution shifts is an essential step in the analysis of medical data. As the first systematic review of domain generalization and domain adaptation on functional brain signals, the article discusses and categorizes various methods, tasks, and datasets in this field. Moreover, it discusses relevant directions for future research.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {255},
numpages = {39},
keywords = {Domain adaptation, domain generalization, functional medical data}
}

@article{10.1145/3654800,
author = {Liu, Shizhan and Lin, Weiyao and Chen, Yihang and Zhang, Yufeng and Dai, Wenrui and See, John and Xiong, Hong-Kai},
title = {A Unified Framework for Jointly Compressing Visual and Semantic Data},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3654800},
doi = {10.1145/3654800},
abstract = {The rapid advancement of multimedia and imaging technologies has resulted in increasingly diverse visual and semantic data. A large range of applications such as remote-assisted driving requires the amalgamated storage and transmission of various visual and semantic data. However, existing works suffer from the limitation of insufficiently exploiting the redundancy between different types of data. In this article, we propose a unified framework to jointly compress a diverse spectrum of visual and semantic data, including images, point clouds, segmentation maps, object attributes, and relations. We develop a unifying process that embeds the representations of these data into a joint embedding graph according to their categories, which enables flexible handling of joint compression tasks for various visual and semantic data. To fully leverage the redundancy between different data types, we further introduce an embedding-based adaptive joint encoding process and a Semantic Adaptation Module to efficiently encode diverse data based on the learned embeddings in the joint embedding graph. Experiments on the Cityscapes, MSCOCO, and KITTI datasets demonstrate the superiority of our framework, highlighting promising steps toward scalable multimedia processing.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {221},
numpages = {24},
keywords = {Joint visual and semantic data compression, visual semantic data, multimedia processing, unified compression framework}
}

@article{10.1145/3654923,
author = {Patel, Liana and Kraft, Peter and Guestrin, Carlos and Zaharia, Matei},
title = {ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654923},
doi = {10.1145/3654923},
abstract = {Applications increasingly leverage mixed-modality data, and must jointly search over vector data, such as embedded images, text and video, as well as structured data, such as attributes and keywords. Proposed methods for this hybrid search setting either suffer from poor performance or support a severely restricted set of search predicates (e.g., only small sets of equality predicates), making them impractical for many applications. To address this, we present ACORN, an approach for performant and predicate-agnostic hybrid search. ACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art graph-based approximate nearest neighbor index, and can be implemented efficiently by extending existing HNSW libraries. ACORN introduces the idea of predicate subgraph traversal to emulate a theoretically ideal, but impractical, hybrid search strategy. ACORN's predicate-agnostic construction algorithm is designed to enable this effective search strategy, while supporting a wide array of predicate sets and query semantics. We systematically evaluate ACORN on both prior benchmark datasets, with simple, low-cardinality predicate sets, and complex multi-modal datasets not supported by prior methods. We show that ACORN achieves state-of-the-art performance on all datasets, outperforming prior methods with 2--1,000\texttimes{} higher throughput at a fixed recall. Our code is available at: https://github.com/stanford-futuredata/ACORN.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {120},
numpages = {27},
keywords = {approximate nearest neighbor search, hybrid search, vector search}
}

@article{10.1145/3654934,
author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick},
title = {Data Acquisition for Improving Model Confidence},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654934},
doi = {10.1145/3654934},
abstract = {In recent years, there has been a growing recognition that high-quality training data is crucial for the performance of machine learning models. This awareness has catalyzed both research endeavors and industrial initiatives dedicated to data acquisition to enhance diverse dimensions of model performance. Among these dimensions, model confidence holds paramount importance; however, it has often been overlooked in prior investigations into data acquisition methodologies. To address this gap, our work focuses on improving the data acquisition process with the goal of enhancing the confidence of Machine Learning models. Specifically, we operate within a practical context where limited samples can be obtained from a large data pool. We employ well-established model confidence metrics as our foundation, and we propose two methodologies, Bulk Acquisition (BA) and Sequential Acquisition (SA), each geared towards identifying the sets of samples that yield the most substantial gains in model confidence. Recognizing the complexity of BA and SA, we introduce two efficient approximate methods, namely kNN-BA and kNN-SA, restricting data acquisition to promising subsets within the data pool. To broaden the applicability of our solutions, we introduce a Distribution-based Acquisition approach that makes minimal assumption regarding the data pool and facilitates the data acquisition across various settings. Through extensive experimentation encompassing diverse datasets, models, and parameter configurations, we demonstrate the efficacy of our proposed methods across a range of tasks. Comparative experiments with alternative applicable baselines underscore the superior performance of our proposed approaches.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {131},
numpages = {25},
keywords = {data acquisition, model confidence}
}

@article{10.1145/3654946,
author = {Zhu, Jingyu and Sun, Yu and Song, Shaoxu and Yuan, Xiaojie},
title = {High Precision ≠ High Cost: Temporal Data Fusion for Multiple Low-Precision Sensors},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654946},
doi = {10.1145/3654946},
abstract = {High-quality data are crucial for practical applications, but obtaining them through high-precision sensors comes at a high cost. To guarantee the trade-off between cost and precision, we may use multiple low-precision sensors to obtain the nearly accurate data fusion results at an affordable cost. The commonly used techniques, such as the Kalman filter and truth discovery methods, typically compute fusion values by combining all the observations according to predictions or sensor reliability. However, low-precision sensors can often cause outliers, and such methods combining all observations are susceptible to interference. To handle this problem, we select a single observation from multiple sensor readings as the fusion result for each timestamp. The selection strategy is guided by the maximum likelihood estimation, to determine the most probable changing trends of fusion results with adjacent timestamps. Our major contributions include (1) the problem formalization and NP-hardness analysis on finding the fusion result with the maximum likelihood w.r.t. local fusion models, (2) exact algorithms based on dynamic programming for tackling the problem, (3) efficient approximation methods with performance guarantees. Experiments on various real datasets and downstream applications demonstrate the superiority and practicality of our work in low-precision sensor data fusion.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {143},
numpages = {27},
keywords = {low-precision sensors, temporal data fusion}
}

@article{10.1145/3654952,
author = {Glaze, Nick and McNeely, Tria and Zhu, Yiwen and Gleeson, Matthew and Serr, Helen and Bhopi, Rajeev and Krishnan, Subru},
title = {Lorentz: Learned SKU Recommendation Using Profile Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654952},
doi = {10.1145/3654952},
abstract = {In response to diverse demands, cloud operators have significantly expanded the array of service offerings, often referred to as Stock Keeping Units (SKUs) available for computing resource configurations. Such diversity has led to increased complexity for customers to choose the appropriate SKU. In the analyzed system, only 43\% of the resource capacity was rightly chosen. Although various automated solutions have attempted to resolve this issue, they often rely on the availability of enriched data, such as workload traces, which are unavailable for newly established services. Since these services amass a substantial volume of telemetry from existing users, cloud operators can leverage this information to better understand customer needs and mitigate the risk of over- or under-provisioning. Furthermore, customer satisfaction feedback serves as a crucial resource for continuous learning and improving the recommendation mechanism. In this paper, we present Lorentz, an intelligent SKU recommender for provisioning new compute resources that circumvents the need for workload traces. Lorentz leverages customer profile data to forecast resource capacities for new users based on detailed profiling of existing users. Furthermore, using a continuous learned feedback loop, Lorentz tailors capacity recommendations according to customer performance vs. cost preferences captured through satisfaction signals. Validated using the production data from provisioned VMs supporting Database Platform X, we demonstrate that Lorentz outperforms user selections and existing defaults, reducing slack by &gt;60\% without increasing throttling. Evaluated using synthetic data, Lorentz's personalization stage iteratively learns the user preferences over time with high accuracy.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {149},
numpages = {25},
keywords = {machine learning, resource management, simulation}
}

@article{10.1145/3654957,
author = {Gong, Yue and Galhotra, Sainyam and Castro Fernandez, Raul},
title = {Nexus: Correlation Discovery over Collections of Spatio-Temporal Tabular Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654957},
doi = {10.1145/3654957},
abstract = {Causal analysis is essential for gaining insights into complex real-world processes and making informed decisions. However, performing accurate causal analysis on observational data is generally infeasible, and therefore, domain experts start exploration with the identification of correlations. The increased availability of data from open government websites, organizations, and scientific studies presents an opportunity to harness observational datasets in assisting domain experts during this exploratory phase.In this work, we introduce Nexus, a system designed to align large repositories of spatio-temporal datasets and identify correlations, facilitating the exploration of causal relationships. Nexus addresses the challenges of aligning tabular datasets across space and time, handling missing data, and identifying correlations deemed "interesting". Empirical evaluation on Chicago Open Data and United Nations datasets demonstrates the effectiveness of Nexus in exposing interesting correlations, many of which have undergone extensive scrutiny by social scientists.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {154},
numpages = {28},
keywords = {correlation analysis, data discovery, hypothesis generation, spatio-temporal data}
}

@article{10.1145/3654963,
author = {Pirhadi, Alireza and Moslemi, Mohammad Hossein and Cloninger, Alexander and Milani, Mostafa and Salimi, Babak},
title = {OTClean: Data Cleaning for Conditional Independence Violations using Optimal Transport},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654963},
doi = {10.1145/3654963},
abstract = {Ensuring Conditional Independence (CI) constraints is pivotal for the development of fair and trustworthy machine learning models. In this paper, we introduce OTClean, a framework that harnesses optimal transport theory for data repair under CI constraints. Optimal transport theory provides a rigorous framework for measuring the discrepancy between probability distributions, thereby ensuring control over data utility. We formulate the data repair problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and propose an alternating method for its solution. However, this approach faces scalability issues due to the computational cost associated with computing optimal transport distances, such as the Wasserstein distance. To overcome these scalability challenges, we reframe our problem as a regularized optimization problem, enabling us to develop an iterative algorithm inspired by Sinkhorn's matrix scaling algorithm, which efficiently addresses high-dimensional and large-scale data. Through extensive experiments, we demonstrate the efficacy and efficiency of our proposed methods, showcasing their practical utility in real-world data cleaning and preprocessing tasks. Furthermore, we provide comparisons with traditional approaches, highlighting the superiority of our techniques in terms of preserving data utility while ensuring adherence to the desired CI constraints.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {160},
numpages = {26},
keywords = {algorithmic fairness, conditional independence, data cleaning, data integrity, data transformation, dataset repair, machine learning}
}

@article{10.1145/3654984,
author = {Chen, Kaiwen and Koudas, Nick},
title = {Unstructured Data Fusion for Schema and Data Extraction},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654984},
doi = {10.1145/3654984},
abstract = {Recently, there has been significant interest in extracting actionable insights from the abundance of unstructured textual data. In this paper, we introduce a novel problem, which we term Semistructured Schema and Data Extraction (SDE). This task aims to enhance and complete tables using information discovered from textual repositories, given partial table specifications in the form of queries. To effectively solve SDE, several challenges must be overcome, which involve transforming the partial table specifications into effective queries, retrieving relevant documents, discerning values for partially specified attributes, inferring additional attributes, and constructing an enriched output table while mitigating the influence of false positives from the retrieval.We propose an end-to-end pipeline for SDE, which consists of a retrieval component and an augmentation component, to address each of the challenges. In the retrieval component, we serialize the partial table specifications into a query and employ a dense passage retrieval algorithm to extract the top-k relevant results from the text repository. Subsequently, the augmentation component ingests the output documents from the retrieval phase and generates an enriched table. We formulate this table enrichment task as a unique sequence-to-sequence task, distinct from traditional approaches, as it operates on multiple documents during generation. Utilizing an interpolation mechanism on the encoder output, our model maintains a nearly constant context length while automatically prioritizing the importance of documents during the generation. Due to the novelty of SDE, we establish a validation methodology, adapting and expanding existing benchmarks with the use of powerful large language models. Our extensive experiments show that our method achieves high accuracy in enriching query tables through multi-document fusion, while also surpassing baseline methods in both accuracy and computational efficiency.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {181},
numpages = {26},
keywords = {data fusion, information extraction, schema extraction}
}

@article{10.1145/3654989,
author = {Jo, Saehan and Trummer, Immanuel},
title = {ThalamusDB: Approximate Query Processing on Multi-Modal Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654989},
doi = {10.1145/3654989},
abstract = {We introduce ThalamusDB, a novel approximate query processing system that processes complex SQL queries on multi-modal data. ThalamusDB supports SQL queries integrating natural language predicates on visual, audio, and text data. To answer such queries, ThalamusDB exploits a collection of zero-shot models in combination with relational processing. ThalamusDB utilizes deterministic approximate query processing, harnessing the relative efficiency of relational processing to mitigate the computational demands of machine learning inference. For evaluating a natural language predicate, ThalamusDB requests a small number of labels from users. User can specify their preferences on the performance objective regarding the three relevant metrics: approximation error, computation time, and labeling overheads. The ThalamusDB query optimizer chooses optimized plans according to user preferences, prioritizing data processing and requested labels to maximize impact. Experiments with several real-world data sets, taken from Craigslist, YouTube, and Netflix, show that ThalamusDB achieves an average speedup of 35.0x over MindsDB, an exact processing baseline, and outperforms ABAE, a sampling-based method, in 78.9\% of cases.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {186},
numpages = {26},
keywords = {multi-modal data, neural models, query processing}
}

@article{10.1145/3654990,
author = {Su, Yongye and Sun, Yinqi and Zhang, Minjia and Wang, Jianguo},
title = {Vexless: A Serverless Vector Data Management System Using Cloud Functions},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654990},
doi = {10.1145/3654990},
abstract = {Cloud functions, exemplified by AWS Lambda and Azure Functions, are emerging as a new computing paradigm in the cloud. They provide elastic, serverless, and low-cost cloud computing, making them highly suitable for bursty and sparse workloads, which are quite common in practice. Thus, there is a new trend in designing data systems that leverage cloud functions. In this paper, we focus on vector databases, which have recently gained significant attention partly due to large language models. In particular, we investigate how to use cloud functions to build high-performance and cost-efficient vector databases. This presents significant challenges in terms of how to perform sharding, how to reduce communication overhead, and how to minimize cold-start times.In this paper, we introduce Vexless, the first vector database system optimized for cloud functions. We present three optimizations to address the challenges. To perform sharding, we propose a global coordinator (orchestrator) that assigns workloads to Cloud function instances based on their available hardware resources. To overcome communication overhead, we propose the use of stateful cloud functions, eliminating the need for costly communications during synchronization. To minimize cold-start overhead, we introduce a workload-aware Cloud function lifetime management strategy. Vexless has been implemented using Azure Functions. Experimental results demonstrate that Vexless can significantly reduce costs, especially on bursty and sparse workloads, compared to cloud VM instances, while achieving similar or higher query performance and accuracy.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {187},
numpages = {26},
keywords = {cloud functions, serverless computing, serverless databases, vector databases}
}

@article{10.1145/3654992,
author = {Wu, Yang and Wan, Yao and Zhang, Hongyu and Sui, Yulei and Wei, Wucai and Zhao, Wei and Xu, Guandong and Jin, Hai},
title = {Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654992},
doi = {10.1145/3654992},
abstract = {The Natural Language to Visualization (NL2Vis) task aims to transform natural-language descriptions into visual representations for a grounded table, enabling users to gain insights from vast amounts of data. Recently, many deep learning-based approaches have been developed for NL2Vis. Despite the considerable efforts made by these approaches, challenges persist in visualizing data sourced from unseen databases or spanning multiple tables. Taking inspiration from the remarkable generation capabilities of Large Language Models (LLMs), this paper conducts an empirical study to evaluate their potential in generating visualizations, and explore the effectiveness of in-context learning prompts for enhancing this task. In particular, we first explore the ways of transforming structured tabular data into sequential text prompts, as to feed them into LLMs and analyze which table content contributes most to the NL2Vis. Our findings suggest that transforming structured tabular data into programs is effective, and it is essential to consider the table schema when formulating prompts. Furthermore, we evaluate two types of LLMs: finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5), against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench). The experimental results reveal that LLMs outperform baselines, with inference-only models consistently exhibiting performance improvements, at times even surpassing fine-tuned models when provided with certain few-shot demonstrations through in-context learning. Finally, we analyze when the LLMs fail in NL2Vis, and propose to iteratively update the results using strategies such as chain-of-thought, role-playing, and code-interpreter. The experimental results confirm the efficacy of iterative updates and hold great potential for future study.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {115},
numpages = {28},
keywords = {code generation, data analysis, data visualization, exploratory study, large language models, natural language processing}
}

@article{10.1145/3656403,
author = {Gladshtein, Vladimir and Zhao, Qiyuan and Ahrens, Willow and Amarasinghe, Saman and Sergey, Ilya},
title = {Mechanised Hypersafety Proofs about Structured Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656403},
doi = {10.1145/3656403},
abstract = {Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally reasoning about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants.   In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of k programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number k of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety reasoning, resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational reasoning about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {173},
numpages = {24},
keywords = {mechanised proofs, relational logic, sparse data structures}
}

@article{10.1145/3656406,
author = {Hong, Jaemin and Ryu, Sukyoung},
title = {Don’t Write, but Return: Replacing Output Parameters with Algebraic Data Types in C-to-Rust Translation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656406},
doi = {10.1145/3656406},
abstract = {Translating legacy system programs from C to Rust is a promising way to enhance their reliability. To alleviate the burden of manual translation, automatic C-to-Rust translation is desirable. However, existing translators fail to generate Rust code fully utilizing Rust’s language features, including algebraic data types. In this work, we focus on tuples and Option/Result types, an important subset of algebraic data types. They are used as functions’ return types to represent those returning multiple values and those that may fail. Due to the absence of these types, C programs use output parameters, i.e., pointer-type parameters for producing outputs, to implement such functions. As output parameters make code less readable and more error-prone, their use is discouraged in Rust. To address this problem, this paper presents a technique for removing output parameters during C-to-Rust translation. This involves three steps: (1) syntactically translating C code to Rust using an existing translator; (2) analyzing the Rust code to extract information related to output parameters; and (3) transforming the Rust code using the analysis result. The second step poses several challenges, including the identification and classification of output parameters. To overcome these challenges, we propose a static analysis based on abstract interpretation, complemented by the notion of abstract read/write sets, which approximate the sets of read/written pointers, and two sensitivities: write set sensitivity and nullity sensitivity. Our evaluation shows that the proposed technique is (1) scalable, with the analysis and transformation of 190k LOC within 213 seconds, (2) useful, with the detection of 1,670 output parameters across 55 real-world C programs, and (3) mostly correct, with 25 out of 26 programs passing their test suites after the transformation.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {176},
numpages = {25},
keywords = {Algebraic Data Type, Automatic Translation, C, Output Parameter, Rust}
}

@article{10.1145/3656414,
author = {Liu, Jiawen and Qu, Weihao and Gaboardi, Marco and Garg, Deepak and Ullman, Jonathan},
title = {Program Analysis for Adaptive Data Analysis},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {PLDI},
url = {https://doi.org/10.1145/3656414},
doi = {10.1145/3656414},
abstract = {Data analyses are usually designed to identify some property of the population from which the data are drawn, generalizing beyond the specific data sample. For this reason, data analyses are often designed in a way that guarantees that they produce a low generalization error.  That is, they are designed so that the result of a data analysis run on a sample data does not differ too much from the result one would achieve by running the analysis over the entire population.   An adaptive data analysis can be seen as a process composed by multiple queries interrogating some data, where the choice of which query to run next may rely on the results of previous queries.  The generalization error of each individual query/analysis can be controlled by using an array of well-established statistical techniques.  However, when queries are arbitrarily composed, the different errors can propagate through the chain of different queries and bring to a high generalization error.  To address this issue, data analysts are designing several techniques that not only guarantee bounds on the generalization errors of single queries, but that also guarantee bounds on the generalization error of the composed analyses.  The choice of which of these techniques to use, often depends on the chain of queries that an adaptive data analysis can generate.   In this work, we consider adaptive data analyses implemented as while-like programs and we design a program analysis which can help with identifying which technique to use to control their generalization errors.  More specifically, we formalize the intuitive notion of adaptivity as a quantitative property of programs.  We do this because the adaptivity level of a data analysis is a key measure to choose the right technique.  Based on this definition, we design a program analysis for soundly approximating this quantity.  The program analysis generates a representation of the data analysis as a weighted dependency graph, where the weight is an upper bound on the number of times each variable can be reached, and uses a path search strategy to guarantee an upper bound on the adaptivity.  We implement our program analysis and show that it can help to analyze the adaptivity of several concrete data analyses with different adaptivity structures.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {184},
numpages = {25},
keywords = {Adaptive data analysis, dependency graph, program analysis}
}

@article{10.1145/3656584,
author = {Ding, Chen and Zhou, Jian and Lu, Kai and Li, Sicen and Xiong, Yiqin and Wan, Jiguang and Zhan, Ling},
title = {D2Comp: Efficient Offload of LSM-tree Compaction with Data Processing Units on Disaggregated Storage},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3656584},
doi = {10.1145/3656584},
abstract = {LSM-based key-value stores suffer from sub-optimal performance due to their slow and heavy background compactions. The compaction brings severe CPU and network overhead on high-speed disaggregated storage. This article further reveals that data-intensive compression in compaction consumes a significant portion of CPU power. Moreover, the multi-threaded compactions cause substantial CPU contention and network traffic during high-load periods. Based on the above observations, we propose fine-grained dynamical compaction offloading by leveraging the modern Data Processing Unit (DPU) to alleviate the CPU and network overhead. To achieve this, we first customized a file system to enable efficient data access for DPU. We then leverage the Arm cores on the DPU to meet the burst CPU and network requirements to reduce resource contention and data movement. We further employ dedicated hardware-based accelerators on the DPU to speed up the compression in compactions. We integrate our DPU-offloaded compaction with RocksDB and evaluate it with NVIDIA’s latest Bluefield-2 DPU on a real system. The evaluation shows that the DPU is an effective solution to solve the CPU bottleneck and reduce data traffic of compaction. The results show that compaction performance is accelerated by 2.86 to 4.03 times, system write and read throughput is improved by up to 3.2 times and 1.4 times respectively, and host CPU contention and network traffic are effectively reduced compared to the fine-tuned CPU-only baseline.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {46},
numpages = {22},
keywords = {LSM-tree, key-value store, data processing units, disaggregated storage}
}

@article{10.1145/3657286,
author = {Gaudreault, Jean-Gabriel and Branco, Paula},
title = {A Systematic Literature Review of Novelty Detection in Data Streams: Challenges and Opportunities},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3657286},
doi = {10.1145/3657286},
abstract = {Novelty detection in data streams is the task of detecting concepts that were not known prior, in streams of data. Many machine learning algorithms have been proposed to detect these novelties, as well as integrate them. This study provides a systematic literature review of the state of novelty detection in data streams, including its advancement in recent years, its main challenges and solutions, an updated taxonomy for the classification of the proposed frameworks, and a comparative analysis of different key algorithms in this field. Additionally, we highlight ongoing challenges and future research directions that could be tackled moving forward.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {243},
numpages = {37},
keywords = {Novelty detection, data streams, data mining, concept evolution, online learning, concept drift}
}

@article{10.1145/3659101,
author = {Tian, Huan and Tang, Jiewen and Li, Jun and Sha, Zhibing and Yang, Fan and Cai, Zhigang and Liao, Jianwei},
title = {Modeling Retention Errors of 3D NAND Flash for Optimizing Data Placement},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1084-4309},
url = {https://doi.org/10.1145/3659101},
doi = {10.1145/3659101},
abstract = {Considering 3D NAND flash has a new property of process variation (PV), which causes different raw bit error rates (RBER) among different layers of the flash block. This article builds a mathematical model for estimating the retention errors of flash cells, by considering the factor of layer-to-layer PV in 3D NAND flash memory, as well as the factors of program/erase (P/E) cycle and retention time of data. Then, it proposes classifying the layers of flash block in 3D NAND flash memory into profitable and unprofitable categories, according to the error correction overhead. After understanding the retention error variation of different layers in 3D NAND flash, we design a mechanism of data placement, which maps the write data onto a suitable layer of flash block, according to the data hotness and the error correction overhead of layers, to boost read performance of 3D NAND flash. The experimental results demonstrate that our proposed retention error estimation model can yield a R2 value of 0.966 on average, verifying the accuracy of the model. Based on the estimated retention error rates of layers, the proposed data placement mechanism can noticeably reduce the read latency by 29.8\% on average, compared with state-of-the-art methods against retention errors for 3D NAND flash memory.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {62},
numpages = {24},
keywords = {Solid-state drivers, 3D flash memories, ECC, reliability, modeling, layer RBER variation}
}

@article{10.1145/3659589,
author = {Zhou, Yexu and Zhao, Haibin and Huang, Yiran and R\"{o}ddiger, Tobias and Kurnaz, Murat and Riedel, Till and Beigl, Michael},
title = {AutoAugHAR: Automated Data Augmentation for Sensor-based Human Activity Recognition},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659589},
doi = {10.1145/3659589},
abstract = {Sensor-based HAR models face challenges in cross-subject generalization due to the complexities of data collection and annotation, impacting the size and representativeness of datasets. While data augmentation has been successfully employed in domains like natural language and image processing, its application in HAR remains underexplored. This study presents AutoAugHAR, an innovative two-stage gradient-based data augmentation optimization framework. AutoAugHAR is designed to take into account the unique attributes of candidate augmentation operations and the unique nature and challenges of HAR tasks. Notably, it optimizes the augmentation pipeline during HAR model training without substantially extending the training duration. In evaluations on eight inertial-measurement-units-based benchmark datasets using five HAR models, AutoAugHAR has demonstrated superior robustness and effectiveness compared to other leading data augmentation frameworks. A salient feature of AutoAugHAR is its model-agnostic design, allowing for its seamless integration with any HAR model without the need for structural modifications. Furthermore, we also demonstrate the generalizability and flexible extensibility of AutoAugHAR on four datasets from other adjacent domains. We strongly recommend its integration as a standard protocol in HAR model training and will release it as an open-source tool1.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {48},
numpages = {27},
keywords = {automated data augmentation, human activity recognition, machine learning}
}

@article{10.1145/3659599,
author = {Wang, Lei and Wang, Xingwei and Zhang, Yu and Ma, Xiaolei and Dai, Haipeng and Zhang, Yong and Li, Zhijun and Gu, Tao},
title = {Accurate Blood Pressure Measurement Using Smartphone's Built-in Accelerometer},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659599},
doi = {10.1145/3659599},
abstract = {Efficient blood pressure (BP) monitoring in everyday contexts stands as a substantial public health challenge that has garnered considerable attention from both industry and academia. Commercial mobile phones have emerged as a promising tool for BP measurement, benefitting from their widespread popularity, portability, and ease of use. Most mobile phone-based systems leverage a combination of the built-in camera and LED to capture photoplethysmography (PPG) signals, which can be used to infer BP by analyzing the blood flow characteristics. However, due to low Signal-to-Noise (SNR), various factors such as finger motion, improper finger placement, skin tattoos, or fluctuations in environmental lighting can distort the PPG signal. These distortions consequentially affect the performance of BP estimation. In this paper, we introduce a novel sensing system that utilizes the built-in accelerometer of a mobile phone to capture seismocardiography (SCG) signals, enabling accurate BP measurement. Our system surpasses previous mobile phone-based BP measurement systems, offering advantages such as high SNR, ease of use, and power efficiency. We propose a triple-stage noise reduction scheme, integrating improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN), recursive least squares (RLS) adaptive filter, and soft-thresholding, to effectively reconstruct high-quality heartbeat waveforms from initially contaminated raw SCG signals. Moreover, we introduce a data augmentation technique encompassing normalization coupled with temporal-sliding, effectively augmenting the diversity of the training sample set. To enable battery efficiency on smartphone, we propose a resource-efficient deep learning model that incorporates resource-efficient convolution, shortcut connections, and Huber loss. We conduct extensive experiments with 70 volunteers, comprising 35 healthy individuals and 35 individuals diagnosed with hypertension, under a user-independent setting. The excellent performance of our system demonstrates its capacity for robust and accurate daily BP measurement.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {57},
numpages = {28},
keywords = {Blood Pressure, Seismocardiography}
}

@article{10.1145/3659620,
author = {Hou, Weiying and Wu, Chenshu},
title = {RFBoost: Understanding and Boosting Deep WiFi Sensing via Physical Data Augmentation},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659620},
doi = {10.1145/3659620},
abstract = {Deep learning shows promising performance in wireless sensing. However, deep wireless sensing (DWS) heavily relies on large datasets. Unfortunately, building comprehensive datasets for DWS is difficult and costly, because wireless data depends on environmental factors and cannot be labeled offline. Despite recent advances in few-shot/cross-domain learning, DWS is still facing data scarcity issues. In this paper, we investigate a distinct perspective of radio data augmentation (RDA) for WiFi sensing and present a data-space solution. Our key insight is that wireless signals inherently exhibit data diversity, contributing more information to be extracted for DWS. We present RFBoost, a simple and effective RDA framework encompassing novel physical data augmentation techniques. We implement RFBoost as a plug-and-play module integrated with existing deep models and evaluate it on multiple datasets. Experimental results demonstrate that RFBoost achieves remarkable average accuracy improvements of 5.4\% on existing models without additional data collection or model modifications, and the best-boosted performance outperforms 11 state-of-the-art baseline models without RDA. RFBoost pioneers the study of RDA, an important yet currently underexplored building block for DWS, which we expect to become a standard DWS component of WiFi sensing and beyond. RFBoost is released at https://github.com/aiot-lab/RFBoost.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {58},
numpages = {26},
keywords = {Data Augmentation, Deep Learning, Wireless Sensing}
}

@article{10.1145/3660634,
author = {Bellandi, Valerio and Ceravolo, Paolo and Maggesi, Jonatan and Maghool, Samira},
title = {Data management for continuous learning in EHR systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1533-5399},
url = {https://doi.org/10.1145/3660634},
doi = {10.1145/3660634},
abstract = {To gain a comprehensive understanding of a patient’s health, advanced analytics must be applied to the data collected by electronic health record (EHR) systems. However, managing and curating this data requires carefully designed workflows. While digitalization and standardization enable continuous health monitoring, missing data values and technical issues can compromise the consistency and timeliness of the data. In this paper, we propose a workflow for developing prognostic models that leverages the SMART BEAR infrastructure and the capabilities of the Big Data Analytics (BDA) engine to homogenize and harmonize data points. Our workflow improves the quality of the data by evaluating different imputation algorithms and selecting one that maintains the distribution and correlation of features similar to the raw data. We applied this workflow to a subset of the data stored in the SMART BEAR repository and examined its impact on the prediction of emerging health states such as cardiovascular disease and mild depression. We also discussed the possibility of model validation by clinicians in the SMART BEAR project, the transmission of subsequent actions in the decision support system, and the estimation of the required number of data points.},
note = {Just Accepted},
journal = {ACM Trans. Internet Technol.},
month = may,
keywords = {Internet of Things, Electronic Health Records, Data Management, Continuous Learning}
}

@article{10.1145/3660637,
author = {Fischer, Joshua David and van der Merwe, Johan and Vandenheever, David},
title = {The Influence that the Complexity of the Three-Dimensional Eye Model Used to Generate Simulated Eye-tracking Data Has on the Gaze Estimation Errors Achieved Using the Data},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3660637},
doi = {10.1145/3660637},
abstract = {Simulated eye-tracking data are an integral tool in the development of eye-tracking methods. Most of the simulated data used in eye-tracking-related research has been generated using low-complexity eye models that include a single spherical corneal surface. This study investigated the influence of eye-model complexity on the ability of simulated eye-tracking data to predict real-world outcomes. The experimental procedures of two pertinent comparative eye-tracking studies were replicated in a simulated environment using various eye-model complexities. The simulated outcomes were then evaluated against the findings of the comparative studies that were derived from real-world outcomes. The simulated outcomes of both comparative studies were significantly influenced by the eye-model complexity. Eye models that included an aspheric corneal surface best replicated experimental eye-tracking outcomes, while including a posterior corneal surface did not improve the ability of simulated data to replicate real-world outcomes. Using a wide-angle eye model that accurately replicates the peripheral optics of the eye did not improve simulated outcomes relative to a paraxial eye model.},
journal = {ACM Trans. Appl. Percept.},
month = nov,
articleno = {2},
numpages = {16},
keywords = {Eye-tracking, simulation, gaze estimation, eye model}
}

@article{10.1145/3660639,
author = {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan, Naren},
title = {Neural Methods for Data-to-text Generation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3660639},
doi = {10.1145/3660639},
abstract = {The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text (D2T) generation. This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that focus not only on the design of linguistically capable systems but also on systems that exhibit fairness and accountability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {89},
numpages = {46},
keywords = {Narration, data-to-text, data-to-text generation, natural language generation}
}

@article{10.1145/3660768,
author = {Landauer, Max and Skopik, Florian and Wurzenberger, Markus},
title = {A Critical Review of Common Log Data Sets Used for Evaluation of Sequence-Based Anomaly Detection Techniques},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660768},
doi = {10.1145/3660768},
abstract = {Log data store event execution patterns that correspond to underlying workflows of systems or applications. While most logs are informative, log data also include artifacts that indicate failures or incidents. Accordingly, log data are often used to evaluate anomaly detection techniques that aim to automatically disclose unexpected or otherwise relevant system behavior patterns. Recently, detection approaches leveraging deep learning have increasingly focused on anomalies that manifest as changes of sequential patterns within otherwise normal event traces. Several publicly available data sets, such as HDFS, BGL, Thunderbird, OpenStack, and Hadoop, have since become standards for evaluating these anomaly detection techniques, however, the appropriateness of these data sets has not been closely investigated in the past. In this paper we therefore analyze six publicly available log data sets with focus on the manifestations of anomalies and simple techniques for their detection. Our findings suggest that most anomalies are not directly related to sequential manifestations and that advanced detection techniques are not required to achieve high detection rates on these data sets.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {61},
numpages = {22},
keywords = {anomaly detection, data sets, log data analysis}
}

@article{10.1145/3662178,
author = {Haghir Chehreghani, Mostafa},
title = {A Review on the Impact of Data Representation on Model Explainability},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3662178},
doi = {10.1145/3662178},
abstract = {In recent years, advanced machine learning and artificial intelligence techniques have gained popularity due to their ability to solve problems across various domains with high performance and quality. However, these techniques are often so complex that they fail to provide simple and understandable explanations for the outputs they generate. To address this issue, the field of explainable artificial intelligence has recently emerged. However, most data generated in different domains are inherently structural; that is, they consist of parts and relationships among them. Such data can be represented using either a simple data-structure or form, such as a vector, or a complex data-structure, such as a graph. The effect of this representation form on the explainability and interpretability of machine learning models is not extensively discussed in the literature. In this survey article, we review efficient algorithms proposed for learning from inherently structured data, emphasizing how their representation form affects the explainability of learning models. A conclusion of our literature review is that using complex forms or data-structures for data representation improves not only the learning performance but also the explainability and transparency of the model.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {263},
numpages = {21},
keywords = {Explainable artificial intelligence, data representation, neural networks, graphs}
}

@article{10.1145/3662179,
author = {Sandeepa, Chamara and Siniarski, Bartlomiej and Kourtellis, Nicolas and Wang, Shen and Liyanage, Madhusanka},
title = {A Survey on Privacy of Personal and Non-Personal Data in B5G/6G Networks},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3662179},
doi = {10.1145/3662179},
abstract = {The upcoming Beyond 5G (B5G) and 6G networks are expected to provide enhanced capabilities such as ultra-high data rates, dense connectivity, and high scalability. It opens many possibilities for a new generation of services driven by Artificial Intelligence (AI) and billions of interconnected smart devices. However, with this expected massive upgrade, the privacy of people, organisations, and states is becoming a rising concern. The recent introduction of privacy laws and regulations for personal and non-personal data signals that global awareness is emerging in the current privacy landscape. Yet, many gaps need to be identified in the case of two data types. If not detected, then they can lead to significant privacy leakages and attacks that will affect billions of people and organisations who utilise B5G/6G. This survey is a comprehensive study of personal and non-personal data privacy in B5G/6G to identify the current progress and future directions to ensure data privacy. We provide a detailed comparison of the two data types and a set of related privacy goals for B5G/6G. Next, we bring data privacy issues with possible solutions. This article also provides future directions to preserve personal and non-personal data privacy in future networks.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {266},
numpages = {37},
keywords = {Non-personal data, personal data, privacy, beyond 5G/6G}
}

@article{10.1145/3663572,
author = {Tang, Shengeng and Xue, Feng and Wu, Jingjing and Wang, Shuo and Hong, Richang},
title = {Gloss-driven Conditional Diffusion Models for Sign Language Production},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3663572},
doi = {10.1145/3663572},
abstract = {Sign Language Production (SLP) aims to convert text or audio sentences into sign language videos corresponding to their semantics, which is challenging due to the diversity and complexity of sign languages, and cross-modal semantic mapping issues. In this work, we propose a Gloss-driven Conditional Diffusion Model (GCDM) for SLP. The core of the GCDM is a diffusion model architecture, in which the sign gloss sequence is encoded by a Transformer-based encoder and input into the diffusion model as a semantic prior condition. In the process of sign pose generation, the textual semantic priors carried in the encoded gloss features are integrated into the embedded Gaussian noise via cross-attention. Subsequently, the model converts the fused features into sign language pose sequences through T-round denoising steps. During the training process, the model uses the ground-truth labels of sign poses as the starting point, generates Gaussian noise through T rounds of noise, and then performs T rounds of denoising to approximate the real sign language gestures. The entire process is constrained by the MAE loss function to ensure that the generated sign language gestures are as close as possible to the real labels. In the inference phase, the model directly randomly samples a set of Gaussian noise, generates multiple sign language gesture sequence hypotheses under the guidance of the gloss sequence, and outputs a high-confidence sign language gesture video by averaging multiple hypotheses. Experimental results on the Phoenix2014T dataset show that the proposed GCDM method achieves competitiveness in both quantitative performance and qualitative visualization.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
keywords = {Sign Language Production, Gloss Semantic Encoding, Diffusion Model, Deep Learning}
}

@article{10.1145/3663759,
author = {Paproki, Anthony and Salvado, Olivier and Fookes, Clinton},
title = {Synthetic Data for Deep Learning in Computer Vision \&amp; Medical Imaging: A Means to Reduce Data Bias},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3663759},
doi = {10.1145/3663759},
abstract = {Deep-learning (DL) performs well in computer-vision and medical-imaging automated decision-making applications. A bottleneck of DL stems from the large amount of labelled data required to train accurate models that generalise well. Data scarcity and imbalance are common problems in imaging applications that can lead DL models towards biased decision making. A solution to this problem is synthetic data. Synthetic data is an inexpensive substitute to real data for improved accuracy and generalisability of DL models. This survey reviews the recent methods published in relation to the creation and use of synthetic data for computer-vision and medical-imaging DL applications. The focus will be on applications that utilised synthetic data to improve DL models by either incorporating an increased diversity of data that is difficult to obtain in real life, or by reducing a bias caused by class imbalance. Computer-graphics software and generative networks are the most popular data generation techniques encountered in the literature. We highlight their suitability for typical computer-vision and medical-imaging applications, and present promising avenues for research to overcome their computational and theoretical limitations.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {271},
numpages = {37},
keywords = {Synthetic data, machine learning, generative adversarial network, deep learning}
}

@article{10.1145/3664290,
author = {Liang, Tianhai and Shen, Qiangqiang and Wang, Shuqin and Chen, Yongyong and Zhang, Guokai and Chen, Junxin},
title = {Data Completion-Guided Unified Graph Learning for Incomplete Multi-View Clustering},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {8},
issn = {1556-4681},
url = {https://doi.org/10.1145/3664290},
doi = {10.1145/3664290},
abstract = {Due to its heterogeneous property, multi-view data has been widely concerned over single-view data for performance improvement. Unfortunately, some instances may be with partially available information because of some uncontrollable factors, for which the incomplete multi-view clustering (IMVC) problem is raised. IMVC aims to partition unlabeled incomplete multi-view data into their clusters by exploiting the heterogeneity of multi-view data and overcoming the difficulty of data loss. However, most existing IMVC methods like BSV, MIC, OMVC, and IVC tend to conduct basic completion processing on the input data, without taking advantage of the correlation between samples and information redundancy. To overcome the above issue, we propose one novel IMVC method named data completion-guided unified graph learning (DCUGL), which could complete the data of missing views and fuse multiple learned view-specific similarity matrices into one unified graph. Specifically, we first reduce the dimension of the input data to learn multiple view-specific similarity matrices. By stacking all view-specific similarity matrices, DCUGL constructs a third-order tensor with the low-rank constraint, such that sample correlation within and between views can be well explored. Finally, by dividing the original data into observed data and unobserved data, DCUGL can infer and complete the missing data according to the view-specific similarity matrices, and obtain a unified graph, which can be directly used for clustering. To solve the proposed model, we design an iterative algorithm, which is based on the alternating direction method of multipliers framework. The proposed model proves to be superior by benchmarking on six challenging datasets compared with state-of-the-art IMVC methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {188},
numpages = {23},
keywords = {Incomplete multi-view clustering, tensor completion, low-rank tensor learning}
}

@article{10.1145/3664603,
author = {Xu, Hang and Chen, Liheng and Gan, Shuitao and Zhang, Chao and Li, Zheming and Ji, Jiangan and Chen, Baojian and Hu, Fan},
title = {Graphuzz: Data-driven Seed Scheduling for Coverage-guided Greybox Fuzzing},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664603},
doi = {10.1145/3664603},
abstract = {Seed scheduling is a critical step of greybox fuzzing, which assigns different weights to seed test cases during seed selection, and significantly impacts the efficiency of fuzzing. Existing seed scheduling strategies rely on manually designed models to estimate the potentials of seeds and determine their weights, which fails to capture the rich information of a seed and its execution and thus the estimation of seeds’ potentials is not optimal. In this article, we introduce a new seed scheduling solution, Graphuzz, for coverage-guided greybox fuzzing, which utilizes deep learning models to estimate the potentials of seeds and works in a data-driven way. Specifically, we propose an extended control flow graph called e-CFG to represent the control-flow and data-flow features of a seed's execution, which is suitable for graph neural networks (GNN) to process and estimate seeds’ potential. We evaluate each seed's code coverage increment and use it as the label to train the GNN model. Further, we propose a self-attention mechanism to enhance the GNN model so that it can capture overlooked features. We have implemented a prototype of Graphuzz based on the baseline fuzzer AFLplusplus. The evaluation results show that our model can estimate the potential of seeds and has the robust capability to generalize to different targets. Furthermore, the evaluation using 12 benchmarks from FuzzBench shows that Graphuzz outperforms AFLplusplus and the state-of-the-art seed scheduling solution K-Scheduler and other coverage-guided fuzzers in terms of code coverage, and the evaluation using 8 benchmarks from Magma shows that Graphuzz outperforms the baseline fuzzer AFLplusplus and SOTA solutions in terms of bug detection.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {185},
numpages = {36},
keywords = {Fuzzing, seed scheduling, graph neural network}
}

@article{10.1145/3664612,
author = {Not, Elena and Leonardi, Chiara and L\'{o}pez-De-Ipi\~{n}a, Diego and Silva Palacios, Daniel and S\'{a}nchez-Corcuera, Ruben and Kazhamiakin, Raman and Gerosa, Matteo},
title = {Designing a Digital Environment to Support the Co-production of Public Services: Balancing Multiple Requirements and Governance Concepts},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3664612},
doi = {10.1145/3664612},
abstract = {This paper investigates the challenges of designing a computer supported collaborative environment aimed at facilitating co-production processes, i.e., those collaborative processes between Public Administrations, private stakeholders and citizens that aim at the design of public services, their implementation, and their shared delivery to the community. We argue that, for such a digital platform, different types of socio-technical requirements should be considered, i.e., those related to governance models and associated collaboration dynamics; requirements that may emerge from the specific type of public service to design; as well as user and technical requirements common to all e-government platforms. This research informed the development and testing of a digital collaboration platform that offers guidance on how to organize a co-production initiative and a network of stakeholders, functionalities to support collaborative work, and enablers (in the form of reusable knowledge and digital resources) to perform the sequence of steps to produce a public service. The lessons learned from the iterative platform development process and a preliminary evaluation study conducted with three Public Administrations in different European countries pointed at specific functionalities that are perceived as most crucial and at different appropriation practices that depend on the organizational structure of the involved Public Administrations and related multi-stakeholder networks. The innovation that is brought about with respect to general-purpose platforms for computer supported cooperative work is represented by the operationalization of co-production processes, with step-by-step guidance and potential reuse (with adaptation) of ready to use resources and processes. Based on the results of our research, general guidelines are also proposed for the design of future digital platforms supporting co-production.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {24},
numpages = {30},
keywords = {ICT-enabled co-production, digital collaboration environment, digital platforms design, co-design with public administrations, public services}
}

@article{10.1145/3665278,
author = {Ren, Haoyu and Anicic, Darko and Li, Xue and Runkler, Thomas},
title = {On-device Online Learning and Semantic Management of TinyML Systems},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/3665278},
doi = {10.1145/3665278},
abstract = {Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning (ML). While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges. This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1)&nbsp;Embedded devices operate in dynamically changing conditions. Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects. However, static models may underperform in the real world due to evolving input data distributions. We propose online learning to enable training on constrained devices, adapting local models toward the latest field conditions. (2)&nbsp;Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices. We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning. This approach ensures optimal performance among distributed devices by knowledge sharing. (3)&nbsp;Moreover, TinyML’s pivotal advantage is widespread adoption. Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures. Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up. We present semantic management for the joint management of models and devices at scale. We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection. The results confirm the effectiveness of our approaches from various perspectives, such as accuracy improvement, resource savings, and engineering effort reduction.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jun,
articleno = {55},
numpages = {32},
keywords = {Tiny machine learning, online learning, federated meta-learning, edge computing, semantic web, knowledge graph, industrial Internet of Things}
}

@article{10.1145/3665331,
author = {Gramoli, Lysa and Cumin, Julien and Lacoche, J\'{e}r\'{e}my and Foulonneau, Anthony and Arnaldi, Bruno and Gouranton, Val\'{e}rie},
title = {Generating and Evaluating Data of Daily Activities with an Autonomous Agent in a Virtual Smart Home},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3665331},
doi = {10.1145/3665331},
abstract = {Training machine learning models to identify human behavior is a difficult yet essential task to develop autonomous and adaptive systems such as smart homes. These models require large and diversified amounts of labeled data to be trained effectively. Due to the high variety of home environments and occupant behaviors, collecting datasets that are representative of all possible homes is a major challenge. In addition, privacy and cost are major hurdles to collect real home data. To avoid these difficulties, one solution consists of training these models using purely synthetic data, which can be generated through the simulation of home and their occupants. Two challenges arise from this approach: designing a methodology with a simulation able to generate credible simulated data and evaluating this credibility. In this article, we explain the methodology used to generate diversified synthetic data of daily activities, through the combination of an agent model to simulate an occupant and a simulated 3D house enriched with sensors and effectors to produce such data. We demonstrate the credibility of the generated synthetic data by comparing their efficacy for training human context understanding models against the efficacy generated by real data. To achieve this, we replicate a real dataset collection setting with our smart home simulator. The occupant is replaced by an autonomous agent following the same experimental protocol used for the real dataset collection. This agent is a BDI-based model enhanced with a scheduler designed to offer a balance between control and autonomy. This balance is useful in synthetic data generation since strong constraints can be imposed on the agent to simulate desired situations while allowing autonomous behaviors outside these constraints to generate diversified data. In our case, the constraints are those imposed during the real dataset collection that we want to replicate. The simulated sensors and effectors were configured to react to the agent’s behaviors similarly to the real ones. We experimentally show that data generated from this simulation are valuable for two human context understanding tasks: current human activity recognition and future human activity prediction. In particular, we show that models trained solely with simulated data can give reasonable predictions about real situations occurring in the original dataset. We also report experimental results regarding statistical analysis and C2ST to assess the credibility of generated data. We discuss the generality of our approach for evaluating the credibility of simulated data from their use as training data.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {13},
numpages = {25},
keywords = {Synthetic data, autonomous agent, activities of daily living, human activity recognition}
}

@article{10.1145/3665495,
author = {Gupta, Ankur and Sawhney, Sahil and Kompella, Kashyap},
title = {The First Principles: Setting the Context for a Safe and Secure Metaverse},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3665495},
doi = {10.1145/3665495},
abstract = {The metaverse delivered through converged and amalgamated technologies holds promise. No wonder technology heavyweights, large corporates, research organizations and businesses cutting across industry verticals are racing to put in place a metaverse-first strategy. The bets on consumers rapidly migrating from traditional social networks and collaborative applications to more immersive digital experiences have been placed. However, the transition is not expected to be seamless. Privacy, safety and security concerns abound in the early versions of the metaverse. Increased regulatory oversight and diverse national laws threaten to derail the hype around the metaverse. It is increasingly clear that the final iteration of the metaverse will need to assuage the concerns of individual users while addressing complex legal and regulatory requirements. Thus, a multi-perspective approach needs to be adopted to help set the agenda for the evolution of the metaverse. This research paper examines the different aspects and challenges which the future metaverse will need to address. A set of “first principles” are formulated, which if implemented will lead to the development of an equitable, inclusive, safe and secure metaverse.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {280},
numpages = {29},
keywords = {Metaverse, metaverse security, first principles for the metaverse}
}

@article{10.1145/3670697,
author = {Peng, Yanguo and Liu, Rongqiao and Guo, Jingjing and Gao, Xiyue and Huang, Luyuan and Tu, Yaofeng},
title = {SecCT: Secure and Scalable Count Query Models on Encrypted Genomic Data},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0934-5043},
url = {https://doi.org/10.1145/3670697},
doi = {10.1145/3670697},
abstract = {Recently, due to the continued reduction in DNA sequencing cost, large-scale genetic samples are being gathered for accelerating predispositions to specific diseases, tailoring treatment of efficient drugs and therapies, and the like. Massive genetic samples are encrypted-and-then-delegated to a public cloud to both save investment and maintenance costs and prevent the potential leakage of sensitive information. However, such a manner compromises the serviceability of a public cloud, since encryption inevitably breaks the semantic information of genetic samples. Secure count query of single-nucleotide polymorphisms (SNPs), as a kernel component for GWASs and related genomic analysis, is attracting much more attention.Existing methods lack provable security, suffer low efficiency caused by multiple interactions with the cloud, and so on. In this paper, a secure virtual CT-Tree (secure vCT-Tree) is carefully constructed to confuse the tree structure by introducing a hash function and a Paillier system. Furthermore, by delegating the secure vCT-Tree to the cloud, concrete models (i.e., SecCT and SecCT+) are presented to resolve secure count query problems on the fly. SecCT+ is a solution based on trusted execution environment while SecCT is a pure software solution. Both models advance the provable security of genetic research and are proven to be secure under the adaptive chosen keyword (query) attack (IND-CKA2) model. Furthermore, massive experiments are evaluated on realistic data to show the superiority of SecCT and SecCT+.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {21},
numpages = {25},
keywords = {Secure count query, genotypic and phenotypic data, IND-CKA security, scalability}
}

@article{10.1145/3672082,
author = {Serra, Flavia and Peralta, Ver\'{o}nika and Marotta, Adriana and Marcel, Patrick},
title = {Use of Context in Data Quality Management: A Systematic Literature Review},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3672082},
doi = {10.1145/3672082},
abstract = {The importance of context in data quality (DQ) was shown many years ago and nowadays is widely accepted. Early approaches and surveys defined DQ as fitness for use and showed the influence of context on DQ. This article presents a Systematic Literature Review (SLR) for investigating how context is taken into account in recent proposals for DQ management (DQM). We specifically present the planning and execution of the SLR, the analysis criteria and our results reflecting the relationship between context and DQ in the state-of-the-art and, particularly, how this context is defined and used for DQM. The SLR is instrumental to the identification of context components and the design of a context formal model.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {19},
numpages = {41},
keywords = {Systematic literature review, data quality, context, data quality methodology}
}

@article{10.1145/3672396,
author = {Liang, Chao and Zhu, Linchao and Yang, Zongxin and Chen, Wei and Yang, Yi},
title = {Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {10},
issn = {1551-6857},
url = {https://doi.org/10.1145/3672396},
doi = {10.1145/3672396},
abstract = {We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {308},
numpages = {19},
keywords = {Deep learning, learn from noisy labels, few-shot learning, prototypical learning}
}

@article{10.1145/3672570,
author = {Banerjee, Prabal and Nikam, Nishant and Mazumdar, Subhra and Ruj, Sushmita},
title = {Cumulus: Blockchain-Enabled Privacy Preserving Data Audit in Cloud},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672570},
doi = {10.1145/3672570},
abstract = {Data owners upload large files to cloud storage servers, but malicious servers may potentially tamper data. To check integrity of remote data, Proof of Retrievability (PoR) schemes were introduced. Existing PoR protocols assume that data owners and third-party auditors are honest and audit only the potentially malicious cloud server to check integrity of stored data. In this paper we consider a system where any party may attempt to cheat others and consider collusion cases. We design a protocol, Cumulus, that is secure under such adversarial assumptions and use blockchain smart contracts to act as mediator in case of dispute and payment settlement. We use state channels to reduce blockchain interactions in order to build a practical audit solution. The security of the protocol has been proven in Universal Composability (UC) framework. Finally, we illustrate several applications of our basic protocol and evaluate practicality of our approach via a prototype implementation for fairly selling large files over the Ethereum platform. We evaluate the prototype and show that our scheme has comparable performance.},
note = {Just Accepted},
journal = {Distrib. Ledger Technol.},
month = nov,
keywords = {PoR, Cloud, Storage, Audit, Blockchain, DLT, Privacy}
}

@article{10.1145/3672614,
author = {Mathieu, Claire and Rajaraman, Rajmohan and Young, Neal E. and Yousefi, Arman},
title = {Competitive Data-Structure Dynamization},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1549-6325},
url = {https://doi.org/10.1145/3672614},
doi = {10.1145/3672614},
abstract = {Data-structure dynamization is a general approach for making static data structures dynamic. It is used extensively in geometric settings and in the guise of so-called merge (or compaction) policies in big-data databases such as LevelDB and Google Bigtable. Previous theoretical work is based on worst-case analyses for uniform inputs—insertions of one item at a time and non-varying read rate. In practice, merge policies must not only handle batch insertions and varying read/write ratios, they can take advantage of such non-uniformity to reduce cost on a per-input basis. To model this, we initiate the study of data-structure dynamization through the lens of competitive analysis via two new online set-cover problems. For each, the input is a sequence of disjoint sets of weighted items. The sets are revealed one at a time. The algorithm must respond to each with a set cover that covers all items revealed so far. It obtains the cover incrementally from the previous cover by adding one or more sets and optionally removing existing sets. For each new set the algorithm incurs build cost equal to the weight of the items in the set. In the first problem the objective is to minimize total build cost plus total query cost, where the algorithm incurs a query cost at each time  (t)  equal to the current cover size. In the second problem, the objective is to minimize the build cost while keeping the query cost from exceeding  (k)  (a given parameter) at any time. We give deterministic online algorithms for both variants, with competitive ratios of  (Theta(log^{*}n))  and  (k) , respectively. The latter ratio is optimal for the second variant.},
journal = {ACM Trans. Algorithms},
month = oct,
articleno = {37},
numpages = {28},
keywords = {Online algorithms, competitive analysis, data-structure dynamization, log-structured merge-tree, compaction}
}

@article{10.1145/3673228,
author = {Snow, Stephen and Khan, Awais Hameed and Day, Kaleb and Matthews, Ben},
title = {Household Wattch: Exploring Opportunities for Surveillance and Consent through Families’ Household Energy Use Data},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3673228},
doi = {10.1145/3673228},
abstract = {Household energy use data may contain sensitive inferences into family life, yet its potential for surveillance is imperfectly understood. To explore this space, we developed Household Wattch, a speculative eco-feedback ‘provotype’ that profiles households according to their energy use data. Evaluated by 16 participants from Australian households engaged in an 18-month energy use monitoring trial, Household Wattch elicited users’ perceptions and expectations about a near future where energy use data is a useful yet potentially sensitive commodity when analysed. We highlight challenges and opportunities for energy use data across three scales: (1) Within the household, (2) Beyond the household (e.g., sharing energy data with third parties) and (3) Post-household (e.g., what happens to energy data when a household re-configures or disbands). Findings suggest users may require support in understanding the sensitivities of their energy use data, particularly when deciding whether to share it with third parties. Opportunities exist for accidental or deliberate surveillance via energy use data, and these need to be identified and managed. Provotypes represent a useful tool for navigating this space, and we provide considerations for how they can support users in speculating over possible energy futures.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {55},
numpages = {30},
keywords = {Energy, electricity, eco-feedback, provotype, energy data, interpersonal, IoT, smart home}
}

@article{10.1145/3674634,
author = {Chen, Yijia and Parreaux, Lionel},
title = {The Long Way to Deforestation: A Type Inference and Elaboration Technique for Removing Intermediate Data Structures},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ICFP},
url = {https://doi.org/10.1145/3674634},
doi = {10.1145/3674634},
abstract = {Deforestation is a compiler optimization that removes intermediate data structure allocations from functional programs to improve their efficiency. This is an old idea, but previous approaches have proved limited or impractical — they either only worked on compositions of predefined combinators (shortcut fusion), or involved the aggressive unfolding of recursive definitions until a depth limit was reached or a reoccurring pattern was found to tie the recursive knot, resulting in impractical algorithmic complexity and large amounts of code duplication. We present Lumberhack, a general-purpose deforestation approach for purely functional call-by-value programs. Lumberhack uses subtype inference to reason about data structure production and consumption and uses an elaboration pass to fuse the corresponding recursive definitions. It fuses large classes of mutually recursive definitions while avoiding much of the unproductive (and sometimes counter-productive) code duplication inherent in previous approaches. We prove the soundness of Lumberhack using logical relations and experimentally demonstrate significant speedups in the standard nofib benchmark suite. We manually adapted nofib programs to call-by-value semantics and compiled them using the OCaml compiler. The average speedup over the 38 benchmarked programs is 8.2\% while the average code size increases by just about 1.79x. In particular, 19 programs see their performance mostly unchanged, 17 programs improve significantly (by an average speedup of 16.6\%), and only three programs visibly worsen (by an average slowdown of 1.8\%). As a point of comparison, we measured that the well-proven but semi-manual list fusion technique of the Glasgow Haskell Compiler (GHC), which only works for call-by-need programs, had an average speedup of 6.5\%. Our technique is in its infancy still and misses many deforestation opportunities. We are confident that further refinements to the core technique will yield greater performance improvements in the future.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {245},
numpages = {35},
keywords = {deforestation, elaboration, elaboration, fusion, fusion, optimization, optimizationdeforestation, type inference, type inference}
}

@article{10.1145/3675389,
author = {Richermoz, Antoine and Neyret, Fabrice},
title = {GigaVoxels DP: Starvation-Less Render and Production for Large and Detailed Volumetric Worlds Walkthrough},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3675389},
doi = {10.1145/3675389},
abstract = {Using voxel hierarchies as a generic 3D scene representation makes ray marching, antialiasing, and LOD easy. The drawback is the huge amount of memory required to store voxels, even with empty space compression. Still, GigaVoxels [Crassin et al. 2009] showed that by using a ray-guided cache to produce and store only visible voxels bricks on demand, it is possible to walk through very large and detailed worlds with real-time performance in bounded GPU memory. However, on-demand production of data during rendering is still challenging in terms of synchronization and starvation of GPU cores. We propose a new GPU-driven algorithm using dynamic parallelism (DP) to minimize these, and a "GPU-cores timeline" profiling tool to analyze them. We validate our model with timings (2\texttimes{} gain) and we illustrate it on various scenes.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = aug,
articleno = {42},
numpages = {11},
keywords = {Computer Graphics, Dynamic Parallelism, GPU-Driven, On-Demand, Real-Time Rendering, Volume Ray-Marching}
}

@article{10.1145/3675405,
author = {Sghaier, Oussama and Amayri, Manar and Bouguila, Nizar},
title = {Libby-Novick Beta-Liouville Distribution for Enhanced Anomaly Detection in Proportional Data},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3675405},
doi = {10.1145/3675405},
abstract = {We consider the problem of anomaly detection in proportional data by investigating the Libby-Novick Beta-Liouville distribution, a novel distribution merging the salient characteristics of Liouville and Libby-Novick Beta distributions. Its main benefit, compared to the typical distributions dedicated to proportional data such as Dirichlet and Beta-Liouville, is its adaptability and explanatory power when dealing with this kind of data. Our goal is to exploit this appropriateness for modeling proportional data to achieve great performance in the anomaly detection task. First, we develop generative models, namely finite mixture models of Libby-Novick Beta-Liouville distributions. Then, we propose two discriminative techniques: Normality scores based on selecting the given distribution to approximate the softmax output vector of a deep classifier and an improved version of Support Vector Machine (SVM) by suggesting a feature mapping approach. We demonstrate the benefits of the presented approaches through a variety of experiments on both image and non-image datasets. The results demonstrate that the proposed anomaly detectors based on the Libby-Novick Beta-Liouville distribution outperform the classical distributions as well as the baseline techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {107},
numpages = {26},
keywords = {Anomaly detection, Libby-Novick Beta-Liouville distribution, finite mixture models, normality scores, SVM, feature mapping}
}

@article{10.1145/3675804,
author = {Denoo, Maarten and Dupont, Bruno and Zaman, Bieke},
title = {Many faces, many names? Ethics in Belgian game development education},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3675804},
doi = {10.1145/3675804},
abstract = {What is nowadays taught to the game creators of tomorrow in terms of ethics? The current study addresses this question by focusing on 11 higher education and continuous vocational training programs for aspiring game developers taught in Belgium. We conducted textual analyses of institutional materials and semi-structured interviews with nine educators. By combining these sources of data, this study identifies three key categories of ethical considerations that are taught to students: content and design impact, workplace standards, and diversity in gaming culture. This study also underscores educators’ proactivity in addressing gaps between curricular content, industry expectations, and student concerns. It is our hope that this study elucidates the critical potential of teaching ethics, providing actionable recommendations for educational institutions to help prepare creators navigate complex moral issues in today's gaming landscape.},
journal = {ACM Games},
month = aug,
articleno = {15},
numpages = {23},
keywords = {Education, Ethical, Societal, and Political Issues, Design}
}

@article{10.1145/3676278,
author = {Balcan, Maria-Florina and Deblasio, Dan and Dick, Travis and Kingsford, Carl and Sandholm, Tuomas and Vitercik, Ellen},
title = {How Much Data Is Sufficient to Learn High-Performing Algorithms?},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {71},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3676278},
doi = {10.1145/3676278},
abstract = {Algorithms often have tunable parameters that impact performance metrics such as runtime and solution quality. For many algorithms used in practice, no parameter settings admit meaningful worst-case bounds, so the parameters are made available for the user to tune. Alternatively, parameters may be tuned implicitly within the proof of a worst-case approximation ratio or runtime bound. Worst-case instances, however, may be rare or nonexistent in practice. A growing body of research has demonstrated that a data-driven approach to parameter tuning can lead to significant improvements in performance. This approach uses a training set of problem instances sampled from an unknown, application-specific distribution and returns a parameter setting with strong average performance on the training set.We provide techniques for deriving generalization guarantees that bound the difference between the algorithm’s average performance over the training set and its expected performance on the unknown distribution. Our results apply no matter how the parameters are tuned, be it via an automated or manual approach. The challenge is that for many types of algorithms, performance is a volatile function of the parameters: slightly perturbing the parameters can cause a large change in behavior. Prior research&nbsp;[e.g., 12, 16, 20, 62] has proved generalization bounds by employing case-by-case analyses of greedy algorithms, clustering algorithms, integer programming algorithms, and selling mechanisms. We streamline these analyses with a general theorem that applies whenever an algorithm’s performance is a piecewise-constant, piecewise-linear, or—more generally—piecewise-structured function of its parameters. Our results, which are tight up to logarithmic factors in the worst case, also imply novel bounds for configuring dynamic programming algorithms from computational biology.},
journal = {J. ACM},
month = oct,
articleno = {32},
numpages = {58},
keywords = {Automated algorithm design, data-driven algorithm design, automated algorithm configuration, machine learning theory, computational biology}
}

@article{10.1145/3676961,
author = {Yang, Xu and Rajbahadur, Gopi krishnan and Lin, Dayi and Wang, Shaowei and Jiang, Zhen Ming (Jack)},
title = {SimClone: Detecting Tabular Data Clones Using Value Similarity},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3676961},
doi = {10.1145/3676961},
abstract = {Data clones are defined as multiple copies of the same data among datasets. The presence of data clones between datasets can cause issues such as difficulties in managing data assets and data license violations when using datasets with clones to build AI software. However, detecting data clones is not trivial. The majority of the prior studies in this area rely on structural information to detect data clones (e.g., font size, column header). However, tabular datasets used to build AI software are typically stored without any structural information. In this article, we propose a novel method called SimClone for data clone detection in tabular datasets without relying on structural information. SimClone method utilizes value similarities for data clone detection. We also propose a visualization approach as a part of our SimClone method to help locate the exact position of the cloned data between a dataset pair. Our results show that our SimClone outperforms the current state-of-the-art method by at least 20\% in terms of both F1-score and AUC. In addition, SimClone’s visualization component helps identify the exact location of the data clone in a dataset with a Precision@10 value of 0.80 in the top 20 true positive predictions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {17},
numpages = {27},
keywords = {Data clone, Tabular clone, Value Similarity, Machine learning datasets}
}

@article{10.1145/3677112,
author = {MacKenzie, Janelle E. and Klarkowski, Madison and Horton, Ella M. and Theobald, Maryanne and Danby, Susan and Kervin, Lisa and Barrie, Lance and Amery, Philippa K. and Andradi, Manesha and Smith, Simon S. and Mandryk, Regan L. and Johnson, Daniel},
title = {Using Psychophysiological Data to Facilitate Reflective Conversations with Children about their Player Experiences},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3677112},
doi = {10.1145/3677112},
abstract = {Given the ubiquity of videogame play by children, concerns about the impacts of videogames, and increasing evidence of the benefits associated with digital play, it is important to evaluate the player experience (pX) of children. Moreover, it is essential to include children's voices in research about technologies designed for their use. However, eliciting rich, first-hand data on their experiences and perspectives is challenging because it requires children to articulate opinions in a pre-established context of general positivity towards games, alongside potential social desirability bias. In this paper, we present a methodology which uses children's psychophysiology as a prompt for reflective interviews, with two commercial off-the-shelf computer games (Rocket League and LEGO Builder's Journey). We consider the utility, reproducibility, reliability and validity of the method. In assessing the method, we discuss children's experience of wearing the sensors, the insights generated from the psychophysiological data, and the understanding that emerged when prompting children to reflect and comment on their own psychophysiological data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {347},
numpages = {33},
keywords = {children, digital games, eye tracking, facial expressions, games user research, gsr, gur, hr, methodology, physiology, player experience, psychophysiology, px}
}

@article{10.1145/3677130,
author = {Gu, Tu and Feng, Kaiyu and Yang, Jingyi and Cong, Gao and Long, Cheng and Zhang, Rui},
title = {BT-Tree: A Reinforcement Learning Based Index for Big Trajectory Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3677130},
doi = {10.1145/3677130},
abstract = {With the increasing availability of trajectory data, it is important to have good indexes to facilitate query processing. In this work, we propose BT-Tree, which is built through a recursive bi-partitioning approach, for the processing of range and KNN queries for past trajectory data. We first propose a cost function based method (CFBM) to build the BT-Tree. Specifically, we design a novel cost function, which incorporates the characteristics of both the data and historical query workload, to decide how to partition a BT-Tree node. Then we propose a reinforcement learning (RL) based method to address CFBM's limitations, such as making locally optimal decisions that may lead to global suboptimality. Experiments on three real datasets with up to 800 million data points show that the CFBM generally outperforms the baselines in terms of query processing time and the RL based method consistently outperforms the baselines and has more significant advantages on larger datasets.},
journal = {Proc. ACM Manag. Data},
month = sep,
articleno = {194},
numpages = {27},
keywords = {deep learning, learned index, reinforcement learning, spatio-temporal data, spatio-temporal query processing}
}

@article{10.1145/3677133,
author = {Feng, Chen and Chen, Xingguang and Guo, Qintian and Zhang, Fangyuan and Wang, Sibo},
title = {Efficient Approximation Algorithms for Minimum Cost Seed Selection with Probabilistic Coverage Guarantee},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3677133},
doi = {10.1145/3677133},
abstract = {Given a social network G, a cost associated with each user, and an influence threshold η, the minimum cost seed selection problem (MCSS) aims to find a set of seeds that minimizes the total cost to reach η users. Existing works are mainly devoted to providing an &lt;u&gt;e&lt;/u&gt;xpected &lt;u&gt;c&lt;/u&gt;overage &lt;u&gt;g&lt;/u&gt;uarantee on reaching η, classified as MCSS-ECG, where their solutions either rely on an impractical influence oracle or cannot attain the expected influence threshold. More importantly, due to the expected coverage guarantee, the actual influence in a campaign may drift from the threshold evidently. Thus, the advertisers would like to request for a probability guarantee of reaching η. This motivates us to further solve the MCSS problem with a &lt;u&gt;p&lt;/u&gt;robabilistic &lt;u&gt;c&lt;/u&gt;overage &lt;u&gt;g&lt;/u&gt;uarantee, termed MCSS-PCG.In this paper, we first propose our algorithm CLEAR to solve MCSS-ECG, which reaches the expected influence threshold without any influence oracle or influence shortfall but a practical approximation ratio. However, the ratio involves an unknown term (i.e., the optimal cost). Thus, we further devise the STAR method to derive a lower bound of the optimal cost and then obtain the first explicit approximation ratio for MCSS-ECG. In MCSS-PCG, it is necessary to estimate the probability that the current seeds reach η, to decide when to stop seed selection. To achieve this, we design a new technique named MRR, which provides efficient probability estimation with a theoretical guarantee. With MRR in hand, we propose our algorithm SCORE for MCSS-PCG, whose performance guarantee is derived by measuring the gap between MCSS-ECG and MCSS-PCG, and applying the theoretical results in MCSS-ECG. Finally, extensive experiments demonstrate that our algorithms achieve up to two orders of magnitude speed-up compared to alternatives while meeting the requirement of MCSS-PCG with the smallest cost.},
journal = {Proc. ACM Manag. Data},
month = sep,
articleno = {197},
numpages = {26},
keywords = {approximation algorithms, minimum cost seed selection, probability estimation}
}

@article{10.1145/3677139,
author = {Ma, Lei and Cao, Lei and VanNostrand, Peter M. and Hofmann, Dennis M. and Su, Yao and Rundensteiner, Elke A.},
title = {Pluto: Sample Selection for Robust Anomaly Detection on Polluted Log Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3677139},
doi = {10.1145/3677139},
abstract = {Log anomaly detection, critical in identifying system failures and preempting security breaches, finds irregular patterns within large volumes of log data. Modern log anomaly detectors rely on training deep learning models on clean anomaly-free log data. However, such clean log data requires expensive and tedious human labeling. In this paper, we thus propose a robust log anomaly detection framework, PlutoNOSPACE, that automatically selects a clean representative sample subset of the polluted log sequence data to train a Transformer-based anomaly detection model. Pluto features three innovations. First, due to localized concentrations of anomalies inherent in the embedding space of log data, Pluto partitions the sequence embedding space generated by the model into regions that then allow it to identify and discard regions that are highly polluted by our pollution level estimation scheme, based on our pollution quantification via Gaussian mixture modeling. Second, for the remaining more slightly polluted regions, we select samples that maximally purify the eigenvector spectrum, which can be transformed into the NP-hard facility location problem; allowing us to leverage its greedy solution with a (1-(1/e)) approximation guarantee in optimality. Third, by iteratively alternating between the above subset selection, a model re-training on the latest subset, and a subset filtering using dynamic training artifacts generated by the latest model, the data selected is progressively refined. The final sample set is used to retrain the final anomaly detection model. Our experiments on four real-world log benchmark datasets demonstrate that by retaining 77.7\% (BGL) to 96.6\% (ThunderBird) of the normal sequences while effectively removing 90.3\% (BGL) to 100.0\% (ThunderBird, HDFS) of the anomalies, Pluto provides a significant absolute F-1 improvement up to 68.86\% (2.16\% → 71.02\%) compared to the state-of-the-art sample selection methods. The implementation of this work is available at https://github.com/LeiMa0324/Pluto-SIGMOD25.},
journal = {Proc. ACM Manag. Data},
month = sep,
articleno = {203},
numpages = {25},
keywords = {anomaly detection, log sequence, polluted data, sample selection}
}

@article{10.1145/3677178,
author = {Pandey, Shailja and Panda, Preeti Ranjan},
title = {NeuroTAP: Thermal and Memory Access Pattern-Aware Data Mapping on 3D DRAM for Maximizing DNN Performance},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3677178},
doi = {10.1145/3677178},
abstract = {Deep neural networks (DNNs) have been widely adopted, owing to break-through performance and high accuracy. DNNs exhibit varying memory behavior involving specific and recognizable memory access patterns and access intensity, depending on the selected data reuse in different layers. Such applications have high memory bandwidth demands due to aggressive computations, performing several billion-floating-point-operations-per-second (BFLOPs). 3D DRAMs, providing very high memory access bandwidth, are extensively employed to break the memory wall, bridging the gap between compute and memory while running DNNs. However, the vertical integration in 3D DRAM introduces serious thermal issues, resulting from high power density and close proximity of memory cells, and requires dynamic thermal management (DTM). To unleash the true potential of 3D DRAM and exploit the enormous bandwidth under thermal constraints, there is a need to intelligently map the DNN application’s data across memory channels, pseudo-channels, and banks, minimizing the effective memory latency and reducing the thermal-induced application slowdown. The specific memory access patterns exhibited by a DNN layer execution are crucial to determine a favorable data mapping method for 3D DRAM dies that potentially causes minimal thermal impact and also maximizes DRAM bandwidth utilization. In this work, we propose an application-aware and thermal-sensitive data mapping that intelligently assigns portions of the 3D DRAM to DNN layers, leveraging the knowledge about layer’s memory access patterns and minimizing DTM-induced performance overheads. Additionally, we also deploy a DRAM low-power states based DTM mechanism to keep the 3D DRAM within safe thermal limits. Using our proposal, we observe a performance improvement of 1\% to 61\%, and memory energy savings of 1\% to 55\% for popular DNNs over state-of-the-art DTM strategies while running DNN inference.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = sep,
articleno = {96},
numpages = {30},
keywords = {3D DRAM, deep neural networks, dynamic thermal management, memory access pattern-awareness, data mapping}
}

@article{10.1145/3678009,
author = {Wu, Jiang and Zhang, Zhuo and Yang, Deheng and Xu, Jianjun and He, Jiayu and Mao, Xiaoguang},
title = {Time-Aware Spectrum-Based Bug Localization for Hardware Design Code with Data Purification},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1544-3566},
url = {https://doi.org/10.1145/3678009},
doi = {10.1145/3678009},
abstract = {The verification of hardware design code is a critical aspect in ensuring the quality and reliability of hardware products. Finding bugs in hardware design code is important for hardware development and is frequently considered as a notoriously challenging and time-consuming activity while being an essential aspect of verification. Thus, bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, there exists an unpredictable time span between the precise origin of a bug and its detected manifestation in prior work without costly formal verification. Locating the bug responsible for the exposed discrepancy between expected and exhibited design behavior remains a major challenge. In this work, we propose Tartan, a Time-aware spectrum-based bug localization with data purification for hardware design code to address these limitations. Tartan integrates hardware-specific timing information with the spectrum and captures the changes of executed statements when the state of the circuit changes to effectively locate bugs. Further, Tartan purifies the spectrum data from the simulation and evaluates the suspiciousness of the statements in the design to indicate the likelihood of being buggy. To evaluate the effectiveness of Tartan, we conduct large-scale experiments on 69 versions of 15 hardware projects by the state-of-the-art bug localization techniques. The experimental results clearly show that Tartan is statistically more effective than the baselines. It provides a new perspective on hardware design code bug localization and brings fresh insights to the community.},
journal = {ACM Trans. Archit. Code Optim.},
month = sep,
articleno = {64},
numpages = {25},
keywords = {Automatic bug localization, time-aware, spectrum, hardware design code, data purification}
}

@article{10.1145/3678578,
author = {Zhang, Panyu and Jung, Gyuwon and Alikhanov, Jumabek and Ahmed, Uzair and Lee, Uichin},
title = {A Reproducible Stress Prediction Pipeline with Mobile Sensor Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678578},
doi = {10.1145/3678578},
abstract = {Recent efforts to predict stress in the wild using mobile technology have increased; however, the field lacks a common pipeline for assessing the impact of factors such as label encoding and feature selection on prediction performance. This gap hinders replication, especially because of a lack of common guidelines for reporting results or privacy concerns that limit access to open codes and datasets. Our study introduces a common pipeline based on a comprehensive literature review and offers comprehensive evaluations of key pipeline factors, promoting independent reproducibility. Our systematic evaluation aimed to validate the findings of previous studies. We identified overfitting and distribution shifts across users as the major reasons for performance limitations. We used K-EmoPhone, a public dataset, for experimentation and a new public dataset---DeepStress---to validate the findings. Furthermore, our results suggest that researchers should carefully consider temporal order in cross-validation settings. Additionally, self-report labels for target users are key to enhancing performance in user-independent scenarios.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {143},
numpages = {35},
keywords = {Mobile Health, Reproducibility, Stress Prediction}
}

@article{10.1145/3678584,
author = {Le, Ha and Lakshminarayanan, Rithika and Li, Jixin and Mishra, Varun and Intille, Stephen},
title = {Collecting Self-reported Physical Activity and Posture Data Using Audio-based Ecological Momentary Assessment},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678584},
doi = {10.1145/3678584},
abstract = {μEMA is a data collection method that prompts research participants with quick, answer-at-a-glance, single-multiple-choice self-report behavioral questions, thus enabling high-temporal-density self-report of up to four times per hour when implemented on a smartwatch. However, due to the small watch screen, μEMA is better used to select among 2 to 5 multiple-choice answers versus allowing the collection of open-ended responses. We introduce an alternative and novel form of micro-interaction self-report using speech input - audio-μEMA- where a short beep or vibration cues participants to verbally report their behavioral states, allowing for open-ended, temporally dense self-reports. We conducted a one-hour usability study followed by a within-subject, 6-day to 21-day free-living feasibility study in which participants self-reported their physical activities and postures once every 2 to 5 minutes. We qualitatively explored the usability of the system and identified factors impacting the response rates of this data collection method. Despite being interrupted 12 to 20 times per hour, participants in the free-living study were highly engaged with the system, with an average response rate of 67.7\% for audio-μEMA for up to 14 days. We discuss the factors that impacted feasibility; some implementation, methodological, and participant challenges we observed; and important considerations relevant to deploying audio-μEMA in real-time activity recognition systems.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {111},
numpages = {35},
keywords = {Audio, Ecological Momentary Assessment, Experience Sampling, Microinteraction, Physical Activity Measurement, Ubiquitous and Wearable Computing}
}

@article{10.1145/3679016,
author = {Cheng, Wenhui and Jiang, Zixian and Xiang, Chaocan and Fu, Jianglan},
title = {Marginal Effect-aware Multiple-Vehicle Scheduling for Road Data Collection: A Near-optimal Result},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1550-4859},
url = {https://doi.org/10.1145/3679016},
doi = {10.1145/3679016},
abstract = {Vehicles equipped with abundant sensors offer a promising way for large-scale, low-cost road data collection. To realize this potential, a well-designed vehicle scheduling scheme is essential for deploying the recruited drivers efficiently. Nevertheless, existing works fail to consider the marginal effect among drivers’ collections. Different from them, this article introduces a, to the best of our knowledge, new multiple-vehicle scheduling problem that jointly optimizes task allocation and vehicle trajectory planning to maximize the overall collection utility by accounting for the marginal effect in drivers’ data collections. However, solving this problem is non-trivial due to its involvement with multiple coupled NP-hard problems. To this end, we propose MeSched, a marginal effect-aware multiple-vehicle scheduling scheme designed for road data collection. Specifically, we first present a greedy-based auxiliary graph construction method to disentangle the initial problem into multiple independent single-vehicle scheduling subproblems. Furthermore, we build an approximate surrogate function that transforms each subproblem into a tractable form involving only a single variable. The theoretical analysis proves that MeSched can achieve a 1-(1/e)¼-approximation ratio in polynomial time. Comprehensive evaluations based on a real-world trajectory dataset of 12,493 vehicles demonstrate that MeSched can significantly improve the collection utility by 104.5\% on average compared with four baselines.},
journal = {ACM Trans. Sen. Netw.},
month = oct,
articleno = {116},
numpages = {21},
keywords = {Vehicular crowdsensing, vehicle scheduling, submodular optimization}
}

@article{10.1145/3680277,
author = {Zhao, Yao and Qu, Youyang and Xiang, Yong and Uddin, Md Palash and Peng, Dezhong and Gao, Longxiang},
title = {A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals and Future Trends},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3680277},
doi = {10.1145/3680277},
abstract = {Recent advances in edge computing&nbsp;(EC) have pushed cloud-based data caching services to edge; however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV), which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {8},
numpages = {34},
keywords = {Edge data integrity verification, edge computing, security, Internet of Things}
}

@article{10.1145/3680281,
author = {Dekhici, Benaissa and Benyahia, Boumediene and Cherki, Brahim and Fiori, Luca and Andreottola, Gianni},
title = {Modeling of biogas production from hydrothermal carbonization products in a continuous anaerobic digester},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3680281},
doi = {10.1145/3680281},
abstract = {The coupling between anaerobic digestion and hydrothermal carbonization (HTC) is a promising alternative for sustainable energy production. This study presents a dynamic model tailored for a lab-scale anaerobic digester operating on HTC products, specifically hydrochar and HTC liquor derived from sewage and agro-industrial digestate. Leveraging a modified version of the Anaerobic Model 2 (AM2), our simplified model of four states integrates pH and biomass decay rates into biomass kinetics. Simulation results of the mode were compared with experimental data collected over 164 days from the digester. The obtained results have proven the ability of the proposed model to predict the trend of the biogas production as well as important measured outputs of the bioreactor. The developed model could be used to control and optimize the performance of the digester, which provides potential for bioenergy production from waste streams such as digestate and digestate treated through the HTC process.},
journal = {ACM Trans. Model. Comput. Simul.},
month = sep,
articleno = {27},
numpages = {19},
keywords = {Anaerobic digestion, hydrothermal carbonization, HTC, mathematical modeling and simulation, sensitivity analysis, AM2 model, bioprocess, model identification and optimization}
}

@article{10.1145/3685929,
author = {Protick, Taufiq Islam and Sabir, Aafaq and Abhinaya, Sb and Bartlett, Aiden and Das, Anupam},
title = {Unveiling Users’ Security and Privacy Concerns Regarding Smart Home IoT Products from Online Reviews},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3685929},
doi = {10.1145/3685929},
abstract = {The Internet of Things (IoT) has revolutionized the global market with lifestyle products such as fitness trackers (FT), smart home speakers (SHS), and surveillance and security camera systems (SSCS). While offering convenience, these products also introduce potential security and privacy (S&amp;P) risks to buyers, often going unnoticed. Consumers’ incomplete mental models of the risks involved and the information asymmetry between buyers and sellers only add to the problem. Understanding consumer concerns in online product reviews can play a crucial role in bridging the gap of such information asymmetry. By establishing a balanced flow of information between buyers and sellers, manufacturers can leverage genuine concerns expressed in reviews to enhance product features while educating users about misinformation in reviews. In this study, we collected FT, SHS, and SSCS product reviews from three Amazon domains: the US, the UK, and India. Using a keyword-based search method focused on S&amp;P concerns, we discovered a considerable number of reviews expressing notable concerns regarding security and privacy. Our qualitative analysis revealed that data security is a common concern across all product types. Further, our quantitative analysis exposed significant geographic variations, with the concern ratio being higher in the US than in the UK for all device types and higher than in the Indian domain for security cameras. These findings highlight the need for tailored security measures and user awareness campaigns in different parts of the world to address the identified concerns effectively.},
journal = {ACM J. Comput. Sustain. Soc.},
month = nov,
articleno = {44},
numpages = {41},
keywords = {IoT devices, mixed-method analysis, security and privacy concerns}
}

@article{10.1145/3686151,
author = {Zhang, Xu and Lin, Zexu and Hu, Xiaoyu and Wang, Jianlei and Lu, Wenpeng and Zhou, Deyu},
title = {SECON: Maintaining Semantic Consistency in Data Augmentation for Code Search},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3686151},
doi = {10.1145/3686151},
abstract = {Efficient code search techniques are crucial in accelerating software development by aiding developers in locating specific code snippets and understanding code functionalities. This study investigates code search methodologies, focusing on the emerging significance of semantic consistency in data augmentation techniques. While existing approaches predominantly enhance raw data, often requiring additional preprocessing and incurring higher training costs, this research introduces a pioneering method operating at the code and query representation levels. By bypassing the need for extensive data processing, this novel approach fosters an interactive alignment between code and query, augmenting the semantic coherence crucial for effective code search. An extensive empirical evaluation of a diverse dataset across multiple programming languages substantiates the efficacy of this approach in significantly enhancing code search model performance compared to traditional methodologies. The implementation is publicly available on GitHub 1, offering an accessible resource for further exploration and application.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = aug,
keywords = {code search, data augmentation, semantic consistency}
}

@article{10.1145/3686905,
author = {Kim, Seyun and Ho, Jonathan and Li, Yinan and Fan, Bonnie and Yang, Willa Yunqi and Ramey, Jessie and Fox, Sarah E. and Zhu, Haiyi and Zimmerman, John and Eslami, Motahhare},
title = {Integrating Equity in Public Sector Data-Driven Decision Making: Exploring the Desired Futures of Underserved Stakeholders},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3686905},
doi = {10.1145/3686905},
abstract = {Public sector agencies aim to innovate not just for efficiency but also to enhance equity. Despite the growing adoption of data-driven decision-making systems in the public sector, efforts to integrate equity as a primary goal often fall short. This typically arises from inadequate early-stage involvement of underserved stakeholders and prevalent misunderstandings concerning the authentic meaning of equity from these stakeholders' perspectives. Our research seeks to address this gap by actively involving undersevered stakeholders in the process of envisioning the integration of equity within public sector data-driven decisions, particularly in the context of a building department in a Northeastern mid-sized U.S. city. Applying a speed dating method with storyboards, we explore diverse equity-centric futures within the realm of local business development, a domain where small businesses, particularly women-and minority-owned businesses, historically confront inequitable distribution of public services. We explored three essential aspects of equity: monitoring equity, resource allocation prioritization, as well as information and equity. Our findings illuminate the complexities of integrating equity into data-driven decisions, offering nuanced insights about the needs of stakeholders. We found that attempts to monitor and incorporate equity goals into public sector decision-making can unexpectedly backfire, inadvertently sparking community apprehension and potentially exacerbating existing inequities. Small business owners, including those identifying as women-and minority-owned, advocated against the use of demographic-based data in equity-focused data-driven decision-making in the public sector, instead emphasizing factors such as community needs, application complexity, and uncertainties inherent in small businesses. Drawing from these insights, we propose design implications to assist designers of public sector data-driven decision-making systems to better accommodate equity considerations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {366},
numpages = {39},
keywords = {data-driven decision, equity, public sector, speed dating}
}

@article{10.1145/3687005,
author = {Berke, Alex and Mahari, Robert and Pentland, Alex and Larson, Kent and Calacci, Dana},
title = {Insights from an Experiment Crowdsourcing Data from Thousands of US Amazon Users: The importance of transparency, money, and data use},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3687005},
doi = {10.1145/3687005},
abstract = {Data generated by users on digital platforms are a crucial resource for advocates and researchers interested in uncovering digital inequities, auditing algorithms, and understanding human behavior. Yet data access is often restricted. How can researchers both effectively and ethically collect user data? This paper shares an innovative approach to crowdsourcing user data to collect otherwise inaccessible Amazon purchase histories, spanning 5 years, from more than 5,000 U.S. users. We developed a data collection tool that prioritizes participant consent and includes an experimental study design. The design allows us to study multiple important aspects of privacy perception and user data sharing behavior, including how socio-demographics, monetary incentives and transparency can impact share rates. Experiment results (N=6,325) reveal both monetary incentives and transparency can significantly increase data sharing. Age, race, education, and gender also played a role, where female and less-educated participants were more likely to share. Our study design enables a unique empirical evaluation of the 'privacy paradox', where users claim to value their privacy more than they do in practice. We set up both real and hypothetical data sharing scenarios and find measurable similarities and differences in share rates across these contexts. For example, increasing monetary incentives had a 6 times higher impact on share rates in real scenarios. In addition, we study participants' opinions on how data should be used by various third parties, again finding that gender, age, education, and race have a significant impact. Notably, the majority of participants disapproved of government agencies using purchase data yet the majority approved of use by researchers. Overall, our findings highlight the critical role that transparency, incentive design, and user demographics play in ethical data collection practices, and provide guidance for future researchers seeking to crowdsource user generated data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {466},
numpages = {48},
keywords = {crowdsourcing, data economy, empirical study, experiment design, privacy-paradox}
}

@article{10.1145/3687265,
author = {Gao, Yicheng and Casale, Giuliano and Singhal, Rekha},
title = {Performance Modeling of Distributed Data Processing in Microservice Applications},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2376-3639},
url = {https://doi.org/10.1145/3687265},
doi = {10.1145/3687265},
abstract = {Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = oct,
articleno = {14},
numpages = {30},
keywords = {Microservice, performance modeling, data placement, data caching, layered network}
}

@article{10.1145/3687301,
author = {Goedegebuure, Abel and Kumara, Indika and Driessen, Stefan and Van Den Heuvel, Willem-Jan and Monsieur, Geert and Tamburri, Damian Andrew and Nucci, Dario Di},
title = {Data Mesh: A Systematic Gray Literature Review},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3687301},
doi = {10.1145/3687301},
abstract = {Data mesh is an emerging domain-driven decentralized data architecture that aims to minimize or avoid operational bottlenecks associated with centralized, monolithic data architectures in enterprises. The topic has piqued the practitioners’ interest, and considerable gray literature exists. At the same time, we observe a lack of academic attempts at defining and building upon the concept. Hence, in this article, we aim to start from the foundations and characterize the data mesh architecture regarding its design principles, architectural components, capabilities, and organizational roles. We systematically collected, analyzed, and synthesized 114 industrial gray literature articles. The resulting review provides insights into practitioners’ perspectives on the four key principles of data mesh: data as a product, domain ownership of data, self-serve data platform, and federated computational governance. Moreover, due to the comparability of data mesh and SOA (service-oriented architecture), we mapped the findings from the gray literature into the reference architectures from the SOA academic literature to create the reference architectures for describing three key dimensions of data mesh: organization of capabilities and roles, development, and runtime. Finally, we discuss open research issues in data mesh, partially based on the findings from the gray literature.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {11},
numpages = {36},
keywords = {Data mesh, principles, reference architectures, research challenges, gray literature review, data architecture, data management}
}

@article{10.1145/3688003,
author = {Wang, ZiXuan and Wang, Pan and Sun, Zhixin and Zhou, Xiaokang and Fu, MengYi and Liu, MinYao and Wang, XinTong and Chen, Lu},
title = {FASDSA: A Flexible Adaptive and Secure Data Sharing Architecture},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4665},
url = {https://doi.org/10.1145/3688003},
doi = {10.1145/3688003},
abstract = {With the development of Web 3.0 and Metaverse technologies, the ability of autonomous vehicles has been dramatically improved. These technologies have decentralized features that break the traditional data-sharing mode, grant users control over their data, and achieve benefits through data sharing, promoting the widespread circulation of data. To ensure data exchange security, flexibility, and reliability, this paper proposes FASDSA: A Flexible, Adaptive, and Secure Data Sharing Architecture for CAVs with Web 3.0 and Metaverse. This architecture has three advantages: First, it adopts a decentralized, federated learning and CAV role division method, which allows different computational power CAVs to participate in data sharing according to their roles, achieving flexible data privacy protection. Second, it has the ability of tampered model detection based on interpretable analysis, which can effectively ensure that the model is not tampered with. Third, it has a reward mechanism based on work contribution and trust assessment, which uses blockchain technology to ensure the continuous security operation of this architecture. To verify the performance of FASDSA, we used the UNSW-NB15 dataset to conduct three experiments. The experimental results indicate that compared to traditional methods, FASDSA possesses greater flexibility and security while maintaining similar or even superior model performance.},
note = {Just Accepted},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = aug,
keywords = {Cooperative Intrusion Detection, Federated Learning, Blockchain, Connected Autonomous Vehicles, Web3.0}
}

@article{10.1145/3688393,
author = {Alzahrani, Naif and Ca\l{}a, Jacek and Missier, Paolo},
title = {Experience: A Comparative Analysis of Multivariate Time-Series Generative Models: A Case Study on Human Activity Data},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3688393},
doi = {10.1145/3688393},
abstract = {Human activity recognition (HAR) is an active research field that has seen great success in recent years due to advances in sensory data collection methods and activity recognition systems. Deep artificial intelligence (AI) models have contributed to the success of HAR systems lately, although still suffering from limitations such as data scarcity, the high costs of labelling data instances, and datasets’ imbalance and bias. The temporal nature of human activity data, represented as time series data, impose an additional challenge to using AI models in HAR, because most state-of-the-art models do not account for the time component of the data instances. These limitations have inspired the time-series research community to design generative models for sequential data, but very little work has been done to evaluate the quality of such models. In this work, we conduct a comparative quality analysis of three generative models for time-series data, using a case study in which we aim to generate sensory human activity data from a seed public dataset. Additionally, we adapt and clearly explain four evaluation methods of synthetic time-series data from the literature and apply them to assess the quality of the synthetic activity data we generate. We show experimentally that high-quality human activity data can be generated using deep generative models, and the synthetic data can thus be used in HAR systems to augment real activity data. We also demonstrate that the chosen evaluation methods effectively ensure that the generated data meets the essential quality benchmarks of realism, diversity, coherence, and utility. Our findings suggest that using deep generative models to produce synthetic human activity data can potentially address challenges related to data scarcity, biases, and expensive labeling. This holds promise for enhancing the efficiency and reliability of HAR systems.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {18},
numpages = {18},
keywords = {Human activity recognition, multivariate time series, generative modeling}
}

@article{10.1145/3688807,
author = {Demmel, Markus and G\"{o}bel, Thomas and Gon\c{c}alves, Patrik and Baier, Harald},
title = {Data Synthesis Is Going Mobile—On Community-Driven Dataset Generation for Android Devices},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3688807},
doi = {10.1145/3688807},
abstract = {Personal electronic devices such as smartphones and smartwatches have become indispensable daily companions, collecting a multitude of personal and sensitive data. As a result, they are of paramount importance in digital forensic examinations. However, there is a lack of publicly available and ready-to-use digital forensic datasets, especially in mobile forensics. This work presents a concept and an open-source proof-of-concept implementation, which simplifies and automates the creation of mobile forensic datasets within the scope of the Android operating system. In contrast to previous approaches, which populate the most common databases of an Android device, our concept is based on community-driven playbooks and makes use of interaction with the actual smartphone GUI. Hence, we are able to generate coherent and realistic traces as they occur in real-world human usage. Our proof-of-concept implementation is based on the standard Android emulation environment and borrows tools from the user interface testing community. Our evaluation shows that our approach actually generates realistic Android datasets. For instance, we can generate traces that cannot be simulated by gestures (e.g., changing the GPS position or triggering incoming phone calls). Recording the actual data synthesis process allows users to either create and share their own playbooks (i.e., the exact instructions for the data synthesis process rather than having to share the full image) or reproduce Android images with different scenarios using playbooks previously created and shared by the community.},
journal = {Digital Threats},
month = oct,
articleno = {30},
numpages = {19},
keywords = {Mobile forensics, Android image, Forensic dataset, Digital corpora, Data synthesis, Data generation, Data synthesis framework, UI testing, Android Emulator, User simulation, Human interaction}
}

@article{10.1145/3688845,
author = {Cruz-Filipe, Lu\'{\i}s and Gaspar, Gra\c{c}a and Nunes, Isabel},
title = {Hypothetical Answers to Continuous Queries Over Data Streams},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1529-3785},
url = {https://doi.org/10.1145/3688845},
doi = {10.1145/3688845},
abstract = {Answers to continuous queries over data streams are often delayed until some relevant input arrives through the data stream. These delays may turn answers when they arrive, obsolete to users who sometimes have to make decisions with no help whatsoever. Therefore, it can be useful to provide hypothetical answers—“given the current information, it is possible that  (X)  will become true at time  (t) ”—instead of no information at all.In this work, we present a semantics for queries and corresponding answers that cover such hypothetical answers, together with an incremental online algorithm for updating the set of facts that are consistent with the currently available information. Our framework also works in a language supporting negation.},
journal = {ACM Trans. Comput. Logic},
month = oct,
articleno = {25},
numpages = {40},
keywords = {continuous query answering, stream reasoning, hypothetical reasoning, temporal Datalog}
}

@article{10.1145/3689043,
author = {Reed, Courtney N. and Benito, Adan L. and Caspe, Franco and McPherson, Andrew P.},
title = {Shifting Ambiguity, Collapsing Indeterminacy: Designing with Data as Baradian Apparatus},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3689043},
doi = {10.1145/3689043},
abstract = {This article examines how digital systems designers distil the messiness and ambiguity of the world into concrete data that can be processed by computing systems. Using Karen Barad's agential realism as a guide, we explore how data is fundamentally entangled with the tools and theories of its measurement. We examine data-enabled artefacts acting as Baradian apparatuses: they do not exist independently of the phenomenon they seek to measure but rather collect and co-produce observations from within their entangled state: the phenomenon and the apparatus co-constitute one another. Connecting Barad's quantum view of indeterminacy to the prevailing HCI discourse on the opportunities and challenges of ambiguity, we suggest that the very act of trying to stabilise a conceptual interpretation of data within an artefact has the paradoxical effect of amplifying and shifting ambiguity in interaction. We illustrate these ideas through three case studies from our own practices of designing digital musical instruments (DMIs). DMIs necessarily encode symbolic and music-theoretical knowledge as part of their internal operation, even as conceptual knowledge is not their intended outcome. In each case, we explore the nature of the apparatus, what phenomena it co-produces, and where the ambiguity lies to suggest approaches for design using these abstract theoretical frameworks.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {73},
numpages = {41},
keywords = {ambiguity, research through design, agential realism, entanglement, mapping, digital musical instruments}
}

@article{10.1145/3689627,
author = {Sharief, Farhana and Ijaz, Humaira and Shojafar, Mohammad and Naeem, Muhammad Asif},
title = {Multi-Class Imbalanced Data Handling with Concept Drift in Fog Computing: A Taxonomy, Review, and Future Directions},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3689627},
doi = {10.1145/3689627},
abstract = {A network of actual physical objects or “IoT components” linked to the internet and equipped with sensors, electronics, software, and network connectivity is known as the Internet of Things (IoT). This ability of the IoT components to gather and share data is made possible by this network connectivity. Many IoT devices are currently operating, which generate a lot of data. When these IoT devices started collecting data, the cloud was the only place to analyze, filter, pre-process, and aggregate it. However, when it comes to IoT, the cloud has restrictions regarding latency and a more centralized method of distributing programs. A new form of computing called Fog computing has been proposed to address the shortcomings of current cloud computing. In an IoT context, sensors regularly communicate signal information, and edge devices process the data obtained from these sensors using Fog computing. The sensors’ internal or external problems, security breaches, or the integration of heterogeneous equipment contribute to the imbalanced data, i.e., comparatively speaking, one class has more instances than the other. As a result of this data, the pattern extraction is imbalanced. Recent attempts have concentrated heavily on binary-class imbalanced concerns with exactly two classes. However, the classification of multi-class imbalanced data is an issue that needs to be fixed in Fog computing, even if it is widespread in other fields, including text categorization, human activity detection, and medical diagnosis. The study intends to deal with this problem. It presents a systematic, thorough, and in-depth comparative analysis of several binary-class and multi-class imbalanced data handling strategies for batch and streaming data in IoT networks and Fog computing. There are five major objectives in this study. First, reviewing the Fog computing concept. Second, outlining the optimization metric used in Fog computing. Third, focusing on binary and multi-class batch data handling for IoT networks and Fog computing. Fourth, reviewing and comparing the current imbalanced data handling methodologies for multi-class data streams. Fifth, explaining how to cope with the concept drift, including novel and recurring classes, targeted optimization measures, and evaluation tools. Finally, the best performance metrics and tools for concept drift, binary-class (batch and stream) data, and multi-class (batch and stream) data are highlighted.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {16},
numpages = {48},
keywords = {Cloud computing, fog computing, Internet of Things (IoT), multi-class imbalanced data stream, concept drift}
}

@article{10.1145/3689775,
author = {Le, Callista and Gopinathan, Kiran and Lee, Koon Wen and Gilbert, Seth and Sergey, Ilya},
title = {Concurrent Data Structures Made Easy},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689775},
doi = {10.1145/3689775},
abstract = {Design of an efficient thread-safe concurrent data structure is a balancing act between its implementation complexity and performance. Lock-based concurrent data structures, which are relatively easy to derive from their sequential counterparts and to prove thread-safe, suffer from poor throughput under even light multi-threaded workload. At the same time, lock-free concurrent structures allow for high throughput, but are notoriously difficult to get right and require careful reasoning to formally establish their correctness.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
In this work, we explore a solution to this conundrum based on a relatively old idea of batch parallelism---an approach for designing high-throughput concurrent data structures via a simple insight: efficiently processing a batch of a priori known operations in parallel is easier than optimising performance for a stream of arbitrary asynchronous requests. Alas, batch-parallel structures have not seen wide practical adoption due to (i) the inconvenience of having to structure multi-threaded programs to explicitly group operations and (ii) the lack of a systematic methodology to implement batch-parallel structures as simply as lock-based ones.
 
 
 
 
 
 
 

 
 
 
 
 
 
 
We present OBatcher---a Multicore OCaml library that streamlines the design, implementation, and usage of batch-parallel structures. OBatcher solves the first challenge (how to use) by suggesting a new lightweight implicit batching design pattern that is built on top of generic asynchronous programming mechanisms. The second challenge (how to implement) is addressed by identifying a family of strategies for converting common sequential structures into the corresponding efficient batch-parallel versions, and by providing a library of functors that embody those strategies. We showcase OBatcher with a diverse set of benchmarks ranging from Red-Black and AVL trees to van Emde Boas trees, skip lists, and a thread-safe implementation of a Datalog solver. Our evaluation of all the implementations on large asynchronous workloads shows that (a) they consistently outperform the corresponding coarse-grained lock-based implementations---the only ones available in OCaml to date, and that (b) their throughput scales reasonably with the number of processors.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {335},
numpages = {29},
keywords = {Multicore OCaml, batch parallelism, shared-memory concurrency}
}

@article{10.1145/3689802,
author = {Nagar, Kartik and Sahoo, Anmol and Chowdhury, Romit Roy and Jagannathan, Suresh},
title = {Automated Robustness Verification of Concurrent Data Structure Libraries against Relaxed Memory Models},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689802},
doi = {10.1145/3689802},
abstract = {Clients reason about the behavior of concurrent data structure                                libraries such as sets, queues, or stacks using specifications that                                capture well-understood correctness conditions, such as                                linearizability. The implementation of these libraries, however,                                focused as they are on performance, may additionally exploit relaxed                                memory behavior allowed by the language or underlying hardware that                                weaken the strong ordering and visibility constraints on shared-memory                                accesses that would otherwise be imposed by a sequentially consistent                                (SC) memory model. As an alternative to developing new specification                                and verification mechanisms for reasoning about libraries under                                relaxed memory model, we instead consider the orthogonal problem of                                library robustness, a property that holds when all possible                                behaviors of a library implementation under relaxed memory model are                                also possible under SC. In this paper, we develop a new                                automated technique for verifying robustness of library                                implementations in the context of a C11-style memory model. This task                                is challenging because a most-general client may invoke an unbounded                                number of concurrently executing library operations that can                                manipulate an unbounded number of shared locations. We establish a                                novel inductive technique for verifying library robustness that                                leverages prior work on the robustness problem for the C11 memory                                model based on the search for a non-robustness witness under SC                                executions. We crucially rely on the fact that this search is carried                                out over SC executions, and use high-level SC specifications                                (including linearizability) of the library to verify the absence of a                                non-robustness witness. Our technique is compositional - we show how                                we can safely preserve robustness of multiple interacting library                                implementations and clients using additional SC fences to guarantee                                robustness of entire executions. Experimental results on a number of                                complex realistic library implementations demonstrate the feasibility                                of our approach.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {362},
numpages = {28},
keywords = {Concurrent Library implementations, Relaxed Memory Models, Robustness}
}

@article{10.1145/3694689,
author = {Yu, Xiao and Liu, Hui and Zhang, Yan and Lin, Yuxiu and Zhang, Caiming},
title = {Hubness-Enabled Clustering and Recovery for Large-Scale Incomplete Multi-View Data},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3694689},
doi = {10.1145/3694689},
abstract = {Incomplete multi-view clustering has gained considerable attention in recent years due to the prevalence of incomplete multi-view data in real-world applications. However, existing methods often struggle to effectively deal with large-scale datasets, particularly those with a significant number of missing instances. To address these issues, we propose a novel method called Hubness-Enabled Clustering and Recovery for Large-Scale Incomplete Multi-View Data (HENRI). HENRI utilizes the consensus hubs of all views to identify informative anchors to handle large-scale incomplete datasets. Furthermore, it incorporates a novel sample-level fusion strategy that effectively integrates information from all views, leading to remarkable outcomes in both cluster formation and missing data reconstruction. HENRI demonstrates exceptional capability in capturing the underlying structures of the data and recovering missing information, even when faced with a significant number of instances with incomplete data in partial views. To validate its effectiveness, we conducted experiments on 6 complete datasets and 31 incomplete datasets, comparing against 11 baseline methods. The results are impressive, demonstrating the superior performance of HENRI over the state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {2},
numpages = {23},
keywords = {Multi-view clustering, incomplete, hubness, large-scale}
}

@article{10.1145/3694979,
author = {Qiu, Jingyi and Song, Aibo and Jin, Jiahui and Chen, Jiaoyan and Zhang, Xinyu and Fang, Xiaolin and Zhang, Tianbo},
title = {Matching Tabular Data to Knowledge Graph with Effective Core Column Set Discovery.},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3694979},
doi = {10.1145/3694979},
abstract = {Matching tabular data to a knowledge graph (KG) is critical for understanding the semantic column types, column relationships, and entities of a table. Existing matching approaches rely heavily on core columns that represent primary subject entities on which other columns in the table depend. However, discovering these core columns before understanding the table’s semantics is challenging. Most prior works use heuristic rules, such as the leftmost column, to discover a single core column, while an insightful discovery of the core column set that accurately captures the dependencies between columns is often overlooked. To address these challenges, we introduce Dependency-aware Core Column Set Discovery (DaCo), an iterative method that uses a novel rough matching strategy to identify both inter-column dependencies and the core column set. Additionally, DaCo can be seamlessly integrated with pre-trained language models, as proposed in the optimization module. Unlike other methods, DaCo does not require labeled data or contextual information, making it suitable for real-world scenarios. In addition, it can identify multiple core columns within a table, which is common in real-world tables. We conduct experiments on six datasets, including five datasets with single core columns and one dataset with multiple core columns. Our experimental results show that DaCo&nbsp; outperforms existing core column set detection methods, further improving the effectiveness of table understanding tasks.},
journal = {ACM Trans. Web},
month = oct,
articleno = {51},
numpages = {27},
keywords = {Table&nbsp;understanding, core column set, semantic dependency}
}

@article{10.1145/3695463,
author = {Yu, Chunqiang and Cheng, Shichao and Zhang, Xianquan and Zhang, Xinpeng and Tang, Zhenjun},
title = {Reversible Data Hiding in Shared JPEG Images},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {12},
issn = {1551-6857},
url = {https://doi.org/10.1145/3695463},
doi = {10.1145/3695463},
abstract = {Reversible data hiding (RDH) in encrypted images has emerged as an effective technique for securely storing and managing confidential images in the cloud. However, most RDH methods in shared images (RDHSI) are designed for uncompressed images and cannot be applied for JPEG images. To address this issue, we propose a novel RDH in shared JPEG images. Our method consists of JPEG image sharing and data hiding in JPEG shares, which are both conducted on JPEG bit-stream. Specifically, the DC appended bits (DCA) and AC appended bits (ACA) derived from the original JPEG bit-stream are shared by ( (k) ,  (n) ) threshold Chinese remainder theorem-based secret sharing (CRTSS) with two different constraints, one for DC sharing and another for AC sharing. The constraint of DC sharing ensures that the DC coefficient shares do not overflow. The constraint of AC sharing ensures the sizes of ACA shares are less than the sizes of the original ACA so that the embedding room can be vacated from each shared JPEG bit-stream. Each data-hider can embed the secret data into the personal JPEG share. The original JPEG image can be recovered losslessly from any  (k)  JPEG shares. The proposed sharing and data hiding are both well compatible with the JPEG standard. Experimental results demonstrate that the proposed method not only well preserves the file size whether the JPEG shares or marked JPEG shares but also achieves outstanding security performance and a high embedding capacity.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
articleno = {372},
numpages = {24},
keywords = {Reversible data hiding, Image sharing, JPEG image, high embedding capacity}
}

@article{10.1145/3696206,
author = {Cao, Chengtai and Zhou, Fan and Dai, Yurou and Wang, Jianping and Zhang, Kunpeng},
title = {A Survey of Mix-based Data Augmentation: Taxonomy, Methods, Applications, and Explainability},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696206},
doi = {10.1145/3696206},
abstract = {Data augmentation (DA) is indispensable in modern machine learning and deep neural networks. The basic idea of DA is to construct new training data to improve the model’s generalization by adding slightly disturbed versions of existing data or synthesizing new data. This survey comprehensively reviews a crucial subset of DA techniques, namely Mix-based Data Augmentation (MixDA), which generates novel samples by combining multiple examples. In contrast to traditional DA approaches that operate on single samples or entire datasets, MixDA stands out due to its effectiveness, simplicity, computational efficiency, theoretical foundation, and broad applicability. We begin by introducing a novel taxonomy that categorizes MixDA into Mixup-based, Cutmix-based, and mixture approaches based on a hierarchical perspective of the data mixing operation. Subsequently, we provide an in-depth review of various MixDA techniques, focusing on their underlying motivations. Owing to its versatility, MixDA has penetrated a wide range of applications, which we also thoroughly investigate in this survey. Moreover, we delve into the underlying mechanisms of MixDA’s effectiveness by examining its impact on model generalization and calibration while providing insights into the model’s behavior by analyzing the inherent properties of MixDA. Finally, we recapitulate the critical findings and fundamental challenges of current MixDA studies while outlining the potential directions for future works. Different from previous related surveys that focus on DA approaches in specific domains (e.g., computer vision and natural language processing) or only review a limited subset of MixDA studies, we are the first to provide a systematical survey of MixDA, covering its taxonomy, methodology, application, and explainability. Furthermore, we provide promising directions for researchers interested in this exciting area.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {37},
numpages = {38},
keywords = {Data augmentation, regularization, generalization, machine learning, deep learning}
}

@article{10.1145/3696391,
author = {Diamant, Jonathan and Landau Feibish, Shir},
title = {SetD4: Sets With Deletions and Decay in the Data Plane},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CoNEXT4},
url = {https://doi.org/10.1145/3696391},
doi = {10.1145/3696391},
abstract = {Sets are a fundamental data type in Computer Science. Data structures used to maintain sets need to enable the insertion and deletion of keys from the set and support a lookup operation to check if a key belongs to the set. Recent advances in programmable networks allow performing fine-grained network telemetry and other network functions right in the data plane, many of which utilize sets. One of the most common data structure in use for maintaining sets in the data plane is the Bloom Filter (BF). Existing implementations of BFs in the data plane support key insertion and lookup, yet due to the harsh processing restrictions of the data plane, they do not support deletions. We present SetD4, the first data structure for maintaining sets in the data plane that supports insertion, lookup and deletion. SetD4 maintains a modified BF, which holds the set, as well as two auxiliary structures that allow the safe removal of keys from the set. In addition, we present a variant of SetD4 that also supports decay, which allows the automatic removal of keys from the structure after a predefined time interval. We analyze SetD4 and show precise error rates for both the decaying and non-decaying structures. We have implemented SetD4 on the Tofino programmable switch and show that it can achieve high accuracy with limited overhead when compared to the current state-of-the-art set-membership data structures in the data plane with better false-positive and false-negative error rates.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {34},
numpages = {22},
keywords = {dynamic set membership, network algorithms, programmable switches}
}

@article{10.1145/3697008,
author = {P\'{e}rez, Beatriz and Rubio, \'{A}ngel Luis and Zapata, Mar\'{\i}a A.},
title = {PROV-IDEA: Supporting Interoperable Schema and Data Provenance within Database Evolution},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697008},
doi = {10.1145/3697008},
abstract = {Database evolution and data provenance are two closely related research fields. On the one hand, the registry (via provenance) of the schema evolution allows the maintenance of its version record. On the other hand, the origin of the data (i.e. its provenance) will always be affected by modifications (i.e. the evolution) in the schema on which they are based. Despite these interrelationships, there are few works in the literature that have proposed advances in that direction. In particular, to the best of our knowledge, there is no research that has resulted in a general and interoperable solution to the problem of managing database evolution using provenance. In this paper we present PROV-IDEA: a PROV-Interoperable Database Evolution Approach. This is a proposal that allows the simultaneous management of the provenance of schemas (of relational databases) and data, using the PROV standard as a way to guarantee interoperability. Furthermore, it is an adaptable and expandable approach (by using PROV templates), which allows a non-intrusive and seamless integration with existing applications, as well as different aspects of provenance information generation. These properties are demonstrated in the article by presenting a proof of concept built on top of a third-party relational database evolution tool.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep
}

@article{10.1145/3698104,
author = {Blanc, Guy},
title = {Subsampling Suffices for Adaptive Data Analysis},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0004-5411},
url = {https://doi.org/10.1145/3698104},
doi = {10.1145/3698104},
abstract = {Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst’s query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of adaptive data analysis was formalized in the seminal works of Dwork et&nbsp;al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014). We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: The only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work. In addition to its simplicity, we demonstrate the utility of this framework by designing mechanisms for two foundational tasks, statistical queries and median finding. In particular, our mechanism for answering the broadly applicable class of statistical queries is both extremely simple and state of the art in many parameter regimes.},
note = {Just Accepted},
journal = {J. ACM},
month = oct,
keywords = {Adaptive data analysis, Statistical queries, Stability of algorithms, Subsampling, Differential privacy}
}

@article{10.1145/3698194,
author = {Michalopoulos, Achilleas and Tsitsigkos, Dimitrios and Bouros, Panagiotis and Mamoulis, Nikos and Terrovitis, Manolis},
title = {Efficient Distance Queries on Non-point Data},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3698194},
doi = {10.1145/3698194},
abstract = {Distance queries, including distance-range queries, k-nearest neighbors search, and distance joins, are very popular in spatial databases. However, they have been studied mainly for point data. Inspired by a recent approach on indexing non-point objects for rectangular range queries, we propose a secondary partitioning approach for space-partitioning indices, which is appropriate for distance queries. Our approach classifies the contents of each primary partition into 16 secondary partitions, taking into consideration the begin and end values of objects with respect to the spatial extent of the primary partition. Based on this, we define algorithms for three popular spatial query types, that avoid duplicate results and skip unnecessary computations. We compare our approach to the previous secondary partitioning method and to state-of-the-art data-partitioning indexing and find that it has a significant performance advantage.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = nov,
articleno = {1},
numpages = {37},
keywords = {Spatial indexing, distance queries, nearest neighbor search, distance join, non-point data}
}

@article{10.1145/3698195,
author = {Hussein, Dina and Belkhouja, Taha and Bhat, Ganapati and Doppa, Jana},
title = {Sensor-Aware Data Imputation for Time-Series Machine Learning on Low-Power Wearable Devices},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3698195},
doi = {10.1145/3698195},
abstract = {Wearable devices that have low-power sensors, processors, and communication capabilities are gaining wide adoption in several health applications. The machine learning algorithms on these devices assume that data from all sensors are available during runtime. However, data from one or more sensors may be unavailable due to energy or communication challenges. This loss of sensor data can result in accuracy degradation of the application. Prior approaches to handle missing data, such as generative models or training multiple classifiers for each combination of missing sensors are not suitable for low-energy wearable devices due to their high overhead at runtime. In contrast to prior approaches, we present an energy-efficient approach, referred to as Sensor-Aware iMputation&nbsp;(SAM), to accurately impute missing data at runtime and recover application accuracy. SAM first uses unsupervised clustering to obtain clusters of similar sensor data patterns. Next, it learns inter-relationship between clusters to obtain imputation patterns for each combination of clusters using a principled sensor-aware search algorithm. Using sensor data for clustering before choosing imputation patterns ensures that the imputation is aware of sensor data observations. Experiments on seven diverse wearable sensor-based time-series datasets demonstrate that SAM is able to maintain accuracy within 5\% of the baseline with no missing data when one sensor is missing. We also compare SAM against generative adversarial imputation networks&nbsp;(GAIN), transformers, and k-nearest neighbor methods. Results show that SAM outperforms all three approaches on average by more than&nbsp;25\% when two sensors are missing with negligible overhead compared to the baseline.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = nov,
articleno = {2},
numpages = {27},
keywords = {Human activity recognition, wearable electronics, missing data detection, data imputation, clustering, health monitoring}
}

@article{10.1145/3698799,
author = {Zheng, Liang and Xiao, Qingjun and Cai, Xuyuan},
title = {A Universal Sketch for Estimating Heavy Hitters and Per-Element Frequency Moments in Data Streams with Bounded Deletions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698799},
doi = {10.1145/3698799},
abstract = {In the field of data stream processing, there are two prevalent models, i.e., insertion-only, and turnstile models. Most previous works were proposed for the insertion-only model, which assumes new elements arrive continuously as a stream, and neglects the possibilities of removing existing elements. In this paper, we make a bounded deletion assumption, putting a constraint on the number of deletions allowed. For such a turnstile stream, we focus on a new problem of universal measurement that estimates multiple kinds of statistical metrics simultaneously using limited memory and in an online fashion, including per-element frequency, heavy hitters, frequency moments, and frequency distribution. There are two key challenges for processing a turnstile stream with bounded deletions. Firstly, most previous methods for detecting heavy hitters cannot ensure a bounded detection error when there are deletion events. Secondly, there is still no prior work to estimate the per-element frequency moments under turnstile model, especially in an online fashion. In this paper, we address the former challenge by proposing a Removable Augmented Sketch, and address the latter by a Removable Universal Sketch, enhanced with an Online Moment Estimator. In addition, we improve the accuracy of frequency estimation by a compressed counter design, which can halve the memory cost of a frequency counter and support addition/minus operations. Our experiments show that our solution outperforms other algorithms by 16\%~69\% in F1 Score of heavy hitter detection, and improves the throughput of frequency moment estimation by 3.0x104 times.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {224},
numpages = {28},
keywords = {data streams, sketch, turnstile model, universal measurement}
}

@article{10.1145/3698811,
author = {Yan, Mengyi and Wang, Yaoshu and Wang, Yue and Miao, Xiaoye and Li, Jianxin},
title = {GIDCL: A Graph-Enhanced Interpretable Data Cleaning Framework with Large Language Models},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698811},
doi = {10.1145/3698811},
abstract = {Data quality is critical across many applications. The utility of data is undermined by various errors, making rigorous data cleaning a necessity. Traditional data cleaning systems depend heavily on predefined rules and constraints, which necessitate significant domain knowledge and manual effort. Moreover, while configuration-free approaches and deep learning methods have been explored, they struggle with complex error patterns, lacking interpretability, requiring extensive feature engineering or labeled data. This paper introduces GIDCL (Graph-enhanced Interpretable Data Cleaning with Large language models), a pioneering framework that harnesses the capabilities of Large Language Models (LLMs) alongside Graph Neural Network (GNN) to address the challenges of traditional and machine learning-based data cleaning methods. By converting relational tables into graph structures, GIDCL utilizes GNN to effectively capture and leverage structural correlations among data, enhancing the model's ability to understand and rectify complex dependencies and errors. The framework's creator-critic workflow innovatively employs LLMs to automatically generate interpretable data cleaning rules and tailor feature engineering with minimal labeled data. This process includes the iterative refinement of error detection and correction models through few-shot learning, significantly reducing the need for extensive manual configuration. GIDCL not only improves the precision and efficiency of data cleaning but also enhances its interpretability, making it accessible and practical for non-expert users. Our extensive experiments demonstrate that GIDCL significantly outperforms existing methods, improving F1-scores by 10\% on average while requiring only 20 labeled tuples.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {236},
numpages = {29},
keywords = {data quality, graph neural network, interpretable, large language models}
}

@article{10.1145/3698812,
author = {Boeschen, Nils and Ziegler, Tobias and Binnig, Carsten},
title = {GOLAP: A GPU-in-Data-Path Architecture for High-Speed OLAP},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {6},
url = {https://doi.org/10.1145/3698812},
doi = {10.1145/3698812},
abstract = {In this paper, we suggest a novel GPU-in-data-path architecture that leverages a GPU to accelerate the I/O path and thus can achieve almost in-memory bandwidth using SSDs. In this architecture, the main idea is to stream data in heavy-weight compressed blocks from SSDs directly into the GPU and decompress it on-the-fly as part of the table scan to inflate data before processing it by downstream query operators. Furthermore, we employ novel GPU-optimized pruning techniques that help us further inflate the perceived read bandwidth. In our evaluation, we show that the GPU-in-data-path architecture can achieve an effective bandwidth of up to 100 GiB/s, surpassing existing in-memory systems' capabilities.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {237},
numpages = {26},
keywords = {dbms gpu-acceleration, olap, ssd storage}
}

@article{10.1145/3699515,
author = {Taha, Kamal},
title = {Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3699515},
doi = {10.1145/3699515},
abstract = {This survey paper presents a comprehensive analysis of crime prediction methodologies, exploring the various techniques and technologies utilized in this area. The paper covers the statistical methods, machine learning algorithms, and deep learning techniques employed to analyze crime data, while also examining their effectiveness and limitations. We propose a methodological taxonomy that classifies crime prediction algorithms into specific techniques. This taxonomy is structured into four tiers, including methodology category, methodology sub-category, methodology techniques, and methodology sub-techniques. Empirical and experimental evaluations are provided to rank the different techniques. The empirical evaluation assesses the crime prediction techniques based on three criteria, while the experimental evaluation ranks the algorithms that employ the same sub-technique, the different sub-techniques that employ the same technique, the different techniques that employ the same methodology sub-category, the different methodology sub-categories within the same category, and the different methodology categories. The combination of methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of crime prediction algorithms, aiding researchers in making informed decisions. Finally, the paper provides a glimpse into the future of crime prediction techniques, highlighting potential advancements and opportunities for further research in this field.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
keywords = {Crime Prediction, Classification, Clustering, Regression, Social Networks, Machine Learning, Deep Learning}
}

@article{10.1145/3699730,
author = {Lyu, Feng and Zhang, Jie and Lu, Huali and Wu, Huaqing and Wu, Fan and Zhang, Yongmin and Zhang, Yaoxue},
title = {SynthCAT: Synthesizing Cellular Association Traces with Fusion of Model-Based and Data-Driven Approaches},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699730},
doi = {10.1145/3699730},
abstract = {The scarcity of publicly available cellular association traces hinders user location-based research and various data-driven services, highlighting the importance of data synthesis in this field. In this paper, we investigate the cellular association trace synthesis (CATS) problem, aiming to generate diverse and realistic cellular association traces based on road segment-based trajectories and corresponding departure times. To substantiate our research, we first gather substantial data, including road segment-based trajectories, base station (BS) distribution, and ground truths of cellular association traces. We then perform systematic data analysis to reveal technical challenges such as disparity in geographic spaces, complex and dynamic BS handover, and poor performance of single-dimension approaches. To address these challenges, we propose SynthCAT, a novel scheme that fuses model-based and data-driven approaches. Specifically, SynthCAT includes: i) A model-based coarse-grained cellular association trace generation component, encompassing GPS reference generation, weighted historical average time generation, Bayesian decision, and time mapping modules. This component establishes a unified GPS space to map road and BS spaces, generates initial time information, synthesizes coarse-grained spatial cellular association traces by following explicit BS handover rules, and maps the corresponding arrival time for each trace point; ii) A fine-grained cellular association trace generation component, which combines model-based and data-driven approaches. This employs a two-stage Autoencoder Generative Adversarial Network (AEGAN) to refine cellular association traces based on the coarse-grained ones. Extensive field experiments validate the efficacy of SynthCAT in terms of trace similarity to ground truths and its efficiency in supporting practical downstream applications.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {161},
numpages = {24},
keywords = {Autoencoder Generative Adversarial Network, Downstream applications support, Model-based, Synthesizing cellular association traces, data-driven fusion}
}

@article{10.1145/3699765,
author = {Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu},
title = {DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
url = {https://doi.org/10.1145/3699765},
doi = {10.1145/3699765},
abstract = {Large language models (LLMs) have the potential to transform digital healthcare, as evidenced by recent advances in LLM-based virtual doctors. However, current approaches rely on patient's subjective descriptions of symptoms, causing increased misdiagnosis. Recognizing the value of daily data from smart devices, we introduce a novel LLM-based multi-turn consultation virtual doctor system, DrHouse, which incorporates three significant contributions: 1) It utilizes sensor data from smart devices in the diagnosis process, enhancing accuracy and reliability. 2) DrHouse leverages continuously updating medical knowledge bases to ensure its model remains at diagnostic standard's forefront. 3) DrHouse introduces a novel diagnostic algorithm that concurrently evaluates potential diseases and their likelihood, facilitating more nuanced and informed medical assessments. Through multi-turn interactions, DrHouse determines the next steps, such as accessing daily data from smart devices or requesting in-lab tests, and progressively refines its diagnoses. Evaluations on three public datasets and our self-collected datasets show that DrHouse can achieve up to an 31.5\% increase in diagnosis accuracy over the state-of-the-art baselines. The results of a 32-participant user study show that 75\% medical experts and 91.7\% test subjects are willing to use DrHouse.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = nov,
articleno = {153},
numpages = {29},
keywords = {Diagnostic Reasoning Systems, Internet of Things, Knowledge Retrieval, LLMs, Proactive Conversational Systems, Sensor Data, Up-to-Date}
}

@article{10.1145/3700791,
author = {Bodaghi, Arezo and Fung, Benjamin C. M. and A. Schmitt, Ketra},
title = {AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1559-1131},
url = {https://doi.org/10.1145/3700791},
doi = {10.1145/3700791},
abstract = {Addressing the challenge of toxic language in online discussions is crucial for the development of effective toxicity detection models. This pioneering work focuses on addressing imbalanced datasets in toxicity detection by introducing a novel approach to augment toxic language data. We create a balanced dataset by instructing fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF). Recognizing the challenges in collecting sufficient toxic samples from social media platforms for building a balanced dataset, our methodology involves sentence-level text data augmentation through paraphrasing existing samples using optimized generative LLMs. Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to fine-tune the model further and align it with human feedback. In other words, we start by fine-tuning a LLM using an instruction dataset, specifically tailored for the task of paraphrasing while maintaining semantic consistency. Next, we apply PPO and a reward function, to further fine-tune (optimize) the instruction-tuned LLM. This RL process guides the model in generating toxic responses. We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly. This approach guides LLMs through PPO and the reward function, transforming minority class samples into augmented versions. The primary goal of our methodology is to create a balanced and diverse dataset to enhance the accuracy and performance of classifiers in identifying instances from the minority class. Utilizing two publicly available toxic datasets, we compared various techniques with our proposed method for generating toxic samples, demonstrating that our approach outperforms all others in producing a higher number of toxic samples. Starting with an initial 16,225 toxic prompts, our method successfully generated 122,951 toxic samples with a toxicity score exceeding 30\%. Subsequently, we developed various classifiers using the generated balanced datasets and applied a cost-sensitive learning approach to the original imbalanced dataset. The findings highlight the superior performance of classifiers trained on data generated using our proposed method. These results highlight the importance of employing RL and a data-agnostic model as a reward mechanism for augmenting toxic data, thereby enhancing the robustness of toxicity detection models.},
note = {Just Accepted},
journal = {ACM Trans. Web},
month = oct,
keywords = {Text Data Augmentation, Imbalanced Toxic Datasets, Large Language Models, Reinforcement Learning}
}

@article{10.1145/3700878,
author = {Xiang, Linhua and Wang, Zengfu},
title = {Joint Mixing Data Augmentation for Skeleton-based Action Recognition},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3700878},
doi = {10.1145/3700878},
abstract = {Skeleton-based action recognition is beneficial for understanding human behavior in videos, and thus has received much attention in recent years as an important research area in action recognition. Current research focuses on designing more advanced algorithms to better extract spatio-temporal information from skeleton data. However, due to the small amount of data in the existing skeleton dataset and the lack of effective data augmentation methods, it is easy to lead to overfitting in model training. To address this challenge, we propose a mix-based data augmentation method, Joint Mixing Data Augmentation (JMDA), which can generally improve the effectiveness and robustness of various skeleton-based action recognition algorithms. In terms of spatial information, we introduce SpatialMix (SM), a method that projects the original 3D skeleton discrete information into a 2D space. Then, SM mixes the projected spatial information between two random samples during the training process to achieve the spatial-based mixing data augmentation. Concerning temporal information, we propose TemporalMix (TM). Leveraging the temporal continuity in skeleton data, we perform a temporal resize operation on the original skeleton data, and then merge two random samples during training to achieve the temporal-based mixed data augmentation. Additionally, we analyze the Feature Mismatch (FM) problem caused by introducing mix-based data augmentation into skeleton data. Then we propose a new data preprocessing method called Feature Alignment (FA) to effectively address this problem and improve model performance. Moreover, we propose a novel training pipeline, Joint Training Strategy (JTS), which combines multiple mix-based data augmentation methods for further improvement of model performance. Specifically, our proposed JMDA is plug-and-play and widely applicable to skeleton-based action recognition models. At the same time, the application of JMDA does not increase the model parameters and there is almost no additional training cost. We conduct extensive experiments on NTU RGB+D 60 and NTU RGB+D 120 datasets to demonstrate the effectiveness and robustness of the proposed JMDA on several mainstream skeleton-based action recognition algorithms.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
keywords = {skeleton-based action recognition, data augmentation, graph convolutional networks}
}

@article{10.1145/3700886,
author = {Zhang, Xian-Rong and Gong, Yue-Jiao and Cao, Zhiguang and Zhang, Jun},
title = {Island-Based Evolutionary Computation with Diverse Surrogates and Adaptive Knowledge Transfer for High-Dimensional Data-Driven Optimization},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700886},
doi = {10.1145/3700886},
abstract = {In recent years, there has been a growing interest in data-driven evolutionary algorithms (DDEAs) employing surrogate models to approximate the objective functions with limited data. However, current DDEAs are primarily designed for lower-dimensional problems and their performance drops significantly when applied to large-scale optimization problems (LSOPs). To address the challenge, this paper proposes an offline DDEA named DSKT-DDEA. DSKT-DDEA leverages multiple islands that utilize different data to establish diverse surrogate models, fostering diverse subpopulations and mitigating the risk of premature convergence. In the intra-island optimization phase, a semi-supervised learning method is devised to fine-tune the surrogates. It not only facilitates data argumentation, but also incorporates the distribution information gathered during the search process to align the surrogates with the evolving local landscapes. Then, in the inter-island knowledge transfer phase, the algorithm incorporates an adaptive strategy that periodically transfers individual information and evaluates the transfer effectiveness in the new environment, facilitating global optimization efficacy. Experimental results demonstrate that our algorithm is competitive with state-of-the-art DDEAs on problems with up to 1000 dimensions, while also exhibiting decent parallelism and scalability. Our DSKT-DDEA is open-source and accessible at: https://github.com/LabGong/DSKT-DDEA.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = nov,
keywords = {Data-driven evolutionary algorithm, large-scale optimization problems, diverse surrogate models, semi-supervised learning, adaptive knowledge transfer}
}

@article{10.1145/3702001,
author = {Yan, Jianrong and Jiang, Wenbin and He, Dongao and Wen, Suyang and Li, Yang and Jin, Hai and Shao, Zhiyuan},
title = {RT-GNN: Accelerating Sparse Graph Neural Networks by Tensor-CUDA Kernel Fusion},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3702001},
doi = {10.1145/3702001},
abstract = {Graph Neural Networks (GNNs) have achieved remarkable successes in various graph-based learning tasks, thanks to their ability to leverage advanced GPUs. However, GNNs currently face challenges arising from the concurrent use of advanced Tensor Cores (TCs) and CUDA Cores (CDs) in GPUs. These challenges are further exacerbated due to repeated, inefficient, and redundant aggregations in GNN that result from the high sparsity and irregular non-zero distribution of real-world graphs. We propose RT-GNN, a GNN framework based on the fusion of advanced TC and CD units, to eliminate the aforementioned redundancies by exploiting the properties of an adjacency matrix. First, a novel GNN representation technique, hierarchical embedding graph (HEG) is proposed to manage the intermediate aggregation results hierarchically, which can further avoid redundancy in intermediate aggregations elegantly. Next, to address the inherent sparsity of graphs, RT-GNN places the blocks (a.k.a tiles) in HEG onto TCs and CDs according to their sparsity by a new block-based row-wise multiplication approach, which assembles TCs and CDs to work concurrently. Experimental results demonstrate that HEG outperforms HAG by an average speedup of 19.3 \texttimes{} for redundancy elimination performance, especially up to 72 \texttimes{} speedup on the dataset of ARXIV. Moreover, for overall performance, RT-GNN outperforms state-of-the-art GNN frameworks (including DGL, HAG, GNNAdvisor, and TC-GNN) by an average factor of 3.1 \texttimes{} while maintaining or even improving the task accuracy.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
keywords = {Graph neural network, Redundancy elimination, GPU Tensor Core, Kernel fusion}
}

@article{10.1145/3702322,
author = {Giaccardi, Elisa and Murray-Rust, Dave and Redstr\"{o}m, Johan and Caramiaux, Baptiste},
title = {Prototyping with Uncertainties: Data, Algorithms, and Research through Design},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3702322},
doi = {10.1145/3702322},
abstract = {Seen both as a resource and an obstacle to clarity, uncertainty is a concept that permeates many areas of design. As the concept gains prominence in Human-Computer Interaction (HCI), this special issue specifically explores the interplay between uncertainty and prototyping in Research through Design (RtD). We first outline three histories of uncertainty in design, in relation to its philosophical significance, its role in statistical and algorithmic processes, and its importance in prototyping. The convergence of these aspects is crucial as design evolves toward more agentive and entangled systems, introducing challenges such as Design as a Probabilistic Outcome. We then investigate the design spaces for engaging with “being uncertain” that emerge from the papers: from nuancing the relationship between designers and quantitative data to blurring the line between humans, fungi, and algorithms. Finally, we illuminate some preliminary threads for how RtD can navigate and engage with these shifting technological and design landscapes thoughtfully.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {68},
numpages = {21},
keywords = {Algorithms, Data, Prototyping, Research through Design, Uncertaintities}
}

@article{10.1145/3702645,
author = {Jaysawal, Bijay Prasad and Huang, Jen-Wei},
title = {SOHUPDS+: An Efficient One-phase Algorithm for Mining High Utility Patterns over a Data Stream},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3702645},
doi = {10.1145/3702645},
abstract = {Existing algorithms for mining high utility patterns over a data stream are two-phase algorithms that are not scalable due to the large number of candidates generation in the first phase, particularly when the minimum utility threshold is low. Moreover, in the second phase, the algorithm needs to scan the database again to find out actual utility for candidates. In this article, we propose one-phase algorithm SOHUPDS (+)  to mine high utility itemsets in the current sliding window of the data stream with respect to absolute or relative minimum utility threshold. To facilitate SOHUPDS (+) , we propose a data structure IUDataListSW (+) , which stores and maintains utility and upper-bound values of the items in the current sliding window when sliding window advances. In addition, we propose a transaction merging strategy, called BitmapTransactionMerging, which saves execution time for utility and upper-bound values computations in denser datasets. Moreover, we propose update strategies to utilize mined high utility patterns from the previous sliding window to update high utility patterns in the current sliding window. The results of experiments illustrate that SOHUPDS (+)  is more efficient than the state-of-the-art algorithms in terms of execution time as well as memory usage in most of the experiments on various datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {16},
numpages = {32},
keywords = {High utility pattern mining, Data stream mining, Data mining, High utility itemsets}
}

@article{10.1145/3702649,
author = {Zhang, Yihong and Hara, Takahiro},
title = {Extracting Political Interest Model from Interaction Data Based on Novel Word-level Bias Assignment},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3702649},
doi = {10.1145/3702649},
abstract = {In democratic countries, political interest is deeply involved in people’s daily lives. Research in political consumerism shows that product purchase decision is also influenced by the political orientation of the consumer. In traditional recommendation system design, user interest in an item is provided by a unified model. Recently, interest disentanglement methods have been introduced. It is shown that by disentangling interest factors such as conformity and private interest, recommendation performance can be significantly improved. However, few studies attempt to disentangle political interest in purchase behavior, which is bipolar. In this article, we propose a method to extract political interest model from e-commerce interaction data, which is supported by a novel word-level political bias assignment. For the bias assignment part, we improved a political bias distilling method. For the political interest model extraction part, we extend a one-side bias method to make it support bipolar bias. We compare our method with state-of-the-art baseline methods in several evaluation settings, and the experimental results show that our method can achieve superior performance. Further investigation shows that our method is consistent with theories of political consumerism.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {18},
numpages = {21},
keywords = {political interest model, word bias, debiased recommendation}
}

@article{10.1145/3702985,
author = {Belgacem, Hichem and Li, Xiaochen and Bianculli, Domenico and Briand, Lionel},
title = {A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms - RCR Report},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702985},
doi = {10.1145/3702985},
abstract = {This paper represents the Replicated Computational Results (RCR) related to our TOSEM paper “A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms”, where we proposed LAFF, an approach to automatically suggest possible values of categorical fields in data entry forms, which is a common user interface feature in many software systems. In this RCR report, we provide details about our replication package. We make available the different scripts needed to fully replicate the results obtained in our paper.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
keywords = {Form filling, Data entry forms, Machine learning, Software data quality, User interfaces}
}

@article{10.1145/3703157,
author = {Adhikari, Saugat and Yan, Da and Jiang, Zhe and Han, Jiao and Xu, Zelin and Zhang, Yupu and Sainju, Arpan and Zhou, Yang},
title = {Scaling Terrain-Aware Spatial Machine Learning for Flood Mapping on Large Scale Earth Imagery Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2374-0353},
url = {https://doi.org/10.1145/3703157},
doi = {10.1145/3703157},
abstract = {The accurate and prompt mapping of flood-affected regions is important for effective disaster management, including damage assessment and relief efforts. While high-resolution optical imagery from satellites during disasters presents an opportunity for automated flood inundation mapping, existing segmentation models face challenges due to noises like cloud cover and tree canopies. Thanks to the digital elevation model (DEM) data readily available from sources such as United States Geological Survey (USGS), terrain guidance was utilized by recent graphical models such as hidden Markov trees (HMTs) to improve segmentation quality. Unfortunately, these methods either can only handle a small area where water levels at different locations are assumed to be consistent, or require restricted assumptions such as there is only one river channel. This paper presents an algorithm for flood extent mapping on large-scale Earth imagery, applicable to a large geographic area with multiple river channels. Since water level can vary a lot from upstream to downstream, we propose to detect river pixels in order to partition the remaining pixels into localized zones, each with a unique water level. In each zone, water at all locations flow to the same river entry point. Pixels in each zone are organized by an HMT to capture water flow directions guided by elevations. Moreover, a novel regularization scheme is designed to enforce inter-zone consistency by penalizing pixel-pairs of adjacent zones that violate terrain guidance. Efficient parallelization is made possible by coloring the zone adjacency graph to identify zones and zone-pairs that have no dependency and hence can be processed in parallel, and incremental one-pass terrain-guided scanning is conducted wherever applicable to reuse computations. Experiments demonstrate that our solution is more accurate than existing solutions, and can efficiently and accurately map out flooding pixels in a giant area of size 24,805 \texttimes{} 40,129. Despite the imbalanced workloads caused by a few large zonal HMTs dominating the serial computing time, our parallelization approach is effective and manages to achieve up to 14.3 \texttimes{} speedup on a machine with Intel Xeon Gold 6126 CPU @ 2.60GHz (24 cores, 48 threads) using 32 threads.},
note = {Just Accepted},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = nov,
keywords = {Flood Extent Mapping, Digital Elevation Model, Parallelization}
}

@article{10.1145/3703626,
author = {Wang, Tao and Zhang, Yushu and Qi, Shuren and Zhao, Ruoyu and Xia, Zhihua and Weng, Jian},
title = {Security and Privacy on Generative Data in AIGC: A Survey},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3703626},
doi = {10.1145/3703626},
abstract = {The advent of artificial intelligence-generated content (AIGC) represents a pivotal moment in the evolution of information technology. With AIGC, it can be effortless to generate high-quality data that is challenging for the public to distinguish. Nevertheless, the proliferation of generative data across cyberspace brings security and privacy issues, including privacy leakages of individuals and media forgery for fraudulent purposes. Consequently, both academia and industry begin to emphasize the trustworthiness of generative data, successively providing a series of countermeasures for security and privacy. In this survey, we systematically review the security and privacy on generative data in AIGC, particularly for the first time analyzing them from the perspective of information security properties. Specifically, we reveal the successful experiences of state-of-the-art countermeasures in terms of the foundational properties of privacy, controllability, authenticity, and compliance, respectively. Finally, we show some representative benchmarks, present a statistical analysis, and summarize the potential exploration directions from each of these properties.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {82},
numpages = {34},
keywords = {Information security, AIGC, generative data, privacy, controllability, authenticity, compliance}
}

@article{10.1145/3703913,
author = {Zhou, Yuxuan and Mingyang, Li and Jingze, Tong and Linlin, Li and Zhiwei, Yang},
title = {SD-Meta: The Software-Defined Network of Human-Centric Metaverse for Multi-Lead or Multi-Media Data in Spread Spectrum Communications},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3703913},
doi = {10.1145/3703913},
abstract = {In this paper, the novel adjacent two-end link or network multi-end link concept of software-defined network is presented to support the data transmission of electroencephalogram, audio and video streaming in spread spectrum communications. Instead of solving the problem of software-defined prioritization scheme in networking calculation and communication, the southbound-northbound structure of existing neural or information network is found in control and perceptual interactions. The proposed framework of multi-lead and multi-media structures allow the human-centric Metaverse to execute different operations for local and global planning schemes with the high-speed lead or media data transmission of spread spectrum streaming, depending on the kind of brain-computer interaction, and similar to the system of multi-modal perception in the routing or switching. Firstly, this research designed the filed programmable gate array for conventional routing nodes in the computation of different neural networks. Secondly, this study demonstrates its enhanced applicability with software core for multiple routers in the computing power network of the Internet of brains in different brain-computer smart terminals. Experiment results also show the benefits of proposed model and structure, and exposing the different running indicators in the simulation.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
keywords = {Software-defined network, human-centric Metaverse, multi-lead and multi-media, Internet of brains}
}

@article{10.1145/3704810,
author = {Betti Sorbelli, Francesco and Ghobadi, Sajjad and Pinotti, Cristina M.},
title = {Single- and Multi-Depot Optimization for UAV-Based IoT Data Collection in Neighborhoods},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3704810},
doi = {10.1145/3704810},
abstract = {In this paper, we investigate the problem of deploying the minimum number of Unmanned Aerial Vehicles (UAVs) and determining their flying tours to collect data from all Internet of Things (IoT) sensors. We study this problem in a scenario with neighborhoods where a UAV can collect data from an IoT sensor if the distance between them is less than the wireless communication range of the IoT sensor. Since UAVs are powered by batteries with a limited amount of energy, we assume that the total energy consumed during the flying tour of each UAV is bounded by a given budget. We present the Minimum rooted drone Deployment Problem with Neighborhoods (MDPN), which is NP − hard, and propose two approximation algorithms for the single-depot case, where one of them is a bi-criteria approximation algorithm that returns a solution whose tour’s cost is violated by a factor of 1 + ϵ. Furthermore, we extend these two algorithms to the multi-depot scenario. Finally, we evaluate our algorithms in three different scenarios: the ideal one where the communication range is a circle and the data transfer rate is constant, and two more realistic scenarios where we introduce some degree of irregularity in the communication range and a non-constant rate in data transfer.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = nov,
keywords = {Unmanned aerial vehicles; Wireless sensor network; Approximation algorithms}
}

@article{10.1145/3705313,
author = {Dang, Xiaochao and Ding, GuoZhen and Dong, Xiaohui and Li, Fengfang and Gao, Shiwei and Wang, Yue},
title = {UIE-Based Relational Extraction Task for Mine Hoist Fault Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3705313},
doi = {10.1145/3705313},
abstract = {Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59\% to 92.51\%.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
keywords = {joint extraction, mechanical problem, mining sector, prompt learning}
}

@article{10.1145/3705609,
author = {Lim, Donghyun and Ziegler, Martin},
title = {Quantitative Coding and Complexity Theory of Continuous Data: Part I: Motivation, Definition, Consequences},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0004-5411},
url = {https://doi.org/10.1145/3705609},
doi = {10.1145/3705609},
abstract = {When encoding real numbers as (necessarily infinite) bit-strings, the na\"{\i}ve binary/decimal expansion is well-known [doi:10.1112/plms/s2-43.6.544]
computably ‘unreasonable’, rendering for example tripling qualitatively discontinuous on Cantor’s sequence space. Encoding reals as sequences of (finite integer numerators and denominators, in binary, of) rational approximations does make common operations qualitatively computable, yet admits no bounds on their computational complexity/quantitative continuity. Dyadic approximations on the other hand are known polynomially, and signed binary expansions even linearly, ‘reasonable’ in a rigorous sense recalled in the introduction of this work. But how distinguish between un/suitable encodings of spaces common in Calculus beyond the reals, such as Banach or Sobolev? With respect to qualitative computability/continuity on topological spaces, the technical condition of admissibility had been identified [doi:10.1016/0304-3975(85)90208-7]
for an encoding over Cantor space (historically called a representation) to be ‘reasonable’ [doi:10.1007/978-3-030-59234-9_9]. Roughly speaking, admissibility requires the representation to be (i) continuous, and to be (ii) maximal with respect to continuous reduction. Admissible representations exist for a large class of spaces. And for (precisely) these does the Kreitz-Weihrauch—sometimes aka Main—Theorem of Computable Analysis hold, which characterizes continuity of functions by continuity of mappings translating codes, so-called realizers. We refine qualitative computability/continuity on topological spaces to quantitative continuity/complexity on metric spaces: by proposing a notion, and investigating the properties, of polynomially/linearly admissible representations. Roughly speaking, these are (i) close to ‘optimally’ continuous, namely linearly/polynomially relative to the space’s entropy; and they are (ii) maximal with respect to relative linear/polynomial quantitatively continuous reductions defined in the main text. Quantitatively admissible representations are closed under composition over generalized ground spaces beyond Cantor’s. Such representations exhibit a quantitative strengthening of the qualitative Main Theorem, namely now characterizing quantitative continuity of functions by quantitative continuity of realizers. A large class of compact metric spaces is shown to admit polynomially admissible representations over compact ultrametric spaces, and some even a generalization of the linearly admissible signed binary encoding. Quantitative admissibility thus provides the desired criterion for complexity-theoretically ‘reasonable’ encodings.},
note = {Just Accepted},
journal = {J. ACM},
month = nov,
keywords = {Computational Complexity of Continuous Data, Generalized Representation, Ultrametric Compact Space}
}

@article{10.1145/3705863,
author = {Hamed, Naeima and Rana, Omer and Orozco-terWengel, Pablo and Goossens, Beno\^{\i}t and Perera, Charith},
title = {A Comparison of Open Data Observatories},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3705863},
doi = {10.1145/3705863},
abstract = {Open Data Observatories refer to online platforms that provide real-time and historical data for a particular application context, e.g., urban/non-urban environments or a specific application domain. They are generally developed to facilitate collaboration within one or more communities through reusable datasets, analysis tools, and interactive visualizations. Open Data Observatories collect and integrate various data from multiple disparate data sources—some providing mechanisms to support real-time data capture and ingest. Data types can include sensor data (soil, weather, traffic, pollution levels) and satellite imagery. Data sources can include Open Data providers, interconnected devices, and services offered through the Internet of Things. The continually increasing volume and variety of such data require timely integration, management, and analysis, yet presented in a way that end-users can easily understand. Data released for open access preserve their value and enable a more in-depth understanding of real-world choices. This survey compares thirteen Open Data Observatories and their data management approaches - investigating their aims, design, and types of data. We conclude with research challenges that influence the implementation of these observatories, outlining some strengths and limitations for each one and recommending areas for improvement. Our goal is to identify best practices learned from the selected observatories to aid the development of new Open Data Observatories.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = nov,
keywords = {Urban and non-urban data observatories, FAIR Open Data principles, Data integration, Data platforms.}
}

@article{10.1145/3705896,
author = {Ribeiro Jesus, Rui Filipe and Rodrigues, Ana and Costa, Carlos},
title = {Unlocking AutoML: Enhancing Data with Deep Learning Algorithms for Medical Imaging},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3705896},
doi = {10.1145/3705896},
abstract = {Deep learning algorithms have become increasingly popular over the years, having proved their efficiency in input-output functions for distinct types of data. This technology is particularly useful in medical imaging, where complex image structures often generate disagreements between medical staff. These technologies can streamline the diagnostic process by performing automatic image analysis, which results in more accurate and reproducible diagnoses. Additionally, these technologies can enhance content retrieval systems by automatically labeling the images based on the structures they possess. Despite the benefits, the mathematical complexity of deep learning algorithms and their training optimizations can be challenging. Automated machine learning provides a solution to this challenge by offering tools that automate the development and training of these algorithms. This makes it possible for users with limited programming experience to take advantage of these powerful technologies to quickly develop and prototype analysis algorithms for their specific needs. This article presents a management platform for deep learning services on the cloud that provides a code-free experience through automated machine learning. The evaluation was done in one of the most demanding scenarios, where the service was integrated into a research pathology PACS to annotate mitotic cells in breast cancer tissue automatically. The annotations are processed by an open-source PACS archive and stored directly on the files, enhancing the image metadata and consequently content retrieval systems. The results of the developed algorithms were compared to the state-of-the-art to evaluate the competitiveness of the solution.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {24},
numpages = {17},
keywords = {Machine learning, deep learning, automated machine learning, automated data annotation, medical imaging, digital pathology}
}

@article{10.1145/3705897,
author = {Begoli, Edmon and Mahbub, Maria and Passarella, Linsey and Srinivasan, Sudarshan},
title = {A Compound Data Poisoning Technique with Significant Adversarial Effects on Transformer-based Sentiment Classification Tasks},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3705897},
doi = {10.1145/3705897},
abstract = {Transformer-based models have demonstrated much success in various natural language processing tasks. However, they are often vulnerable to adversarial attacks, such as data poisoning, which can intentionally fool the model into generating incorrect results. In this article, we present a novel, compound variant of a data poisoning attack on a transformer-based model that maximizes the poisoning effect while minimizing the scope of poisoning. We do so by combining the established data poisoning technique (label flipping) with a novel adversarial artifact selection and insertion technique aimed at minimizing detectability and the scope of the poisoning footprint. We find that by using a combination of these two techniques, we achieve a state-of-the-art attack success rate of approximately 90\% while poisoning only 0.5\% of the original training set, thus minimizing the scope and detectability of the poisoning action. These findings have the potential to advance the development of better data poisoning detection methods.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {22},
numpages = {15},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@article{10.1145/3706062,
author = {Sui, Yongduo and Wang, Shuyao and Sun, Jie and Liu, Zhiyuan and Cui, Qing and Li, Longfei and Zhou, Jun and Wang, Xiang and He, Xiangnan},
title = {A Simple Data Augmentation for Graph Classification: A Perspective of Equivariance and Invariance},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3706062},
doi = {10.1145/3706062},
abstract = {In graph classification, the out-of-distribution (OOD) issue is attracting great attention. To address this issue, a prevailing idea is to learn stable features, on the assumption that they are substructures causally determining the label and that their relationship with the label is stable to the distributional uncertainty. In contrast, the complementary parts termed environmental features, fail to determine the label solely and hold varying relationships with the label, thus ascribed to the possible reason for the distribution shift. Existing generalization efforts mainly encourage the model's insensitivity to environmental features. While the sensitivity to stable features is promising to distinguish the crucial clues from the distributional uncertainty but largely unexplored. A paradigm of simultaneously exploring the sensitivity to stable features and insensitivity to environmental features is until-now lacking to achieve the generalizable graph classification, to the best of our knowledge. In this work, we conjecture that generalizable models should be sensitive to stable features and insensitive to environmental features. To this end, we propose a simple yet effective augmentation strategy for graph classification: Equivariant and Invariant Cross-Data Augmentation (EI-CDA). By employing equivariance, given a pair of input graphs, we first estimate their stable and environmental features via masks. Then we linearly mix the estimated stable features of two graphs and encourage the model predictions faithfully reflect their mixed semantics. Meanwhile, by using invariance, we swap the estimated environmental features of two graphs and keep the predictions invariant. This simple yet effective strategy endows the models with both sensitivity to stable features and insensitivity to environmental features. Extensive experiments show that EI-CDA significantly improves performance and outperforms leading baselines. Our codes are available at: https://github.com/yongduosui/EI-GNN.},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
keywords = {Graph Data Augmentation, Graph Classification, Distribution Shift}
}

@article{10.1145/3706063,
author = {Carvalho, Jason and Daga, Enrico and Mulholland, Paul and Asprino, Luigi and Uwasomba, Chukwudi and Daquino, Marilena and Gangemi, Aldo and Maguire, Mark and Stoneman, Adam},
title = {Integrating Citizen Experiences in Cultural Heritage Archives with a Linked Non-Open Data Hub},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3706063},
doi = {10.1145/3706063},
abstract = {This article explores and presents innovative methods and technologies for supporting citizen curation of cultural heritage. Relevant outcomes of the Social Participation, Cohesion and Inclusion through Cultural Engagement (SPICE) project are presented, focusing on enhancing the state of content management and delivery strategies in museums and memory institutions. We argue that citizen curation requires a principled way of managing and integrating citizen responses, contributions and dataflows in the domain of cultural heritage, raising challenges and opportunities such as integration of distributed and diverse data sources, authoritativeness, interdependence, privacy, data reuse and rights management. The solution is a Linked Data Hub (LDH), which integrates museum collections and user-generated content and repurposes them to end-user systems tailored to specific use cases for citizens and museum practitioners. Such LDH must be non-open, by offering an approach that gives citizens and organisations, such as museums or engagement agencies, meaningful control over their data by implementing user-tailored policies and negotiated access and terms of use. Additionally, our solution addresses privacy violations in user-contributed content by offering a near-real-time content monitoring framework. We present the LDH discussing pilot applications within the EU-funded project SPICE, including ‘Deep Viewpoints’, which currently supports the Irish Museum of Modern Art (IMMA) in citizen curation activities. Overall, this article serves as a critical milestone in closing functional gaps and advancing the state of technology in managing citizen responses and contributions in the cultural heritage domain.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {71},
numpages = {39},
keywords = {Cultural Heritage, Linked Data, Citizen Curation, Data Management, Data Integration, RDF, SPARQL}
}

@article{10.1145/3707457,
author = {Vitale, Antonio and Oliveto, Rocco and Scalabrino, Simone},
title = {A Catalog of Data Smells for Coding Tasks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707457},
doi = {10.1145/3707457},
abstract = {Large Language Models (LLMs) are increasingly becoming fundamental in supporting software developers in coding tasks. The massive datasets used for training LLMs are often collected automatically, leading to the introduction of data smells. Previous work addressed this issue by using quality filters to handle some specific smells. Still, the literature lacks a systematic catalog of the data smells for coding tasks currently known. This paper presents a Systematic Literature Review (SLR) focused on articles that introduce LLMs for coding tasks. We first extracted the quality filters adopted for training and testing such LLMs, inferred the root problem behind their adoption (data smells for coding tasks), and defined a taxonomy of such smells. Our results highlight discrepancies in the adoption of quality filters between pre-training and fine-tuning stages and across different coding tasks, shedding light on areas for improvement in LLM-based software development support.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {LLMs for coding tasks, data smells, data quality, Systematic Literature Review}
}

@article{10.1145/3707459,
author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Lavista Ferres, Juan M. and Banerjee, Arindam},
title = {TorchGeo: Deep Learning With Geospatial Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2374-0353},
url = {https://doi.org/10.1145/3707459},
doi = {10.1145/3707459},
abstract = {Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that may have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for uncurated geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.},
note = {Just Accepted},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = dec,
keywords = {deep learning, computer vision, remote sensing, earth observation, satellite imagery, geospatial, datasets, samplers, transforms, models}
}

@article{10.1145/3707647,
author = {Candela, Gustavo},
title = {Browsing Linked Open Data in Cultural Heritage: a shareable visual configuration approach},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3707647},
doi = {10.1145/3707647},
abstract = {Over the last decade Cultural Heritage (CH) organizations have been exploring new ways to make their content available and browsable using the Semantic Web and Linked Open Data (LOD). The objective of the present study was to introduce a framework to browse LOD in CH institutions using a shareable visual configuration approach. The framework was then applied to a selection of LOD repositories. Common features and best practices were identified. A detailed analysis, that can be useful for organizations interested in publishing and explore their datasets, is provided. Open issues requiring further work are outlined.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = dec,
keywords = {Semantic Web, Linked Open Data, Data visualisation, Cultural Heritage}
}

@article{10.1145/3707693,
author = {Caroprese, Luciano and Pisani, Francesco and Veloso, Bruno Miguel and Konig, Matthias and Manco, Giuseppe and Hoos, Holger and Gama, Joao},
title = {Modelling Concept Drift in Dynamic Data Streams for Recommender Systems},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707693},
doi = {10.1145/3707693},
abstract = {Recommendation systems play a crucial role in modern e-commerce and streaming services. However, the limited availability of public datasets hampers the rapid development of more efficient and accurate recommendation algorithms within the research community. This work introduces a stream-based data generator designed to generate user preferences for a set of items while accommodating progressive changes in user preferences. The underlying principle involves using user/item embeddings to derive preferences by exploring the proximity of these embeddings. Whether randomly generated or learned from a real finite data stream, these embeddings serve as the basis for generating new preferences. We investigate how this fundamental model can adapt to shifts in user behavior over time; in our framework, changes correspond to alterations in the structure of the tripartite graph, reflecting modifications in the underlying embeddings. Through an analysis of real-life data streams, we demonstrate that the proposed model is effective in capturing actual preferences and the changes that they can exhibit over time. Thus, we characterize these changes and develop a generalized method capable of simulating realistic data, thereby generating streams with similar yet controllable drift dynamics.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = dec,
keywords = {Recommender Systems, Collaborative Filtering, Concept Drift Adaptation, Data Generation}
}

@article{10.1145/3708322,
author = {Mirpour Marzuni, Saeed and Toosi, Adel and Savadi, Abdorreza and Naghibzadeh, Mahmud and Taniar, David},
title = {Optimizing Geo-Distributed Data Processing with Resource Heterogeneity over the Internet},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1533-5399},
url = {https://doi.org/10.1145/3708322},
doi = {10.1145/3708322},
abstract = {The traditional MapReduce frameworks were originally designed for processing data within a single cluster and are not suitable for handling geo-distributed data. Consequently, alternative approaches such as Hierarchical and Geo-Hadoop have been proposed to address this limitation. However, these approaches still face challenges in efficiently managing inter-cluster data transfer, particularly considering the heterogeneity of clusters and varying bandwidth among them. Moreover, the need to transmit results to a central global reducer for geo-distributed MapReduce operations adds unnecessary complexity. To tackle these issues, we introduce Extended Cross-MapReduce (ECMR), a framework that integrates resource heterogeneity and network links in geo-distributed MapReduce workflows. ECMR optimizes data management and determines the necessary data volume for generating final results. To enhance performance, ECMR leverages the overlap between data transfer and execution time by utilizing multiple global reducers and grouping temporary results that require data transfer over the Internet. In ECMR, we propose a bipartite graph and extend the Gale-Shapley algorithm to determine the optimal number of clusters and select the most suitable locations for global reducers. Through extensive experimental evaluations conducted on a real testbed, we demonstrate the effectiveness of our proposed ECMR method. The results exhibit significant improvements over traditional Hierarchical and Geo-Hadoop approaches, achieving reductions of up to 81\% and 85\% in overall makespan, respectively.},
note = {Just Accepted},
journal = {ACM Trans. Internet Technol.},
month = dec,
keywords = {MapReduce, Geo-distributed Data, Data Centers, Big Data, Multi-cluster}
}

@article{10.1145/3708346,
author = {Chen, Yipei and Yuen, Hua and Ma, Baojun and Wang, Limin and Qian, Yu},
title = {Beyond Songs: Analyzing User Sentiment Through Music Playlists and Multimodal Data},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3708346},
doi = {10.1145/3708346},
abstract = {The automatic recognition of user sentiments through their music listening behavior is an important research task in cognitive studies. Whereas prior studies were conducted to identify the sentiment conveyed (or evoked) by a song that a user listens to at a particular time, we argue that a more effective method would be to identify the user’s induced sentiment based on the comprehensive list of songs they have listened to (e.g., the sequence of music being played). However, recognizing the sentiment information induced by a playlist using machine learning techniques is much more challenging than identifying the sentiment induced by a single song, as it is difficult to obtain accurately labeled training samples for playlists. In this study, we developed the List-Song Relationship Factorization (LSRF) model with the objective of efficiently identifying sentiments induced by playlists. This model employs two side information constraints: the sentiment similarity between songs, based on multimodal information, and the co-occurrence of songs in playlists. These constraints enable the simultaneous co-clustering of songs and playlists. The experimental results demonstrate that the proposed model efficiently and consistently identifies sentiment information evoked by either playlists or individual songs.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
keywords = {Music sentiment, playlist, matrix factorization, multimodality data, co-clustering}
}

@article{10.1145/3708473,
author = {Manke, Ruchira and Wardat, Mohammad and Khomh, Foutse and Rajan, Hridesh},
title = {Leveraging Data Characteristics for Bug Localization in Deep Learning Programs},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708473},
doi = {10.1145/3708473},
abstract = {Deep Learning (DL) is a class of machine learning algorithms that are used in a wide variety of applications. Like any software system, DL programs can have bugs. To support bug localization in DL programs, several tools have been proposed in the past. As most of the bugs that occur due to improper model structure known as structural bugs lead to inadequate performance during training, it is challenging for developers to identify the root cause and address these bugs. To support bug detection and localization in DL programs, in this paper, we propose Theia, which detects and localizes structural bugs in DL programs. Unlike the previous works, Theia considers the training dataset characteristics to automatically detect bugs in DL programs developed using two deep learning libraries, Keras and PyTorch. Since training the DL models is a time-consuming process, Theia detects these bugs at the beginning of the training process and alerts the developer with informative messages containing the bug's location and actionable fixes which will help them to improve the structure of the model. We evaluated Theia on a benchmark of 40 real-world buggy DL programs obtained from Stack Overflow. Our results show that Theia successfully localizes 57/75 structural bugs in 40 buggy programs, whereas NeuraLint, a state-of-the-art approach capable of localizing structural bugs before training localizes 17/75 bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {deep learning bugs, bug localization, debugging, program analysis}
}

@article{10.1145/3708549,
author = {Babanejaddehaki, Ghazaleh and An, Aijun and Papagelis, Manos},
title = {Disease Outbreak Detection and Forecasting: A Review of Methods and Data Sources},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708549},
doi = {10.1145/3708549},
abstract = {Infectious diseases occur when pathogens from other individuals or animals infect a person, causing harm to both individuals and society. Outbreaks of such diseases can pose a significant threat to human health. However, early detection and tracking of these outbreaks have the potential to reduce mortality rates. To address these threats, public health authorities have endeavored to establish comprehensive mechanisms for collecting disease data. Many countries have implemented infectious disease surveillance systems, with epidemic detection as a primary objective. The clinical healthcare system, local/state health agencies, federal agencies, academic/professional groups, and collaborating governmental entities all play pivotal roles within this system. Moreover, search engines and social media platforms can serve as valuable tools for monitoring disease trends. The Internet and social media have become significant platforms where users share information about their preferences and relationships. This real-time information can be harnessed to gauge the influence of ideas and societal opinions, proving highly useful across various domains and research areas, such as marketing campaigns, financial predictions, and public health. This article provides a review of the existing standard methods developed by researchers for detecting outbreaks using time series data. These methods leverage various data sources, including conventional data sources and social media data or Internet data sources. The review particularly concentrates on works published within the timeframe of 2015 to 2022.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = dec,
keywords = {outbreak detection, outbreak forecasting, social media, surveillance systems, neural networks, machine learning, statistical analysis, Time series}
}

@article{10.1145/3709158,
author = {Singh, Akarsh and Sural, Shounak and Sengupta, Tirthankar and Sural, Shamik},
title = {AVChain: Trusted Sharing of Autonomous Vehicle Crash Incident Data using Interoperating HyperLedger Fabric Networks and IPFS},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709158},
doi = {10.1145/3709158},
abstract = {Autonomous vehicles (AVs) are gaining in popularity over the years as a viable cab service apps as well as for personal use. However, incidents of crashes involving AVs continue to occur, adversely affecting their prospects for widespread acceptance by both end users and regulatory authorities. While such cases are routinely investigated, in the absence of a human to testify on what caused the crash, one has to rely solely on available data. It is therefore imperative that the data logged by AVs is accessible to the concerned parties in a trustworthy manner. In this paper, we present AVChain - a novel framework for using a permissioned blockchain like HyperLedger Fabric (HLF) to record and share AV data comprised of sensors, actuators, maps, planning algorithms and machine learning models so that the data stays immutable even in the face of cross blaming among involved parties. Since the data volume is extremely large, we appropriately compress and down sample the same before storing in a distributed file system, namely, IPFS (Inter-Planetary File System). The hashes of such IPFS data called Content Ids (CIDs) are committed to the HLF network for making them tamper proof. The HLF ledger can later be queried to obtain the CIDs, which are then further used to retrieve and un-compress the original data from IPFS. Effectiveness and usability of AVChain is demonstrated by generating autonomous vehicle data from CARLA, which is a widely used open source AV simulator. For sharing AV data across organizations like sensor and actuator suppliers, map service providers, machine learning model developers and law enforcement authorities, the Weaver tool has been used to make multiple HLF networks interoperate. We have also developed a web application to demonstrate the working of AVChain. Results of an extensive set of experiments establish the efficacy of our approach.},
note = {Just Accepted},
journal = {Distrib. Ledger Technol.},
month = dec,
keywords = {Autonomous vehicles, Trusted data sharing, HyperLedger Fabric, IPFS, CARLA, Blockchain interoperability}
}

@article{10.1145/3711118,
author = {Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia},
title = {Data-centric Artificial Intelligence: A Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3711118},
doi = {10.1145/3711118},
abstract = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jan,
keywords = {Artificial intelligence, machine learning, data-centric AI}
}

@article{10.14778/2994509.2994523,
author = {Tong, Yongxin and She, Jieying and Ding, Bolin and Chen, Lei and Wo, Tianyu and Xu, Ke},
title = {Online minimum matching in real-time spatial data: experiments and analysis},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994523},
doi = {10.14778/2994509.2994523},
abstract = {Recently, with the development of mobile Internet and smartphones, the &lt;u&gt;o&lt;/u&gt;nline &lt;u&gt;m&lt;/u&gt;inimum &lt;u&gt;b&lt;/u&gt;ipartite &lt;u&gt;m&lt;/u&gt;atching in real time spatial data (OMBM) problem becomes popular. Specifically, given a set of service providers with specific locations and a set of users who dynamically appear one by one, the OMBM problem is to find a maximum-cardinality matching with minimum total distance following that once a user appears, s/he must be immediately matched to an unmatched service provider, which cannot be revoked, before subsequent users arrive. To address this problem, existing studies mainly focus on analyzing the worst-case competitive ratios of the proposed online algorithms, but study on the performance of the algorithms in practice is absent. In this paper, we present a comprehensive experimental comparison of the representative algorithms of the OMBM problem. Particularly, we observe a surprising result that the simple and efficient greedy algorithm, which has been considered as the worst due to its exponential worst-case competitive ratio, is significantly more effective than other algorithms. We investigate the results and further show that the competitive ratio of the worst case of the greedy algorithm is actually just a constant, 3.195, in the average-case analysis. We try to clarify a 25-year misunderstanding towards the greedy algorithm and justify that the greedy algorithm is not bad at all. Finally, we provide a uniform implementation for all the algorithms of the OMBM problem and clarify their strengths and weaknesses, which can guide practitioners to select appropriate algorithms for various scenarios.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1053–1064},
numpages = {12}
}

@article{10.14778/3352063.3352130,
author = {Kandula, Srikanth and Lee, Kukjin and Chaudhuri, Surajit and Friedman, Marc},
title = {Experiences with approximating queries in Microsoft's production big-data clusters},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352130},
doi = {10.14778/3352063.3352130},
abstract = {With the rapidly growing volume of data, it is more attractive than ever to leverage approximations to answer analytic queries. Sampling is a powerful technique which has been studied extensively from the point of view of facilitating approximation. Yet, there has been no large-scale study of effectiveness of sampling techniques in big data systems. In this paper, we describe an in-depth study of the sampling-based approximation techniques that we have deployed in Microsoft's big data clusters. We explain the choices we made to implement approximation, identify the usage cases, and study detailed data that sheds insight on the usefulness of doing sampling based approximation.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2131–2142},
numpages = {12}
}

@article{10.14778/3368289.3368300,
author = {Li, Conggai and Zhang, Fan and Zhang, Ying and Qin, Lu and Zhang, Wenjie and Lin, Xuemin},
title = {Efficient progressive minimum k-core search},
year = {2019},
issue_date = {November 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3368289.3368300},
doi = {10.14778/3368289.3368300},
abstract = {As one of the most representative cohesive subgraph models, k-core model has recently received significant attention in the literature. In this paper, we investigate the problem of the minimum k-core search: given a graph G, an integer k and a set of query vertices Q = {q}, we aim to find the smallest k-core subgraph containing every query vertex q ϵ Q. It has been shown that this problem is NP-hard with a huge search space, and it is very challenging to find the optimal solution. There are several heuristic algorithms for this problem, but they rely on simple scoring functions and there is no guarantee as to the size of the resulting subgraph, compared with the optimal solution. Our empirical study also indicates that the size of their resulting subgraphs may be large in practice. In this paper, we develop an effective and efficient progressive algorithm, namely PSA, to provide a good trade-off between the quality of the result and the search time. Novel lower and upper bound techniques for the minimum k-core search are designed. Our extensive experiments on 12 real-life graphs demonstrate the effectiveness and efficiency of the new techniques.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {362–375},
numpages = {14}
}

@article{10.14778/3461535.3461536,
author = {Zhao, Jianwen and Tao, Yufei},
title = {Minimum vertex augmentation},
year = {2021},
issue_date = {May 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3461535.3461536},
doi = {10.14778/3461535.3461536},
abstract = {This paper introduces a class of graph problems named minimum vertex augmentation (MVA). Given an input graph G where each vertex carries a binary color 0 or 1, we want to flip the colors of the fewest 0-vertices such that the subgraph induced by all the (original and new) 1-vertices satisfies a user-defined predicate π. In other words, the goal is to minimally augment the subset of 1-vertices to uphold the property π. Different formulations of π instantiate the framework into concrete problems at the core of numerous applications. We first describe a suite of techniques for solving MVA problems with strong performance guarantees, and then present a generic algorithmic paradigm that a user can instantiate to deal with ad-hoc MVA problems. The effectiveness and efficiency of our solutions are verified with an extensive experimental evaluation.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1454–1466},
numpages = {13}
}

@article{10.14778/3476311.3476378,
author = {Lee, Rubao and Zhou, Minghong and Li, Chi and Hu, Shenggang and Teng, Jianping and Li, Dongyang and Zhang, Xiaodong},
title = {The art of balance: a RateupDB™ experience of building a CPU/GPU hybrid database product},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476378},
doi = {10.14778/3476311.3476378},
abstract = {GPU-accelerated database systems have been studied for more than 10 years, ranging from prototyping development to industry products serving in multiple domains of data applications. Existing GPU database research solutions are often focused on specific aspects in parallel algorithms and system implementations for specific features, while industry product development generally concentrates on delivering a whole system by considering its holistic performance and cost. Aiming to fill this gap between academic research and industry development, we present a comprehensive industry product study on a complete CPU/GPU HTAP system, called RateupDB. We firmly believe "the art of balance" addresses major issues in the development of RateupDB. Specifically, we consider balancing multiple factors in the software development cycle, such as the trade-off between OLAP and OLTP, the trade-off between system performance and development productivity, and balanced choices of algorithms in the product. We also present RateupDB's complete TPC-H test performance to demonstrate its significant advantages over other existing GPU DBMS products.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2999–3013},
numpages = {15}
}

@article{10.14778/3476311.3476393,
author = {Weikum, Gerhard},
title = {Knowledge graphs 2021: a data odyssey},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476393},
doi = {10.14778/3476311.3476393},
abstract = {Providing machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing vision and challenge for AI. Over the last 15 years, huge knowledge bases, also known as knowledge graphs, have been automatically constructed from web data, and have become a key asset for search engines and other use cases. Machine knowledge can be harnessed to semantically interpret texts in news, social media and web tables, contributing to question answering, natural language processing and data analytics. This position paper reviews these advances and discusses lessons learned. It highlights the role of "DB thinking" in building and maintaining high-quality knowledge bases from web contents. Moreover, the paper identifies open challenges and new research opportunities. In particular, extracting quantitative measures of entities (e.g., height of buildings or energy efficiency of cars), from text and web tables, presents an opportunity to further enhance the scope and value of knowledge bases.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3233–3238},
numpages = {6}
}

@article{10.14778/3514061.3514074,
author = {Cheng, Kewei and Li, Xian and Xu, Yifan Ethan and Dong, Xin Luna and Sun, Yizhou},
title = {PGE: robust product graph embedding learning for error detection},
year = {2022},
issue_date = {February 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3514061.3514074},
doi = {10.14778/3514061.3514074},
abstract = {Although product graphs (PGs) have gained increasing attentions in recent years for their successful applications in product search and recommendations, the extensive power of PGs can be limited by the inevitable involvement of various kinds of errors. Thus, it is critical to validate the correctness of triples in PGs to improve their reliability. Knowledge graph (KG) embedding methods have strong error detection abilities. Yet, existing KG embedding methods may not be directly applicable to a PG due to its distinct characteristics: (1) PG contains rich textual signals, which necessitates a joint exploration of both text information and graph structure; (2) PG contains a large number of attribute triples, in which attribute values are represented by free texts. Since free texts are too flexible to define entities in KGs, traditional way to map entities to their embeddings using ids is no longer appropriate for attribute value representation; (3) Noisy triples in a PG mislead the embedding learning and significantly hurt the performance of error detection. To address the aforementioned challenges, we propose an end-to-end noise-tolerant embedding learning framework, PGE, to jointly leverage both text information and graph structure in PG to learn embeddings for error detection. Experimental results on real-world product graph demonstrate the effectiveness of the proposed framework comparing with the state-of-the-art approaches.},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {1288–1296},
numpages = {9}
}

@article{10.14778/3551793.3551861,
author = {Xia, Siyuan and Zhu, Zhiru and Zhu, Chris and Zhao, Jinjin and Chard, Kyle and Elmore, Aaron J. and Foster, Ian and Franklin, Michael and Krishnan, Sanjay and Fernandez, Raul Castro},
title = {Data station: delegated, trustworthy, and auditable computation to enable data-sharing consortia with a data escrow},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3551793.3551861},
doi = {10.14778/3551793.3551861},
abstract = {Pooling and sharing data increases and distributes its value. But since data cannot be revoked once shared, scenarios that require controlled release of data for regulatory, privacy, and legal reasons default to not sharing. Because selectively controlling what data to release is difficult, the few data-sharing consortia that exist are often built around data-sharing agreements resulting from long and tedious one-off negotiations.We introduce Data Station, a data escrow designed to enable the formation of data-sharing consortia. Data owners share data with the escrow knowing it will not be released without their consent. Data users delegate their computation to the escrow. The data escrow relies on delegated computation to execute queries without releasing the data first. Data Station leverages hardware enclaves to generate trust among participants, and exploits the centralization of data and computation to generate an audit log.We evaluate Data Station on machine learning and data-sharing applications while running on an untrusted intermediary. In addition to important qualitative advantages, we show that Data Station: i) outperforms federated learning baselines in accuracy and runtime for the machine learning application; ii) is orders of magnitude faster than alternative secure data-sharing frameworks; and iii) introduces small overhead on the critical path.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3172–3185},
numpages = {14}
}

@article{10.14778/3554821.3554832,
author = {Chen, Jianjun and Ding, Yonghua and Liu, Ye and Li, Fangshi and Zhang, Li and Zhang, Mingyi and Wei, Kui and Cao, Lixun and Zou, Dan and Liu, Yang and Zhang, Lei and Shi, Rui and Ding, Wei and Wu, Kai and Luo, Shangyu and Sun, Jason and Liang, Yuming},
title = {ByteHTAP: bytedance's HTAP system with high data freshness and strong data consistency},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554832},
doi = {10.14778/3554821.3554832},
abstract = {In recent years, at ByteDance, we see more and more business scenarios that require performing complex analysis over freshly imported data, together with transaction support and strong data consistency. In this paper, we describe our journey of building ByteHTAP, an HTAP system with high data freshness and strong data consistency. It adopts a separate-engine and shared-storage architecture. Its modular system design fully utilizes an existing ByteDance's OLTP system and an open source OLAP system. This choice saves us a lot of resources and development time and allows easy future extensions such as replacing the query processing engine with other alternatives.ByteHTAP can provide high data freshness with less than one second delay, which enables many new business opportunities for our customers. Customers can also configure different data freshness thresholds based on their business needs. ByteHTAP also provides strong data consistency through global timestamps across its OLTP and OLAP system, which greatly relieves application developers from handling complex data consistency issues by themselves. In addition, we introduce some important performance optimizations to ByteHTAP, such as pushing computations to the storage layer and using delete bitmaps to efficiently handle deletes. Lastly, we will share our lessons and best practices in developing and running ByteHTAP in production.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3411–3424},
numpages = {14}
}

@article{10.14778/3565816.3565820,
author = {Shaham, Sina and Ghinita, Gabriel and Shahabi, Cyrus},
title = {Models and mechanisms for spatial data fairness},
year = {2022},
issue_date = {October 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3565816.3565820},
doi = {10.14778/3565816.3565820},
abstract = {Fairness in data-driven decision-making studies scenarios where individuals from certain population segments may be unfairly treated when being considered for loan or job applications, access to public resources, or other types of services. In location-based applications, decisions are based on individual whereabouts, which often correlate with sensitive attributes such as race, income, and education.While fairness has received significant attention recently, e.g., in machine learning, there is little focus on achieving fairness when dealing with location data. Due to their characteristics and specific type of processing algorithms, location data pose important fairness challenges. We introduce the concept of spatial data fairness to address the specific challenges of location data and spatial queries. We devise a novel building block to achieve fairness in the form of fair polynomials. Next, we propose two mechanisms based on fair polynomials that achieve individual spatial fairness, corresponding to two common location-based decision-making types: distance-based and zone-based. Extensive experimental results on real data show that the proposed mechanisms achieve spatial fairness without sacrificing utility.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {167–179},
numpages = {13}
}

@article{10.14778/3583140.3583166,
author = {Azizi, Ilias and Echihabi, Karima and Palpanas, Themis},
title = {ELPIS: Graph-Based Similarity Search for Scalable Data Science},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583166},
doi = {10.14778/3583140.3583166},
abstract = {The recent popularity of learned embeddings has fueled the growth of massive collections of high-dimensional (high-d) vectors that model complex data. Finding similar vectors in these collections is at the core of many important and practical data science applications. The data series community has developed tree-based similarity search techniques that outperform state-of-the-art methods on large collections of both data series and generic high-d vectors, on all scenarios except for no-guarantees ng-approximate search, where graph-based approaches designed by the high-d vector community achieve the best performance. However, building graph-based indexes is extremely expensive both in time and space. In this paper, we bring these two worlds together, study the corresponding solutions and their performance behavior, and propose ELPIS, a new strong baseline that takes advantage of the best features of both to achieve a superior performance in terms of indexing and ng-approximate search in-memory. ELPIS builds the index 3x-8x faster than competitors, using 40\% less memory. It also achieves a high recall of 0.99, up to 2x faster than the state-of-the-art methods, and answers 1-NN queries up to one order of magnitude faster.},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {1548–1559},
numpages = {12}
}

@article{10.14778/3594512.3594522,
author = {Marinelli, Eugenio and Yan, Yiqing and Magnone, Virginie and Dumargne, Charlotte and Barbry, Pascal and Heinis, Thomas and Appuswamy, Raja},
title = {Towards Migration-Free "Just-in-Case" Data Archival for Future Cloud Data Lakes Using Synthetic DNA},
year = {2023},
issue_date = {April 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3594512.3594522},
doi = {10.14778/3594512.3594522},
abstract = {Given the growing adoption of AI, cloud data lakes are facing the need to support cost-effective "just-in-case" data archival over long time periods to meet regulatory compliance requirements. Unfortunately, current media technologies suffer from fundamental issues that will soon, if not already, make cost-effective data archival infeasible. In this paper, we present a vision for redesigning the archival tier of cloud data lakes based on a novel, obsolescence-free storage medium-synthetic DNA. In doing so, we make two contributions: (i) we highlight the challenges in using DNA for data archival and list several open research problems, (ii) we outline OligoArchive-DSM (OA-DSM)-an end-to-end DNA storage pipeline that we are developing to demonstrate the feasibility of our vision.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {1923–1929},
numpages = {7}
}

@article{10.14778/3598581.3598601,
author = {Sudhir, Sivaprasad and Tao, Wenbo and Laptev, Nikolay and Habis, Cyrille and Cafarella, Michael and Madden, Samuel},
title = {Pando: Enhanced Data Skipping with Logical Data Partitioning},
year = {2023},
issue_date = {May 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3598581.3598601},
doi = {10.14778/3598581.3598601},
abstract = {With enormous volumes of data, quickly retrieving data that is relevant to a query is essential for achieving high performance. Modern cloud-based database systems often partition the data into blocks and employ various techniques to skip irrelevant blocks during query execution. Several algorithms, often based on historical properties of a workload of queries run over the data, have been proposed to tune the physical layout of data to reduce the number of blocks accessed. The effectiveness of these methods at skipping blocks depends on what metadata is stored and how well the physical data layout aligns with the queries. Existing work on automatic physical database design misses significant opportunities in skipping blocks because it ignores logical predicates in the workload that exhibit strongly correlated results. In this paper, we present Pando which enables significantly better block skipping than past methods by informing physical layout decisions with correlation-aware logical partitioning. Across a range of benchmark and real-world workloads, Pando attains up to 2.8X reduction in the number of blocks scanned and up to 2.3X speedup in end-to-end query execution time over the state-of-the-art techniques.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {2316–2329},
numpages = {14}
}

@article{10.14778/3603581.3603604,
author = {Pedreira, Pedro and Erling, Orri and Karanasos, Konstantinos and Schneider, Scott and McKinney, Wes and Valluri, Satya R and Zait, Mohamed and Nadeau, Jacques},
title = {The Composable Data Management System Manifesto},
year = {2023},
issue_date = {June 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3603581.3603604},
doi = {10.14778/3603581.3603604},
abstract = {The requirement for specialization in data management systems has evolved faster than our software development practices. After decades of organic growth, this situation has created a siloed landscape composed of hundreds of products developed and maintained as monoliths, with limited reuse between systems. This fragmentation has resulted in developers often reinventing the wheel, increased maintenance costs, and slowed down innovation. It has also affected the end users, who are often required to learn the idiosyncrasies of dozens of incompatible SQL and non-SQL API dialects, and settle for systems with incomplete functionality and inconsistent semantics. In this vision paper, considering the recent popularity of open source projects aimed at standardizing different aspects of the data stack, we advocate for a paradigm shift in how data management systems are designed. We believe that by decomposing these into a modular stack of reusable components, development can be streamlined while creating a more consistent experience for users. Towards that goal, we describe the state-of-the-art, principal open source technologies, and highlight open questions and areas where additional research is needed. We hope this work will foster collaboration, motivate further research, and promote a more composable future for data management.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {2679–2685},
numpages = {7}
}

@article{10.14778/3611479.3611520,
author = {Tong, Yulai and Liu, Jiazhen and Wang, Hua and Zhou, Ke and He, Rongfeng and Zhang, Qin and Wang, Cheng},
title = {Sieve: A Learned Data-Skipping Index for Data Analytics},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611520},
doi = {10.14778/3611479.3611520},
abstract = {Modern data analytics services are coupled with external data storage services, making I/O from remote cloud storage one of the dominant costs for query processing. Techniques such as columnar block-based data organization and compression have become standard practices for these services to save storage and processing cost. However, the problem of effectively skipping irrelevant blocks at low overhead is still open. Existing data-skipping efforts maintain lightweight summaries (e.g., min/max, histograms) for each block to filter irrelevant data. However, such techniques ignore patterns in real-world data, enabling ineffective use of the storage budget and may cause serious false positives.This paper presents Sieve, a learning-enhanced index designed to efficiently filter out irrelevant blocks by capturing data patterns. Specifically, Sieve utilizes piece-wise linear functions to capture block distribution trends over the key space. Based on the captured trends, Sieve trades off storage consumption and false positives by grouping neighboring keys with similar block distributions into a single region. We have evaluated Sieve using Presto, and experiments on real-world datasets demonstrate that Sieve achieves up to 80\% reduction in blocks accessed and 42\% reduction in query times compared to its counterparts.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3214–3226},
numpages = {13}
}

@article{10.14778/3611479.3611533,
author = {Eltabakh, Mohamed Y. and Kunjir, Mayuresh and Elmagarmid, Ahmed K. and Ahmad, Mohammad Shahmeer},
title = {Cross Modal Data Discovery over Structured and Unstructured Data Lakes},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611533},
doi = {10.14778/3611479.3611533},
abstract = {Organizations are collecting increasingly large amounts of data for data-driven decision making. These data are often dumped into a centralized repository, e.g., a data lake, consisting of thousands of structured and unstructured datasets. Perversely, such mixture makes the problem of discovering tables or documents that are relevant to a user's query very challenging. Despite the recent efforts in data discovery, the problem remains widely open especially in the two fronts of (1) discovering relationships and relatedness across structured and unstructured datasets-where existing techniques suffer from either scalability, being customized for a specific problem type (e.g., entity matching or data integration), or demolishing the structural properties on its way, and (2) developing a holistic system for integrating various similarity measurements and sketches in an effective way to boost the discovery accuracy.In this paper, we propose a new data discovery system, named CMDL, for addressing these two limitations. CMDL supports the data discovery process over both structured and unstructured data while retaining the structural properties of tables. As a result, CMDL is the only system to date that empowers end-users to seamlessly pipeline the discovery tasks across the two modalities. We propose a novel multi-modal embedding representation that captures the similarities between text documents and tabular columns. The model training relies on labeled datasets generated though weak supervision, and thus the system is domain agnostic and easily generalizable. We evaluate CMDL on three real-world data lakes with diverse applications and show that our system is significantly more effective for cross-modality discovery compared to the search-based baseline techniques. Moreover, CMDL is more accurate and robust to different data types and distributions compared to the state-of-the-art systems that are limited to only the structured datasets.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3377–3390},
numpages = {14}
}

@article{10.14778/3611540.3611569,
author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},
title = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611569},
doi = {10.14778/3611540.3611569},
abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3848–3860},
numpages = {13}
}

@article{10.14778/3611540.3611595,
author = {Bylois, Niels and Neven, Frank and Vansummeren, Stijn},
title = {CM-Explorer: Dissecting Data Ingestion Problems},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611595},
doi = {10.14778/3611540.3611595},
abstract = {Data ingestion validation, the task of certifying the quality of continuously collected data, is crucial to ensure trustworthiness of analytics insights. A widely used approach for validating data quality is to specify, either manually or automatically, so-called data unit tests that check whether data quality metrics lie within expected bounds. We employ conditional unit tests based on conditional metrics (CMs) that compute data quality signals over specific parts of the ingestion data and therefore allow for a fine-grained detection of errors. A violated conditional unit test specifies a set of erroneous tuples in a natural way: the subrelation that its CM refers to. Unfortunately, the downside of their fine-grained nature is that violating unit tests are often correlated: a single error in an ingestion batch may cause multiple tests (each referring to different parts of the batch) to fail. The key challenge is therefore to untangle this correlation and filter out the most relevant violated conditional unit tests, i.e., tests that identify a core set of erroneous tuples and act as an explanation for the errors. We present CM-Explorer, a system that supports data stewards in quickly finding the most relevant violated conditional unit tests. The system consists of three components: (1) a graph explorer for visualizing the correlation structure of the violated unit tests; (2) a relation explorer for browsing the tuples selected by conditional unit tests; and, (3) a history explorer to get insight why conditional unit tests are violated. In this paper, we discuss these components and present the different scenarios that we make available for the demonstration.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3958–3961},
numpages = {4}
}

@article{10.14778/3611540.3611635,
author = {Cheung, Alvin and Ahmad, Maaz Bin Safeer and Haynes, Brandon and Kittivorawong, Chanwut and Laddad, Shadaj and Liu, Xiaoxuan and Wang, Chenglong and Yan, Cong},
title = {Towards Auto-Generated Data Systems},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611635},
doi = {10.14778/3611540.3611635},
abstract = {After decades of progress, database management systems (DBMSs) are now the backbones of many data applications that we interact with on a daily basis. Yet, with the emergence of new data types and hardware, building and optimizing new data systems remain as difficult as the heyday of relational databases. In this paper, we summarize our work towards automating the building and optimization of data systems. Drawing from our own experience, we further argue that any automation technique must address three aspects: user specification, code generation, and result validation. We conclude by discussing a case study using videos data processing, along with opportunities for future research towards designing data systems that are automatically generated.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4116–4129},
numpages = {14}
}

@article{10.14778/3625054.3625060,
author = {Liu, Rui and Park, Kwanghyun and Psallidas, Fotis and Zhu, Xiaoyong and Mo, Jinghui and Sen, Rathijit and Interlandi, Matteo and Karanasos, Konstantinos and Tian, Yuanyuan and Camacho-Rodr\'{\i}guez, Jes\'{u}s},
title = {Optimizing Data Pipelines for Machine Learning in Feature Stores},
year = {2023},
issue_date = {September 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3625054.3625060},
doi = {10.14778/3625054.3625060},
abstract = {Data pipelines (i.e., converting raw data to features) are critical for machine learning (ML) models, yet their development and management is time-consuming. Feature stores have recently emerged as a new "DBMS-for-ML" with the premise of enabling data scientists and engineers to define and manage their data pipelines. While current feature stores fulfill their promise from a functionality perspective, they are resource-hungry---with ample opportunities for implementing database-style optimizations to enhance their performance. In this paper, we propose a novel set of optimizations specifically targeted for point-in-time join, which is a critical operation in data pipelines. We implement these optimizations on top of Feathr: a widely-used feature store, and evaluate them on use cases from both the TPCx-AI benchmark and real-world online retail scenarios. Our thorough experimental analysis shows that our optimizations can accelerate data pipelines by up to 3\texttimes{} over state-of-the-art baselines.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {4230–4239},
numpages = {10}
}

@article{10.14778/3625054.3625064,
author = {Lian, Jinqing and Zhang, Xinyi and Shao, Yingxia and Pu, Zenglin and Xiang, Qingfeng and Li, Yawen and Cui, Bin},
title = {ContTune: Continuous Tuning by Conservative Bayesian Optimization for Distributed Stream Data Processing Systems},
year = {2023},
issue_date = {September 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3625054.3625064},
doi = {10.14778/3625054.3625064},
abstract = {The past decade has seen rapid growth of distributed stream data processing systems. Under these systems, a stream application is realized as a Directed Acyclic Graph (DAG) of operators, where the level of parallelism of each operator has a substantial impact on its overall performance. However, finding optimal levels of parallelism remains challenging. Most existing methods are heavily coupled with the topological graph of operators, unable to efficiently tune under-provisioned jobs. They either insufficiently use previous tuning experience by treating successively tuning independently, or explore the configuration space aggressively, violating the Service Level Agreements (SLA).To address the above problems, we propose ContTune, a continuous tuning system for stream applications. It is equipped with a novel Big-small algorithm, in which the Big phase decouples the tuning from the topological graph by decomposing the job tuning problem into sub-problems that can be solved concurrently. We propose a conservative Bayesian Optimization (CBO) technique in the Small phase to speed up the tuning process by utilizing the previous observations. It leverages the state-of-the-art (SOTA) tuning method as conservative exploration to avoid SLA violations. Experimental results show that ContTune reduces up to 60.75\% number of reconfigurations under synthetic workloads and up to 57.5\% number of reconfigurations under real workloads, compared to the SOTA method DS2.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {4282–4295},
numpages = {14}
}

@article{10.14778/3632093.3632100,
author = {Li, Xiao and Li, Huan and Lu, Hua and Jensen, Christian S. and Pandey, Varun and Markl, Volker},
title = {Missing Value Imputation for Multi-Attribute Sensor Data Streams via Message Propagation},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632100},
doi = {10.14778/3632093.3632100},
abstract = {Sensor data streams occur widely in various real-time applications in the context of the Internet of Things (IoT). However, sensor data streams feature missing values due to factors such as sensor failures, communication errors, or depleted batteries. Missing values can compromise the quality of real-time analytics tasks and downstream applications. Existing imputation methods either make strong assumptions about streams or have low efficiency. In this study, we aim to accurately and efficiently impute missing values in data streams that satisfy only general characteristics in order to benefit real-time applications more widely. First, we propose a message propagation imputation network (MPIN) that is able to recover the missing values of data instances in a time window. We give a theoretical analysis of why MPIN is effective. Second, we present a continuous imputation framework that consists of data update and model update mechanisms to enable MPIN to perform continuous imputation both effectively and efficiently. Extensive experiments on multiple real datasets show that MPIN can outperform the existing data imputers by wide margins and that the continuous imputation framework is efficient and accurate.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {345–358},
numpages = {14}
}

@article{10.14778/3641204.3641211,
author = {Xing, Junjie and Wang, Xinyu and Jagadish, H. V.},
title = {Data-Driven Insight Synthesis for Multi-Dimensional Data},
year = {2024},
issue_date = {January 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3641204.3641211},
doi = {10.14778/3641204.3641211},
abstract = {Exploratory data analysis can uncover interesting data insights from data. Current methods utilize "interestingness measures" designed based on system designers' perspectives, thus inherently restricting the insights to their defined scope. These systems, consequently, may not adequately represent a broader range of user interests. Furthermore, most existing approaches that formulate "interestingness measure" are rule-based, which makes them inevitably brittle and often requires holistic re-design when new user needs are discovered.This paper presents a data-driven technique for deriving an "interestingness measure" that learns from annotated data. We further develop an innovative annotation algorithm that significantly reduces the annotation cost, and an insight synthesis algorithm based on the Markov Chain Monte Carlo method for efficient discovery of interesting insights. We consolidate these ideas into a system. Our experimental outcomes and user studies demonstrate that DAISY can effectively discover a broad range of interesting insights, thereby substantially advancing the current state-of-the-art.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1007–1019},
numpages = {13}
}

@article{10.14778/3648160.3648167,
author = {Yuan, Hao and Liu, Yajiong and Zhang, Yanfeng and Ai, Xin and Wang, Qiange and Chen, Chaoyi and Gu, Yu and Yu, Ge},
title = {Comprehensive Evaluation of GNN Training Systems: A Data Management Perspective},
year = {2024},
issue_date = {February 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3648160.3648167},
doi = {10.14778/3648160.3648167},
abstract = {Many Graph Neural Network (GNN) training systems have emerged recently to support efficient GNN training. Since GNNs embody complex data dependencies between training samples, the training of GNNs should address distinct challenges different from DNN training in data management, such as data partitioning, batch preparation for mini-batch training, and data transferring between CPUs and GPUs. These factors, which take up a large proportion of training time, make data management in GNN training more significant. This paper reviews GNN training from a data management perspective and provides a comprehensive analysis and evaluation of the representative approaches. We conduct extensive experiments on various benchmark datasets and show many interesting and valuable results. We also provide some practical tips learned from these experiments, which are helpful for designing GNN training systems in the future.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1241–1254},
numpages = {14}
}

@article{10.14778/3648160.3648173,
author = {Chen, Xin and Shi, Jieming and Peng, You and Lin, Wenqing and Wang, Sibo and Zhang, Wenjie},
title = {Minimum Strongly Connected Subgraph Collection in Dynamic Graphs},
year = {2024},
issue_date = {February 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3648160.3648173},
doi = {10.14778/3648160.3648173},
abstract = {Real-world directed graphs are dynamically changing, and it is important to identify and maintain the strong connectivity information between nodes, which is useful in numerous applications. Given an input graph G, we study a new problem, minimum strongly connected subgraph collection (MSCSC), which asks for a complete collection of subgraphs, each of which contains a maximal set of nodes that are strongly connected to each other via minimum number of edges in G.MSCSC is NP-hard, and its computation and maintenance are challenging, especially on large-scale dynamic graphs. Thus, we resort to approximate MSCSC with theoretical guarantees. We develop a series of approximate MSCSC methods for both static and dynamic graphs. Specifically, we first develop a static MSCSC method MSC that only needs one scan of the graph G, runs in linear time w.r.t., the number of edges, and provides rigorous approximation guarantees. Then, based on MSC, we leverage a reduced directed acyclic graph of G to design incremental MSCSC method MSCi with two variants to handle edge insertions efficiently. We further develop MSCd that updates MSCSC under edge deletions by efficiently scanning only locally affected subgraphs. Moreover, to demonstrate the high utility, we conduct two use case studies to apply our MSCSC methods to boost the efficiency of dynamic strongly connected component (SCC) maintenance and dynamic SCC-based reachability index maintenance. Extensive experiments on 8 large graphs, including 3 billion-edge graphs, validate the superior efficiency of our methods.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1324–1336},
numpages = {13}
}

@article{10.14778/3648160.3648183,
author = {Lv, Yangming and Zhang, Kai and Wang, Ziming and Zhang, Xiaodong and Lee, Rubao and He, Zhenying and Jing, Yinan and Wang, X. Sean},
title = {RTScan: Efficient Scan with Ray Tracing Cores},
year = {2024},
issue_date = {February 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3648160.3648183},
doi = {10.14778/3648160.3648183},
abstract = {Indexing is a core technique for accelerating predicate evaluation in databases. After many years of effort, the indexing performance has reached its peak on the existing hardware infrastructure. We propose to use ray tracing (RT) cores to move the indexing performance and efficiency to another level by addressing the following technical challenges: (1) the lack of an efficient mapping of predicate evaluation to a ray tracing job and (2) the poor performance by the heavy and imbalanced ray load when processing skewed datasets. These challenges set obstacles to effectively exploiting RT cores for predicate evaluation.In this paper, we propose RTScan, an approach that leverages RT cores to accelerate index scans. RTScan transforms the evaluation of conjunctive predicates into an efficient ray tracing job in a three-dimensional space. A set of techniques are designed in RTScan, i.e., Uniform Encoding, Data Sieving, and Matrix RT Refine, which significantly enhances the parallelism of scans on RT cores while lightening and balancing the ray load. With the proposed techniques, RTScan achieves high performance for datasets with either uniform or skewed distributions and queries with different selectivities. Extensive evaluations demonstrate that RTScan enhances the scan performance on RT cores by five orders of magnitude and outperforms the state-of-the-art approach on CPU by up to 4.6\texttimes{}.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1460–1472},
numpages = {13}
}

@article{10.14778/3659437.3659446,
author = {Wu, Biao and Huang, Qiang and Tung, Anthony K. H.},
title = {From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying},
year = {2024},
issue_date = {April 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3659437.3659446},
doi = {10.14778/3659437.3659446},
abstract = {Safeguarding the Intellectual Property (IP) of data has become critically important as machine learning applications continue to proliferate, and their success heavily relies on the quality of training data. While various mechanisms exist to secure data during storage, transmission, and consumption, fewer studies have been developed to detect whether they are already leaked for model training without authorization. This issue is particularly challenging due to the absence of information and control over the training process conducted by potential attackers.In this paper, we concentrate on the domain of tabular data and introduce a novel methodology, Local Distribution Shifting Synthesis (LDSS), to detect leaked data that are used to train classification models. The core concept behind LDSS involves injecting a small volume of synthetic data-characterized by local shifts in class distribution-into the owner's dataset. This enables the effective identification of models trained on leaked data through model querying alone, as the synthetic data injection results in a pronounced disparity in the predictions of models trained on leaked and modified datasets. LDSS is model-oblivious and hence compatible with a diverse range of classification models. We have conducted extensive experiments on seven types of classification models across five real-world datasets. The comprehensive results affirm the reliability, robustness, fidelity, security, and efficiency of LDSS. Extending LDSS to regression tasks further highlights its versatility and efficacy compared with baseline methods.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1898–1910},
numpages = {13}
}

@article{10.14778/3659437.3659455,
author = {Mohapatra, Shubhankar and Zong, Jianqiao and Kerschbaum, Florian and He, Xi},
title = {Differentially Private Data Generation with Missing Data},
year = {2024},
issue_date = {April 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3659437.3659455},
doi = {10.14778/3659437.3659455},
abstract = {Despite several works that succeed in generating synthetic data with differential privacy (DP) guarantees, they are inadequate for generating high-quality synthetic data when the input data has missing values. In this work, we formalize the problems of DP synthetic data with missing values and propose three effective adaptive strategies that significantly improve the utility of the synthetic data on four real-world datasets with different types and levels of missing data and privacy requirements. We also identify the relationship between privacy impact for the complete ground truth data and incomplete data for these DP synthetic data generation algorithms. We model the missing mechanisms as a sampling process to obtain tighter upper bounds for the privacy guarantees to the ground truth data. Overall, this study contributes to a better understanding of the challenges and opportunities for using private synthetic data generation algorithms in the presence of missing data.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {2022–2035},
numpages = {14}
}

@article{10.14778/3665844.3665850,
author = {Daliri, Majid and Freire, Juliana and Musco, Christopher and Santos, A\'{e}cio and Zhang, Haoxiang},
title = {Sampling Methods for Inner Product Sketching},
year = {2024},
issue_date = {May 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3665844.3665850},
doi = {10.14778/3665844.3665850},
abstract = {Recently, Bessa et al. (PODS 2023) showed that sketches based on coordinated weighted sampling theoretically and empirically outperform popular linear sketching methods like Johnson-Lindentrauss projection and CountSketch for the ubiquitous problem of inner product estimation. We further develop this finding by introducing and analyzing two alternative sampling-based methods. In contrast to the computationally expensive algorithm in Bessa et al., our methods run in linear time (to compute the sketch) and perform better in practice, significantly beating linear sketching on a variety of tasks. For example, they provide state-of-the-art results for estimating the correlation between columns in unjoined tables, a problem that we show how to reduce to inner product estimation in a black-box way. While based on known sampling techniques (threshold and priority sampling) we introduce significant new theoretical analysis to prove approximation guarantees for our methods.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2185–2197},
numpages = {13}
}

@article{10.14778/3665844.3665861,
author = {Nagda, Heena and Singhal, Shubhendra Pal and Amiri, Mohammad Javad and Loo, Boon Thau},
title = {Rashnu: Data-Dependent Order-Fairness},
year = {2024},
issue_date = {May 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3665844.3665861},
doi = {10.14778/3665844.3665861},
abstract = {Distributed data management systems use state Machine Replication (SMR) to provide fault tolerance. The SMR algorithm enables Byzantine Fault-Tolerant (BFT) protocols to guarantee safety and liveness despite the malicious failure of nodes. However, SMR does not prevent the adversarial manipulation of the order of transactions, where the order assigned by a malicious leader differs from the order in that transactions are received from clients. While order-fairness has been recently studied in a few protocols, such protocols rely on synchronized clocks, suffer from liveness issues, or incur significant performance overhead. This paper presents Rashnu, a high-performance fair ordering protocol. Rashnu is motivated by the fact that fair ordering among two transactions is needed only when both transactions access a shared resource. Based on this observation, we define the notion of data-dependent order fairness where replicas capture only the order of data-dependent transactions and the leader uses these orders to propose a dependency graph that represents fair ordering among transactions. Replicas then execute transactions using the dependency graph, resulting in the parallel execution of independent transactions. We implemented a prototype of Rashnu where our experimental evaluation reveals the low overhead of providing order-fairness in Rashnu.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2335–2348},
numpages = {14}
}

@article{10.14778/3675034.3675038,
author = {Zheng, Leqian and Xu, Lei and Wang, Cong and Wang, Sheng and Hu, Yuke and Qin, Zhan and Li, Feifei and Ren, Kui},
title = {SWAT: A System-Wide Approach to Tunable Leakage Mitigation in Encrypted Data Stores},
year = {2024},
issue_date = {June 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3675034.3675038},
doi = {10.14778/3675034.3675038},
abstract = {Numerous studies have underscored the significant privacy risks associated with various leakage patterns in encrypted data stores. While many solutions have been proposed to mitigate these leakages, they either (1) incur substantial overheads, (2) focus on specific subsets of leakage patterns, or (3) apply the same security notion across various workloads, thereby impeding the attainment of fine-tuned privacy-efficiency trade-offs. In light of various detrimental leakage patterns, this paper starts with an investigation into which specific leakage patterns require our focus in the contexts of key-value, range-query, and dynamic workloads, respectively. Subsequently, we introduce new security notions tailored to the specific privacy requirements of these workloads. Accordingly, we propose and instantiate Swat, an efficient construction that progressively enables these workloads, while provably mitigating system-wide leakage via a suite of algorithms with tunable privacy-efficiency trade-offs. We conducted extensive experiments and compiled a detailed result analysis, showing the efficiency of our solution. Swat is about an order of magnitude slower than an encryption-only data store that reveals various leakage patterns and is two orders of magnitude faster than a trivial zero-leakage solution. Meanwhile, the performance of Swat remains highly competitive compared to other designs that mitigate specific types of leakage.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2445–2458},
numpages = {14}
}

@article{10.14778/3681954.3681957,
author = {Campos, David and Yang, Bin and Kieu, Tung and Zhang, Miao and Guo, Chenjuan and Jensen, Christian S.},
title = {QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681957},
doi = {10.14778/3681954.3681957},
abstract = {We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes. It is thus attractive to be able to deploy machine learning models, e.g., for classification, on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers. To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits. The resulting quantized models are then calibrated using back-propagation with the full training data to ensure accuracy. This one-time calibration works for deployments in static environments. However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions with the original training data. The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus cannot be assumed to be always available on edge devices. The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive. We propose QCore to enable continual calibration on the edge. First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths. We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data. Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation. An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2708–2721},
numpages = {14}
}

@article{10.14778/3681954.3681968,
author = {Ma, Yuxin and Gong, Ping and Wu, Tianming and Yi, Jiawei and Yang, Chengru and Li, Cheng and Peng, Qirong and Xie, Guiming and Bao, Yongcheng and Liu, Haifeng and Xu, Yinlong},
title = {Eliminating Data Processing Bottlenecks in GNN Training over Large Graphs via Two-level Feature Compression},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681968},
doi = {10.14778/3681954.3681968},
abstract = {Training GNNs over large graphs faces a severe data processing bottleneck, involving both sampling and feature loading. To tackle this issue, we introduce F2CGT, a fast GNN training system incorporating feature compression. To avoid potential accuracy degradation, we propose a two-level, hybrid feature compression approach that applies different compression methods to various graph nodes. This differentiated choice strikes a balance between rounding errors, compression ratios, model accuracy loss, and preprocessing costs. Our theoretical analysis proves that this approach offers convergence and comparable model accuracy as the conventional training without feature compression. Additionally, we also co-design the on-GPU cache sub-system with compression-enabled training within F2CGT. The new cache sub-system, driven by a cost model, runs new cache policies to carefully choose graph nodes with high access frequencies, and well partitions the spare GPU memory for various types of graph data, for improving cache hit rates. Finally, extensive evaluation of F2CGT on two popular GNN models and four datasets, including three large public datasets, demonstrates that F2CGT achieves a compression ratio of up to 128 and provides GNN training speedups of 1.23-2.56\texttimes{} and 3.58--71.46\texttimes{} for single-machine and distributed training, respectively, with up to 32 GPUs and marginal accuracy loss.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2854–2866},
numpages = {13}
}

@article{10.14778/3681954.3681978,
author = {Schmidl, Sebastian and Naumann, Felix and Papenbrock, Thorsten},
title = {AutoTSAD: Unsupervised Holistic Anomaly Detection for Time Series Data},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681978},
doi = {10.14778/3681954.3681978},
abstract = {Detecting anomalous subsequences in time series data is one of the key tasks in time series analytics, having applications in environmental monitoring, preventive healthcare, predictive maintenance, and many further areas. Data scientists have developed various anomaly detection algorithms with individual strengths, such as the ability to detect repeating anomalies, anomalies in non-periodic time series, or anomalies with varying lengths. For a given dataset and task, the best algorithm with a suitable parameterization and, in some cases, sufficient training data, usually solves the anomaly detection problem well. However, given the high number of existing algorithms, their numerous parameters, and a pervasive lack of training data and domain knowledge, effective anomaly detection is still a complex task that heavily relies on manual experimentation.We propose the unsupervised AutoTSAD system, which parameterizes, executes, and ensembles various highly effective anomaly detection algorithms. The ensembling system automatically presents an aggregated anomaly scoring for an arbitrary time series without a need for training data or parameter expertise. Our experiments show that AutoTSAD offers an anomaly detection accuracy comparable to the best manually optimized anomaly detection algorithms, and can significantly outperform existing method selection and ensembling approaches for time series anomaly detection.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2987–3002},
numpages = {16}
}

@article{10.14778/3681954.3681980,
author = {Ramos, Maria and Azevedo, Jo\~{a}o and Kingsbury, Kyle and Pereira, Jos\'{e} and Esteves, T\^{a}nia and Macedo, Ricardo and Paulo, Jo\~{a}o},
title = {When Amnesia Strikes: Understanding and Reproducing Data Loss Bugs with Fault Injection},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681980},
doi = {10.14778/3681954.3681980},
abstract = {We present LazyFS, a new fault injection tool that simplifies the debugging and reproduction of complex data durability bugs experienced by databases, key-value stores, and other data-centric systems in crashes. Our tool simulates persistence properties of POSIX file systems (e.g., operations ordering and atomicity) and enables users to inject lost and torn write faults with a precise and controlled approach. Further, it provides profiling information about the system's operations flow and persisted data, enabling users to better understand the root cause of errors.We use LazyFS to study seven important systems: PostgreSQL, etcd, Zookeeper, Redis, LevelDB, PebblesDB, and Lightning Network. Our fault injection campaign shows that LazyFS automates and facilitates the reproduction of five known bug reports containing manual and complex reproducibility steps. Further, it aids in understanding and reproducing seven ambiguous bugs reported by users. Finally, LazyFS is used to find eight new bugs, which lead to data loss, corruption, and unavailability.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3017–3030},
numpages = {14}
}

@article{10.14778/3681954.3681983,
author = {Takagi, Shun and Xiong, Li and Kato, Fumiyuki and Cao, Yang and Yoshikawa, Masatoshi},
title = {HRNet: Differentially Private Hierarchical and Multi-Resolution Network for Human Mobility Data Synthesization},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681983},
doi = {10.14778/3681954.3681983},
abstract = {Human mobility data offers valuable insights for many applications such as urban planning and pandemic response, but its use also raises privacy concerns. In this paper, we introduce the Hierarchical and Multi-Resolution Network (HRNet), a novel deep generative model specifically designed to synthesize realistic human mobility data while guaranteeing differential privacy. We first identify the key difficulties inherent in learning human mobility data under differential privacy. In response to these challenges, HRNet integrates three components: a hierarchical location encoding mechanism, multi-task learning across multiple resolutions, and private pre-training. These elements collectively enhance the model's ability under the constraints of differential privacy. Through extensive comparative experiments utilizing a real-world dataset, HRNet demonstrates a marked improvement over existing methods in balancing the utility-privacy trade-off.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3058–3071},
numpages = {14}
}

@article{10.14778/3681954.3681988,
author = {Wang, Haibo},
title = {Enhancing Accuracy for Super Spreader Identification in High-Speed Data Streams},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681988},
doi = {10.14778/3681954.3681988},
abstract = {This paper addresses the challenge of identifying super spreaders within large, high-speed data streams. In these streams, data is segmented into flows, with each flow's spread defined as the number of distinct items it contains. A super spreader is characterized as a flow with a notably large spread. Current compact solutions, known as sketches, are designed to fit within the constrained memory of online devices. However, they struggle to accurately track the spread of all flows due to the substantial memory requirement for monitoring a single flow --- a problem exacerbated when numerous flows are involved. To overcome these limitations, this study proposes a more precise sketch-based approach. Our solution introduces an innovative non-duplicate sampler that effectively eliminates duplicates, allowing for accurate post-sampling count of flow spread using only counters. Additionally, it incorporates an exponential-weakening decay technique to highlight large flows, markedly enhancing the accuracy of super spreader identification. We offer a comprehensive theoretical analysis of our method. Trace-driven experiments validate that our approach statistically surpasses existing state-of-the-art solutions in identifying super spreaders. It also demonstrates the lowest time required to restore super spreaders and significantly reduces bandwidth consumption by an order of magnitude when offline restoration is conducted remotely.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3124–3137},
numpages = {14}
}

@article{10.14778/3681954.3681989,
author = {Seeman, Jeremy and Sexton, William and Pujol, David and Machanavajjhala, Ashwin},
title = {Privately Answering Queries on Skewed Data via Per-Record Differential Privacy},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681989},
doi = {10.14778/3681954.3681989},
abstract = {We consider the problem of the private release of statistics (like payroll) where it is critical to preserve the contribution made by a small number of outlying large entities. We propose a privacy formalism, per-record zero concentrated differential privacy (PzCDP), where the privacy loss associated with each record is a public function of that record's value. Unlike other formalisms which provide different privacy losses to different records, PzCDP's privacy loss depends explicitly on the confidential data. We define our formalism, derive its properties, and propose mechanisms which satisfy PzCDP that are uniquely suited to publishing skewed or heavy-tailed statistics, where a small number of records contribute substantially to query answers. This targeted relaxation helps overcome the difficulties of applying standard DP to these data products.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3138–3150},
numpages = {13}
}

@article{10.14778/3681954.3681996,
author = {Zuo, Rundong and Li, Guozhong and Cao, Rui and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S},
title = {DARKER: Efficient Transformer with Data-Driven Attention Mechanism for Time Series},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3681996},
doi = {10.14778/3681954.3681996},
abstract = {Transformer-based models have facilitated numerous applications with superior performance. A key challenge in transformers is the quadratic dependency of its training time complexity on the length of the input sequence. A recent popular solution is using random feature attention (RFA) to approximate the costly vanilla attention mechanism. However, RFA relies on only a single, fixed projection for approximation, which does not capture the input distribution and can lead to low efficiency and accuracy, especially on time series data. In this paper, we propose DARKER, an efficient transformer with a novel DAta-dRiven KERnel-based attention mechanism. To precisely present the technical details, this paper discusses them with a fundamental time series task, namely, time series classification (tsc). First, the main novelty of DARKER lies in approximating the softmax kernel by learning multiple machine learning models with trainable weights as multiple projections offline, moving beyond the limitation of a fixed projection. Second, we propose a projection index (called pIndex) to efficiently search the most suitable projection for the input for training transformer. As a result, the overall time complexity of DARKER is linear with the input length. Third, we propose an indexing technique for efficiently computing the inputs required for transformer training. Finally, we evaluate our method on 14 real-world and 2 synthetic time series datasets. The experiments show that DARKER is 3\texttimes{}-4\texttimes{} faster than vanilla transformer and 1.5\texttimes{}-3\texttimes{} faster than other SOTAs for long sequences. In addition, the accuracy of DARKER is comparable to or higher than that of all compared transformers.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3229–3242},
numpages = {14}
}

@article{10.14778/3681954.3682004,
author = {Si, Michelle and Pei, Jian},
title = {Counterfactual Explanation of Shapley Value in Data Coalitions},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682004},
doi = {10.14778/3681954.3682004},
abstract = {The Shapley value is widely used for data valuation in data markets. However, explaining the Shapley value of an owner in a data coalition is an unexplored and challenging task. To tackle this, we formulate the problem of finding the counterfactual explanation of Shapley value in data coalitions. Essentially, given two data owners A and B such that A has a higher Shapley value than B, a counter-factual explanation is a smallest subset of data entries in A such that transferring the subset from A to B makes the Shapley value of A less than that of B. We show that counterfactual explanations always exist, but finding an exact counterfactual explanation is NP-hard. Using Monte Carlo estimation to approximate counterfactual explanations directly according to the definition is still very costly, since we have to estimate the Shapley values of owners A and B after each possible subset shift. We develop a series of heuristic techniques to speed up computation by estimating differential Shapley values, computing the power of singular data entries, and shifting subsets greedily, culminating in the SV-Exp algorithm. Our experimental results on real datasets clearly demonstrate the efficiency of our method and the effectiveness of counterfactuals in interpreting the Shapley value of an owner.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3332–3345},
numpages = {14}
}

@article{10.14778/3681954.3682005,
author = {Zhang, Yi and Chen, Peter Baile and Ives, Zachary G.},
title = {Searching Data Lakes for Nested and Joined Data},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682005},
doi = {10.14778/3681954.3682005},
abstract = {Exploratory data science is driving new platforms that assist data scientists with everyday tasks, such as integration and wrangling, to assemble training datasets. Such tools take scientists' work-in-progress data as a search object (table or JSON) and find relevant supplementary data from an organizational data lake, which can be unioned or joined with the current data. Existing data lake search tools find single, relational tables to match or join with a search object. Yet many data science applications revolve around hierarchical data, which can only be matched by creating views that simultaneously join and transform several tables in the data lake. In this paper, we extend the Juneau data lake search system [46] for this broader class of matches at scale. Our contribution is a general framework for efficiently merging ranked results to match hierarchical data, leveraging novel techniques for indexing and sketching, and incorporating existing single-table search techniques and ranking functions. We experimentally validate our methods' benefits and broad applicability using real data from data science computational notebooks. Our results indicate that, with different ranking functions, our approach can return the optimal set of views up to 4.8x faster and 43\% more related compared to heuristics, and increase the data domain coverage by up to 28\%. In a case study to show the utility of our results to data science downstream tasks, we reduce regression error by up to 6.6\%, and improve classification accuracy by up to 19.5\%.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3346–3359},
numpages = {14}
}

@article{10.14778/3681954.3682013,
author = {Hansert, Patrick and Michel, Sebastian},
title = {Partition, Don't Sort! Compression Boosters for Cloud Data Ingestion Pipelines},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682013},
doi = {10.14778/3681954.3682013},
abstract = {Data Lakes deployed in the cloud are a go-to solution for enterprise data storage. While the pay-as-you-go cost model allows flexible resource allocation and billing, it mandates an efficient use of resources like CPU hours, network traffic, and used storage. The distributed nature of cloud environments necessitates partitioning the data and processing these partitions separately. In this work, we put forward a practical solution to improve the efficiency of compression algorithms on Dremel-encoded data by clustering similarly structured nested data at ingestion time, such that compressible partitions can be created. We propose a clustering approach inspired by decision trees that outpaces even the naive partition-then-sort approach by up to factor 17.44 while also boosting the compression by up to factor 2. We further show that when sorting the individual buckets, a compression boost that is competitive with the well-established increasing-cardinality heuristic can be achieved, but at a lower ingestion time.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3456–3469},
numpages = {14}
}

@article{10.14778/3681954.3682014,
author = {Erfanian, Mahdi and Jagadish, H. V. and Asudeh, Abolfazl},
title = {Chameleon: Foundation Models for Fairness-Aware Multi-Modal Data Augmentation to Enhance Coverage of Minorities},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682014},
doi = {10.14778/3681954.3682014},
abstract = {Potential harms from the under-representation of minorities in data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge.With recent generative AI advancements, large language and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a dataset with minimal addition of synthetically generated tuples to enhance the coverage of the under-represented groups. Our system applies quality and outlier-detection tests to ensure the quality and semantic integrity of the generated tuples. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies to provide a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our proposed algorithms, illustrate our approach's effectiveness, as the model's unfairness in a downstream task significantly dropped after data repair using Chameleon.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3470–3483},
numpages = {14}
}

@article{10.14778/3681954.3682016,
author = {Mohr-Daurat, Hubert and Theodorakis, Georgios and Pirk, Holger},
title = {Hardware-Efficient Data Imputation through DBMS Extensibility},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682016},
doi = {10.14778/3681954.3682016},
abstract = {The separation of data and code/queries has served Data Management Systems (DBMSs) well for decades. However, while the resulting soundness and rigidity are the basis for many performance-oriented optimizations, it lacks the flexibility to efficiently support modern data science applications: data cleansing, data ingestion/augmentation or generative models. To support such applications without sacrificing performance, we propose a new logical data model called Homoiconic Collection Processing (HCP). HCP is based on a well-known Meta-Programming concept called Homoiconicity (a unified representation for code and data).In a DBMS, HCP supports the storage of "classic" relational data but also allows the storage and evaluation of code fragments we refer to as "Homoiconic Expressions". Homoiconic Expressions enable applications such as data imputation directly in the database kernel. Implemented na\"{\i}vely, such flexibility would come at a prohibitive cost in terms of performance. To make HCP performance-competitive with highly-tuned in-memory DBMSs, we develop a novel storage and processing model called Shape-Wise Microbatching (SWM) and implement it in a system called BOSS. BOSS is performance-competitive with high-performance DBMSs while offering unprecedented extensibility. To demonstrate the extensibility, we implement an extension for impute-and-query workloads: BOSS outperforms state-of-the-art homoiconic runtimes and data imputation systems by two to five orders of magnitude.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3497–3510},
numpages = {14}
}

@article{10.14778/3681954.3682022,
author = {Wang, Zuozhi and Huang, Yicong and Ni, Shengquan and Kumar, Avinash and Alsudais, Sadeem and Liu, Xiaozhen and Lin, Xinyuan and Ding, Yunyan and Li, Chen},
title = {Texera: A System for Collaborative and Interactive Data Analytics Using Workflows},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682022},
doi = {10.14778/3681954.3682022},
abstract = {Domain experts play an important role in data science, as their knowledge can unlock valuable insights from data. As they often lack technical skills required to analyze data, they need collaborations with technical experts. In these joint efforts, productive collaborations are critical not only in the phase of constructing a data science task, but more importantly, during the execution of a task. This need stems from the inherent complexity of data science, which often involves user-defined functions or machine-learning operations. Consequently, collaborators want various interactions during runtime, such as pausing/resuming the execution, inspecting an operator's state, and modifying an operator's logic. To achieve the goal, in the past few years we have been developing an open-source system called Texera to support collaborative data analytics using GUI-based workflows as cloud services. In this paper, we present a holistic view of several important design principles we followed in the design and implementation of the system. We focus on different methods of sending messages to running workers, how these methods are adopted to support various runtime interactions from users, and their trade-offs on both performance and consistency. These principles enable Texera to provide powerful user interactions during a workflow execution to facilitate efficient collaborations in data analytics.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3580–3588},
numpages = {9}
}

@article{10.14778/3681954.3682026,
author = {Yu, Geoffrey X. and Wu, Ziniu and Kossmann, Ferdi and Li, Tianyu and Markakis, Markos and Ngom, Amadou and Madden, Samuel and Kraska, Tim},
title = {Blueprinting the Cloud: Unifying and Automatically Optimizing Cloud Data Infrastructures with BRAD},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682026},
doi = {10.14778/3681954.3682026},
abstract = {Modern organizations manage their data with a wide variety of specialized cloud database engines (e.g., Aurora, BigQuery, etc.). However, designing and managing such infrastructures is hard. Developers must consider many possible designs with non-obvious performance consequences; moreover, current software abstractions tightly couple applications to specific systems (e.g., with engine-specific clients), making it difficult to change after initial deployment. A better solution would virtualize cloud data management, allowing developers to declaratively specify their workload requirements and rely on automated solutions to design and manage the physical realization. In this paper, we present a technique called blueprint planning that achieves this vision. The key idea is to project data infrastructure design decisions into a unified design space (blueprints). We then systematically search over candidate blueprints using cost-based optimization, leveraging learned models to predict the utility of a blueprint on the workload. We use this technique to build BRAD, the first cloud data virtualization system. BRAD users issue queries to a single SQL interface that can be backed by multiple cloud database services. BRAD automatically selects the most suitable engine for each query, provisions and manages resources to minimize costs, and evolves the infrastructure to adapt to workload shifts. Our evaluation shows that BRAD meet user-defined performance targets and improve cost-savings by 1.6--13\texttimes{} compared to serverless auto-scaling or HTAP systems.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3629–3643},
numpages = {15}
}

@article{10.14778/3685800.3685808,
author = {Paduroiu, Andrei and Wi, Sungheun and Yan, Yan and Burd, Roni and Farchtchi, Ruhollah and Fumarola, Giovanni Matteo},
title = {Membrane - Safe and Performant Data Access Controls in Apache Spark in the Presence of Imperative Code},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685808},
doi = {10.14778/3685800.3685808},
abstract = {Data Governance is an increasingly critical feature of modern cloud database systems, enabling administrators to set granular access policies on their data. AWS customers want to define row or column filtering on their blob storage data and access it using popular tools such as Apache Spark. AWS EMR provides a managed and serverless solution that lets users run Spark jobs in the AWS cloud with imperative and declarative programming against their data, while securely enforcing the fine-grained access controls defined on those datasets. Spark runs its compiler and scheduler alongside the user application and embeds user-defined functions in query plans, giving a threat actor direct access to its memory space. This introduces attack vectors such as information disclosure or privilege escalation during policy enforcement, in addition to well-researched threats such as SQL side channel attacks. In this paper, we present Membrane: a novel approach to secure query plans with declarative and imperative code. The innovation comes from splitting the Spark driver in two in order to rewrite query plans with security boundaries while avoiding traditional tradeoffs when using container isolation techniques. The approach described herein enables applying fine grained data access controls to both SQL and map-reduce Spark jobs, with negligible performance and cost differences.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {3813–3826},
numpages = {14}
}

@article{10.14778/3685800.3685810,
author = {Yi, Peng and Liang, Lei and Zhang, Da and Chen, Yong and Zhu, Jinye and Liu, Xiangyu and Tang, Kun and Chen, Jialin and Lin, Hao and Qiu, Leijie and Zhou, Jun},
title = {KGFabric: A Scalable Knowledge Graph Warehouse for Enterprise Data Interconnection},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685810},
doi = {10.14778/3685800.3685810},
abstract = {Based on the diversified application scenarios at Ant Group, we built the Ant Knowledge Graph Platform (AKGP). It has constructed numerous domain-specific knowledge graphs related to merchants, companies, accounts, products, and more. AKGP manages trillions of structured knowledge graphs, serving search, recommendation, risk control and other businesses. However, as the demand increasing for various workloads such as graph pattern matching, graph representation learning, and cross-domain knowledge reuse, the existing warehouse systems based on relational DBMS or graph databases are unable to meet the requirements. To address these issues, we propose KGFabric, an industrial-scale knowledge graph management system built on the distributed file system (DFS). KGFabric offers a nearline knowledge storage engine that utilizes a Semantic-enhanced Programmable Graph (SPG) model, which is compatible with the Labeled Property Graph (LPG) model. The data is persistently stored in DFS, such as HDFS, which leverages the POSIX file system API, making it suitable for deployment in multi-cloud environment at low cost. KGFabric provides a native graph-based and hybrid storage format that can serve as a shared backend for parallel graph computing systems, significantly accelerating the analysis of multi-workload. Additionally, KGFabric includes a graph fabric framework that minimizes data duplication and guarantees data security.KGFabric is able to manage Peta-scale data and has supported graph fabric and analysis with over 100 billion relations at Ant Group. We conduct experiments on various datasets to evaluate the performance of KGFabric. Compared with popular relational DBMS and graph databases, the storage space for semantic relations is reduced by over 90\%. The performance of graph fabric improves by 21\texttimes{} in real-world workloads. In multi-hop semantic graph analysis, KGFabric enhances performance by 100\texttimes{}.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {3841–3854},
numpages = {14}
}

@article{10.14778/3685800.3685817,
author = {Zhang, Xinchun and Kashaf, Aqsa and Zou, Yihan and Zhang, Wei and Liao, Weibo and Song, Haoxiang and Ye, Jintao and Li, Yakun and Shi, Rui and Tian, Yong and Feng, Wei and Chen, Binbin and Chen, Zuzhi and Zhang, Tieying and Tang, Yongping},
title = {ResLake: Towards Minimum Job Latency and Balanced Resource Utilization in Geo-Distributed Job Scheduling},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685817},
doi = {10.14778/3685800.3685817},
abstract = {At internet scale companies like ByteDance, data is generated and consumed at enormously high speed by many different applications. Achieving low latency on such big data jobs is an important problem. However, the naive approach of aggregating all the data required by a job to a single location is not always feasible in a geo-distributed environment. Similarly, existing approaches in geo-distributed job scheduling often try to minimize WAN usage, which may come at the cost of latency. Another crucial element to ensure low latency is resource load balancing among DCs, which enables flexibility in job scheduling and avoids resource bottlenecks. Therefore, to minimize latency, optimizing job completion time (JCT) while maintaining resource utilization balance is important. To this end, we propose ResLake, a global scheduling platform for data-intensive workloads. ResLake aims to reduce JCT of geo-distributed applications while balancing the compute (CPU/Memory) and storage (Disk) usages across DCs and efficiently using WAN interconnections. We have deployed ResLake in ByteDance's production for over 1.5 years. ResLake has scheduled billions of jobs since its deployment. We find that ResLake improves JCT of jobs by at least 20\%, and can improve resource utilization balance across DCs by up to 53\%.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {3934–3946},
numpages = {13}
}

@article{10.14778/3685800.3685834,
author = {Okolnychyi, Anton and Sun, Chao and Tanimura, Kazuyuki and Spitzer, Russell and Blue, Ryan and Ho, Szehon and Gu, Yufei and Lakkundi, Vishwanath and Tsai, DB},
title = {Petabyte-Scale Row-Level Operations in Data Lakehouses},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685834},
doi = {10.14778/3685800.3685834},
abstract = {Data lakehouses combine the almost infinite scale and diverse tooling of a data lake with the reliability and functionality of a data warehouse. This paper presents extensions that enhance data lake-houses using Apache Iceberg and Apache Spark with performant petabyte-scale row-level operations. The framework is capable of handling both high-density and sparse modifications by either materializing changes at the file level during writes or producing equality and position deletes that are lazily merged with existing data during reads. The paper also outlines essential improvements in determining and applying row-level changes: eliminating expensive shuffles with storage-partitioned joins, minimizing write amplification with runtime filtering, and optimizing the layout of output data with adaptive writes. Our evaluation demonstrates the relative strengths and weaknesses of the various materialization strategies, highlighting the use cases that require each technique. We also show an order of magnitude improvement in performance after our enhancements.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4159–4172},
numpages = {14}
}

@article{10.14778/3685800.3685841,
author = {Roy, Senjuti Basu and Schieber, Baruch and Talmon, Nimrod},
title = {Fairness in Preference Queries: Social Choice Theories Meet Data Management},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685841},
doi = {10.14778/3685800.3685841},
abstract = {Given a large number (notationally m) of users' (members or voters) preferences as inputs over a large number of items or candidates (notationally n), preference queries leverage different preference aggregation methods to aggregate individual preferences in a systematic manner and come up with a single output (either a complete order or top-k, ordered or unordered) that is most representative of the users' preferences. The goal of this 1.5 hour lecture style tutorial is to adapt different preference aggregation methods from social choice theories, summarize how existing research has handled fairness over these methods, identify their limitations, and outline new research directions.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4225–4228},
numpages = {4}
}

@article{10.14778/3685800.3685847,
author = {Pedreira, Pedro and Majeti, Deepak and Erling, Orri},
title = {Composable Data Management: An Execution Overview},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685847},
doi = {10.14778/3685800.3685847},
abstract = {The trend of decomposing monolithic data management systems into a stack of reusable components has quickly gained momentum across the industry. Although a series of open-source projects have emerged targeting different layers of the stack, execution engines are of special importance due to the complexity they encapsulate, and the demand to optimize price-performance. In this tutorial, we will survey the space of composability in data management, focusing on the execution layer. We will discuss the main APIs, integration with existing and novel data management systems, and how specialized behavior can be accommodated by using extensibility APIs. With an emphasis on analytics, we will take a deeper dive into performance, discussing modern aspects of vectorization, compressed (encoding-aware) execution, and adaptivity. While the presentation is contextualized using real-world examples and experience while developing the Velox open-source execution engine and integrations with existing systems like Presto (Prestissimo) and Spark (Gluten), the concepts and techniques discussed are generally applicable to other execution engines. Finally, we will discuss future trends and ongoing work regarding novel file formats, compressed execution opportunities, and nascent hardware acceleration efforts, highlighting current challenges and open questions. With a survey of the state-of-the-art in this space, we hope this tutorial will help motivate individuals and organizations to embrace composability and promote collaborations across related projects.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4249–4252},
numpages = {4}
}

@article{10.14778/3685800.3685879,
author = {Ding, Xiaoou and Song, Yichen and Wang, Hongzhi and Yang, Donghua and Wang, Chen and Wang, Jianmin},
title = {Clean4TSDB: A Data Cleaning Tool for Time Series Databases},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685879},
doi = {10.14778/3685800.3685879},
abstract = {Billions of data points are generated by devices equipped with thousands of sensors, leading to significant data quality issues in time series data. These errors not only complicate time series data management but also compromise the accuracy and reliability of analysis based on such data. Given the noteworthy characteristics of time series data, existing cleaning methods struggle to provide adequate repairs, and tools supporting expressive constraints for time series remain scarce. To address this, we develop Clean4TSDB, a specialized data cleaning system for time series databases. This system integrates three key modules: expressive data quality constraint discovery, violation detection, and multivariate time series repairing, forming a comprehensive "profiling-detection-repair" workflow. Technically, we introduce TSDD, a data quality constraint that effectively captures contextual relationships within multivariate time series, and implement an efficient algorithm for its automated mining. Leveraging both row- and column-based constraints, we propose an effective time series cleaning algorithm. From a system standpoint, Clean4TSDB is pre-configured for seamless integration with time series databases like Apache IoTDB. Using user-provided and algorithmically-mined constraints, it effectively identifies various error patterns and offers reliable cleaning solutions. Furthermore, we establish a comprehensive library of state-of-the-art time series repair algorithms to meet the diverse needs of different management scenarios.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4377–4380},
numpages = {4}
}

@article{10.14778/3685800.3685897,
author = {van Renen, Alexander and Stoian, Mihail and Kipf, Andreas},
title = {DataLoom: Simplifying Data Loading with LLMs},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685897},
doi = {10.14778/3685800.3685897},
abstract = {Schema discovery and data loading is a crucial step in any data analysis pipeline. While this used to be a rare task, in the highly dynamic field of machine learning and modern business intelligence on top of data lakes, today it has become a frequent, but often underestimated, activity. Existing tools often focus on single files, presume prior knowledge of the data on the user's side or a significant amount of manual labor.In this paper, we improve the process of mapping a "chaotic" set of files to an initial database schema that can then be iteratively refined and loaded. The idea is to take the previously tedious parts of this process and automate them through the use of Large Language Models (LLMs) while leaving already well-understood problems such as constraint discovery to existing algorithms. We thus carefully orchestrate the use of LLMs for the "soft" problems and traditional algorithms for the "hard" problems. This creates a more seamless schema discovery and data loading experience that minimizes the time to first insight for users. We show this vision on modern schema discovery and data loading in our web-based prototype called DataLoom that serves as our demonstration.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4449–4452},
numpages = {4}
}

@article{10.14778/3685800.3685901,
author = {Gao, Chang and Zhang, Tianlong and Zeng, Yuxiang and Xu, Yi and Li, Shuyuan and Zhang, Yuanyuan},
title = {Swift: A Data-Driven Flight Planning System at Scale},
year = {2024},
issue_date = {August 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3685800.3685901},
doi = {10.14778/3685800.3685901},
abstract = {Flight planning, a pivotal challenge in the airline industry, strives to achieve economic and flexible scheduling of airplanes to serve designated flight itineraries. As the demand for air transportation soars, traditional planning methods can be inefficient in managing large-scale flights. Thus, we introduce Swift, a data-driven system tailored to enhance the scalability and effectiveness of flight planning. Swift primarily employs the bipartite graph model to derive optimal and economic flight plans for airlines. Our method not only minimizes the number of required planes but also ensures a balanced workload across these planes. Furthermore, Swift offers the capability of dynamic updates to flight plans in response to unexpected incidents at airports, such as bad weather conditions. Besides, Swift incorporates other functionalities like predicting future flight demand and monitoring real-time flight trajectories. Conference participants can interact with this system and explore our flight planning solution in real-world scenarios.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {4465–4468},
numpages = {4}
}

@article{10.1613/jair.1.13353,
author = {Pakzad, Atefe and Analoui, Morteza},
title = {A Word Selection Method for Producing Interpretable Distributional Semantic Word Vectors },
year = {2022},
issue_date = {Jan 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {72},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13353},
doi = {10.1613/jair.1.13353},
abstract = {Distributional semantic models represent the meaning of words as vectors. We introduce a selection method to learn a vector space that each of its dimensions is a natural word. The selection method starts from the most frequent words and selects a subset, which has the best performance. The method produces a vector space that each of its dimensions is a word. This is the main advantage of the method compared to fusion methods such as NMF, and neural embedding models. We apply the method to the ukWaC corpus and train a vector space of N=1500 basis words. We report tests results on word similarity tasks for MEN, RG-65, SimLex-999, and WordSim353 gold datasets. Also, results show that reducing the number of basis vectors from 5000 to 1500 reduces accuracy by about 1.5-2\%. So, we achieve good interpretability without a large penalty. Interpretability evaluation results indicate that the word vectors obtained by the proposed method using N=1500 are more interpretable than word embedding models, and the baseline method. We report the top 15 words of 1500 selected basis words in this paper.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {1281–1305},
numpages = {25},
keywords = {distributional semantic vectors; basis vectors; basis words; interpretable; word selection method; projection function}
}

@article{10.1613/jair.1.15313,
author = {Roveri, Marco and Di Ciccio, Claudio and Di Francescomarino, Chiara and Ghidini, Chiara},
title = {Computing Unsatisfiable Cores for LTLf Specifications},
year = {2024},
issue_date = {Sep 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {80},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.15313},
doi = {10.1613/jair.1.15313},
abstract = {Linear-time temporal logic on finite traces (LTLf) is rapidly becoming a de-facto standard to produce specifications in many application domains (including planning, business process management, run-time monitoring, and reactive synthesis). Several studies have challenged the satisfiability problem thus far. In this paper, we focus instead on unsatisfiable LTLf specifications, with the objective of extracting the subset of formulae that cause inconsistencies within them, i.e., the unsatisfiable cores. We provide four algorithms to this end, which leverage the adaptation of a range of state-of-the-art algorithms to LTLf satisfiability checking. We implement those algorithms extending the respective implementations and carry out an experimental evaluation on a set of reference benchmarks, restricting to the unsatisfiable specifications. The results put in evidence that the different algorithms and tools exhibit complementary features determining their efficiency and efficacy. Indeed, our findings suggest exploring different strategies and algorithmic solutions for the extraction of unsatisfiable cores from LTLf specifications, thus confirming the challenging and multi-faceted nature of this problem.},
journal = {J. Artif. Int. Res.},
month = jun,
numpages = {42}
}

@article{10.1613/jair.1.15956,
author = {Schidler, Andre and Szeider, Stefan},
title = {SAT-based Decision Tree Learning for Large Data Sets},
year = {2024},
issue_date = {Sep 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {80},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.15956},
doi = {10.1613/jair.1.15956},
abstract = {Decision trees of low depth are beneficial for understanding and interpreting the data they represent. Unfortunately, finding a decision tree of lowest complexity (depth or size) that correctly represents given data is NP-hard. Hence known algorithms either (i) utilize heuristics that do not minimize the depth or (ii) are exact but scale only to small or medium-sized instances. We propose a new hybrid approach to decision tree learning, combining heuristic and exact methods in a novel way. More specifically, we employ SAT encodings repeatedly to local parts of a decision tree provided by a standard heuristic, leading to an overall reduction in complexity. This allows us to scale the power of exact SAT-based methods to comparatively very large data sets. We evaluate our new approach experimentally on a range of real-world instances that contain up to several thousand samples. In almost all cases, our method successfully decreases the complexity of the initial decision tree; often, the decrease is significant.},
journal = {J. Artif. Int. Res.},
month = jul,
numpages = {44}
}

@article{10.1613/jair.1.15985,
author = {Yang, Yilin and Adamczewski, Kamil and Li, Xiaoxiao and Sutherland, Danica J. and Park, Mijung},
title = {Differentially Private Neural Tangent Kernels (DP-NTK) for Privacy-Preserving Data Generation},
year = {2024},
issue_date = {Jan 2025},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {81},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.15985},
doi = {10.1613/jair.1.15985},
abstract = {Maximum mean discrepancy (MMD) is a particularly useful distance metric for differentially private data generation: when used with finite-dimensional features, it allows us to summarize and privatize the data distribution once, which we can repeatedly use during generator training without further privacy loss. An important question in this framework is, then, what features are useful to distinguish between real and synthetic data distributions, and whether those enable us to generate quality synthetic data. This work considers using the features of neural tangent kernels (NTKs), more precisely empirical NTKs (e-NTKs). We find that, perhaps surprisingly, the expressiveness of the untrained e-NTK features is comparable to that of the features taken from pre-trained perceptual features using public data. As a result, our method improves the privacy-accuracy trade-off compared to other state-of-the-art methods, without relying on any public data, as demonstrated on several tabular and image benchmark datasets.},
journal = {J. Artif. Int. Res.},
month = nov,
numpages = {18}
}

@article{10.5555/2503308.2503347,
author = {Lui, Yui Man},
title = {Human gesture recognition on product manifolds},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classification is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {3297–3321},
numpages = {25},
keywords = {Grassmann manifolds, action recognition, gesture recognition, kinect data, one-shot-learning, product manifolds}
}

@article{10.5555/3007225.3007256,
author = {Burgess, Scott and Page, Brian},
title = {Cuda programming in the core curriculum: a preliminary study},
year = {2016},
issue_date = {October 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {32},
number = {1},
issn = {1937-4771},
abstract = {Educators often struggle with student indifference to systems courses such as architecture and operating systems. Parallel computing skills are increasingly important to nurture in young computer scientists, but often are only available in advanced courses. This project melds the excitement of parallel programming with core architecture and operating systems courses. We attempt to increase student engagement and improve the presentation of core systems concepts through the design and implementation of CUDA programming laboratory exercises. We describe the exercises and analyze the effectiveness of the new pedagogical materials.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {155–161},
numpages = {7}
}

@article{10.5555/3122009.3176854,
author = {Lin, Lin and Li, Jia},
title = {Clustering with hidden Markov model on variable blocks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3913–3961},
numpages = {49},
keywords = {Gaussian mixture model, hidden Markov model, modal Baum-Welch algorithm, modal clustering}
}

@article{10.5555/3122009.3242083,
author = {Athreya, Avanti and Fishkind, Donniell E. and Tang, Minh and Priebe, Carey E. and Park, Youngser and Vogelstein, Joshua T. and Levin, Keith and Lyzinski, Vince and Qin, Yichen},
title = {Statistical inference on random dot product graphs: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {8393–8484},
numpages = {92},
keywords = {adjacency spectral embedding, laplacian spectral embedding, multi-sample graph hypothesis testing, random dot product graph, semiparametric modeling}
}

@article{10.5555/3455716.3455842,
author = {Guo, Xin and Hu, Ting and Wu, Qiang},
title = {Distributed minimum error entropy algorithms},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with su_cient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an ination of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {126},
numpages = {31},
keywords = {information theoretic learning, minimum error entropy, distributed method, semi-supervised data, reproducing kernel Hilbert space}
}

@article{10.5555/3586589.3586890,
author = {Cai, T. Tony and Ma, Rong},
title = {Theoretical foundations of t-SNE for visualizing high-dimensional clustered data},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {This paper investigates the theoretical foundations of the t-distributed stochastic neighbor embedding (t-SNE) algorithm, a popular nonlinear dimension reduction and data visualization method. A novel theoretical framework for the analysis of t-SNE based on the gradient descent approach is presented. For the early exaggeration stage of t-SNE, we show its asymptotic equivalence to power iterations based on the underlying graph Laplacian, characterize its limiting behavior, and uncover its deep connection to Laplacian spectral clustering, and fundamental principles including early stopping as implicit regularization. The results explain the intrinsic mechanism and the empirical benefits of such a computational strategy. For the embedding stage of t-SNE, we characterize the kinematics of the low-dimensional map throughout the iterations, and identify an amplification phase, featuring the intercluster repulsion and the expansive behavior of the low-dimensional map, and a stabilization phase. The general theory explains the fast convergence rate and the exceptional empirical performance of t-SNE for visualizing clustered data, brings forth the interpretations of the t-SNE visualizations, and provides theoretical guidance for applying t-SNE and selecting its tuning parameters in various applications.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {301},
numpages = {54},
keywords = {clustering, data visualization, foundation of data science, nonlinear dimension reduction, t-SNE}
}

@article{10.5555/3648699.3648714,
author = {Vadillo, Jon and Santana, Roberto and Lozano, Jose A.},
title = {Extending adversarial attacks to produce adversarial class probability distributions},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Despite the remarkable performance and generalization levels of deep learning models in a wide range of artificial intelligence tasks, it has been demonstrated that these models can be easily fooled by the addition of imperceptible yet malicious perturbations to natural inputs. These altered inputs are known in the literature as adversarial examples. In this paper, we propose a novel probabilistic framework to generalize and extend adversarial attacks in order to produce a desired probability distribution for the classes when we apply the attack method to a large number of inputs. This novel attack paradigm provides the adversary with greater control over the target model, thereby exposing, in a wide range of scenarios, threats against deep learning models that cannot be conducted by the conventional paradigms. We introduce four different strategies to efficiently generate such attacks, and illustrate our approach by extending multiple adversarial attack algorithms. We also experimentally validate our approach for the spoken command classification task and the Tweet emotion classification task, two exemplary machine learning problems in the audio and text domain, respectively. Our results demonstrate that we can closely approximate any probability distribution for the classes while maintaining a high fooling rate and even prevent the attacks from being detected by label-shift detection methods.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {15},
numpages = {42},
keywords = {adversarial examples, deep neural networks, robust classification, class probability distributions, linear programming}
}

@article{10.5555/3648699.3648722,
author = {Weinstein, Eli N. and Miller, Jeffrey W.},
title = {Bayesian data selection},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Insights into complex, high-dimensional data can be obtained by discovering features of the data that match or do not match a model of interest. To formalize this task, we introduce the "data selection" problem: finding a lower-dimensional statistic--such as a subset of variables--that is well fit by a given parametric model of interest. A fully Bayesian approach to data selection would be to parametrically model the value of the statistic, nonparametrically model the remaining "background" components of the data, and perform standard Bayesian model selection for the choice of statistic. However, fitting a nonparametric model to high-dimensional data tends to be highly inefficient, statistically and computationally. We propose a novel score for performing data selection, the "Stein volume criterion (SVC)", that does not require fitting a nonparametric model. The SVC takes the form of a generalized marginal likelihood with a kernelized Stein discrepancy in place of the Kullback-Leibler divergence. We prove that the SVC is consistent for data selection, and establish consistency and asymptotic normality of the corresponding generalized posterior on parameters. We apply the SVC to the analysis of single-cell RNA sequencing data sets using probabilistic principal components analysis and a spin glass model of gene regulation.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {23},
numpages = {72},
keywords = {Bayesian nonparametrics, Bayesian theory, consistency, misspecification, Stein discrepancy}
}

@article{10.5555/3648699.3648787,
author = {Gu, Yuqi and Erosheva, Elena A. and Xu, Gongjun and Dunson, David B.},
title = {Dimension-grouped mixed membership models for multivariate categorical data},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Mixed Membership Models (MMMs) are a popular family of latent structure models for complex multivariate data. Instead of forcing each subject to belong to a single cluster, MMMs incorporate a vector of subject-specific weights characterizing partial membership across clusters. With this exibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In this article, we propose a new class of Dimension-Grouped MMMs (Gro-M3s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M3s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identifiability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically confirm the identifiability results. We illustrate the new methodology through applications to a functional disability survey dataset and a personality test dataset.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {88},
numpages = {49},
keywords = {Bayesian methods, grade of membership model, identifiability, mixed membership model, multivariate categorical data, probabilistic tensor decomposition}
}

@article{10.5555/3648699.3649047,
author = {Sesia, Matteo and Favaro, Stefano and Dobriban, Edgar},
title = {Conformal frequency estimation using discrete sketched data with coverage for distinct queries},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {This paper develops conformal inference methods to construct a confidence interval for the frequency of a queried object in a very large discrete data set, based on a sketch with a lower memory footprint. This approach requires no knowledge of the data distribution and can be combined with any sketching algorithm, including but not limited to the renowned count-min sketch, the count-sketch, and variations thereof. After explaining how to achieve marginal coverage for exchangeable random queries, we extend our solution to provide stronger inferences that can account for the discreteness of the data and for heterogeneous query frequencies, increasing also robustness to possible distribution shifts. These results are facilitated by a novel conformal calibration technique that guarantees valid coverage for a large fraction of distinct random queries. Finally, we show our methods have improved empirical performance compared to existing frequentist and Bayesian alternatives in simulations as well as in examples of text and SARS-CoV-2 DNA data.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {348},
numpages = {80},
keywords = {conformal inference, discrete data, distribution shifts, sketching, uncertainty}
}
