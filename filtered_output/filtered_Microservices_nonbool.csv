key,title,authors,year,journal,abstract,doi
10.1109/TNET.2020.2973800,Vehicular-OBUs-As-On-Demand-Fogs: Resource and Context Aware Deployment of Containerized Micro-Services,"Sami, Hani; Mourad, Azzam; El-Hajj, Wassim",2020,IEEE/ACM Trans. Netw.,"Observing the headway in vehicular industry, new applications are developed demanding more resources. For instance, real-time vehicular applications require fast processing of the vast amount of generated data by vehicles in order to maintain service availability and reachability while driving. Fog devices are capable of bringing cloud intelligence near the edge, making them a suitable candidate to process vehicular requests. However, their location, processing power, and technology used to host and update services affect their availability and performance while considering the mobility patterns of vehicles. In this paper, we overcome the aforementioned limitations by taking advantage of the evolvement of On-Board Units, Kubeadm Clustering, Docker Containerization, and micro-services technologies. In this context, we propose an efficient resource and context aware approach for deploying containerized micro-services on on-demand fogs called Vehicular-OBUs-As-On-Demand-Fogs. Our proposed scheme embeds (1) a Kubeadm based approach for clustering OBUs and enabling on-demand micro-services deployment with the least costs and time using Docker containerization technology, (2) a hybrid multi-layered networking architecture to maintain reachability between the requesting user and available vehicular fog cluster, and (3) a vehicular multi-objective container placement model for producing efficient vehicles selection and services distribution. An Evolutionary Memetic Algorithm is elaborated to solve our vehicular container placement problem. Experiments and simulations demonstrate the relevance and efficiency of our approach compared to other recent techniques in the literature.",10.1109/TNET.2020.2973800
10.1109/TNET.2024.3366561,"SPRIGHT: High-Performance eBPF-Based Event-Driven, Shared-Memory Processing for Serverless Computing","Qi, Shixiong; Monis, Leslie; Zeng, Ziteng; Wang, Ian-Chin; Ramakrishnan, K. K.",2024,IEEE/ACM Trans. Netw.,"Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. ‘Cold-start’ latency is another deterrent. We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF’s socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$10times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; less CPU usage under realistic workloads. Additionally, eBPF benefits SPRIGHT, by replacing heavyweight serverless components, allowing us to keep functions ‘warm’ with negligible penalty. Our preliminary experimental results show that SPRIGHT achieves an order of magnitude improvement in throughput and latency compared to Knative, while substantially reducing CPU usage, and obviates the need for ‘cold-start’.",10.1109/TNET.2024.3366561
10.1109/TNET.2024.3394514,Zero+: Monitoring Large-Scale Cloud-Native Infrastructure Using One-Sided RDMA,"Song, Zhuo; Wu, Jiejian; Ma, Teng; Wang, Zhe; Kong, Linghe; Wen, Zhenzao; Li, Jingxuan; Lu, Yang; Yang, Yong; Ma, Tao; Liu, Zheng; Chen, Guihai",2024,IEEE/ACM Trans. Netw.,"Cloud services have shifted from monolithic designs to microservices running on cloud-native infrastructure with monitoring systems to ensure service level agreements (SLAs). However, traditional monitoring systems no longer meet the demands of cloud-native monitoring. In Alibaba’s “double eleven” shopping festival, it is observed that the monitor occupies resources of the monitored infrastructure and even disrupts services. In this paper, we propose a novel monitoring system named Zero+ for cloud-native monitoring. Zero+ achieves zero overhead in collecting raw metrics using one-sided remote direct memory access (RDMA) and remedies network congestion by adopting a receiver-driven flow control scheme. Zero+ also features a priority queue mechanism to meet different quality of service requirements and an efficient batch processing design to relieve CPU occupation. Zero+ has been deployed and evaluated in four different clusters with heterogeneous RDMA NIC devices and architectures in Alibaba Cloud. Results show that Zero+ achieves no CPU occupation at the monitored host and supports &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1sim 10k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$0.1sim 1s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; sampling interval using a single thread for network I/O. Zero+ significantly relieves the incast issue and maintains &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$80sim 95\%$ &lt;/tex-math&gt;&lt;/inline-formula&gt; of bandwidth utilization in several clusters when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts. Zero+ also ensures services with high priority accomplish collecting metrics earlier than low priority ones by at least &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$400 mu s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts.",10.1109/TNET.2024.3394514
10.1109/TNET.2024.3400953,DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Wang, Ziliang; Zhu, Shiyi; Li, Jianguo; Jiang, Wei; Ramakrishnan, K. K.; Yan, Meng; Zhang, Xiaohong; Liu, Alex X.",2024,IEEE/ACM Trans. Netw.,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",10.1109/TNET.2024.3400953
10.1145/3104028,Architectural Principles for Cloud Software,"Pahl, Claus; Jamshidi, Pooyan; Zimmermann, Olaf",2018,ACM Trans. Internet Technol.,"A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.",10.1145/3104028
10.1145/3226644,A Unified Model for the Mobile-Edge-Cloud Continuum,"Baresi, L.; Mendon\c{c}a, D. F.; Garriga, M.; Guinea, S.; Quattrocchi, G.",2019,ACM Trans. Internet Technol.,"Technologies such as mobile, edge, and cloud computing have the potential to form a computing continuum for new, disruptive applications. At runtime, applications can choose to execute parts of their logic on different infrastructures that constitute the continuum, with the goal of minimizing latency and battery consumption and maximizing availability. In this article, we propose A3-E, a unified model for managing the life cycle of continuum applications. In particular, A3-E exploits the Functions-as-a-Service model to bring computation to the continuum in the form of microservices. Furthermore, A3-E selects where to execute a certain function based on the specific context and user requirements. The article also presents a prototype framework that implements the concepts behind A3-E. Results show that A3-E is capable of dynamically deploying microservices and routing the application’s requests, reducing latency by up to 90\% when using edge instead of cloud resources, and battery consumption by 74\% when computation has been offloaded.",10.1145/3226644
10.1145/3293455,RESTful API Automated Test Case Generation with EvoMaster,"Arcuri, Andrea",2019,ACM Trans. Softw. Eng. Methodol.,"RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.",10.1145/3293455
10.1145/3381452,Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems,"Tsigkanos, Christos; Garriga, Martin; Baresi, Luciano; Ghezzi, Carlo",2020,ACM Trans. Internet Technol.,"Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.",10.1145/3381452
10.1145/3418899,A Black-box Monitoring Approach to Measure Microservices Runtime Performance,"Brondolin, Rolando; Santambrogio, Marco D.",2020,ACM Trans. Archit. Code Optim.,"Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.",10.1145/3418899
10.1145/3466696,Autonomic Security Management for IoT Smart Spaces,"Lin, Changyuan; Khazaei, Hamzeh; Walenstein, Andrew; Malton, Andrew",2021,ACM Trans. Internet Things,"Embedded sensors and smart devices have turned the environments around us into smart spaces that could automatically evolve, depending on the needs of users, and adapt to the new conditions. While smart spaces are beneficial and desired in many aspects, they could be compromised and expose privacy, security, or render the whole environment a hostile space in which regular tasks cannot be accomplished anymore. In fact, ensuring the security of smart spaces is a very challenging task due to the heterogeneity of devices, vast attack surface, and device resource limitations. The key objective of this study is to minimize the manual work in enforcing the security of smart spaces by leveraging the autonomic computing paradigm in the management of IoT environments. More specifically, we strive to build an autonomic manager that can monitor the smart space continuously, analyze the context, plan and execute countermeasures to maintain the desired level of security, and reduce liability and risks of security breaches. We follow the microservice architecture pattern and propose a generic ontology named Secure Smart Space Ontology (SSSO) for describing dynamic contextual information in security-enhanced smart spaces. Based on SSSO, we build an autonomic security manager with four layers that continuously monitors the managed spaces, analyzes contextual information and events, and automatically plans and implements adaptive security policies.As the evaluation, focusing on a current BlackBerry customer problem, we deployed the proposed autonomic security manager to maintain the security of a smart conference room with 32 devices and 66 services. The high performance of the proposed solution was also evaluated on a large-scale deployment with over 1.8 million triples.",10.1145/3466696
10.1145/3470658,Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability,"Avasalcai, Cosmin; Tsigkanos, Christos; Dustdar, Schahram",2021,ACM Trans. Internet Technol.,"Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.",10.1145/3470658
10.1145/3478680,"Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities","Siqueira, Frank; Davis, Joseph G.",2021,ACM Comput. Surv.,"Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.",10.1145/3478680
10.1145/3502724,Dynamic Evaluation of Microservice Granularity Adaptation,"Hassan, Sara; Bahsoon, Rami; Minku, Leandro; Ali, Nour",2022,ACM Trans. Auton. Adapt. Syst.,"Microservices have gained acceptance in software industries as an emerging architectural style for autonomic, scalable, and more reliable computing. Among the critical microservice architecture design decisions is when to adapt the granularity of a microservice architecture by merging/decomposing microservices. No existing work investigates the following question: How can we reason about the trade-off between predicted benefits and cost of pursuing microservice granularity adaptation under uncertainty? To address this question, we provide a novel formulation of the decision problem to pursue granularity adaptation as a real options problem. We propose a novel evaluation process for dynamically evaluating granularity adaptation design decisions under uncertainty. Our process is based on a novel combination of real options and the concept of Bayesian surprises. We show the benefits of our evaluation process by comparing it to four representative industrial microservice runtime monitoring tools, which can be used for retrospective evaluation for granularity adaptation decisions. Our comparison shows that our process can supersede and/or complement these tools. We implement a microservice application—Filmflix—using Amazon Web Service Lambda and use this implementation as a case study to show the unique benefit of our process compared to traditional application of real options analysis.",10.1145/3502724
10.1145/3508360,The Serverless Computing Survey: A Technical Primer for Design Architecture,"Li, Zijun; Guo, Linsong; Cheng, Jiagan; Chen, Quan; He, Bingsheng; Guo, Minyi",2022,ACM Comput. Surv.,"The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.",10.1145/3508360
10.1145/3532183,"Microservice Security Metrics for Secure Communication, Identity Management, and Observability","Zdun, Uwe; Queval, Pierre-Jean; Simhandl, Georg; Scandariato, Riccardo; Chakravarty, Somik; Jelic, Marjan; Jovanovic, Aleksandar",2023,ACM Trans. Softw. Eng. Methodol.,"Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems’ dataset through statistical methods.",10.1145/3532183
10.1145/3539606,"Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges","Carri\'{o}n, Carmen",2022,ACM Comput. Surv.,"Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.",10.1145/3539606
10.1145/3549539,Focused Layered Performance Modelling by Aggregation,"Islam, Farhana; Petriu, Dorina; Woodside, Murray",2022,ACM Trans. Model. Perform. Eval. Comput. Syst.,"Performance models of server systems, based on layered queues, may be very complex. This is particularly true for cloud-based systems based on microservices, which may have hundreds of distinct components, and for models derived by automated data analysis. Often only a few of these many components determine the system performance, and a smaller simplified model is all that is needed. To assist an analyst, this work describes a focused model that includes the important components (the focus) and aggregates the rest in groups, called dependency groups. The method Focus-based Simplification with Preservation of Tasks described here fills an important gap in a previous method by the same authors. The use of focused models for sensitivity predictions is evaluated empirically in the article on a large set of randomly generated models. It is found that the accuracy depends on a “saturation ratio” (SR) between the highest utilization value in the model and the highest value of a component excluded from the focus; evidence suggests that SR must be at least 2 and must be larger to evaluate larger model changes. This dependency was captured in an “Accurate Sensitivity Hypothesis” based on SR, which can be used to indicate trustable sensitivity results.",10.1145/3549539
10.1145/3583563,Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment,"Camilli, Matteo; Colarusso, Carmine; Russo, Barbara; Zimeo, Eugenio",2023,ACM Trans. Softw. Eng. Methodol.,"The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.",10.1145/3583563
10.1145/3585009,White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study,"Zhang, Man; Arcuri, Andrea; Li, Yonggang; Liu, Yang; Xue, Kaiming",2023,ACM Trans. Softw. Eng. Methodol.,"Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation.",10.1145/3585009
10.1145/3592598,Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions,"Pallewatta, Samodha; Kostakos, Vassilis; Buyya, Rajkumar",2023,ACM Comput. Surv.,"The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.",10.1145/3592598
10.1145/3611312,Strega: An HTTP Server for FPGAs,"Maschi, Fabio; Alonso, Gustavo",2024,ACM Trans. Reconfigurable Technol. Syst.,"The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega, an open source1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7&nbsp;M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.",10.1145/3611312
10.1145/3617507,SensiX++: Bringing MLOps and Multi-tenant Model Serving to Sensory Edge Devices,"Min, Chulhong; Mathur, Akhil; Acer, Utku G\""{u}nay; Montanari, Alessandro; Kawsar, Fahim",2023,ACM Trans. Embed. Comput. Syst.,"We present SensiX++, a multi-tenant runtime for adaptive model execution with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT sensors. SensiX++ operates on two fundamental principles: highly modular componentisation to externalise data operations with clear abstractions and document-centric manifestation for system-wide orchestration. First, a data coordinator manages the lifecycle of sensors and serves models with correct data through automated transformations. Next, a resource-aware model server executes multiple models in isolation through model abstraction, pipeline automation, and feature sharing. An adaptive scheduler then orchestrates the best-effort executions of multiple models across heterogeneous accelerators, balancing latency and throughput. Finally, microservices with REST APIs serve synthesised model predictions, system statistics, and continuous deployment. Collectively, these components enable SensiX++ to serve multiple models efficiently with fine-grained control on edge devices while minimising data operation redundancy, managing data and device heterogeneity, and reducing resource contention. We benchmark SensiX++ with 10 different vision and acoustics models across various multi-tenant configurations on different edge accelerators (Jetson AGX and Coral TPU) designed for sensory devices. We report on the overall throughput and quantified benefits of various automation components of SensiX++ and demonstrate its efficacy in significantly reducing operational complexity and lowering the effort to deploy, upgrade, reconfigure, and serve embedded models on edge devices.",10.1145/3617507
10.1145/3622787,MicroProf: Code-level Attribution of Unnecessary Data Transfer in Microservice Applications,"Tariq, Syed Salauddin Mohammad; Menard, Lance; Su, Pengfei; Roy, Probir",2023,ACM Trans. Archit. Code Optim.,"The microservice architecture style has gained popularity due to its ability to fault isolation, ease of scaling applications, and developer’s agility. However, writing applications in the microservice design style has its challenges. Due to the loosely coupled nature, services communicate with others through standard communication APIs. This incurs significant overhead in the application due to communication protocol and data transformation. An inefficient service communication at the microservice application logic can further overwhelm the application. We perform a grey literature review showing that unnecessary data transfer is a real challenge in the industry. To the best of our knowledge, no effective tool is currently available to accurately identify the origins of unnecessary microservice communications that lead to significant performance overhead and provide guidance for optimization.To bridge the knowledge gap, we propose MicroProf, a dynamic program analysis tool to detect unnecessary data transfer in Java-based microservice applications. At the implementation level, MicroProf proposes novel techniques such as remote object sampling and hardware debug registers to monitor remote object usage. MicroProf reports the unnecessary data transfer at the application source code level. Furthermore, MicroProf pinpoints the opportunities for communication API optimization. MicroProf is evaluated on four well-known applications involving two real-world applications and two benchmarks, identifying five inefficient remote invocations. Guided by MicroProf, API optimization achieves an 87.5\% reduction in the number of fields within REST API responses. The empirical evaluation further reveals that the optimized services experience a speedup of up to 4.59\texttimes{}.",10.1145/3622787
10.1145/3630006,Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing,"Zhao, Laiping; Cui, Yushuai; Yang, Yanan; Zhou, Xiaobo; Qiu, Tie; Li, Keqiu; Bao, Yungang",2024,ACM Trans. Comput. Syst.,"Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3\%, CPU utilization by 38.6\%, and memory bandwidth utilization by 45.4\% while guaranteeing the tail latency requirement.",10.1145/3630006
10.1145/3631607,Optimizing Resource Management for Shared Microservices: A Scalable System Design,"Luo, Shutian; Lin, Chenyu; Ye, Kejiang; Xu, Guoyao; Zhang, Liping; Yang, Guodong; Xu, Huanle; Xu, Chengzhong",2024,ACM Trans. Comput. Syst.,"A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements.We present an efficient resource management system, namely Erms, for guaranteeing SLAs with high probability in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5\texttimes{} and more importantly, lead to a reduction in resource usage by 1.6\texttimes{}, compared to state-of-the-art approaches.",10.1145/3631607
10.1145/3665223,"IMAGE: An Open-Source, Extensible Framework for Deploying Accessible Audio and Haptic Renderings of Web Graphics","Regimbal, Juliette; Blum, Jeffrey R.; Kuo, Cyan; Cooperstock, Jeremy R.",2024,ACM Trans. Access. Comput.,"For accessibility practitioners, creating and deploying novel multimedia interactions for people with disabilities is a nontrivial task. As a result, many projects aiming to support such accessibility needs come and go or never make it to a public release. To reduce the overhead involved in deploying and maintaining a system that transforms web content into multimodal renderings, we created an open source, modular microservices architecture as part of the IMAGE project. This project aims to design richer means of interacting with web graphics than is afforded by a screen reader and text descriptions alone. To benefit the community of accessibility software developers, we discuss this architecture and explain how it provides support for several multimodal processing pipelines. Beyond illustrating the initial use case that motivated this effort, we further describe two use cases outside the scope of our project to explain how a team could use the architecture to develop and deploy accessible solutions for their own work. We then discuss our team’s experience working with the IMAGE architecture, informed by discussions with six project members, and provide recommendations to other practitioners considering applying the framework to their own accessibility projects.",10.1145/3665223
10.1145/3674726,HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Zhu, Zhouruixing; Lee, Cheryl; Tang, Xiaoying; He, Pinjia",2024,ACM Trans. Softw. Eng. Methodol.,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",10.1145/3674726
10.1145/3674734,Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency,"Antoniou, Georgia; Bartolini, Davide; Volos, Haris; Kleanthous, Marios; Wang, Zhe; Kalaitzidis, Kleovoulos; Rollet, Tom; Li, Ziwei; Mutlu, Onur; Sazeides, Yiannakis; Haj Yahya, Jawad",2024,ACM Trans. Archit. Code Optim.,"Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70\% with limited performance degradation (at most 2\%).",10.1145/3674734
10.1145/3676167,Dynamically Balancing Load with Overload Control for Microservices,"Bhattacharya, Ratnadeep; Gao, Yuan; Wood, Timothy",2024,ACM Trans. Auton. Adapt. Syst.,"The microservices architecture simplifies application development by breaking monolithic applications into manageable microservices. However, this distributed microservice “service mesh” leads to new challenges due to the more complex application topology. Particularly, each service component scales up and down independently creating load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this article, we propose a self-managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes. BLOC uses overload control approaches to provide feedback to the load balancers. We show that this performs significantly better in solving the incast problem in microservice architectures. A critical component of BLOC is the dynamic capacity estimation algorithm. We show that a well-tuned capacity estimate can outperform even join-the-shortest-queue, a nearly optimal algorithm, while a reasonable dynamic estimate still outperforms Least Connection, a distributed implementation of join-the-shortest-queue. Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 –4 times and the tail, 99th percentile, latency by 2 times.",10.1145/3676167
10.1145/3687265,Performance Modeling of Distributed Data Processing in Microservice Applications,"Gao, Yicheng; Casale, Giuliano; Singhal, Rekha",2024,ACM Trans. Model. Perform. Eval. Comput. Syst.,"Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.",10.1145/3687265
10.1145/3687301,Data Mesh: A Systematic Gray Literature Review,"Goedegebuure, Abel; Kumara, Indika; Driessen, Stefan; Van Den Heuvel, Willem-Jan; Monsieur, Geert; Tamburri, Damian Andrew; Nucci, Dario Di",2024,ACM Comput. Surv.,"Data mesh is an emerging domain-driven decentralized data architecture that aims to minimize or avoid operational bottlenecks associated with centralized, monolithic data architectures in enterprises. The topic has piqued the practitioners’ interest, and considerable gray literature exists. At the same time, we observe a lack of academic attempts at defining and building upon the concept. Hence, in this article, we aim to start from the foundations and characterize the data mesh architecture regarding its design principles, architectural components, capabilities, and organizational roles. We systematically collected, analyzed, and synthesized 114 industrial gray literature articles. The resulting review provides insights into practitioners’ perspectives on the four key principles of data mesh: data as a product, domain ownership of data, self-serve data platform, and federated computational governance. Moreover, due to the comparability of data mesh and SOA (service-oriented architecture), we mapped the findings from the gray literature into the reference architectures from the SOA academic literature to create the reference architectures for describing three key dimensions of data mesh: organization of capabilities and roles, development, and runtime. Finally, we discuss open research issues in data mesh, partially based on the findings from the gray literature.",10.1145/3687301
10.1145/3691630,On the Understandability of Design-Level Security Practices in Infrastructure-as-Code Scripts and Deployment Architectures,"Ntentos, Evangelos; Lueger, Nicole Elisabeth; Simhandl, Georg; Zdun, Uwe; Schneider, Simon; Scandariato, Riccardo; Ferreyra, Nicol\'{a}s E. D\'{\i}az",2024,ACM Trans. Softw. Eng. Methodol.,"Infrastructure as Code (IaC) automates IT infrastructure deployment, which is particularly beneficial for continuous releases, for instance, in the context of microservices and cloud systems. Despite its flexibility in application architecture, neglecting security can lead to vulnerabilities. The lack of comprehensive architectural security guidelines for IaC poses challenges in adhering to best practices. We studied how developers interpret IaC scripts (source code) in two IaC technologies, Ansible and Terraform, compared to semi-formal IaC deployment architecture models and metrics regarding design-level security understanding. In a controlled experiment involving ninety-four participants, we assessed the understandability of IaC-based deployment architectures through source code inspection compared to semi-formal representations in models and metrics.We hypothesized that providing semi-formal IaC deployment architecture models and metrics as supplementary material would significantly improve the comprehension of IaC security-related practices, as measured by task correctness. Our findings suggest that semi-formal IaC deployment architecture models and metrics as supplementary material enhance the understandability of IaC security-related practices without significantly increasing duration. We also observed a significant correlation between task correctness and duration when models and metrics were provided.",10.1145/3691630
10.1145/3700436,The Tale of Errors in Microservices,"Lee, I-Ting Angelina; Zhang, Zhizhou; Parwal, Abhishek; Chabbi, Milind",2024,Proc. ACM Meas. Anal. Comput. Syst.,"Microservice architecture is the computing paradigm of choice for large, service-oriented software catering to real-time requests. Individual programs in such a system perform Remote Procedure Calls (RPCs) to other microservices to accomplish sub-tasks. Microservices are designed to be robust; top-level requests can succeed despite errors returned from RPC sub-tasks, referred to as non-fatal errors. Because of this design, the top-level microservices tend to ''live with'' non-fatal errors. Hence, a natural question to ask is ''how prevalent are non-fatal errors and what impact do they have on the exposed latency of top-level requests?''In this paper, we present a large-scale study of errors in microservices. We answer the aforementioned question by analyzing 11 Billion RPCs covering 1,900 user-facing endpoints at the Uber serving requests of hundreds of millions of active users. To assess the latency impact of non-fatal errors, we develop a methodology that projects potential latency savings for a given request as if the time spent on failing APIs were eliminated. This estimator allows ranking and bubbling up those APIs that are worthy of further investigations, where the non-fatal errors likely resulted in operational inefficiencies. Finally, we employ our error detection and impact estimation techniques to pinpoint operational inefficiencies, which a) result in a tail latency reduction of a critical endpoint by 30\% and b) offer insights into common inefficiency-introducing patterns.",10.1145/3700436
10.1145/3702978,Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features,"Gu, Wenwei; Liu, Jinyang; Chen, Zhuangbin; Zhang, Jianping; Su, Yuxin; Gu, Jiazhen; Feng, Cong; Yang, Zengyin; Yang, Yongqiang; Lyu, Michael R.",2024,ACM Trans. Softw. Eng. Methodol.,"Cloud systems, typically comprised of various components (e.g., microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e., key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.",10.1145/3702978
10.14778/3476311.3476360,Query-driven video event processing for the internet of multimedia things,"Yadav, Piyush; Salwala, Dhaval; Pontes, Felipe Arruda; Dhingra, Praneet; Curry, Edward",2021,Proc. VLDB Endow.,"Advances in Deep Neural Network (DNN) techniques have revolutionized video analytics and unlocked the potential for querying and mining video event patterns. This paper details GNOSIS, an event processing platform to perform near-real-time video event detection in a distributed setting. GNOSIS follows a serverless approach where its component acts as independent microservices and can be deployed at multiple nodes. GNOSIS uses a declarative query-driven approach where users can write customize queries for spatiotemporal video event reasoning. The system converts the incoming video streams into a continuous evolving graph stream using machine learning (ML) and DNN models pipeline and applies graph matching for video event pattern detection. GNOSIS can perform both stateful and stateless video event matching. To improve Quality of Service (QoS), recent work in GNOSIS incorporates optimization techniques like adaptive scheduling, energy efficiency, and content-driven windows. This paper demonstrates the Occupational Health and Safety query use cases to show the GNOSIS efficacy.",10.14778/3476311.3476360
10.5555/3381555.3381567,Design and development of an open private educational cloud storage solution for application development,"Pyatt, Kevin; Lotfy, Mohamed",2019,J. Comput. Sci. Coll.,"CS, IT, and software engineering students need to learn and master web application development and cloud computing skills on an educational full-stack Cloud architecture. In this paper we provide the design of an educational full-stack architecture integrating a private storage cloud. The focus was on how to integrate, configure, secure and deploy web-software applications, microservices and private storage within a full-stack architecture. In this research study we determined the feasibility of designing and developing an open private-cloud storage solution for cloud software. We also share the results of the deployment and testing of phases 0 and 1 of the educational full-stack architecture.",
10.5555/3575846.3575851,The structure and delivery of an advanced systems administration IT course,"Lotfy, Mohamed; Fredrickson, Christian",2022,J. Comput. Sci. Coll.,"IT program courses should allow students to acquire the needed IT skills to enable them to be job ready by graduation. To prepare the IT graduates to current and future practices of virtualized computing resources, which integrate IT systems and services, DevOps and microservices, systems administration courses need to introduce students to current tools used to administer and manage continuous integration and deployment of infrastructure and services. In this paper, we provide the structure, components, hands-on assignments, and the virtual environment of a senior-level competency-based advanced Unix/Linux systems administration course that is delivered face-to-face and online. The course introduces current organizational practices of continuous integration and delivery of services. Student course evaluation and what helped them learn the most are also presented and discussed.",
