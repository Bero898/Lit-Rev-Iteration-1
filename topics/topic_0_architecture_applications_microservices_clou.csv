text,Document Title,Abstract,source,bool,processed_text
"Vehicular-OBUs-As-On-Demand-Fogs: Resource and Context Aware Deployment of Containerized Micro-Services Observing the headway in vehicular industry, new applications are developed demanding more resources. For instance, real-time vehicular applications require fast processing of the vast amount of generated data by vehicles in order to maintain service availability and reachability while driving. Fog devices are capable of bringing cloud intelligence near the edge, making them a suitable candidate to process vehicular requests. However, their location, processing power, and technology used to host and update services affect their availability and performance while considering the mobility patterns of vehicles. In this paper, we overcome the aforementioned limitations by taking advantage of the evolvement of On-Board Units, Kubeadm Clustering, Docker Containerization, and micro-services technologies. In this context, we propose an efficient resource and context aware approach for deploying containerized micro-services on on-demand fogs called Vehicular-OBUs-As-On-Demand-Fogs. Our proposed scheme embeds (1) a Kubeadm based approach for clustering OBUs and enabling on-demand micro-services deployment with the least costs and time using Docker containerization technology, (2) a hybrid multi-layered networking architecture to maintain reachability between the requesting user and available vehicular fog cluster, and (3) a vehicular multi-objective container placement model for producing efficient vehicles selection and services distribution. An Evolutionary Memetic Algorithm is elaborated to solve our vehicular container placement problem. Experiments and simulations demonstrate the relevance and efficiency of our approach compared to other recent techniques in the literature.",Vehicular-OBUs-As-On-Demand-Fogs: Resource and Context Aware Deployment of Containerized Micro-Services,"Observing the headway in vehicular industry, new applications are developed demanding more resources. For instance, real-time vehicular applications require fast processing of the vast amount of generated data by vehicles in order to maintain service availability and reachability while driving. Fog devices are capable of bringing cloud intelligence near the edge, making them a suitable candidate to process vehicular requests. However, their location, processing power, and technology used to host and update services affect their availability and performance while considering the mobility patterns of vehicles. In this paper, we overcome the aforementioned limitations by taking advantage of the evolvement of On-Board Units, Kubeadm Clustering, Docker Containerization, and micro-services technologies. In this context, we propose an efficient resource and context aware approach for deploying containerized micro-services on on-demand fogs called Vehicular-OBUs-As-On-Demand-Fogs. Our proposed scheme embeds (1) a Kubeadm based approach for clustering OBUs and enabling on-demand micro-services deployment with the least costs and time using Docker containerization technology, (2) a hybrid multi-layered networking architecture to maintain reachability between the requesting user and available vehicular fog cluster, and (3) a vehicular multi-objective container placement model for producing efficient vehicles selection and services distribution. An Evolutionary Memetic Algorithm is elaborated to solve our vehicular container placement problem. Experiments and simulations demonstrate the relevance and efficiency of our approach compared to other recent techniques in the literature.",ACM,no,"['resource', 'context', 'aware', 'deployment', 'containerized', 'vehicular', 'industry', 'new', 'developed', 'resource', 'instance', 'vehicular', 'require', 'fast', 'processing', 'vast', 'amount', 'generated', 'vehicle', 'order', 'maintain', 'availability', 'driving', 'fog', 'device', 'capable', 'bringing', 'intelligence', 'near', 'edge', 'making', 'suitable', 'candidate', 'process', 'vehicular', 'request', 'however', 'location', 'processing', 'power', 'technology', 'used', 'host', 'update', 'affect', 'availability', 'performance', 'considering', 'mobility', 'pattern', 'vehicle', 'paper', 'overcome', 'aforementioned', 'limitation', 'taking', 'advantage', 'unit', 'clustering', 'docker', 'containerization', 'technology', 'context', 'propose', 'efficient', 'resource', 'context', 'aware', 'deploying', 'containerized', 'fog', 'called', 'proposed', 'scheme', 'based', 'clustering', 'enabling', 'deployment', 'least', 'cost', 'time', 'using', 'docker', 'containerization', 'technology', 'hybrid', 'networking', 'maintain', 'user', 'available', 'vehicular', 'fog', 'cluster', 'vehicular', 'container', 'placement', 'model', 'producing', 'efficient', 'vehicle', 'selection', 'distribution', 'evolutionary', 'algorithm', 'elaborated', 'solve', 'vehicular', 'container', 'placement', 'problem', 'experiment', 'simulation', 'demonstrate', 'relevance', 'efficiency', 'compared', 'recent', 'technique', 'literature']"
"SPRIGHT: High-Performance eBPF-Based Event-Driven, Shared-Memory Processing for Serverless Computing Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. ‘Cold-start’ latency is another deterrent. We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF’s socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$10times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; less CPU usage under realistic workloads. Additionally, eBPF benefits SPRIGHT, by replacing heavyweight serverless components, allowing us to keep functions ‘warm’ with negligible penalty. Our preliminary experimental results show that SPRIGHT achieves an order of magnitude improvement in throughput and latency compared to Knative, while substantially reducing CPU usage, and obviates the need for ‘cold-start’.","SPRIGHT: High-Performance eBPF-Based Event-Driven, Shared-Memory Processing for Serverless Computing","Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. ‘Cold-start’ latency is another deterrent. We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF’s socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$10times $ &lt;/tex-math&gt;&lt;/inline-formula&gt; less CPU usage under realistic workloads. Additionally, eBPF benefits SPRIGHT, by replacing heavyweight serverless components, allowing us to keep functions ‘warm’ with negligible penalty. Our preliminary experimental results show that SPRIGHT achieves an order of magnitude improvement in throughput and latency compared to Knative, while substantially reducing CPU usage, and obviates the need for ‘cold-start’.",ACM,no,"['spright', 'processing', 'serverless', 'computing', 'serverless', 'computing', 'promise', 'efficient', 'compute', 'capability', 'environment', 'however', 'existing', 'solution', 'platform', 'include', 'goal', 'serverless', 'computing', 'additionally', 'serverless', 'platform', 'lack', 'dataplane', 'optimization', 'achieve', 'efficient', 'function', 'chain', 'facilitate', 'popular', 'development', 'paradigm', 'use', 'complex', 'duplicate', 'capability', 'building', 'function', 'chain', 'performance', 'latency', 'another', 'describe', 'spright', 'lightweight', 'responsive', 'serverless', 'framework', 'spright', 'exploit', 'shared', 'memory', 'processing', 'dramatically', 'improves', 'scalability', 'dataplane', 'avoiding', 'unnecessary', 'protocol', 'processing', 'overhead', 'spright', 'extensively', 'leverage', 'processing', 'extended', 'berkeley', 'packet', 'filter', 'ebpf', 'use', 'ebpf', 'message', 'mechanism', 'support', 'shared', 'memory', 'processing', 'overhead', 'strictly', 'compared', 'spright', 'achieves', 'dataplane', 'performance', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'less', 'cpu', 'usage', 'realistic', 'workload', 'additionally', 'ebpf', 'benefit', 'spright', 'replacing', 'serverless', 'allowing', 'u', 'keep', 'function', 'preliminary', 'experimental', 'result', 'show', 'spright', 'achieves', 'order', 'magnitude', 'improvement', 'throughput', 'latency', 'compared', 'reducing', 'cpu', 'usage', 'need']"
"Zero+: Monitoring Large-Scale Cloud-Native Infrastructure Using One-Sided RDMA Cloud services have shifted from monolithic designs to microservices running on cloud-native infrastructure with monitoring systems to ensure service level agreements (SLAs). However, traditional monitoring systems no longer meet the demands of cloud-native monitoring. In Alibaba’s “double eleven” shopping festival, it is observed that the monitor occupies resources of the monitored infrastructure and even disrupts services. In this paper, we propose a novel monitoring system named Zero+ for cloud-native monitoring. Zero+ achieves zero overhead in collecting raw metrics using one-sided remote direct memory access (RDMA) and remedies network congestion by adopting a receiver-driven flow control scheme. Zero+ also features a priority queue mechanism to meet different quality of service requirements and an efficient batch processing design to relieve CPU occupation. Zero+ has been deployed and evaluated in four different clusters with heterogeneous RDMA NIC devices and architectures in Alibaba Cloud. Results show that Zero+ achieves no CPU occupation at the monitored host and supports &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1sim 10k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$0.1sim 1s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; sampling interval using a single thread for network I/O. Zero+ significantly relieves the incast issue and maintains &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$80sim 95\%$ &lt;/tex-math&gt;&lt;/inline-formula&gt; of bandwidth utilization in several clusters when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts. Zero+ also ensures services with high priority accomplish collecting metrics earlier than low priority ones by at least &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$400 mu s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts.",Zero+: Monitoring Large-Scale Cloud-Native Infrastructure Using One-Sided RDMA,"Cloud services have shifted from monolithic designs to microservices running on cloud-native infrastructure with monitoring systems to ensure service level agreements (SLAs). However, traditional monitoring systems no longer meet the demands of cloud-native monitoring. In Alibaba’s “double eleven” shopping festival, it is observed that the monitor occupies resources of the monitored infrastructure and even disrupts services. In this paper, we propose a novel monitoring system named Zero+ for cloud-native monitoring. Zero+ achieves zero overhead in collecting raw metrics using one-sided remote direct memory access (RDMA) and remedies network congestion by adopting a receiver-driven flow control scheme. Zero+ also features a priority queue mechanism to meet different quality of service requirements and an efficient batch processing design to relieve CPU occupation. Zero+ has been deployed and evaluated in four different clusters with heterogeneous RDMA NIC devices and architectures in Alibaba Cloud. Results show that Zero+ achieves no CPU occupation at the monitored host and supports &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1sim 10k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts with &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$0.1sim 1s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; sampling interval using a single thread for network I/O. Zero+ significantly relieves the incast issue and maintains &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$80sim 95\%$ &lt;/tex-math&gt;&lt;/inline-formula&gt; of bandwidth utilization in several clusters when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts. Zero+ also ensures services with high priority accomplish collecting metrics earlier than low priority ones by at least &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$400 mu s$ &lt;/tex-math&gt;&lt;/inline-formula&gt; when monitoring &lt;inline-formula&gt; &lt;tex-math notation=""LaTeX""&gt;$1k$ &lt;/tex-math&gt;&lt;/inline-formula&gt; hosts.",ACM,no,"['monitoring', 'infrastructure', 'using', 'rdma', 'shifted', 'monolithic', 'design', 'running', 'infrastructure', 'monitoring', 'ensure', 'level', 'agreement', 'slas', 'however', 'traditional', 'monitoring', 'longer', 'meet', 'demand', 'monitoring', 'alibaba', 'shopping', 'observed', 'monitor', 'resource', 'monitored', 'infrastructure', 'even', 'paper', 'propose', 'novel', 'monitoring', 'named', 'monitoring', 'achieves', 'zero', 'overhead', 'collecting', 'metric', 'using', 'remote', 'direct', 'memory', 'access', 'rdma', 'network', 'congestion', 'adopting', 'flow', 'control', 'scheme', 'also', 'feature', 'priority', 'queue', 'mechanism', 'meet', 'different', 'quality', 'requirement', 'efficient', 'batch', 'processing', 'design', 'cpu', 'occupation', 'deployed', 'evaluated', 'four', 'different', 'cluster', 'heterogeneous', 'rdma', 'device', 'alibaba', 'result', 'show', 'achieves', 'cpu', 'occupation', 'monitored', 'host', 'support', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'host', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'sampling', 'using', 'single', 'network', 'significantly', 'incast', 'issue', 'maintains', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'bandwidth', 'utilization', 'several', 'cluster', 'monitoring', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'host', 'also', 'ensures', 'high', 'priority', 'accomplish', 'collecting', 'metric', 'earlier', 'low', 'priority', 'one', 'least', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'monitoring', 'lt', 'gt', 'lt', 'latex', 'gt', 'lt', 'gt', 'lt', 'gt', 'host']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,yes,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",IEEE journal,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"Architectural Principles for Cloud Software A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.",Architectural Principles for Cloud Software,"A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.",ACM,no,"['architectural', 'principle', 'distributed', 'providing', 'resource', 'virtualization', 'resource', 'provisioning', 'deployed', 'managed', 'dynamically', 'discus', 'building', 'block', 'architectural', 'style', 'capture', 'architectural', 'principle', 'pattern', 'form', 'challenge', 'resulting', 'distributed', 'heterogeneous', 'cause', 'uncertainty', 'addressed', 'define', 'principle', 'pattern', 'needed', 'effective', 'development', 'operation', 'adaptive']"
"A Unified Model for the Mobile-Edge-Cloud Continuum Technologies such as mobile, edge, and cloud computing have the potential to form a computing continuum for new, disruptive applications. At runtime, applications can choose to execute parts of their logic on different infrastructures that constitute the continuum, with the goal of minimizing latency and battery consumption and maximizing availability. In this article, we propose A3-E, a unified model for managing the life cycle of continuum applications. In particular, A3-E exploits the Functions-as-a-Service model to bring computation to the continuum in the form of microservices. Furthermore, A3-E selects where to execute a certain function based on the specific context and user requirements. The article also presents a prototype framework that implements the concepts behind A3-E. Results show that A3-E is capable of dynamically deploying microservices and routing the application’s requests, reducing latency by up to 90\% when using edge instead of cloud resources, and battery consumption by 74\% when computation has been offloaded.",A Unified Model for the Mobile-Edge-Cloud Continuum,"Technologies such as mobile, edge, and cloud computing have the potential to form a computing continuum for new, disruptive applications. At runtime, applications can choose to execute parts of their logic on different infrastructures that constitute the continuum, with the goal of minimizing latency and battery consumption and maximizing availability. In this article, we propose A3-E, a unified model for managing the life cycle of continuum applications. In particular, A3-E exploits the Functions-as-a-Service model to bring computation to the continuum in the form of microservices. Furthermore, A3-E selects where to execute a certain function based on the specific context and user requirements. The article also presents a prototype framework that implements the concepts behind A3-E. Results show that A3-E is capable of dynamically deploying microservices and routing the application’s requests, reducing latency by up to 90\% when using edge instead of cloud resources, and battery consumption by 74\% when computation has been offloaded.",ACM,no,"['unified', 'model', 'continuum', 'technology', 'mobile', 'edge', 'computing', 'potential', 'form', 'computing', 'continuum', 'new', 'runtime', 'choose', 'execute', 'part', 'logic', 'different', 'infrastructure', 'continuum', 'goal', 'minimizing', 'latency', 'battery', 'consumption', 'maximizing', 'availability', 'article', 'propose', 'unified', 'model', 'managing', 'life', 'cycle', 'continuum', 'particular', 'exploit', 'model', 'bring', 'computation', 'continuum', 'form', 'furthermore', 'selects', 'execute', 'certain', 'function', 'based', 'specific', 'context', 'user', 'requirement', 'article', 'also', 'present', 'prototype', 'framework', 'implement', 'concept', 'behind', 'result', 'show', 'capable', 'dynamically', 'deploying', 'routing', 'request', 'reducing', 'latency', 'using', 'edge', 'instead', 'resource', 'battery', 'consumption', 'computation']"
"Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.",Cloud Deployment Tradeoffs for the Analysis of Spatially Distributed Internet of Things Systems,"Internet-enabled devices operating in the physical world are increasingly integrated in modern distributed systems. We focus on systems where the dynamics of spatial distribution is crucial; in such cases, devices may need to carry out complex computations (e.g., analyses) to check satisfaction of spatial requirements. The requirements are partly global—as the overall system should achieve certain goals—and partly individual, as each entity may have different goals. Assurance may be achieved by keeping a model of the system at runtime, monitoring events that lead to changes in the spatial environment, and performing requirements analysis. However, computationally intensive runtime spatial analysis cannot be supported by resource-constrained devices and may be offloaded to the cloud. In such a scenario, multiple challenges arise regarding resource allocation, cost, performance, among other dimensions. In particular, when the workload is unknown at the system’s design time, it may be difficult to guarantee application-service-level agreements, e.g., on response times. To address and reason on these challenges, we first instantiate complex computations as microservices and integrate them to an IoT-cloud architecture. Then, we propose alternative cloud deployments for such an architecture—based on virtual machines, containers, and the recent Functions-as-a-Service paradigm. Finally, we assess the feasibility and tradeoffs of the different deployments in terms of scalability, performance, cost, resource utilization, and more. We adopt a workload scenario from a known dataset of taxis roaming in Beijing, and we derive other workloads to represent unexpected request peaks and troughs. The approach may be replicated in the design process of similar classes of spatially distributed IoT systems.",ACM,no,"['deployment', 'tradeoff', 'analysis', 'distributed', 'internet', 'thing', 'device', 'operating', 'physical', 'world', 'increasingly', 'integrated', 'modern', 'distributed', 'focus', 'dynamic', 'spatial', 'distribution', 'crucial', 'case', 'device', 'may', 'need', 'carry', 'complex', 'computation', 'analysis', 'check', 'spatial', 'requirement', 'requirement', 'overall', 'achieve', 'certain', 'individual', 'entity', 'may', 'different', 'goal', 'assurance', 'may', 'achieved', 'keeping', 'model', 'runtime', 'monitoring', 'event', 'lead', 'change', 'spatial', 'environment', 'performing', 'requirement', 'analysis', 'however', 'computationally', 'intensive', 'runtime', 'spatial', 'analysis', 'supported', 'device', 'may', 'scenario', 'multiple', 'challenge', 'arise', 'regarding', 'resource', 'allocation', 'cost', 'performance', 'among', 'dimension', 'particular', 'workload', 'design', 'time', 'may', 'difficult', 'guarantee', 'agreement', 'response', 'time', 'address', 'reason', 'challenge', 'first', 'instantiate', 'complex', 'computation', 'integrate', 'propose', 'alternative', 'deployment', 'virtual', 'machine', 'container', 'recent', 'paradigm', 'finally', 'assess', 'feasibility', 'tradeoff', 'different', 'deployment', 'term', 'scalability', 'performance', 'cost', 'resource', 'utilization', 'adopt', 'workload', 'scenario', 'known', 'dataset', 'derive', 'workload', 'represent', 'unexpected', 'request', 'may', 'replicated', 'design', 'process', 'similar', 'class', 'distributed', 'iot']"
"Autonomic Security Management for IoT Smart Spaces Embedded sensors and smart devices have turned the environments around us into smart spaces that could automatically evolve, depending on the needs of users, and adapt to the new conditions. While smart spaces are beneficial and desired in many aspects, they could be compromised and expose privacy, security, or render the whole environment a hostile space in which regular tasks cannot be accomplished anymore. In fact, ensuring the security of smart spaces is a very challenging task due to the heterogeneity of devices, vast attack surface, and device resource limitations. The key objective of this study is to minimize the manual work in enforcing the security of smart spaces by leveraging the autonomic computing paradigm in the management of IoT environments. More specifically, we strive to build an autonomic manager that can monitor the smart space continuously, analyze the context, plan and execute countermeasures to maintain the desired level of security, and reduce liability and risks of security breaches. We follow the microservice architecture pattern and propose a generic ontology named Secure Smart Space Ontology (SSSO) for describing dynamic contextual information in security-enhanced smart spaces. Based on SSSO, we build an autonomic security manager with four layers that continuously monitors the managed spaces, analyzes contextual information and events, and automatically plans and implements adaptive security policies.As the evaluation, focusing on a current BlackBerry customer problem, we deployed the proposed autonomic security manager to maintain the security of a smart conference room with 32 devices and 66 services. The high performance of the proposed solution was also evaluated on a large-scale deployment with over 1.8 million triples.",Autonomic Security Management for IoT Smart Spaces,"Embedded sensors and smart devices have turned the environments around us into smart spaces that could automatically evolve, depending on the needs of users, and adapt to the new conditions. While smart spaces are beneficial and desired in many aspects, they could be compromised and expose privacy, security, or render the whole environment a hostile space in which regular tasks cannot be accomplished anymore. In fact, ensuring the security of smart spaces is a very challenging task due to the heterogeneity of devices, vast attack surface, and device resource limitations. The key objective of this study is to minimize the manual work in enforcing the security of smart spaces by leveraging the autonomic computing paradigm in the management of IoT environments. More specifically, we strive to build an autonomic manager that can monitor the smart space continuously, analyze the context, plan and execute countermeasures to maintain the desired level of security, and reduce liability and risks of security breaches. We follow the microservice architecture pattern and propose a generic ontology named Secure Smart Space Ontology (SSSO) for describing dynamic contextual information in security-enhanced smart spaces. Based on SSSO, we build an autonomic security manager with four layers that continuously monitors the managed spaces, analyzes contextual information and events, and automatically plans and implements adaptive security policies.As the evaluation, focusing on a current BlackBerry customer problem, we deployed the proposed autonomic security manager to maintain the security of a smart conference room with 32 devices and 66 services. The high performance of the proposed solution was also evaluated on a large-scale deployment with over 1.8 million triples.",ACM,no,"['autonomic', 'security', 'management', 'iot', 'smart', 'space', 'embedded', 'sensor', 'smart', 'device', 'environment', 'around', 'u', 'smart', 'space', 'could', 'automatically', 'evolve', 'depending', 'need', 'user', 'adapt', 'new', 'condition', 'smart', 'space', 'beneficial', 'desired', 'many', 'aspect', 'could', 'expose', 'privacy', 'security', 'render', 'whole', 'environment', 'space', 'regular', 'task', 'fact', 'ensuring', 'security', 'smart', 'space', 'challenging', 'task', 'due', 'heterogeneity', 'device', 'vast', 'attack', 'surface', 'device', 'resource', 'limitation', 'key', 'objective', 'study', 'minimize', 'manual', 'work', 'enforcing', 'security', 'smart', 'space', 'leveraging', 'autonomic', 'computing', 'paradigm', 'management', 'iot', 'environment', 'specifically', 'build', 'autonomic', 'manager', 'monitor', 'smart', 'space', 'continuously', 'analyze', 'context', 'plan', 'execute', 'maintain', 'desired', 'level', 'security', 'reduce', 'risk', 'security', 'breach', 'follow', 'pattern', 'propose', 'generic', 'ontology', 'named', 'secure', 'smart', 'space', 'ontology', 'describing', 'dynamic', 'contextual', 'information', 'smart', 'space', 'based', 'build', 'autonomic', 'security', 'manager', 'four', 'layer', 'continuously', 'monitor', 'managed', 'space', 'analyzes', 'contextual', 'information', 'event', 'automatically', 'plan', 'implement', 'adaptive', 'security', 'evaluation', 'focusing', 'current', 'customer', 'problem', 'deployed', 'proposed', 'autonomic', 'security', 'manager', 'maintain', 'security', 'smart', 'room', 'device', 'high', 'performance', 'proposed', 'solution', 'also', 'evaluated', 'deployment', 'million']"
"Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.",Adaptive Management of Volatile Edge Systems at Runtime With Satisfiability,"Edge computing offers the possibility of deploying applications at the edge of the network. To take advantage of available devices’ distributed resources, applications often are structured as microservices, often having stringent requirements of low latency and high availability. However, a decentralized edge system that the application may be intended for is characterized by high volatility, due to devices making up the system being unreliable or leaving the network unexpectedly. This makes application deployment and assurance that it will continue to operate under volatility challenging. We propose an adaptive framework capable of deploying and efficiently maintaining a microservice-based application at runtime, by tackling two intertwined problems: (i) finding a microservice placement across device hosts and (ii) deriving invocation paths that serve it. Our objective is to maintain correct functionality by satisfying given requirements in terms of end-to-end latency and availability, in a volatile edge environment. We evaluate our solution quantitatively by considering performance and failure recovery.",ACM,no,"['adaptive', 'management', 'volatile', 'edge', 'runtime', 'edge', 'computing', 'offer', 'possibility', 'deploying', 'edge', 'network', 'take', 'advantage', 'available', 'device', 'distributed', 'resource', 'often', 'structured', 'often', 'requirement', 'low', 'latency', 'high', 'availability', 'however', 'decentralized', 'edge', 'may', 'intended', 'characterized', 'high', 'due', 'device', 'making', 'unreliable', 'leaving', 'network', 'make', 'deployment', 'assurance', 'continue', 'operate', 'challenging', 'propose', 'adaptive', 'framework', 'capable', 'deploying', 'efficiently', 'maintaining', 'runtime', 'two', 'problem', 'finding', 'placement', 'across', 'device', 'host', 'ii', 'invocation', 'path', 'serve', 'objective', 'maintain', 'correct', 'functionality', 'given', 'requirement', 'term', 'latency', 'availability', 'volatile', 'edge', 'environment', 'evaluate', 'solution', 'quantitatively', 'considering', 'performance', 'failure', 'recovery']"
"Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.","Service Computing for Industry 4.0: State of the Art, Challenges, and Research Opportunities","Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.",ACM,no,"['computing', 'industry', 'state', 'art', 'challenge', 'research', 'opportunity', 'recent', 'advance', 'adoption', 'information', 'communication', 'technology', 'manufacturing', 'process', 'known', 'industry', 'smart', 'manufacturing', 'provide', 'u', 'manufacturing', 'sector', 'evolve', 'decade', 'result', 'initiative', 'manufacturing', 'started', 'integrate', 'series', 'emerging', 'technology', 'process', 'change', 'way', 'product', 'designed', 'consumed', 'article', 'provides', 'comprehensive', 'review', 'computing', 'employed', 'develop', 'required', 'infrastructure', 'industry', 'identifies', 'major', 'challenge', 'research', 'opportunity', 'particular', 'attention', 'increasingly', 'recognized', 'offering', 'promising', 'developing', 'innovative', 'industrial', 'literature', 'review', 'based', 'current', 'state', 'art', 'computing', 'industry', 'described', 'large', 'recently', 'published', 'research', 'paper', 'helped', 'u', 'identify', 'explore', 'series', 'challenge', 'opportunity', 'development', 'emerging', 'technology', 'goal', 'facilitating', 'widespread', 'adoption']"
"The Serverless Computing Survey: A Technical Primer for Design Architecture The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.",The Serverless Computing Survey: A Technical Primer for Design Architecture,"The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.",ACM,no,"['serverless', 'computing', 'survey', 'technical', 'design', 'development', 'infrastructure', 'emergence', 'computing', 'promising', 'deploying', 'serverless', 'computing', 'recently', 'attracted', 'attention', 'industry', 'academia', 'due', 'inherent', 'scalability', 'flexibility', 'serverless', 'computing', 'becomes', 'pervasive', 'internet', 'despite', 'momentum', 'community', 'existing', 'challenge', 'compromise', 'still', 'advanced', 'research', 'solution', 'explore', 'potential', 'serverless', 'computing', 'model', 'contribution', 'knowledge', 'article', 'survey', 'elaborates', 'research', 'domain', 'serverless', 'context', 'decoupling', 'four', 'stack', 'layer', 'virtualization', 'orchestration', 'coordination', 'inspired', 'security', 'model', 'highlight', 'key', 'implication', 'limitation', 'work', 'layer', 'make', 'suggestion', 'potential', 'challenge', 'field', 'future', 'serverless', 'computing']"
"Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.","Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges","Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.",ACM,no,"['kubernetes', 'scheduling', 'taxonomy', 'ongoing', 'issue', 'challenge', 'continuous', 'integration', 'enables', 'development', 'using', 'container', 'virtualization', 'technology', 'container', 'orchestration', 'kubernetes', 'become', 'de', 'facto', 'standard', 'simplify', 'deployment', 'however', 'developing', 'efficient', 'orchestration', 'challenge', 'article', 'focus', 'specifically', 'scheduler', 'key', 'orchestrator', 'task', 'physical', 'resource', 'container', 'scheduling', 'designed', 'based', 'different', 'quality', 'qos', 'parameter', 'provide', 'limited', 'response', 'time', 'efficient', 'energy', 'consumption', 'better', 'resource', 'utilization', 'thing', 'article', 'aim', 'establish', 'insight', 'knowledge', 'kubernetes', 'scheduling', 'find', 'main', 'gap', 'thus', 'guide', 'future', 'research', 'area', 'therefore', 'conduct', 'study', 'empirical', 'research', 'kubernetes', 'scheduling', 'technique', 'present', 'new', 'taxonomy', 'kubernetes', 'scheduling', 'challenge', 'future', 'direction', 'research', 'opportunity', 'also', 'discussed']"
"Focused Layered Performance Modelling by Aggregation Performance models of server systems, based on layered queues, may be very complex. This is particularly true for cloud-based systems based on microservices, which may have hundreds of distinct components, and for models derived by automated data analysis. Often only a few of these many components determine the system performance, and a smaller simplified model is all that is needed. To assist an analyst, this work describes a focused model that includes the important components (the focus) and aggregates the rest in groups, called dependency groups. The method Focus-based Simplification with Preservation of Tasks described here fills an important gap in a previous method by the same authors. The use of focused models for sensitivity predictions is evaluated empirically in the article on a large set of randomly generated models. It is found that the accuracy depends on a “saturation ratio” (SR) between the highest utilization value in the model and the highest value of a component excluded from the focus; evidence suggests that SR must be at least 2 and must be larger to evaluate larger model changes. This dependency was captured in an “Accurate Sensitivity Hypothesis” based on SR, which can be used to indicate trustable sensitivity results.",Focused Layered Performance Modelling by Aggregation,"Performance models of server systems, based on layered queues, may be very complex. This is particularly true for cloud-based systems based on microservices, which may have hundreds of distinct components, and for models derived by automated data analysis. Often only a few of these many components determine the system performance, and a smaller simplified model is all that is needed. To assist an analyst, this work describes a focused model that includes the important components (the focus) and aggregates the rest in groups, called dependency groups. The method Focus-based Simplification with Preservation of Tasks described here fills an important gap in a previous method by the same authors. The use of focused models for sensitivity predictions is evaluated empirically in the article on a large set of randomly generated models. It is found that the accuracy depends on a “saturation ratio” (SR) between the highest utilization value in the model and the highest value of a component excluded from the focus; evidence suggests that SR must be at least 2 and must be larger to evaluate larger model changes. This dependency was captured in an “Accurate Sensitivity Hypothesis” based on SR, which can be used to indicate trustable sensitivity results.",ACM,no,"['focused', 'layered', 'performance', 'modelling', 'aggregation', 'performance', 'model', 'server', 'based', 'layered', 'queue', 'may', 'complex', 'particularly', 'true', 'based', 'may', 'hundred', 'distinct', 'model', 'derived', 'automated', 'analysis', 'often', 'many', 'determine', 'performance', 'smaller', 'simplified', 'model', 'needed', 'assist', 'work', 'describes', 'focused', 'model', 'includes', 'important', 'focus', 'aggregate', 'rest', 'group', 'called', 'dependency', 'group', 'method', 'simplification', 'task', 'described', 'fill', 'important', 'gap', 'previous', 'method', 'author', 'use', 'focused', 'model', 'sensitivity', 'prediction', 'evaluated', 'article', 'large', 'set', 'generated', 'model', 'found', 'accuracy', 'depends', 'ratio', 'sr', 'highest', 'utilization', 'value', 'model', 'highest', 'value', 'focus', 'evidence', 'suggests', 'sr', 'must', 'least', 'must', 'larger', 'evaluate', 'larger', 'model', 'change', 'dependency', 'captured', 'accurate', 'sensitivity', 'based', 'sr', 'used', 'indicate', 'sensitivity', 'result']"
"Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.",Placement of Microservices-based IoT Applications in Fog Computing: A Taxonomy and Future Directions,"The Fog computing paradigm utilises distributed, heterogeneous and resource-constrained devices at the edge of the network for efficient deployment of latency-critical and bandwidth-hungry IoT application services. Moreover, MicroService Architecture (MSA) is increasingly adopted to keep up with the rapid development and deployment needs of fast-evolving IoT applications. Due to the fine-grained modularity of the microservices and their independently deployable and scalable nature, MSA exhibits great potential in harnessing Fog and Cloud resources, thus giving rise to novel paradigms like Osmotic computing. The loosely coupled nature of the microservices, aided by the container orchestrators and service mesh technologies, enables the dynamic composition of distributed and scalable microservices to achieve diverse performance requirements of the IoT applications using distributed Fog resources. To this end, efficient placement of microservice plays a vital role, and scalable placement algorithms are required to utilise the said characteristics of the MSA while overcoming novel challenges introduced by the architecture. Thus, we present a comprehensive taxonomy of recent literature on microservices-based IoT applications placement within Fog computing environments. Furthermore, we organise multiple taxonomies to capture the main aspects of the placement problem, analyse and classify related works, identify research gaps within each category, and discuss future research directions.",ACM,no,"['placement', 'iot', 'fog', 'computing', 'taxonomy', 'future', 'direction', 'fog', 'computing', 'paradigm', 'distributed', 'heterogeneous', 'device', 'edge', 'network', 'efficient', 'deployment', 'iot', 'moreover', 'msa', 'increasingly', 'adopted', 'keep', 'rapid', 'development', 'deployment', 'need', 'iot', 'due', 'modularity', 'independently', 'deployable', 'scalable', 'nature', 'msa', 'exhibit', 'great', 'potential', 'harnessing', 'fog', 'resource', 'thus', 'giving', 'rise', 'novel', 'paradigm', 'like', 'osmotic', 'computing', 'loosely', 'coupled', 'nature', 'container', 'orchestrator', 'mesh', 'technology', 'enables', 'dynamic', 'composition', 'distributed', 'scalable', 'achieve', 'diverse', 'performance', 'requirement', 'iot', 'using', 'distributed', 'fog', 'resource', 'end', 'efficient', 'placement', 'play', 'vital', 'role', 'scalable', 'placement', 'algorithm', 'required', 'utilise', 'characteristic', 'msa', 'overcoming', 'novel', 'challenge', 'introduced', 'thus', 'present', 'comprehensive', 'taxonomy', 'recent', 'literature', 'iot', 'placement', 'within', 'fog', 'computing', 'environment', 'furthermore', 'multiple', 'taxonomy', 'capture', 'main', 'aspect', 'placement', 'problem', 'analyse', 'classify', 'related', 'work', 'identify', 'research', 'gap', 'within', 'category', 'discus', 'future', 'research', 'direction']"
"Strega: An HTTP Server for FPGAs The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega, an open source1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7&nbsp;M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.",Strega: An HTTP Server for FPGAs,"The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega, an open source1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7&nbsp;M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.",ACM,no,"['strega', 'http', 'server', 'computer', 'landscape', 'new', 'opportunity', 'challenge', 'constraint', 'brought', 'one', 'hand', 'hardware', 'performance', 'reduce', 'deployment', 'cost', 'hand', 'provider', 'cpu', 'time', 'allocated', 'client', 'offloading', 'infrastructure', 'task', 'hardware', 'accelerator', 'well', 'network', 'function', 'protocol', 'support', 'higher', 'networking', 'layer', 'still', 'missing', 'limiting', 'potential', 'accelerator', 'article', 'present', 'strega', 'open', 'transfer', 'protocol', 'http', 'server', 'enables', 'crucial', 'functionality', 'function', 'called', 'restful', 'protocol', 'experimental', 'analysis', 'show', 'single', 'strega', 'node', 'throughput', 'http', 'request', 'per', 'second', 'latency', 'low', 'outperforming', 'nginx', 'running', 'metric', 'even', 'alternative', 'traditional', 'flow', 'bus', 'work', 'pave', 'way', 'running', 'directly', 'cpu', 'overhead', 'full', 'potential', 'distributed']"
"SensiX++: Bringing MLOps and Multi-tenant Model Serving to Sensory Edge Devices We present SensiX++, a multi-tenant runtime for adaptive model execution with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT sensors. SensiX++ operates on two fundamental principles: highly modular componentisation to externalise data operations with clear abstractions and document-centric manifestation for system-wide orchestration. First, a data coordinator manages the lifecycle of sensors and serves models with correct data through automated transformations. Next, a resource-aware model server executes multiple models in isolation through model abstraction, pipeline automation, and feature sharing. An adaptive scheduler then orchestrates the best-effort executions of multiple models across heterogeneous accelerators, balancing latency and throughput. Finally, microservices with REST APIs serve synthesised model predictions, system statistics, and continuous deployment. Collectively, these components enable SensiX++ to serve multiple models efficiently with fine-grained control on edge devices while minimising data operation redundancy, managing data and device heterogeneity, and reducing resource contention. We benchmark SensiX++ with 10 different vision and acoustics models across various multi-tenant configurations on different edge accelerators (Jetson AGX and Coral TPU) designed for sensory devices. We report on the overall throughput and quantified benefits of various automation components of SensiX++ and demonstrate its efficacy in significantly reducing operational complexity and lowering the effort to deploy, upgrade, reconfigure, and serve embedded models on edge devices.",SensiX++: Bringing MLOps and Multi-tenant Model Serving to Sensory Edge Devices,"We present SensiX++, a multi-tenant runtime for adaptive model execution with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT sensors. SensiX++ operates on two fundamental principles: highly modular componentisation to externalise data operations with clear abstractions and document-centric manifestation for system-wide orchestration. First, a data coordinator manages the lifecycle of sensors and serves models with correct data through automated transformations. Next, a resource-aware model server executes multiple models in isolation through model abstraction, pipeline automation, and feature sharing. An adaptive scheduler then orchestrates the best-effort executions of multiple models across heterogeneous accelerators, balancing latency and throughput. Finally, microservices with REST APIs serve synthesised model predictions, system statistics, and continuous deployment. Collectively, these components enable SensiX++ to serve multiple models efficiently with fine-grained control on edge devices while minimising data operation redundancy, managing data and device heterogeneity, and reducing resource contention. We benchmark SensiX++ with 10 different vision and acoustics models across various multi-tenant configurations on different edge accelerators (Jetson AGX and Coral TPU) designed for sensory devices. We report on the overall throughput and quantified benefits of various automation components of SensiX++ and demonstrate its efficacy in significantly reducing operational complexity and lowering the effort to deploy, upgrade, reconfigure, and serve embedded models on edge devices.",ACM,no,"['bringing', 'mlops', 'model', 'serving', 'sensory', 'edge', 'device', 'present', 'runtime', 'adaptive', 'model', 'execution', 'integrated', 'mlops', 'edge', 'device', 'camera', 'iot', 'sensor', 'operates', 'two', 'fundamental', 'principle', 'highly', 'modular', 'operation', 'clear', 'abstraction', 'orchestration', 'first', 'manages', 'lifecycle', 'sensor', 'serf', 'model', 'correct', 'automated', 'transformation', 'next', 'model', 'server', 'multiple', 'model', 'isolation', 'model', 'abstraction', 'pipeline', 'automation', 'feature', 'sharing', 'adaptive', 'scheduler', 'execution', 'multiple', 'model', 'across', 'heterogeneous', 'accelerator', 'balancing', 'latency', 'throughput', 'finally', 'rest', 'apis', 'serve', 'model', 'prediction', 'statistic', 'continuous', 'deployment', 'enable', 'serve', 'multiple', 'model', 'efficiently', 'control', 'edge', 'device', 'operation', 'redundancy', 'managing', 'device', 'heterogeneity', 'reducing', 'resource', 'contention', 'benchmark', 'different', 'vision', 'model', 'across', 'various', 'configuration', 'different', 'edge', 'accelerator', 'designed', 'sensory', 'device', 'report', 'overall', 'throughput', 'benefit', 'various', 'automation', 'demonstrate', 'efficacy', 'significantly', 'reducing', 'operational', 'complexity', 'lowering', 'effort', 'deploy', 'upgrade', 'serve', 'embedded', 'model', 'edge', 'device']"
"Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3\%, CPU utilization by 38.6\%, and memory bandwidth utilization by 45.4\% while guaranteeing the tail latency requirement.",Component-distinguishable Co-location and Resource Reclamation for High-throughput Computing,"Cloud service providers improve resource utilization by co-locating latency-critical (LC) workloads with best-effort batch (BE) jobs in datacenters. However, they usually treat multi-component LCs as monolithic applications and treat BEs as “second-class citizens” when allocating resources to them. Neglecting the inconsistent interference tolerance abilities of LC components and the inconsistent preemption loss of BE workloads can result in missed co-location opportunities for higher throughput.We present Rhythm, a co-location controller that deploys workloads and reclaims resources rhythmically for maximizing the system throughput while guaranteeing LC service’s tail latency requirement. The key idea is to differentiate the BE throughput launched with each LC component, that is, components with higher interference tolerance can be deployed together with more BE jobs. It also assigns different reclamation priority values to BEs by evaluating their preemption losses into a multi-level reclamation queue. We implement and evaluate Rhythm using workloads in the form of containerized processes and microservices. Experimental results show that it can improve the system throughput by 47.3\%, CPU utilization by 38.6\%, and memory bandwidth utilization by 45.4\% while guaranteeing the tail latency requirement.",ACM,no,"['resource', 'reclamation', 'computing', 'provider', 'improve', 'resource', 'utilization', 'lc', 'workload', 'batch', 'job', 'however', 'usually', 'treat', 'monolithic', 'treat', 'citizen', 'resource', 'neglecting', 'interference', 'tolerance', 'ability', 'lc', 'loss', 'workload', 'result', 'opportunity', 'higher', 'present', 'controller', 'deploys', 'workload', 'resource', 'maximizing', 'throughput', 'guaranteeing', 'lc', 'tail', 'latency', 'requirement', 'key', 'idea', 'throughput', 'lc', 'higher', 'interference', 'tolerance', 'deployed', 'together', 'job', 'also', 'different', 'reclamation', 'priority', 'value', 'evaluating', 'loss', 'reclamation', 'queue', 'implement', 'evaluate', 'using', 'workload', 'form', 'containerized', 'process', 'experimental', 'result', 'show', 'improve', 'throughput', 'cpu', 'utilization', 'memory', 'bandwidth', 'utilization', 'guaranteeing', 'tail', 'latency', 'requirement']"
"Optimizing Resource Management for Shared Microservices: A Scalable System Design A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements.We present an efficient resource management system, namely Erms, for guaranteeing SLAs with high probability in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5\texttimes{} and more importantly, lead to a reduction in resource usage by 1.6\texttimes{}, compared to state-of-the-art approaches.",Optimizing Resource Management for Shared Microservices: A Scalable System Design,"A common approach to improving resource utilization in data centers is to adaptively provision resources based on the actual workload. One fundamental challenge of doing this in microservice management frameworks, however, is that different components of a service can exhibit significant differences in their impact on end-to-end performance. To make resource management more challenging, a single microservice can be shared by multiple online services that have diverse workload patterns and SLA requirements.We present an efficient resource management system, namely Erms, for guaranteeing SLAs with high probability in shared microservice environments. Erms profiles microservice latency as a piece-wise linear function of the workload, resource usage, and interference. Based on this profiling, Erms builds resource scaling models to optimally determine latency targets for microservices with complex dependencies. Erms also designs new scheduling policies at shared microservices to further enhance resource efficiency. Experiments across microservice benchmarks as well as trace-driven simulations demonstrate that Erms can reduce SLA violation probability by 5\texttimes{} and more importantly, lead to a reduction in resource usage by 1.6\texttimes{}, compared to state-of-the-art approaches.",ACM,no,"['optimizing', 'resource', 'management', 'shared', 'scalable', 'design', 'common', 'improving', 'resource', 'utilization', 'center', 'adaptively', 'provision', 'resource', 'based', 'actual', 'workload', 'one', 'fundamental', 'challenge', 'management', 'framework', 'however', 'different', 'exhibit', 'significant', 'difference', 'impact', 'performance', 'make', 'resource', 'management', 'challenging', 'single', 'shared', 'multiple', 'online', 'diverse', 'workload', 'pattern', 'sla', 'present', 'efficient', 'resource', 'management', 'namely', 'erms', 'guaranteeing', 'slas', 'high', 'probability', 'shared', 'environment', 'erms', 'profile', 'latency', 'linear', 'function', 'workload', 'resource', 'usage', 'interference', 'based', 'profiling', 'erms', 'build', 'resource', 'scaling', 'model', 'optimally', 'determine', 'latency', 'target', 'complex', 'dependency', 'erms', 'also', 'design', 'new', 'scheduling', 'policy', 'shared', 'enhance', 'resource', 'efficiency', 'experiment', 'across', 'benchmark', 'well', 'simulation', 'demonstrate', 'erms', 'reduce', 'sla', 'violation', 'probability', 'lead', 'reduction', 'resource', 'usage', 'compared']"
"Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70\% with limited performance degradation (at most 2\%).",Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency,"Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70\% with limited performance degradation (at most 2\%).",ACM,no,"['agile', 'core', 'latency', 'critical', 'optimizing', 'transition', 'latency', 'running', 'modern', 'exhibit', 'request', 'pattern', 'implemented', 'using', 'multiple', 'strict', 'latency', 'requirement', 'characteristic', 'render', 'existing', 'cpu', 'sleep', 'state', 'ineffective', 'due', 'performance', 'overhead', 'caused', 'state', 'transition', 'latency', 'besides', 'state', 'transition', 'latency', 'another', 'important', 'performance', 'overhead', 'sleep', 'state', 'latency', 'time', 'required', 'state', 'cache', 'content', 'metadata', 'state', 'transition', 'latency', 'latency', 'particularly', 'performance', 'latency', 'critical', 'short', 'execution', 'time', 'prior', 'work', 'focus', 'mitigating', 'effect', 'transition', 'latency', 'optimizing', 'request', 'scheduling', 'work', 'propose', 'redesign', 'core', 'particular', 'introduce', 'new', 'agile', 'core', 'drastically', 'reduces', 'performance', 'overhead', 'caused', 'sleep', 'state', 'transition', 'latency', 'latency', 'maintaining', 'significant', 'energy', 'saving', 'achieves', 'goal', 'implementing', 'power', 'preserving', 'state', 'core', 'keeping', 'active', 'analysis', 'set', 'based', 'intel', 'server', 'show', 'manages', 'reduce', 'energy', 'consumption', 'limited', 'performance', 'degradation']"
"Dynamically Balancing Load with Overload Control for Microservices The microservices architecture simplifies application development by breaking monolithic applications into manageable microservices. However, this distributed microservice “service mesh” leads to new challenges due to the more complex application topology. Particularly, each service component scales up and down independently creating load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this article, we propose a self-managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes. BLOC uses overload control approaches to provide feedback to the load balancers. We show that this performs significantly better in solving the incast problem in microservice architectures. A critical component of BLOC is the dynamic capacity estimation algorithm. We show that a well-tuned capacity estimate can outperform even join-the-shortest-queue, a nearly optimal algorithm, while a reasonable dynamic estimate still outperforms Least Connection, a distributed implementation of join-the-shortest-queue. Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 –4 times and the tail, 99th percentile, latency by 2 times.",Dynamically Balancing Load with Overload Control for Microservices,"The microservices architecture simplifies application development by breaking monolithic applications into manageable microservices. However, this distributed microservice “service mesh” leads to new challenges due to the more complex application topology. Particularly, each service component scales up and down independently creating load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this article, we propose a self-managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes. BLOC uses overload control approaches to provide feedback to the load balancers. We show that this performs significantly better in solving the incast problem in microservice architectures. A critical component of BLOC is the dynamic capacity estimation algorithm. We show that a well-tuned capacity estimate can outperform even join-the-shortest-queue, a nearly optimal algorithm, while a reasonable dynamic estimate still outperforms Least Connection, a distributed implementation of join-the-shortest-queue. Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 –4 times and the tail, 99th percentile, latency by 2 times.",ACM,no,"['dynamically', 'balancing', 'load', 'overload', 'control', 'simplifies', 'development', 'breaking', 'monolithic', 'manageable', 'however', 'distributed', 'mesh', 'lead', 'new', 'challenge', 'due', 'complex', 'topology', 'particularly', 'scale', 'independently', 'creating', 'load', 'imbalance', 'problem', 'shared', 'backend', 'accessed', 'multiple', 'traditional', 'load', 'balancing', 'algorithm', 'port', 'well', 'distributed', 'load', 'balancer', 'deployed', 'article', 'propose', 'load', 'balancing', 'bloc', 'provides', 'consistent', 'response', 'time', 'user', 'without', 'using', 'centralized', 'metadata', 'store', 'explicit', 'messaging', 'node', 'bloc', 'us', 'overload', 'control', 'provide', 'feedback', 'load', 'balancer', 'show', 'performs', 'significantly', 'better', 'solving', 'incast', 'problem', 'critical', 'bloc', 'dynamic', 'capacity', 'estimation', 'algorithm', 'show', 'capacity', 'estimate', 'even', 'nearly', 'optimal', 'algorithm', 'reasonable', 'dynamic', 'estimate', 'still', 'outperforms', 'least', 'connection', 'distributed', 'implementation', 'evaluating', 'framework', 'found', 'bloc', 'improves', 'response', 'time', 'distribution', 'range', 'percentile', 'time', 'tail', 'percentile', 'latency', 'time']"
"Performance Modeling of Distributed Data Processing in Microservice Applications Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.",Performance Modeling of Distributed Data Processing in Microservice Applications,"Microservice applications are increasingly adopted in distributed data processing systems, such as in mobile edge computing and data mesh architectures. However, existing performance models of such systems fall short in providing comprehensive insights into the intricate interplay between data placement and data processing. To address these issues, this article proposes a novel class of performance models that enables joint analysis of data storage access workflows, caching, and queueing contention. Our proposed models introduce a notion of access path for data items to model hierarchical data locality constraints. We develop analytical solutions to efficiently approximate the performance metrics of these models under different data caching policies, finding in particular conditions under which the underlying Markov chain admits a product-form solution. Extensive trace-driven simulations based on real-world datasets indicate that service and data placement policies based on our proposed models can respectively improve by up to 35\% and 37\% the average response time in edge and data mesh case studies.",ACM,no,"['performance', 'modeling', 'distributed', 'processing', 'increasingly', 'adopted', 'distributed', 'processing', 'mobile', 'edge', 'computing', 'mesh', 'however', 'existing', 'performance', 'model', 'short', 'providing', 'comprehensive', 'insight', 'intricate', 'interplay', 'placement', 'processing', 'address', 'issue', 'article', 'proposes', 'novel', 'class', 'performance', 'model', 'enables', 'analysis', 'storage', 'access', 'workflow', 'caching', 'contention', 'proposed', 'model', 'introduce', 'access', 'path', 'item', 'model', 'hierarchical', 'constraint', 'develop', 'analytical', 'solution', 'efficiently', 'performance', 'metric', 'model', 'different', 'caching', 'policy', 'finding', 'particular', 'condition', 'underlying', 'chain', 'solution', 'extensive', 'simulation', 'based', 'datasets', 'indicate', 'placement', 'policy', 'based', 'proposed', 'model', 'respectively', 'improve', 'average', 'response', 'time', 'edge', 'mesh', 'case', 'study']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,yes,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",IEEE journal,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"Microservices-based architecture for fault diagnosis in tele-rehabilitation equipment operated via Internet This paper presents the design of a microservices based architecture allows early fault detection and diagnosis on a remote controlled physical rehabilitation machine using the Internet as a communication channel. Aforementioned architecture is composed of three layers: the low layer which collects variables from the rehabilitation machine components, using Internet of Things protocols. The middle layer which analyses the provided variables and diagnoses the component status, using fuzzy logic. And finally, the upper layer which makes decisions depending on the diagnosis data. The proposed architecture is suitable for heterogeneous systems.This paper also shows how this architecture fulfills the specific and rigorous safety measures for critical mission devices like technical aids for health-care.",Microservices-based architecture for fault diagnosis in tele-rehabilitation equipment operated via Internet,"This paper presents the design of a microservices based architecture allows early fault detection and diagnosis on a remote controlled physical rehabilitation machine using the Internet as a communication channel. Aforementioned architecture is composed of three layers: the low layer which collects variables from the rehabilitation machine components, using Internet of Things protocols. The middle layer which analyses the provided variables and diagnoses the component status, using fuzzy logic. And finally, the upper layer which makes decisions depending on the diagnosis data. The proposed architecture is suitable for heterogeneous systems.This paper also shows how this architecture fulfills the specific and rigorous safety measures for critical mission devices like technical aids for health-care.",IEEE conference,no,"['fault', 'diagnosis', 'equipment', 'operated', 'via', 'internet', 'paper', 'present', 'design', 'based', 'allows', 'early', 'fault', 'detection', 'diagnosis', 'remote', 'controlled', 'physical', 'rehabilitation', 'machine', 'using', 'internet', 'communication', 'channel', 'aforementioned', 'composed', 'three', 'layer', 'low', 'layer', 'collect', 'variable', 'rehabilitation', 'machine', 'using', 'internet', 'thing', 'protocol', 'middle', 'layer', 'analysis', 'provided', 'variable', 'diagnosis', 'status', 'using', 'fuzzy', 'logic', 'finally', 'layer', 'make', 'decision', 'depending', 'diagnosis', 'proposed', 'suitable', 'heterogeneous', 'paper', 'also', 'show', 'specific', 'safety', 'measure', 'critical', 'mission', 'device', 'like', 'technical', 'aid']"
"Research on SFC Deployment with Variable Granularity based on Microservice Architecture Network function virtualization (NFV) solves the problem of inflexibility in traditional network, but there are still some problems with service function chain (SFC) in some usage scenarios. Aiming at the optimization problem of end-to-end delay and resource overhead of SFC in NFV. Firstly, according to the user's service function connection request, the VNF is functionally divided into the most fine-grained atomNF according to the operation logic which is based on microservice architecture, and the redundant logical units are eliminated. Then, the path is planned according to the resource status of nodes and deployment units. Finally, the atomNF in the same node is aggregated in a single container. Simulation results show that the proposed method can optimize the end-to-end delay and node resource utilization.",Research on SFC Deployment with Variable Granularity based on Microservice Architecture,"Network function virtualization (NFV) solves the problem of inflexibility in traditional network, but there are still some problems with service function chain (SFC) in some usage scenarios. Aiming at the optimization problem of end-to-end delay and resource overhead of SFC in NFV. Firstly, according to the user's service function connection request, the VNF is functionally divided into the most fine-grained atomNF according to the operation logic which is based on microservice architecture, and the redundant logical units are eliminated. Then, the path is planned according to the resource status of nodes and deployment units. Finally, the atomNF in the same node is aggregated in a single container. Simulation results show that the proposed method can optimize the end-to-end delay and node resource utilization.",IEEE conference,no,"['research', 'sfc', 'deployment', 'variable', 'granularity', 'based', 'network', 'function', 'virtualization', 'nfv', 'solves', 'problem', 'traditional', 'network', 'still', 'problem', 'function', 'chain', 'sfc', 'usage', 'scenario', 'aiming', 'optimization', 'problem', 'delay', 'resource', 'overhead', 'sfc', 'nfv', 'firstly', 'according', 'user', 'function', 'connection', 'request', 'divided', 'according', 'operation', 'logic', 'based', 'redundant', 'logical', 'unit', 'eliminated', 'path', 'planned', 'according', 'resource', 'status', 'node', 'deployment', 'unit', 'finally', 'node', 'aggregated', 'single', 'container', 'simulation', 'result', 'show', 'proposed', 'method', 'optimize', 'delay', 'node', 'resource', 'utilization']"
"CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS Future mobility systems and their components are increasingly defined by their software. The complexity of these cooperative intelligent transport systems (C-ITS) and the ever-changing requirements posed at the software require continual software updates. The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use simulations as one core methodology. The availability of such simulation architectures is a common interest among many stakeholders, especially in the field of automated driving. That is why we propose CARLOS - an open, modular, and scalable simulation framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems. We provide core building blocks for this framework and explain how it can be used and extended by the community. Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration. In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing. We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwth-aachen/carlos.","CARLOS: An Open, Modular, and Scalable Simulation Framework for the Development and Testing of Software for C-ITS","Future mobility systems and their components are increasingly defined by their software. The complexity of these cooperative intelligent transport systems (C-ITS) and the ever-changing requirements posed at the software require continual software updates. The dynamic nature of the system and the practically innumerable scenarios in which different software components work together necessitate efficient and automated development and testing procedures that use simulations as one core methodology. The availability of such simulation architectures is a common interest among many stakeholders, especially in the field of automated driving. That is why we propose CARLOS - an open, modular, and scalable simulation framework for the development and testing of software in C-ITS that leverages the rich CARLA and ROS ecosystems. We provide core building blocks for this framework and explain how it can be used and extended by the community. Its architecture builds upon modern microservice and DevOps principles such as containerization and continuous integration. In our paper, we motivate the architecture by describing important design principles and showcasing three major use cases - software prototyping, data-driven development, and automated testing. We make CARLOS and example implementations of the three use cases publicly available at github.com/ika-rwth-aachen/carlos.",IEEE conference,no,"['carlos', 'open', 'modular', 'scalable', 'simulation', 'framework', 'development', 'testing', 'future', 'mobility', 'increasingly', 'defined', 'complexity', 'cooperative', 'intelligent', 'transport', 'requirement', 'require', 'update', 'dynamic', 'nature', 'scenario', 'different', 'work', 'together', 'necessitate', 'efficient', 'automated', 'development', 'testing', 'procedure', 'use', 'simulation', 'one', 'core', 'methodology', 'availability', 'simulation', 'common', 'interest', 'among', 'many', 'stakeholder', 'especially', 'field', 'automated', 'driving', 'propose', 'carlos', 'open', 'modular', 'scalable', 'simulation', 'framework', 'development', 'testing', 'leverage', 'rich', 'ro', 'ecosystem', 'provide', 'core', 'building', 'block', 'framework', 'explain', 'used', 'extended', 'community', 'build', 'upon', 'modern', 'devops', 'principle', 'containerization', 'continuous', 'integration', 'paper', 'motivate', 'describing', 'important', 'design', 'principle', 'three', 'major', 'use', 'case', 'development', 'automated', 'testing', 'make', 'carlos', 'example', 'implementation', 'three', 'use', 'case', 'publicly', 'available']"
"A Method for Designing and Analyzing Automotive Software Architecture: A Case Study for an Autonomous Electric Vehicle Software complexity is increased in automotive systems because many software functions are required for autonomous driving, electrified vehicles, and connected cars. In addition, autonomous driving requires centralized software that generally decreases evolvability with many connections. Thus, the automotive industry adopted the microservice architecture within the service-oriented architecture (SOA), which was already being used in distributed computing environments in the information and communication technology (ICT) industry. However, the software characteristics of an automotive system are different from those of an ICT system. Automotive software generally fulfills safety and real-time requirements that are not required in ICT software. Another challenge is integrating electric control units (ECUs) because software platforms supporting SOA require relatively high computational power and network bandwidth, which increases ECU cost. Thus, the deployment of software functions must be considered before integrating ECUs to find an optimal design solution for evolvability, dependability, real-time performance, cost, etc. However, many OEMs integrate ECUs based on deploying vehicular features without software architecture. It causes optimality problems during integrating ECUs. We propose component-based sensor-process-actuator architectural style for high-level architecture to handle quality attributes. Software architecture for an autonomous electrified vehicle will be presented with the proposed architectural style. The architecture is used to deploy software components and integrated ECUs with empirical quantitative analysis. Four design patterns for dependability with the architectural style will also be introduced.",A Method for Designing and Analyzing Automotive Software Architecture: A Case Study for an Autonomous Electric Vehicle,"Software complexity is increased in automotive systems because many software functions are required for autonomous driving, electrified vehicles, and connected cars. In addition, autonomous driving requires centralized software that generally decreases evolvability with many connections. Thus, the automotive industry adopted the microservice architecture within the service-oriented architecture (SOA), which was already being used in distributed computing environments in the information and communication technology (ICT) industry. However, the software characteristics of an automotive system are different from those of an ICT system. Automotive software generally fulfills safety and real-time requirements that are not required in ICT software. Another challenge is integrating electric control units (ECUs) because software platforms supporting SOA require relatively high computational power and network bandwidth, which increases ECU cost. Thus, the deployment of software functions must be considered before integrating ECUs to find an optimal design solution for evolvability, dependability, real-time performance, cost, etc. However, many OEMs integrate ECUs based on deploying vehicular features without software architecture. It causes optimality problems during integrating ECUs. We propose component-based sensor-process-actuator architectural style for high-level architecture to handle quality attributes. Software architecture for an autonomous electrified vehicle will be presented with the proposed architectural style. The architecture is used to deploy software components and integrated ECUs with empirical quantitative analysis. Four design patterns for dependability with the architectural style will also be introduced.",IEEE conference,no,"['method', 'designing', 'analyzing', 'automotive', 'case', 'study', 'autonomous', 'electric', 'vehicle', 'complexity', 'increased', 'automotive', 'many', 'function', 'required', 'autonomous', 'driving', 'vehicle', 'connected', 'addition', 'autonomous', 'driving', 'requires', 'centralized', 'generally', 'decrease', 'many', 'connection', 'thus', 'automotive', 'industry', 'adopted', 'within', 'soa', 'already', 'used', 'distributed', 'computing', 'environment', 'information', 'communication', 'technology', 'ict', 'industry', 'however', 'characteristic', 'automotive', 'different', 'ict', 'automotive', 'generally', 'safety', 'requirement', 'required', 'ict', 'another', 'challenge', 'integrating', 'electric', 'control', 'unit', 'ecus', 'platform', 'supporting', 'soa', 'require', 'relatively', 'high', 'computational', 'power', 'network', 'bandwidth', 'increase', 'cost', 'thus', 'deployment', 'function', 'must', 'considered', 'integrating', 'ecus', 'find', 'optimal', 'design', 'solution', 'dependability', 'performance', 'cost', 'etc', 'however', 'many', 'integrate', 'ecus', 'based', 'deploying', 'vehicular', 'feature', 'without', 'cause', 'problem', 'integrating', 'ecus', 'propose', 'architectural', 'style', 'handle', 'quality', 'attribute', 'autonomous', 'vehicle', 'presented', 'proposed', 'architectural', 'style', 'used', 'deploy', 'integrated', 'ecus', 'empirical', 'quantitative', 'analysis', 'four', 'design', 'pattern', 'dependability', 'architectural', 'style', 'also', 'introduced']"
"A Comparative Study of Monolithic and Microservices Architectures in Machine Learning Scenarios Choosing the most suitable architecture for applications is not an easy decision. While the software giants have almost all put in place the microservices architecture, on smaller platforms such decision it is not so obvious. In the healthcare domain and specifically when accomplishing Machine Learning (ML) tasks in this domain, considering its special characteristics, the decision should be made based on specific metrics. In the context of the beHEALTHIER platform, a platform that is able to handle heterogeneous healthcare data towards their successful management and analysis by applying various ML tasks, such research gap was fully investigated. There has been conducted an experiment by installing the platform in three (3) different architectural ways, referring to the monolithic architecture, the clustered microservices architecture exploiting docker compose, and the microservices architecture exploiting Kubernetes cluster. For these three (3) environments, time-based measurements were made for each Application Programming Interface (API) of the diverse platform’s functionalities (i.e., components) and useful conclusions were drawn towards the adoption of the most suitable software architecture.",A Comparative Study of Monolithic and Microservices Architectures in Machine Learning Scenarios,"Choosing the most suitable architecture for applications is not an easy decision. While the software giants have almost all put in place the microservices architecture, on smaller platforms such decision it is not so obvious. In the healthcare domain and specifically when accomplishing Machine Learning (ML) tasks in this domain, considering its special characteristics, the decision should be made based on specific metrics. In the context of the beHEALTHIER platform, a platform that is able to handle heterogeneous healthcare data towards their successful management and analysis by applying various ML tasks, such research gap was fully investigated. There has been conducted an experiment by installing the platform in three (3) different architectural ways, referring to the monolithic architecture, the clustered microservices architecture exploiting docker compose, and the microservices architecture exploiting Kubernetes cluster. For these three (3) environments, time-based measurements were made for each Application Programming Interface (API) of the diverse platform’s functionalities (i.e., components) and useful conclusions were drawn towards the adoption of the most suitable software architecture.",IEEE conference,no,"['comparative', 'study', 'monolithic', 'machine', 'learning', 'scenario', 'choosing', 'suitable', 'easy', 'decision', 'almost', 'put', 'place', 'smaller', 'platform', 'decision', 'healthcare', 'domain', 'specifically', 'machine', 'learning', 'ml', 'task', 'domain', 'considering', 'special', 'characteristic', 'decision', 'made', 'based', 'specific', 'metric', 'context', 'platform', 'platform', 'able', 'handle', 'heterogeneous', 'healthcare', 'towards', 'successful', 'management', 'analysis', 'applying', 'various', 'ml', 'task', 'research', 'gap', 'fully', 'conducted', 'experiment', 'platform', 'three', 'different', 'architectural', 'way', 'monolithic', 'exploiting', 'docker', 'compose', 'exploiting', 'kubernetes', 'cluster', 'three', 'environment', 'measurement', 'made', 'programming', 'interface', 'api', 'diverse', 'platform', 'functionality', 'useful', 'conclusion', 'towards', 'adoption', 'suitable']"
"A platform-independent communication framework for the simplified development of shop-floor applications as microservice components For flexible and highly networked industry 4.0 production processes, software components are becoming more and more significant for reconfiguring production systems or facilitating complex functionalities. Standards for communication such as OPC UA have a crucial role in the data exchange required in this context. However, these systems adopt the static properties from their domain, which have led to their success and widespread use in Industry 4.0. The most critical core difficulties with OPC UA are the definition of communication mechanisms and the data model to be used in a strictly coupled environment. Because of their central client-server architecture and missing communication patterns, these systems do not offer the necessary flexibility and platform independence to use the entire spectrum of possible software tools efficiently. Especially negatively affected is the initial effort to integrate such systems and to guarantee the scalability of the infrastructure later on. With a focus on these challenges, the message-based communication framework XSC has been developed for the shop-floor, which uses the high-performance ZeroMQ framework and the data format Google Protocol Buffers for platform independence and a high degree of efficiency. It has a scalable and distributed multi-agent architecture that provides a distributed registry for the usage and provision of microservice components. Besides, multiple communication patterns were provided to meet the requirements of both environments, shop-floor, and cloud computing applications.",A platform-independent communication framework for the simplified development of shop-floor applications as microservice components,"For flexible and highly networked industry 4.0 production processes, software components are becoming more and more significant for reconfiguring production systems or facilitating complex functionalities. Standards for communication such as OPC UA have a crucial role in the data exchange required in this context. However, these systems adopt the static properties from their domain, which have led to their success and widespread use in Industry 4.0. The most critical core difficulties with OPC UA are the definition of communication mechanisms and the data model to be used in a strictly coupled environment. Because of their central client-server architecture and missing communication patterns, these systems do not offer the necessary flexibility and platform independence to use the entire spectrum of possible software tools efficiently. Especially negatively affected is the initial effort to integrate such systems and to guarantee the scalability of the infrastructure later on. With a focus on these challenges, the message-based communication framework XSC has been developed for the shop-floor, which uses the high-performance ZeroMQ framework and the data format Google Protocol Buffers for platform independence and a high degree of efficiency. It has a scalable and distributed multi-agent architecture that provides a distributed registry for the usage and provision of microservice components. Besides, multiple communication patterns were provided to meet the requirements of both environments, shop-floor, and cloud computing applications.",IEEE conference,no,"['communication', 'framework', 'simplified', 'development', 'flexible', 'highly', 'industry', 'production', 'process', 'becoming', 'significant', 'production', 'facilitating', 'complex', 'functionality', 'standard', 'communication', 'crucial', 'role', 'exchange', 'required', 'context', 'however', 'adopt', 'static', 'property', 'domain', 'led', 'success', 'widespread', 'use', 'industry', 'critical', 'core', 'difficulty', 'definition', 'communication', 'mechanism', 'model', 'used', 'strictly', 'coupled', 'environment', 'central', 'missing', 'communication', 'pattern', 'offer', 'necessary', 'flexibility', 'platform', 'independence', 'use', 'entire', 'possible', 'tool', 'efficiently', 'especially', 'affected', 'initial', 'effort', 'integrate', 'guarantee', 'scalability', 'infrastructure', 'later', 'focus', 'challenge', 'communication', 'framework', 'developed', 'us', 'framework', 'format', 'google', 'protocol', 'platform', 'independence', 'high', 'degree', 'efficiency', 'scalable', 'distributed', 'provides', 'distributed', 'registry', 'usage', 'provision', 'besides', 'multiple', 'communication', 'pattern', 'provided', 'meet', 'requirement', 'environment', 'computing']"
"On-Demand and Automatic Deployment of Microservice at the Edge Based on NGSI-LD This paper focuses on a new approach to conceiving “virtual sensors” operating in smart environments, which are abstracted components able to map different behaviours on the same Internet of Things (IoT)-based infrastructures according to the needs of the high-level applications. To realize “virtual sensors”, it is necessary to codify user requests in an automation process for the deployment at the Edge of the microservices (MSs) that satisfy such requests. We present a solution that implements all the necessary functionalities to bind the user application with the Edge device in charge to execute the “virtual sensors”. In particular, the solution we propose is based on the FIWARE NGSI-LD information model, which helps us to standardize the communication among the different entities involved in the process. Moreover, the paper describes the reference architecture we designed, provides the implementation details of our first prototype and reports the results of our evaluation experiments.",On-Demand and Automatic Deployment of Microservice at the Edge Based on NGSI-LD,"This paper focuses on a new approach to conceiving “virtual sensors” operating in smart environments, which are abstracted components able to map different behaviours on the same Internet of Things (IoT)-based infrastructures according to the needs of the high-level applications. To realize “virtual sensors”, it is necessary to codify user requests in an automation process for the deployment at the Edge of the microservices (MSs) that satisfy such requests. We present a solution that implements all the necessary functionalities to bind the user application with the Edge device in charge to execute the “virtual sensors”. In particular, the solution we propose is based on the FIWARE NGSI-LD information model, which helps us to standardize the communication among the different entities involved in the process. Moreover, the paper describes the reference architecture we designed, provides the implementation details of our first prototype and reports the results of our evaluation experiments.",IEEE conference,no,"['automatic', 'deployment', 'edge', 'based', 'paper', 'focus', 'new', 'virtual', 'sensor', 'operating', 'smart', 'environment', 'able', 'map', 'different', 'behaviour', 'internet', 'thing', 'iot', 'infrastructure', 'according', 'need', 'realize', 'virtual', 'sensor', 'necessary', 'user', 'request', 'automation', 'process', 'deployment', 'edge', 'ms', 'satisfy', 'request', 'present', 'solution', 'implement', 'necessary', 'functionality', 'user', 'edge', 'device', 'charge', 'execute', 'virtual', 'sensor', 'particular', 'solution', 'propose', 'based', 'fiware', 'information', 'model', 'help', 'u', 'communication', 'among', 'different', 'entity', 'involved', 'process', 'moreover', 'paper', 'describes', 'reference', 'designed', 'provides', 'implementation', 'detail', 'first', 'prototype', 'report', 'result', 'evaluation', 'experiment']"
"Load Balancing for Microservice Service Meshes Microservices break down monolithic applications into smaller components and have become a popular model for application deployment. In this model, load balancing has moved from the server to the request side where a load balancer resides with each upstream service.Least Connection, a derivative of the Join the Shortest Queue (JSQ), is a popular algorithm used in the microservice architecture. Despite JSQ being proven to be nearly optimal in certain scenarios, Least Connection significantly underperforms in microservices and edge environments.My thesis aims to adapt load balancing to the microservices environment such that they can autonomously choose from a collection of approaches to mitigate widening of response time distributions.",Load Balancing for Microservice Service Meshes,"Microservices break down monolithic applications into smaller components and have become a popular model for application deployment. In this model, load balancing has moved from the server to the request side where a load balancer resides with each upstream service.Least Connection, a derivative of the Join the Shortest Queue (JSQ), is a popular algorithm used in the microservice architecture. Despite JSQ being proven to be nearly optimal in certain scenarios, Least Connection significantly underperforms in microservices and edge environments.My thesis aims to adapt load balancing to the microservices environment such that they can autonomously choose from a collection of approaches to mitigate widening of response time distributions.",IEEE conference,no,"['load', 'balancing', 'mesh', 'break', 'monolithic', 'smaller', 'become', 'popular', 'model', 'deployment', 'model', 'load', 'balancing', 'moved', 'server', 'request', 'side', 'load', 'balancer', 'upstream', 'connection', 'shortest', 'queue', 'popular', 'algorithm', 'used', 'despite', 'proven', 'nearly', 'optimal', 'certain', 'scenario', 'least', 'connection', 'significantly', 'edge', 'aim', 'adapt', 'load', 'balancing', 'environment', 'autonomously', 'choose', 'collection', 'mitigate', 'response', 'time', 'distribution']"
"On Microservice Architecture Based Communication Environment for Cycling Map Developing and Maintenance Simulator Urban transport infrastructure nowadays involves environmentally friendly modes of transport, the most democratic of which is cycling. Citizens will use bicycles if a reasonably designed cycle path scheme will be provided. Cyclists also need to know the characteristics and load of the planned route before the trip. Prediction can be provided by simulation, but it is often necessary to use heterogeneous and distributed models that require a specific communication environment to ensure interaction. The article describes the easy communication environment that is used to provide microservices communication and data exchange in a bicycle route design and maintenance multi-level simulator.",On Microservice Architecture Based Communication Environment for Cycling Map Developing and Maintenance Simulator,"Urban transport infrastructure nowadays involves environmentally friendly modes of transport, the most democratic of which is cycling. Citizens will use bicycles if a reasonably designed cycle path scheme will be provided. Cyclists also need to know the characteristics and load of the planned route before the trip. Prediction can be provided by simulation, but it is often necessary to use heterogeneous and distributed models that require a specific communication environment to ensure interaction. The article describes the easy communication environment that is used to provide microservices communication and data exchange in a bicycle route design and maintenance multi-level simulator.",IEEE conference,no,"['based', 'communication', 'environment', 'map', 'developing', 'maintenance', 'simulator', 'urban', 'transport', 'infrastructure', 'nowadays', 'involves', 'mode', 'transport', 'citizen', 'use', 'designed', 'cycle', 'path', 'scheme', 'provided', 'also', 'need', 'know', 'characteristic', 'load', 'planned', 'route', 'prediction', 'provided', 'simulation', 'often', 'necessary', 'use', 'heterogeneous', 'distributed', 'model', 'require', 'specific', 'communication', 'environment', 'ensure', 'interaction', 'article', 'describes', 'easy', 'communication', 'environment', 'used', 'provide', 'communication', 'exchange', 'route', 'design', 'maintenance', 'simulator']"
"Design of a Geographic Information System for Forest and Land Fires Based on a Real-Time Database on Microservices Infrastructure Forest and land fires are an increasingly common problem in Indonesia. Fire cases that often occur require a system that is able to detect fires and provide information to users remotely to reduce the impact of fires. Along with the development of hardware technology such as computers, the use of GIS seems to be an effective shortcut in analyzing an event. Kubernetes is an open source platform for managing containerized application workloads, offering declarative configuration and automation. This research designs a Google Maps API System tool for forest and land fires using a real-time database on microservices infrastructure with outputs in the form of fire locations and the results of sensor readings used. Broadly speaking, the processes that occur in the design of the location of forest fire points will be detected by sensors. Then firebase will store forest fire data which will simultaneously be updated on the website. Clients can see the point of forest fires through a browser on their respective desktops. Based on the results of the performance tests that have been carried out, it can be concluded that the use of Kubernetes microclusters can provide advantages when compared to those built monolithically, because Kubernetes microclusters have several advantages, namely having the Horizontal Pod Autoscaler feature, and the Kubernetes microcluster manages components and related services well. Then for each test performed, there was no significant change in memory usage. In the analysis of the results of the comparison data with 7 tests that have been carried out there are 6 tests which mean that the service built with the Kubernetes microcluster is superior to the monolithic one, namely hits per second 2354 ms, latency 3599 ms, response code 720 success code, cpu utilization 13.84%, error rate error rate 0.00%, and throughput 112/sec.",Design of a Geographic Information System for Forest and Land Fires Based on a Real-Time Database on Microservices Infrastructure,"Forest and land fires are an increasingly common problem in Indonesia. Fire cases that often occur require a system that is able to detect fires and provide information to users remotely to reduce the impact of fires. Along with the development of hardware technology such as computers, the use of GIS seems to be an effective shortcut in analyzing an event. Kubernetes is an open source platform for managing containerized application workloads, offering declarative configuration and automation. This research designs a Google Maps API System tool for forest and land fires using a real-time database on microservices infrastructure with outputs in the form of fire locations and the results of sensor readings used. Broadly speaking, the processes that occur in the design of the location of forest fire points will be detected by sensors. Then firebase will store forest fire data which will simultaneously be updated on the website. Clients can see the point of forest fires through a browser on their respective desktops. Based on the results of the performance tests that have been carried out, it can be concluded that the use of Kubernetes microclusters can provide advantages when compared to those built monolithically, because Kubernetes microclusters have several advantages, namely having the Horizontal Pod Autoscaler feature, and the Kubernetes microcluster manages components and related services well. Then for each test performed, there was no significant change in memory usage. In the analysis of the results of the comparison data with 7 tests that have been carried out there are 6 tests which mean that the service built with the Kubernetes microcluster is superior to the monolithic one, namely hits per second 2354 ms, latency 3599 ms, response code 720 success code, cpu utilization 13.84%, error rate error rate 0.00%, and throughput 112/sec.",IEEE conference,no,"['design', 'information', 'forest', 'land', 'fire', 'based', 'database', 'infrastructure', 'forest', 'land', 'fire', 'increasingly', 'common', 'problem', 'fire', 'case', 'often', 'occur', 'require', 'able', 'detect', 'fire', 'provide', 'information', 'user', 'remotely', 'reduce', 'impact', 'fire', 'along', 'development', 'hardware', 'technology', 'computer', 'use', 'seems', 'effective', 'analyzing', 'event', 'kubernetes', 'open', 'source', 'platform', 'managing', 'containerized', 'workload', 'offering', 'declarative', 'configuration', 'automation', 'research', 'design', 'google', 'map', 'api', 'tool', 'forest', 'land', 'fire', 'using', 'database', 'infrastructure', 'output', 'form', 'fire', 'location', 'result', 'sensor', 'reading', 'used', 'process', 'occur', 'design', 'location', 'forest', 'fire', 'point', 'detected', 'sensor', 'store', 'forest', 'fire', 'simultaneously', 'updated', 'website', 'client', 'point', 'forest', 'fire', 'respective', 'based', 'result', 'performance', 'test', 'carried', 'use', 'kubernetes', 'provide', 'advantage', 'compared', 'built', 'kubernetes', 'several', 'advantage', 'namely', 'horizontal', 'pod', 'autoscaler', 'feature', 'kubernetes', 'manages', 'related', 'well', 'test', 'performed', 'significant', 'change', 'memory', 'usage', 'analysis', 'result', 'comparison', 'test', 'carried', 'test', 'mean', 'built', 'kubernetes', 'superior', 'monolithic', 'one', 'namely', 'hit', 'per', 'second', 'm', 'latency', 'm', 'response', 'code', 'success', 'code', 'cpu', 'utilization', 'error', 'rate', 'error', 'rate', 'throughput']"
"PISCO: A smart simulator to deploy energy saving methods in microservices based networks Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present PISCO: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. PISCO is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, PISCO allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies.Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work “Kaligreen” to demonstrate the effectiveness of PISCO.",PISCO: A smart simulator to deploy energy saving methods in microservices based networks,"Nowadays, many researchers work to identify microservices-based application deployments and scheduling solutions to save energy without decreasing functional QoS. In this work, we present PISCO: A simulator that allows facing this challenge in a simple and efficient way, enabling its users to focus uniquely on microservices deployment/scheduling algorithms and its hardware/software repercussions (load vs. energy consumption) without worrying about low-level network configurations or operating system issues. PISCO is able to deploy and schedule (move, duplicate, start/stop) microservices and their dependencies on various devices with software and hardware heterogeneity (CPU, bandwidth, RAM, Battery, etc.), taking into account various scheduling heuristics algorithms: centralized vs non-centralized. To do this, PISCO allows deploying custom network topologies based on client-server schemes or p2p distributions, where devices can (dis)appear, turn on/off obeying random circumstances or user strategies.Finally, the simulator performs relevant operations such as QoS definition, resource monitoring, calculation of energy saved and consumption tracking (at device and network level). We tested some ideas based on our previous work “Kaligreen” to demonstrate the effectiveness of PISCO.",IEEE conference,no,"['pisco', 'smart', 'simulator', 'deploy', 'energy', 'saving', 'method', 'based', 'network', 'nowadays', 'many', 'researcher', 'work', 'identify', 'deployment', 'scheduling', 'solution', 'save', 'energy', 'without', 'decreasing', 'functional', 'qos', 'work', 'present', 'pisco', 'simulator', 'allows', 'facing', 'challenge', 'simple', 'efficient', 'way', 'enabling', 'user', 'focus', 'algorithm', 'load', 'energy', 'consumption', 'without', 'network', 'configuration', 'operating', 'issue', 'pisco', 'able', 'deploy', 'schedule', 'move', 'duplicate', 'dependency', 'various', 'device', 'hardware', 'heterogeneity', 'cpu', 'bandwidth', 'battery', 'etc', 'taking', 'account', 'various', 'scheduling', 'heuristic', 'algorithm', 'centralized', 'v', 'pisco', 'allows', 'deploying', 'custom', 'network', 'topology', 'based', 'scheme', 'distribution', 'device', 'turn', 'random', 'circumstance', 'user', 'simulator', 'performs', 'relevant', 'operation', 'qos', 'definition', 'resource', 'monitoring', 'calculation', 'energy', 'consumption', 'tracking', 'device', 'network', 'level', 'tested', 'idea', 'based', 'previous', 'work', 'demonstrate', 'effectiveness', 'pisco']"
"Towards Microservices-Aware Autoscaling: A Review This research paper elaborates an overview of auto scaling solutions for microservices-based applications deployed with containers. Two main features may characterize the efficiency of an autoscaler: analysis strategy launched to identify the root cause of resource saturation, and resource allocation strategy which selects the eligible components for scaling and calculates the required amount of resources. However, existing solutions do not consider the specificity of microservice architecture in their analysis and resource allocation strategies, which may lead to wrong root cause identification and unnecessary resource allocation. In this paper, we investigate and classify existing autoscalers dealing with containers in microservice context. We additionally specify the strength and the shortcomings of each category. As a conclusion, we report the challenges of such solutions and provide recommendations for future works enabling the development of microservices-aware autoscalers.",Towards Microservices-Aware Autoscaling: A Review,"This research paper elaborates an overview of auto scaling solutions for microservices-based applications deployed with containers. Two main features may characterize the efficiency of an autoscaler: analysis strategy launched to identify the root cause of resource saturation, and resource allocation strategy which selects the eligible components for scaling and calculates the required amount of resources. However, existing solutions do not consider the specificity of microservice architecture in their analysis and resource allocation strategies, which may lead to wrong root cause identification and unnecessary resource allocation. In this paper, we investigate and classify existing autoscalers dealing with containers in microservice context. We additionally specify the strength and the shortcomings of each category. As a conclusion, we report the challenges of such solutions and provide recommendations for future works enabling the development of microservices-aware autoscalers.",IEEE conference,no,"['towards', 'autoscaling', 'review', 'research', 'paper', 'elaborates', 'overview', 'auto', 'scaling', 'solution', 'deployed', 'container', 'two', 'main', 'feature', 'may', 'characterize', 'efficiency', 'autoscaler', 'analysis', 'strategy', 'identify', 'root', 'cause', 'resource', 'resource', 'allocation', 'strategy', 'selects', 'scaling', 'calculates', 'required', 'amount', 'resource', 'however', 'existing', 'solution', 'consider', 'specificity', 'analysis', 'resource', 'allocation', 'strategy', 'may', 'lead', 'root', 'cause', 'identification', 'unnecessary', 'resource', 'allocation', 'paper', 'investigate', 'classify', 'existing', 'autoscalers', 'dealing', 'container', 'context', 'additionally', 'specify', 'strength', 'shortcoming', 'category', 'conclusion', 'report', 'challenge', 'solution', 'provide', 'recommendation', 'future', 'work', 'enabling', 'development', 'autoscalers']"
"Streamlining Software Release Process and Resource Management for Microservice-based Architecture on multi-cloud The software development process is more flexible with the concept of containerization in the microservice platform. This research is on three key components to resolve problems faced by the developers and DevOps teams in the IT industry. First, the development phase expects a fully automated software release process from to the deployment phase and then optimize processes tailored to Docker, and Kubernetes, in microservice-based applications. Then streamline the process and leverage the container orchestration technologies to monitor the main aspect of the development lifecycle through the multi-cloud deployment on demand of the growth of day-today releases on multi-regions. A centralized monitoring platform is developed to monitor the deployed applications and that provides comprehensive visibility regarding performance and health of microservices. At the stage of scalarization in microservices, Vertical Pod Autoscaling (VPA) and Horizontal Pod Autoscaling (HPA) are available approaches for resource allocation, and they require measuring the minimum and maximum resource limits. As a result, an intelligent resource allocation system is proposed using a combination of Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (Bi-LSTM) algorithms to cater to dynamic resource allocation, optimizing scalability, and improving cost-efficiency. This research aims to achieve practical insights into the IT industry’s automated deployment, managing, scaling, and monitoring of microservice-based applications through the mentioned components.",Streamlining Software Release Process and Resource Management for Microservice-based Architecture on multi-cloud,"The software development process is more flexible with the concept of containerization in the microservice platform. This research is on three key components to resolve problems faced by the developers and DevOps teams in the IT industry. First, the development phase expects a fully automated software release process from to the deployment phase and then optimize processes tailored to Docker, and Kubernetes, in microservice-based applications. Then streamline the process and leverage the container orchestration technologies to monitor the main aspect of the development lifecycle through the multi-cloud deployment on demand of the growth of day-today releases on multi-regions. A centralized monitoring platform is developed to monitor the deployed applications and that provides comprehensive visibility regarding performance and health of microservices. At the stage of scalarization in microservices, Vertical Pod Autoscaling (VPA) and Horizontal Pod Autoscaling (HPA) are available approaches for resource allocation, and they require measuring the minimum and maximum resource limits. As a result, an intelligent resource allocation system is proposed using a combination of Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (Bi-LSTM) algorithms to cater to dynamic resource allocation, optimizing scalability, and improving cost-efficiency. This research aims to achieve practical insights into the IT industry’s automated deployment, managing, scaling, and monitoring of microservice-based applications through the mentioned components.",IEEE conference,no,"['streamlining', 'release', 'process', 'resource', 'management', 'development', 'process', 'flexible', 'concept', 'containerization', 'platform', 'research', 'three', 'key', 'resolve', 'problem', 'faced', 'developer', 'devops', 'team', 'industry', 'first', 'development', 'phase', 'fully', 'automated', 'release', 'process', 'deployment', 'phase', 'optimize', 'process', 'tailored', 'docker', 'kubernetes', 'process', 'leverage', 'container', 'orchestration', 'technology', 'monitor', 'main', 'aspect', 'development', 'lifecycle', 'deployment', 'demand', 'growth', 'release', 'centralized', 'monitoring', 'platform', 'developed', 'monitor', 'deployed', 'provides', 'comprehensive', 'visibility', 'regarding', 'performance', 'health', 'stage', 'vertical', 'pod', 'autoscaling', 'horizontal', 'pod', 'autoscaling', 'available', 'resource', 'allocation', 'require', 'measuring', 'minimum', 'maximum', 'resource', 'limit', 'result', 'intelligent', 'resource', 'allocation', 'proposed', 'using', 'combination', 'neural', 'network', 'long', 'memory', 'algorithm', 'dynamic', 'resource', 'allocation', 'optimizing', 'scalability', 'improving', 'research', 'aim', 'achieve', 'practical', 'insight', 'industry', 'automated', 'deployment', 'managing', 'scaling', 'monitoring']"
"Latency-Aware Microservice Deployment for Edge AI Enabled Video Analytics Video analytics plays a pivotal role in public safety (e.g., criminal suspect detection, traffic flow count, and illegal parking management), which assists the polices in monitoring all anomalous events in the street. In this paper, we consider the scenario with multiple video analytics applications from a single video stream. However, traditional monolithic architecture based video analytics applications shall seriously increase the response latency due to the resource contention of repetitive components. Therefore, we utilize the microservice architecture based video analytics (MAVA) to share the universal microser-vices in different applications, which shall decrease the response latency by reducing the computation load and increasing the resource utilization. To further achieve fast and accurate video analytics, the video analytics microservices are deployed in the edge closing to the cameras and users, and artificial intelligence (AI) methods are used in the microservices to realize specified functions. Therefore, an edge AI enabled MAVA (EAI-MAVA) architecture is proposed to achieve accurate video analytics in real-time. Furthermore, we formulate a microservice deployment problem to determine the location of each microservice in EAI-MAVA, which minimizes the response latency of all applications by considering the resource demands of microservices and the resource constraints of heterogeneous edge devices. Finally, a greedy-based heuristic algorithm is proposed to solve the non-convex microservice deployment problem, which obtains a sub-optimal solution with small loss of accuracy and reduces the solution time obviously.",Latency-Aware Microservice Deployment for Edge AI Enabled Video Analytics,"Video analytics plays a pivotal role in public safety (e.g., criminal suspect detection, traffic flow count, and illegal parking management), which assists the polices in monitoring all anomalous events in the street. In this paper, we consider the scenario with multiple video analytics applications from a single video stream. However, traditional monolithic architecture based video analytics applications shall seriously increase the response latency due to the resource contention of repetitive components. Therefore, we utilize the microservice architecture based video analytics (MAVA) to share the universal microser-vices in different applications, which shall decrease the response latency by reducing the computation load and increasing the resource utilization. To further achieve fast and accurate video analytics, the video analytics microservices are deployed in the edge closing to the cameras and users, and artificial intelligence (AI) methods are used in the microservices to realize specified functions. Therefore, an edge AI enabled MAVA (EAI-MAVA) architecture is proposed to achieve accurate video analytics in real-time. Furthermore, we formulate a microservice deployment problem to determine the location of each microservice in EAI-MAVA, which minimizes the response latency of all applications by considering the resource demands of microservices and the resource constraints of heterogeneous edge devices. Finally, a greedy-based heuristic algorithm is proposed to solve the non-convex microservice deployment problem, which obtains a sub-optimal solution with small loss of accuracy and reduces the solution time obviously.",IEEE conference,no,"['deployment', 'edge', 'ai', 'enabled', 'video', 'analytics', 'video', 'analytics', 'play', 'pivotal', 'role', 'public', 'safety', 'detection', 'traffic', 'flow', 'count', 'management', 'assist', 'monitoring', 'event', 'paper', 'consider', 'scenario', 'multiple', 'video', 'analytics', 'single', 'video', 'stream', 'however', 'traditional', 'monolithic', 'based', 'video', 'analytics', 'shall', 'increase', 'response', 'latency', 'due', 'resource', 'contention', 'therefore', 'utilize', 'based', 'video', 'analytics', 'share', 'universal', 'different', 'shall', 'decrease', 'response', 'latency', 'reducing', 'computation', 'load', 'increasing', 'resource', 'utilization', 'achieve', 'fast', 'accurate', 'video', 'analytics', 'video', 'analytics', 'deployed', 'edge', 'camera', 'user', 'artificial', 'intelligence', 'ai', 'method', 'used', 'realize', 'specified', 'function', 'therefore', 'edge', 'ai', 'enabled', 'proposed', 'achieve', 'accurate', 'video', 'analytics', 'furthermore', 'formulate', 'deployment', 'problem', 'determine', 'location', 'minimizes', 'response', 'latency', 'considering', 'resource', 'demand', 'resource', 'constraint', 'heterogeneous', 'edge', 'device', 'finally', 'heuristic', 'algorithm', 'proposed', 'solve', 'deployment', 'problem', 'solution', 'small', 'loss', 'accuracy', 'reduces', 'solution', 'time']"
"Container-Based Microservices SDN Control Plane for Open Disaggregated Optical Networks Optical networks are undergoing a massive transformation heading the open optical systems with services automation. The Software Defined Networking (SDN), Open Line System (OLS) disaggregation and standardization of data models including IETF, OpenConfig, OpenROADM, Transport-API, are the key factors driving this transformation. In this paper, we first introduce the evolution of optical networks, then we present a container-based microservices SDN control platform and explain how this optical network control platform as a service supports this evolution. Finally, a demonstration of the container-based microservices platform with 4-step scenario is presented and validated.",Container-Based Microservices SDN Control Plane for Open Disaggregated Optical Networks,"Optical networks are undergoing a massive transformation heading the open optical systems with services automation. The Software Defined Networking (SDN), Open Line System (OLS) disaggregation and standardization of data models including IETF, OpenConfig, OpenROADM, Transport-API, are the key factors driving this transformation. In this paper, we first introduce the evolution of optical networks, then we present a container-based microservices SDN control platform and explain how this optical network control platform as a service supports this evolution. Finally, a demonstration of the container-based microservices platform with 4-step scenario is presented and validated.",IEEE conference,no,"['sdn', 'control', 'plane', 'open', 'disaggregated', 'optical', 'network', 'optical', 'network', 'massive', 'transformation', 'open', 'optical', 'automation', 'defined', 'networking', 'sdn', 'open', 'line', 'model', 'including', 'key', 'factor', 'driving', 'transformation', 'paper', 'first', 'introduce', 'evolution', 'optical', 'network', 'present', 'sdn', 'control', 'platform', 'explain', 'optical', 'network', 'control', 'platform', 'support', 'evolution', 'finally', 'demonstration', 'platform', 'scenario', 'presented', 'validated']"
"Study of Microservice Execution Framework Using Spoken Dialogue Agents Japan is currently facing super-aging society and the assistive technology for self-help and mutual aid of the elderly is becoming urgent. The purpose of this paper is to build a system that can execute various services through dialogue with agents, in order to support elderly people who cannot use Internet services due to lack of access to devices. To achieve the goal, we discuss a framework for executing microservices using dialogue agents. More specifically, the framework consists of the next two essential elements: (1) Managing user information for the various microservices centrally. (2) Configuring the behavior of the agents when executing the services correctly. In the proposed method, we first discuss each element in detail. Then, we demonstrate the effectiveness of the framework by applying it to the actual integration of a dialogue agent and several microservices.",Study of Microservice Execution Framework Using Spoken Dialogue Agents,"Japan is currently facing super-aging society and the assistive technology for self-help and mutual aid of the elderly is becoming urgent. The purpose of this paper is to build a system that can execute various services through dialogue with agents, in order to support elderly people who cannot use Internet services due to lack of access to devices. To achieve the goal, we discuss a framework for executing microservices using dialogue agents. More specifically, the framework consists of the next two essential elements: (1) Managing user information for the various microservices centrally. (2) Configuring the behavior of the agents when executing the services correctly. In the proposed method, we first discuss each element in detail. Then, we demonstrate the effectiveness of the framework by applying it to the actual integration of a dialogue agent and several microservices.",IEEE conference,no,"['study', 'execution', 'framework', 'using', 'dialogue', 'agent', 'currently', 'facing', 'technology', 'mutual', 'aid', 'elderly', 'becoming', 'urgent', 'purpose', 'paper', 'build', 'execute', 'various', 'dialogue', 'agent', 'order', 'support', 'elderly', 'people', 'use', 'internet', 'due', 'lack', 'access', 'device', 'achieve', 'goal', 'discus', 'framework', 'executing', 'using', 'dialogue', 'agent', 'specifically', 'framework', 'consists', 'next', 'two', 'essential', 'element', 'managing', 'user', 'information', 'various', 'configuring', 'behavior', 'agent', 'executing', 'correctly', 'proposed', 'method', 'first', 'discus', 'element', 'detail', 'demonstrate', 'effectiveness', 'framework', 'applying', 'actual', 'integration', 'dialogue', 'agent', 'several']"
"Intelligent Agent support for Topology Learning in microservices-based SDN Controller The softwarization of networks is increasingly spreading, and one of the main paradigms is SDN (Software-Defined Networking), which allows overcoming the limitations mainly arising from the integration of the control plane and the forwarding plane within the network devices. It extracts the control plane to place it within a new logically centralized component: the SDN controller. Since this is a monolithic architecture that limits reliability and scalability, distributed solutions based on microservices have been proposed in the literature. In parallel, Agents are fully intelligent, atomic, and autonomous decision-making units that can be flexibly recomposed to create a completely autonomous network system. They also have the ability to replicate single or multiple decision-making processes that collaborate with each other. The development of future networks such as 5G, including 6G, is pushing towards the concept of network management automation and integration of intelligence, making agents an excellent means to meet this trend. This paper first introduces intelligence in the form of agents to a distributed SDN controller based on microservices, by implementing two new functionalities: topology _learning and shortest path, Then, it leverages a microservices-based SDN solution based on Ryu SDN framework, named MSN, to run agents in a Docker Container environment. Multiple measurements were performed locally in a single machine. Results show the topology learning performances compared with several network topologies. Moreover, the shortest patti agent experimental evaluations show the knowledge size depends on the network topology and the performances of different algorithms.",Intelligent Agent support for Topology Learning in microservices-based SDN Controller,"The softwarization of networks is increasingly spreading, and one of the main paradigms is SDN (Software-Defined Networking), which allows overcoming the limitations mainly arising from the integration of the control plane and the forwarding plane within the network devices. It extracts the control plane to place it within a new logically centralized component: the SDN controller. Since this is a monolithic architecture that limits reliability and scalability, distributed solutions based on microservices have been proposed in the literature. In parallel, Agents are fully intelligent, atomic, and autonomous decision-making units that can be flexibly recomposed to create a completely autonomous network system. They also have the ability to replicate single or multiple decision-making processes that collaborate with each other. The development of future networks such as 5G, including 6G, is pushing towards the concept of network management automation and integration of intelligence, making agents an excellent means to meet this trend. This paper first introduces intelligence in the form of agents to a distributed SDN controller based on microservices, by implementing two new functionalities: topology _learning and shortest path, Then, it leverages a microservices-based SDN solution based on Ryu SDN framework, named MSN, to run agents in a Docker Container environment. Multiple measurements were performed locally in a single machine. Results show the topology learning performances compared with several network topologies. Moreover, the shortest patti agent experimental evaluations show the knowledge size depends on the network topology and the performances of different algorithms.",IEEE conference,no,"['intelligent', 'agent', 'support', 'topology', 'learning', 'sdn', 'controller', 'network', 'increasingly', 'one', 'main', 'paradigm', 'sdn', 'networking', 'allows', 'overcoming', 'limitation', 'mainly', 'integration', 'control', 'plane', 'plane', 'within', 'network', 'device', 'extract', 'control', 'plane', 'place', 'within', 'new', 'logically', 'centralized', 'sdn', 'controller', 'since', 'monolithic', 'limit', 'reliability', 'scalability', 'distributed', 'solution', 'based', 'proposed', 'literature', 'parallel', 'agent', 'fully', 'intelligent', 'autonomous', 'unit', 'flexibly', 'create', 'completely', 'autonomous', 'network', 'also', 'ability', 'single', 'multiple', 'process', 'collaborate', 'development', 'future', 'network', 'including', 'pushing', 'towards', 'concept', 'network', 'management', 'automation', 'integration', 'intelligence', 'making', 'agent', 'excellent', 'mean', 'meet', 'trend', 'paper', 'first', 'introduces', 'intelligence', 'form', 'agent', 'distributed', 'sdn', 'controller', 'based', 'implementing', 'two', 'new', 'functionality', 'topology', 'shortest', 'path', 'leverage', 'sdn', 'solution', 'based', 'sdn', 'framework', 'named', 'run', 'agent', 'docker', 'container', 'environment', 'multiple', 'measurement', 'performed', 'locally', 'single', 'machine', 'result', 'show', 'topology', 'learning', 'performance', 'compared', 'several', 'network', 'topology', 'moreover', 'shortest', 'agent', 'experimental', 'evaluation', 'show', 'knowledge', 'size', 'depends', 'network', 'topology', 'performance', 'different', 'algorithm']"
"From Models to Microservices: Easily Operationalizing Machine Learning models Although Machine Learning and Deep Learning have advanced significantly in their ability to perform supervised, unsupervised and reinforcement learning tasks quite well, integrating them into applications for which they are meant to provide intelligence is not as seamless as it should be. There are many reasons for this - primary being that the kind of skills needed to understand the business need, understand the data, develop models, develop applications, and integrate models with applications are different and it is very difficult for one person or just a group of application developers or just a group of data scientists to have all these skills. This makes it challenging to productionalize the developed ML models fast or at all in order to experiment with them further. This paper demonstrates how certain components of the Acumos AI platform project can be used to take models developed using H2o, Java, Spark to production by deploying them as microservices, automatically. The same concept can be (and has been extended by Acumos) to Python, R and ONNX models. What this enables is - multiple heterogeneous models written by different developers or different teams or different organizations in different languages and frameworks becoming functioning microservices that would provide intelligent APIs to the business application in question. These models can then be easily shared with different individuals and organizations and operationalized easily. Using Acumos, these microservices can be deployed such that they also communicate and co-ordinate with each other to do much more complex tasks. We will talk about the 5G Network Slicing usecase, create an ML model for 5G Network Slicing and use certain Acumos components to make it shareable and operationalize it as a predicting microservice.",From Models to Microservices: Easily Operationalizing Machine Learning models,"Although Machine Learning and Deep Learning have advanced significantly in their ability to perform supervised, unsupervised and reinforcement learning tasks quite well, integrating them into applications for which they are meant to provide intelligence is not as seamless as it should be. There are many reasons for this - primary being that the kind of skills needed to understand the business need, understand the data, develop models, develop applications, and integrate models with applications are different and it is very difficult for one person or just a group of application developers or just a group of data scientists to have all these skills. This makes it challenging to productionalize the developed ML models fast or at all in order to experiment with them further. This paper demonstrates how certain components of the Acumos AI platform project can be used to take models developed using H2o, Java, Spark to production by deploying them as microservices, automatically. The same concept can be (and has been extended by Acumos) to Python, R and ONNX models. What this enables is - multiple heterogeneous models written by different developers or different teams or different organizations in different languages and frameworks becoming functioning microservices that would provide intelligent APIs to the business application in question. These models can then be easily shared with different individuals and organizations and operationalized easily. Using Acumos, these microservices can be deployed such that they also communicate and co-ordinate with each other to do much more complex tasks. We will talk about the 5G Network Slicing usecase, create an ML model for 5G Network Slicing and use certain Acumos components to make it shareable and operationalize it as a predicting microservice.",IEEE conference,no,"['model', 'easily', 'machine', 'learning', 'model', 'although', 'machine', 'learning', 'deep', 'learning', 'advanced', 'significantly', 'ability', 'perform', 'supervised', 'unsupervised', 'reinforcement', 'learning', 'task', 'quite', 'well', 'integrating', 'provide', 'intelligence', 'seamless', 'many', 'reason', 'primary', 'kind', 'skill', 'needed', 'understand', 'business', 'need', 'understand', 'develop', 'model', 'develop', 'integrate', 'model', 'different', 'difficult', 'one', 'person', 'group', 'developer', 'group', 'scientist', 'skill', 'make', 'challenging', 'developed', 'ml', 'model', 'fast', 'order', 'experiment', 'paper', 'demonstrates', 'certain', 'acumos', 'ai', 'platform', 'project', 'used', 'take', 'model', 'developed', 'using', 'java', 'spark', 'production', 'deploying', 'automatically', 'concept', 'extended', 'acumos', 'python', 'model', 'enables', 'multiple', 'heterogeneous', 'model', 'written', 'different', 'developer', 'different', 'team', 'different', 'organization', 'different', 'language', 'framework', 'becoming', 'functioning', 'would', 'provide', 'intelligent', 'apis', 'business', 'question', 'model', 'easily', 'shared', 'different', 'individual', 'organization', 'easily', 'using', 'acumos', 'deployed', 'also', 'communicate', 'much', 'complex', 'task', 'talk', 'network', 'slicing', 'create', 'ml', 'model', 'network', 'slicing', 'use', 'certain', 'acumos', 'make', 'predicting']"
"CaMP-INC: Components-aware Microservices Placement for In-Network Computing Cloud-Edge Continuum Microservices are a promising technology for future networks, and many research efforts have been devoted to optimally placing microservices in cloud data centers. However, microservices deployment in edge and in-network devices is more expensive than the cloud. Additionally, several works do not consider the main requirements of microservice architecture, such as service registry, failure detection, and each microservice's specific database. This paper investigates the problem of placing components (i.e. microservices and their corresponding databases) while considering physical nodes' failure and the distance to service registries. We propose a Components-aware Microservices Placement for In-Network Computing Cloud-Edge Continuum (CaMP-INC). We formulate an Integer Linear Programming (ILP) problem with the objective of cost minimization. Due to the problem's $\mathcal{NP}$-hardness, we propose a heuristic solution. Numerical results demonstrate that our proposed solution CaMP-INC reduces the total cost by 15.8% on average and has a superior performance in terms of latency minimization compared to benchmarks.",CaMP-INC: Components-aware Microservices Placement for In-Network Computing Cloud-Edge Continuum,"Microservices are a promising technology for future networks, and many research efforts have been devoted to optimally placing microservices in cloud data centers. However, microservices deployment in edge and in-network devices is more expensive than the cloud. Additionally, several works do not consider the main requirements of microservice architecture, such as service registry, failure detection, and each microservice's specific database. This paper investigates the problem of placing components (i.e. microservices and their corresponding databases) while considering physical nodes' failure and the distance to service registries. We propose a Components-aware Microservices Placement for In-Network Computing Cloud-Edge Continuum (CaMP-INC). We formulate an Integer Linear Programming (ILP) problem with the objective of cost minimization. Due to the problem's $\mathcal{NP}$-hardness, we propose a heuristic solution. Numerical results demonstrate that our proposed solution CaMP-INC reduces the total cost by 15.8% on average and has a superior performance in terms of latency minimization compared to benchmarks.",IEEE conference,no,"['placement', 'computing', 'continuum', 'promising', 'technology', 'future', 'network', 'many', 'research', 'effort', 'optimally', 'placing', 'center', 'however', 'deployment', 'edge', 'device', 'expensive', 'additionally', 'several', 'work', 'consider', 'main', 'requirement', 'registry', 'failure', 'detection', 'specific', 'database', 'paper', 'investigates', 'problem', 'placing', 'corresponding', 'database', 'considering', 'physical', 'node', 'failure', 'distance', 'registry', 'propose', 'placement', 'computing', 'continuum', 'formulate', 'linear', 'programming', 'problem', 'objective', 'cost', 'minimization', 'due', 'problem', 'propose', 'heuristic', 'solution', 'numerical', 'result', 'demonstrate', 'proposed', 'solution', 'reduces', 'total', 'cost', 'average', 'superior', 'performance', 'term', 'latency', 'minimization', 'compared', 'benchmark']"
"An NFV and microservice based architecture for on-the-fly component provisioning in content delivery networks Content Delivery Networks (CDNs) deliver content (e.g. Web pages, videos) to geographically distributed end-users over the Internet. Some contents do sometimes attract the attention of a large group of end-users. This often leads to flash crowds which can cause major issues such as outage in the CDN. Microservice architectural style aims at decomposing monolithic systems into smaller components which can be independently deployed, upgraded and disposed. Network Function Virtualization (NFV) is an emerging technology that aims to reduce costs and bring agility by decoupling network functions from the underlying hardware. This paper leverages the NFV and microservice architectural style to propose an architecture for on-the-fly CDN component provisioning to tackle issues such as flash crowds. In the proposed architecture, CDN components are designed as sets of microservices which interact via RESTFul Web services and are provisioned as Virtual Network Functions (VNFs), which are deployed and orchestrated on-the-fly. We have built a prototype in which a CDN surrogate server, designed as a set of microservices, is deployed on-the-fly. The prototype is deployed on SAVI, a Canadian distributed test bed for future Internet applications. The performance is also evaluated.",An NFV and microservice based architecture for on-the-fly component provisioning in content delivery networks,"Content Delivery Networks (CDNs) deliver content (e.g. Web pages, videos) to geographically distributed end-users over the Internet. Some contents do sometimes attract the attention of a large group of end-users. This often leads to flash crowds which can cause major issues such as outage in the CDN. Microservice architectural style aims at decomposing monolithic systems into smaller components which can be independently deployed, upgraded and disposed. Network Function Virtualization (NFV) is an emerging technology that aims to reduce costs and bring agility by decoupling network functions from the underlying hardware. This paper leverages the NFV and microservice architectural style to propose an architecture for on-the-fly CDN component provisioning to tackle issues such as flash crowds. In the proposed architecture, CDN components are designed as sets of microservices which interact via RESTFul Web services and are provisioned as Virtual Network Functions (VNFs), which are deployed and orchestrated on-the-fly. We have built a prototype in which a CDN surrogate server, designed as a set of microservices, is deployed on-the-fly. The prototype is deployed on SAVI, a Canadian distributed test bed for future Internet applications. The performance is also evaluated.",IEEE conference,no,"['nfv', 'based', 'provisioning', 'content', 'delivery', 'network', 'content', 'delivery', 'network', 'deliver', 'content', 'web', 'video', 'geographically', 'distributed', 'internet', 'content', 'attention', 'large', 'group', 'often', 'lead', 'cause', 'major', 'issue', 'cdn', 'architectural', 'style', 'aim', 'decomposing', 'monolithic', 'smaller', 'independently', 'deployed', 'upgraded', 'network', 'function', 'virtualization', 'nfv', 'emerging', 'technology', 'aim', 'reduce', 'cost', 'bring', 'agility', 'decoupling', 'network', 'function', 'underlying', 'hardware', 'paper', 'leverage', 'nfv', 'architectural', 'style', 'propose', 'cdn', 'provisioning', 'tackle', 'issue', 'proposed', 'cdn', 'designed', 'set', 'interact', 'via', 'restful', 'web', 'provisioned', 'virtual', 'network', 'function', 'vnfs', 'deployed', 'orchestrated', 'built', 'prototype', 'cdn', 'server', 'designed', 'set', 'deployed', 'prototype', 'deployed', 'distributed', 'test', 'future', 'internet', 'performance', 'also', 'evaluated']"
"Enabling the Deployment of Any-Scale Robotic Applications in Microservice Architectures through Automated Containerization* In an increasingly automated world – from ware-house robots to self-driving cars – streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at github.com/ika-rwth-aachen/dorotos.",Enabling the Deployment of Any-Scale Robotic Applications in Microservice Architectures through Automated Containerization*,"In an increasingly automated world – from ware-house robots to self-driving cars – streamlining the development and deployment process and operations of robotic applications becomes ever more important. Automated DevOps processes and microservice architectures have already proven successful in other domains such as large-scale customer-oriented web services (e.g., Netflix). We recommend to employ similar microservice architectures for the deployment of small- to large-scale robotic applications in order to accelerate development cycles, loosen functional dependence, and improve resiliency and elasticity. In order to facilitate involved DevOps processes, we present and release a tooling suite for automating the development of microservices for robotic applications based on the Robot Operating System (ROS). Our tooling suite covers the automated minimal containerization of ROS applications, a collection of useful machine learning-enabled base container images, as well as a CLI tool for simplified interaction with container images during the development phase. Within the scope of this paper, we embed our tooling suite into the overall context of streamlined robotics deployment and compare it to alternative solutions. We release our tools as open-source software at github.com/ika-rwth-aachen/dorotos.",IEEE conference,no,"['enabling', 'deployment', 'robotic', 'automated', 'containerization', 'increasingly', 'automated', 'world', 'robot', 'streamlining', 'development', 'deployment', 'process', 'operation', 'robotic', 'becomes', 'ever', 'important', 'automated', 'devops', 'process', 'already', 'proven', 'successful', 'domain', 'web', 'netflix', 'recommend', 'employ', 'similar', 'deployment', 'robotic', 'order', 'accelerate', 'development', 'cycle', 'functional', 'dependence', 'improve', 'resiliency', 'elasticity', 'order', 'facilitate', 'involved', 'devops', 'process', 'present', 'release', 'tooling', 'suite', 'automating', 'development', 'robotic', 'based', 'robot', 'operating', 'ro', 'tooling', 'suite', 'cover', 'automated', 'minimal', 'containerization', 'ro', 'collection', 'useful', 'machine', 'base', 'container', 'image', 'well', 'tool', 'simplified', 'interaction', 'container', 'image', 'development', 'phase', 'within', 'scope', 'paper', 'tooling', 'suite', 'overall', 'context', 'streamlined', 'robotics', 'deployment', 'compare', 'alternative', 'solution', 'release', 'tool']"
"Microservice Driven Parallel Framework for Image Collection and Processing in the Large-scale Laser Beam Collimation System Laser beam collimation involving image collection and processing is an essential task in the high-power laser experiment. With the development of worldwide high-power laser facilities, both the type and the amount of image collection devices have increased, which contributes to a large-scale heterogeneous image collection system and a highly concurrent image data environment. It is challenging our current image collection and processing system, and requiring a more efficient control pattern. Parallelism by utilizing the multi-thread technology is an effective solution, while a big obstacle is that image collection devices can not be parallel driven, thus we obtain the pseudo parallelism. To address this issue, giving the existing microservice based distributed architecture utilized by the laser beam collimation system, we equip each of these devices with an embedded computer. We conduct the co-design of hardware and software for each image collection device, such that each embedded computer serves as an independent and parallel servant of image collection and processing. This parallel framework is expected to reduce the time consumption of image acquisition and processing. What's more, hardware customization can be further implemented.",Microservice Driven Parallel Framework for Image Collection and Processing in the Large-scale Laser Beam Collimation System,"Laser beam collimation involving image collection and processing is an essential task in the high-power laser experiment. With the development of worldwide high-power laser facilities, both the type and the amount of image collection devices have increased, which contributes to a large-scale heterogeneous image collection system and a highly concurrent image data environment. It is challenging our current image collection and processing system, and requiring a more efficient control pattern. Parallelism by utilizing the multi-thread technology is an effective solution, while a big obstacle is that image collection devices can not be parallel driven, thus we obtain the pseudo parallelism. To address this issue, giving the existing microservice based distributed architecture utilized by the laser beam collimation system, we equip each of these devices with an embedded computer. We conduct the co-design of hardware and software for each image collection device, such that each embedded computer serves as an independent and parallel servant of image collection and processing. This parallel framework is expected to reduce the time consumption of image acquisition and processing. What's more, hardware customization can be further implemented.",IEEE conference,no,"['driven', 'parallel', 'framework', 'image', 'collection', 'processing', 'laser', 'beam', 'collimation', 'laser', 'beam', 'collimation', 'involving', 'image', 'collection', 'processing', 'essential', 'task', 'laser', 'experiment', 'development', 'laser', 'facility', 'type', 'amount', 'image', 'collection', 'device', 'increased', 'contributes', 'heterogeneous', 'image', 'collection', 'highly', 'concurrent', 'image', 'environment', 'challenging', 'current', 'image', 'collection', 'processing', 'requiring', 'efficient', 'control', 'pattern', 'parallelism', 'utilizing', 'technology', 'effective', 'solution', 'big', 'image', 'collection', 'device', 'parallel', 'driven', 'thus', 'obtain', 'parallelism', 'address', 'issue', 'giving', 'existing', 'based', 'distributed', 'utilized', 'laser', 'beam', 'collimation', 'device', 'embedded', 'computer', 'conduct', 'hardware', 'image', 'collection', 'device', 'embedded', 'computer', 'serf', 'independent', 'parallel', 'image', 'collection', 'processing', 'parallel', 'framework', 'expected', 'reduce', 'time', 'consumption', 'image', 'acquisition', 'processing', 'hardware', 'customization', 'implemented']"
"An Microservices-Based Openstack Monitoring Tool Monitoring of openstack clouds is an imperative necessity for cloud providers and administrators to analyze and discover what is happening in the cloud. In this paper, a microservices-based openstack monitoring system, namely openstack-reporter, is proposed, aiming at monitoring openstack clouds and providing a convenient tool for the administrators. Openstack-reporter adopting microservices architecture consists of three constituent components where each component is responsible for a single aspect. The connection among these components is implemented with the help of kubernetes DNS-based service discovery and Role-based access control (RBAC) mechanisms. The management of these three components is performed by kubernetes which is devoted to automate rollouts, rollbacks. We have released the openstack-reporter, docker images and kubernetes configurations which can be accessed publicly. One can easily build the openstack monitoring system just by deploying the openstack-reporter in our kubernetes cluster. To validate the performance of the proposed monitoring system, an openstack platform and openstack-reporter have been built in our datacenter and the management center respectively, and the results are displayed.",An Microservices-Based Openstack Monitoring Tool,"Monitoring of openstack clouds is an imperative necessity for cloud providers and administrators to analyze and discover what is happening in the cloud. In this paper, a microservices-based openstack monitoring system, namely openstack-reporter, is proposed, aiming at monitoring openstack clouds and providing a convenient tool for the administrators. Openstack-reporter adopting microservices architecture consists of three constituent components where each component is responsible for a single aspect. The connection among these components is implemented with the help of kubernetes DNS-based service discovery and Role-based access control (RBAC) mechanisms. The management of these three components is performed by kubernetes which is devoted to automate rollouts, rollbacks. We have released the openstack-reporter, docker images and kubernetes configurations which can be accessed publicly. One can easily build the openstack monitoring system just by deploying the openstack-reporter in our kubernetes cluster. To validate the performance of the proposed monitoring system, an openstack platform and openstack-reporter have been built in our datacenter and the management center respectively, and the results are displayed.",IEEE conference,no,"['openstack', 'monitoring', 'tool', 'monitoring', 'openstack', 'imperative', 'necessity', 'provider', 'administrator', 'analyze', 'discover', 'happening', 'paper', 'openstack', 'monitoring', 'namely', 'proposed', 'aiming', 'monitoring', 'openstack', 'providing', 'convenient', 'tool', 'administrator', 'adopting', 'consists', 'three', 'constituent', 'responsible', 'single', 'aspect', 'connection', 'among', 'implemented', 'help', 'kubernetes', 'discovery', 'access', 'control', 'mechanism', 'management', 'three', 'performed', 'kubernetes', 'automate', 'released', 'docker', 'image', 'kubernetes', 'configuration', 'accessed', 'publicly', 'one', 'easily', 'build', 'openstack', 'monitoring', 'deploying', 'kubernetes', 'cluster', 'validate', 'performance', 'proposed', 'monitoring', 'openstack', 'platform', 'built', 'datacenter', 'management', 'center', 'respectively', 'result', 'displayed']"
"Beyond Monoliths: An In-Depth Analysis of Microservices Adoption in the Era of Kubernetes In today’s rapidly evolving technology environment, software development and deployment paradigms have drastically changed from traditional monolithic architectures to highly scalable flexible microservices, facilitated by the widespread adoption of Kubernetes. Our study explores the complexities of this shift in the era of Kubernetes and examines the challenges, benefits, and best practices associated with microservice adoption. The focus of our research is to implement a MERN application on Kubernetes, which is a common and efficient approach controlling modern web applications. Using infrastructure as a code principle, we use Terraform to deliver resources on AWS, laying a solid foundation for our deployment pipeline. This pipeline, developed by CircleCI, performs core tasks such as creating and pushing Docker images to Docker Hub, ensuring seamless integration, and moving code changes through our adopted GitOps workflow so, we promote a declarative approach to infrastructure management, where application repositories act as a single source of truth. Any deviations from the desired state are quickly resolved with ArgoCD, maintaining consistency and reliability within the Kubernetes cluster. In addition, our research includes a comparative analysis of scaling strategies as opposed to virtual machine scaling and container scaling in a Kubernetes ecosystem. Furthermore, we explore various deployment strategies available in Kubernetes, evaluating their efficiency in different scenarios and use cases.",Beyond Monoliths: An In-Depth Analysis of Microservices Adoption in the Era of Kubernetes,"In today’s rapidly evolving technology environment, software development and deployment paradigms have drastically changed from traditional monolithic architectures to highly scalable flexible microservices, facilitated by the widespread adoption of Kubernetes. Our study explores the complexities of this shift in the era of Kubernetes and examines the challenges, benefits, and best practices associated with microservice adoption. The focus of our research is to implement a MERN application on Kubernetes, which is a common and efficient approach controlling modern web applications. Using infrastructure as a code principle, we use Terraform to deliver resources on AWS, laying a solid foundation for our deployment pipeline. This pipeline, developed by CircleCI, performs core tasks such as creating and pushing Docker images to Docker Hub, ensuring seamless integration, and moving code changes through our adopted GitOps workflow so, we promote a declarative approach to infrastructure management, where application repositories act as a single source of truth. Any deviations from the desired state are quickly resolved with ArgoCD, maintaining consistency and reliability within the Kubernetes cluster. In addition, our research includes a comparative analysis of scaling strategies as opposed to virtual machine scaling and container scaling in a Kubernetes ecosystem. Furthermore, we explore various deployment strategies available in Kubernetes, evaluating their efficiency in different scenarios and use cases.",IEEE conference,no,"['beyond', 'monolith', 'analysis', 'adoption', 'era', 'kubernetes', 'today', 'rapidly', 'evolving', 'technology', 'environment', 'development', 'deployment', 'paradigm', 'drastically', 'changed', 'traditional', 'monolithic', 'highly', 'scalable', 'flexible', 'facilitated', 'widespread', 'adoption', 'kubernetes', 'study', 'explores', 'complexity', 'shift', 'era', 'kubernetes', 'examines', 'challenge', 'benefit', 'best', 'practice', 'associated', 'adoption', 'focus', 'research', 'implement', 'kubernetes', 'common', 'efficient', 'controlling', 'modern', 'web', 'using', 'infrastructure', 'code', 'principle', 'use', 'terraform', 'deliver', 'resource', 'aws', 'solid', 'foundation', 'deployment', 'pipeline', 'pipeline', 'developed', 'performs', 'core', 'task', 'creating', 'pushing', 'docker', 'image', 'docker', 'hub', 'ensuring', 'seamless', 'integration', 'moving', 'code', 'change', 'adopted', 'gitops', 'workflow', 'promote', 'declarative', 'infrastructure', 'management', 'repository', 'act', 'single', 'source', 'truth', 'desired', 'state', 'quickly', 'maintaining', 'consistency', 'reliability', 'within', 'kubernetes', 'cluster', 'addition', 'research', 'includes', 'comparative', 'analysis', 'scaling', 'strategy', 'virtual', 'machine', 'scaling', 'container', 'scaling', 'kubernetes', 'ecosystem', 'furthermore', 'explore', 'various', 'deployment', 'strategy', 'available', 'kubernetes', 'evaluating', 'efficiency', 'different', 'scenario', 'use', 'case']"
"An application of microservices architecture pattern to create a modular computer numerical control system Currently, the most common approach to Computer Numerical Control (CNC) system design is a monolithic architecture. However, the introduction of the concept of Cyber-Physical Production Systems (CPPS) requires a paradigm shift in the design of control systems. This paper suggests a new approach to developing modular industrial equipment using a microservices architecture pattern. Microservices architecture features are addressed, as well as advantages and disadvantages. A heterogeneous computer network, where nodes communicate via a message queue, is proposed as a basis for the computer numerical control system. Fault tolerance is provided by modules full autonomy and reliable messaging. Furthermore, NoSQL database, guaranteeing high data accessibility and increased data access speed, is applied. An apparatus for selective photopolymer laser curing of free-form surfaces is considered as an example. Common setup structure, as well as main hardware and software modules, are described. Moreover, a distributed network latency simulation was carried out to prove the viability of the proposed microservices architecture.",An application of microservices architecture pattern to create a modular computer numerical control system,"Currently, the most common approach to Computer Numerical Control (CNC) system design is a monolithic architecture. However, the introduction of the concept of Cyber-Physical Production Systems (CPPS) requires a paradigm shift in the design of control systems. This paper suggests a new approach to developing modular industrial equipment using a microservices architecture pattern. Microservices architecture features are addressed, as well as advantages and disadvantages. A heterogeneous computer network, where nodes communicate via a message queue, is proposed as a basis for the computer numerical control system. Fault tolerance is provided by modules full autonomy and reliable messaging. Furthermore, NoSQL database, guaranteeing high data accessibility and increased data access speed, is applied. An apparatus for selective photopolymer laser curing of free-form surfaces is considered as an example. Common setup structure, as well as main hardware and software modules, are described. Moreover, a distributed network latency simulation was carried out to prove the viability of the proposed microservices architecture.",IEEE conference,no,"['pattern', 'create', 'modular', 'computer', 'numerical', 'control', 'currently', 'common', 'computer', 'numerical', 'control', 'design', 'monolithic', 'however', 'introduction', 'concept', 'production', 'cpps', 'requires', 'paradigm', 'shift', 'design', 'control', 'paper', 'suggests', 'new', 'developing', 'modular', 'industrial', 'equipment', 'using', 'pattern', 'feature', 'addressed', 'well', 'advantage', 'disadvantage', 'heterogeneous', 'computer', 'network', 'node', 'communicate', 'via', 'message', 'queue', 'proposed', 'basis', 'computer', 'numerical', 'control', 'fault', 'tolerance', 'provided', 'module', 'full', 'autonomy', 'reliable', 'messaging', 'furthermore', 'nosql', 'database', 'guaranteeing', 'high', 'accessibility', 'increased', 'access', 'speed', 'applied', 'laser', 'surface', 'considered', 'example', 'common', 'setup', 'structure', 'well', 'main', 'hardware', 'module', 'described', 'moreover', 'distributed', 'network', 'latency', 'simulation', 'carried', 'proposed']"
"Low Power Real Time GPS Tracking Enabled with RTOS and Serverless Architecture In this day and age everything is monitored and tracked. GPS tracking plays one of the most vital roles in our everyday life. Tracking entities is important in many fields including fleet management, personal security and even operations management. The proposed work details the low power analysis carried out on the built real time GPS tracking system. Incorporating microservices and modern architecture, the system provides live tracking solution along with analyses of the travel history. The use of serverless architecture augments and provides an intelligible design for the system. Equipped with mobile and web application, this system delivers live tracking results to the end users. Customized geofences enables the email alert service to the user in case of a breach in the boundary.",Low Power Real Time GPS Tracking Enabled with RTOS and Serverless Architecture,"In this day and age everything is monitored and tracked. GPS tracking plays one of the most vital roles in our everyday life. Tracking entities is important in many fields including fleet management, personal security and even operations management. The proposed work details the low power analysis carried out on the built real time GPS tracking system. Incorporating microservices and modern architecture, the system provides live tracking solution along with analyses of the travel history. The use of serverless architecture augments and provides an intelligible design for the system. Equipped with mobile and web application, this system delivers live tracking results to the end users. Customized geofences enables the email alert service to the user in case of a breach in the boundary.",IEEE conference,no,"['low', 'power', 'real', 'time', 'gps', 'tracking', 'enabled', 'serverless', 'day', 'age', 'monitored', 'gps', 'tracking', 'play', 'one', 'vital', 'role', 'life', 'tracking', 'entity', 'important', 'many', 'field', 'including', 'management', 'security', 'even', 'operation', 'management', 'proposed', 'work', 'detail', 'low', 'power', 'analysis', 'carried', 'built', 'real', 'time', 'gps', 'tracking', 'incorporating', 'modern', 'provides', 'live', 'tracking', 'solution', 'along', 'analysis', 'history', 'use', 'serverless', 'provides', 'design', 'equipped', 'mobile', 'web', 'live', 'tracking', 'result', 'end', 'user', 'customized', 'enables', 'user', 'case', 'breach', 'boundary']"
"Building Agile Workflow Microservice System for HPC Applications Based on Fast-start OSv The advances of containers have significantly promoted the development of microservice architecture. This architecture splits a monolithic application into multiple independent components and the container orchestrator manages these components by the container in the cloud environment. The feasibility of deploying high performance computing(HPC) applications as microservices has been proven, but the existing container orchestrator incurs a large performance overhead as there is interference between different containers on the same physical host. In this paper, we design an agile workflow microservice system for HPC applications with fast-start OSv. We consider improving HPC workflow performance from two aspects: single OSv startup time optimization and workflow orchestration optimization. For single OSv startup time optimization, we design a fast-start OSv by analyzing the process of OSv startup and finding an optimization by modifying OSv source code. In this way, we get nearly 50% improvement of startup time. For workflow orchestration optimization, we propose four optimization techniques to speed up the execution of workflow by jointly considering OSv and workflow features, namely: node fusion, node merge, image preload, boot delay. Furthermore, we utilize our fast startup OSv to design an orchestration system for efficiently building an agile HPC workflow microservice by Kubevirt. Our experimental results optimization microservice system reduces the execution time by 30% compared with the original deployment with docker.",Building Agile Workflow Microservice System for HPC Applications Based on Fast-start OSv,"The advances of containers have significantly promoted the development of microservice architecture. This architecture splits a monolithic application into multiple independent components and the container orchestrator manages these components by the container in the cloud environment. The feasibility of deploying high performance computing(HPC) applications as microservices has been proven, but the existing container orchestrator incurs a large performance overhead as there is interference between different containers on the same physical host. In this paper, we design an agile workflow microservice system for HPC applications with fast-start OSv. We consider improving HPC workflow performance from two aspects: single OSv startup time optimization and workflow orchestration optimization. For single OSv startup time optimization, we design a fast-start OSv by analyzing the process of OSv startup and finding an optimization by modifying OSv source code. In this way, we get nearly 50% improvement of startup time. For workflow orchestration optimization, we propose four optimization techniques to speed up the execution of workflow by jointly considering OSv and workflow features, namely: node fusion, node merge, image preload, boot delay. Furthermore, we utilize our fast startup OSv to design an orchestration system for efficiently building an agile HPC workflow microservice by Kubevirt. Our experimental results optimization microservice system reduces the execution time by 30% compared with the original deployment with docker.",IEEE conference,no,"['building', 'agile', 'workflow', 'hpc', 'based', 'osv', 'advance', 'container', 'significantly', 'development', 'split', 'monolithic', 'multiple', 'independent', 'container', 'orchestrator', 'manages', 'container', 'environment', 'feasibility', 'deploying', 'high', 'performance', 'computing', 'hpc', 'proven', 'existing', 'container', 'orchestrator', 'large', 'performance', 'overhead', 'interference', 'different', 'container', 'physical', 'host', 'paper', 'design', 'agile', 'workflow', 'hpc', 'osv', 'consider', 'improving', 'hpc', 'workflow', 'performance', 'two', 'aspect', 'single', 'osv', 'startup', 'time', 'optimization', 'workflow', 'orchestration', 'optimization', 'single', 'osv', 'startup', 'time', 'optimization', 'design', 'osv', 'analyzing', 'process', 'osv', 'startup', 'finding', 'optimization', 'modifying', 'osv', 'source', 'code', 'way', 'get', 'nearly', 'improvement', 'startup', 'time', 'workflow', 'orchestration', 'optimization', 'propose', 'four', 'optimization', 'technique', 'speed', 'execution', 'workflow', 'considering', 'osv', 'workflow', 'feature', 'namely', 'node', 'fusion', 'node', 'image', 'boot', 'delay', 'furthermore', 'utilize', 'fast', 'startup', 'osv', 'design', 'orchestration', 'efficiently', 'building', 'agile', 'hpc', 'workflow', 'experimental', 'result', 'optimization', 'reduces', 'execution', 'time', 'compared', 'original', 'deployment', 'docker']"
"A Review of Container level Autoscaling for Microservices-based Applications This paper presents an overview of autoscaling solutions for microservices-based applications during elastic treatment. Actually, most of existing work propose solutions dealing with elasticity at the VM level. These solutions launch elasticity when VM overload is detected or predicted. Few solutions treat this issue at the container level and deal with elasticity of microservices- based applications. In addition, these latter use ideas employed at the VM level without considering the specificity of microservice architecture. In this paper, we study and classify existing autoscalers dealing with containers and deploying microservices-based applications. We explain the strength and the shortcomings of each category. As a conclusion, we describe the challenges of autoscaling treatment and we give recommendations for future solutions.",A Review of Container level Autoscaling for Microservices-based Applications,"This paper presents an overview of autoscaling solutions for microservices-based applications during elastic treatment. Actually, most of existing work propose solutions dealing with elasticity at the VM level. These solutions launch elasticity when VM overload is detected or predicted. Few solutions treat this issue at the container level and deal with elasticity of microservices- based applications. In addition, these latter use ideas employed at the VM level without considering the specificity of microservice architecture. In this paper, we study and classify existing autoscalers dealing with containers and deploying microservices-based applications. We explain the strength and the shortcomings of each category. As a conclusion, we describe the challenges of autoscaling treatment and we give recommendations for future solutions.",IEEE conference,no,"['review', 'container', 'level', 'autoscaling', 'paper', 'present', 'overview', 'autoscaling', 'solution', 'elastic', 'treatment', 'actually', 'existing', 'work', 'propose', 'solution', 'dealing', 'elasticity', 'vm', 'level', 'solution', 'elasticity', 'vm', 'overload', 'detected', 'predicted', 'solution', 'treat', 'issue', 'container', 'level', 'deal', 'elasticity', 'based', 'addition', 'latter', 'use', 'idea', 'employed', 'vm', 'level', 'without', 'considering', 'specificity', 'paper', 'study', 'classify', 'existing', 'autoscalers', 'dealing', 'container', 'deploying', 'explain', 'strength', 'shortcoming', 'category', 'conclusion', 'describe', 'challenge', 'autoscaling', 'treatment', 'give', 'recommendation', 'future', 'solution']"
"Towards a Modular Digital Twin Microservice Architecture for Urban Multi-Energy Systems This paper proposes a formal definition of Digital Twins (DTs) and presents general Building Blocks (BBs) advocating for a microservice architecture. The BBs are categorized into data pipeline, trust and security, and service components. The definition arises from a list of DT requirements for Multi-Energy System (MES) applications and Use Cases (UCs). To create this requirements list, existing DT architectures and concepts are examined for their UCs and requirements that they fulfill.",Towards a Modular Digital Twin Microservice Architecture for Urban Multi-Energy Systems,"This paper proposes a formal definition of Digital Twins (DTs) and presents general Building Blocks (BBs) advocating for a microservice architecture. The BBs are categorized into data pipeline, trust and security, and service components. The definition arises from a list of DT requirements for Multi-Energy System (MES) applications and Use Cases (UCs). To create this requirements list, existing DT architectures and concepts are examined for their UCs and requirements that they fulfill.",IEEE conference,no,"['towards', 'modular', 'digital', 'twin', 'urban', 'paper', 'proposes', 'formal', 'definition', 'digital', 'twin', 'present', 'general', 'building', 'block', 'pipeline', 'trust', 'security', 'definition', 'arises', 'list', 'dt', 'requirement', 'me', 'use', 'case', 'create', 'requirement', 'list', 'existing', 'dt', 'concept', 'examined', 'requirement', 'fulfill']"
"Research on A Lightweight Service Mesh Architecture Using eBPF Many applications deployed in the cloud are typically refactored as widgets in microservices, and these components run as containers in a Kubernetes environment. The containers then run on clusters of physical servers that interoperate with the data center network. With this deployment, a large number of microservice containers running comes with resource consumption. The prevailing management architecture of microservices deepens this resource consumption on physical servers and increases network latency. We believe that this scenario could be further optimized. In this paper, we argue for an improved service mesh architecture design (compared to the traditional one). It reduces the resource consumption and improves the performance of deployed microservice applications, while providing some end-to-end visibility to debug problems that occur in microservices. We present the design and provide preliminary implementation details using eBPF technology to illustrate the feasibility of the system.",Research on A Lightweight Service Mesh Architecture Using eBPF,"Many applications deployed in the cloud are typically refactored as widgets in microservices, and these components run as containers in a Kubernetes environment. The containers then run on clusters of physical servers that interoperate with the data center network. With this deployment, a large number of microservice containers running comes with resource consumption. The prevailing management architecture of microservices deepens this resource consumption on physical servers and increases network latency. We believe that this scenario could be further optimized. In this paper, we argue for an improved service mesh architecture design (compared to the traditional one). It reduces the resource consumption and improves the performance of deployed microservice applications, while providing some end-to-end visibility to debug problems that occur in microservices. We present the design and provide preliminary implementation details using eBPF technology to illustrate the feasibility of the system.",IEEE conference,no,"['research', 'lightweight', 'mesh', 'using', 'ebpf', 'many', 'deployed', 'typically', 'refactored', 'run', 'container', 'kubernetes', 'environment', 'container', 'run', 'cluster', 'physical', 'server', 'center', 'network', 'deployment', 'large', 'number', 'container', 'running', 'come', 'resource', 'consumption', 'prevailing', 'management', 'resource', 'consumption', 'physical', 'server', 'increase', 'network', 'latency', 'believe', 'scenario', 'could', 'optimized', 'paper', 'argue', 'improved', 'mesh', 'design', 'compared', 'traditional', 'one', 'reduces', 'resource', 'consumption', 'improves', 'performance', 'deployed', 'providing', 'visibility', 'debug', 'problem', 'occur', 'present', 'design', 'provide', 'preliminary', 'implementation', 'detail', 'using', 'ebpf', 'technology', 'illustrate', 'feasibility']"
"Implementing DevOps Practices in CPPS Using Microservices and GitOps As digitalization shapes Industry 4.0, integrating software engineering practices into the manufacturing domain becomes a key focus. DevOps is such a practice and methodology that enhances collaboration and automates processes in software development and IT operations. In previous work, we discussed challenges and opportunities of applying the DevOps paradigm in Cyber-Physical Production Systems (CPPS) engineering, which are complex systems that integrate physical components with computer-controlled operations. This paper instantiates our DevOps4CPPS framework, spotlighting the application of GitOps and microservices in a CPPS environment. A real-world case study from laser ultra short pulse manufacturing illustrates how the approach effectively addresses the inherent challenges of CPPS, boosting agility and operational performance, while reducing downtime. The paper provides guidance to researchers and practitioners on applying DevOps practices to CPPS, and suggests future research directions.",Implementing DevOps Practices in CPPS Using Microservices and GitOps,"As digitalization shapes Industry 4.0, integrating software engineering practices into the manufacturing domain becomes a key focus. DevOps is such a practice and methodology that enhances collaboration and automates processes in software development and IT operations. In previous work, we discussed challenges and opportunities of applying the DevOps paradigm in Cyber-Physical Production Systems (CPPS) engineering, which are complex systems that integrate physical components with computer-controlled operations. This paper instantiates our DevOps4CPPS framework, spotlighting the application of GitOps and microservices in a CPPS environment. A real-world case study from laser ultra short pulse manufacturing illustrates how the approach effectively addresses the inherent challenges of CPPS, boosting agility and operational performance, while reducing downtime. The paper provides guidance to researchers and practitioners on applying DevOps practices to CPPS, and suggests future research directions.",IEEE conference,no,"['implementing', 'devops', 'practice', 'cpps', 'using', 'gitops', 'digitalization', 'shape', 'industry', 'integrating', 'engineering', 'practice', 'manufacturing', 'domain', 'becomes', 'key', 'focus', 'devops', 'practice', 'methodology', 'enhances', 'collaboration', 'automates', 'process', 'development', 'operation', 'previous', 'work', 'discussed', 'challenge', 'opportunity', 'applying', 'devops', 'paradigm', 'production', 'cpps', 'engineering', 'complex', 'integrate', 'physical', 'operation', 'paper', 'framework', 'gitops', 'cpps', 'environment', 'case', 'study', 'laser', 'short', 'manufacturing', 'effectively', 'address', 'inherent', 'challenge', 'cpps', 'agility', 'operational', 'performance', 'reducing', 'paper', 'provides', 'guidance', 'researcher', 'practitioner', 'applying', 'devops', 'practice', 'cpps', 'suggests', 'future', 'research', 'direction']"
"Containerized Development and Microservices for Self-Driving Vehicles: Experiences & Best Practices In this paper, experiences and best practices from using containerized software microservices for self-driving vehicles are shared. We applied the containerized software paradigm successfully to both the software development and deployment to turn our software architecture in the vehicles following the idea of microservices. Key enabling elements include onboarding of new developers, both researchers and students, traceable development and packaging, convenient and bare-bone deployment, and traceably archiving binary distributions of our quickly evolving software environment. In this paper, we share our experience from working one year with containerized development and deployment for our self-driving vehicles highlighting our reflections and application-specific shortcomings, our approach uses several components from the widely used Docker ecosystem, but the discussion in this paper generalizes these concepts. We conclude that the growingly complex automotive software systems in combination with their computational platforms should be rather understood as data centers on wheels to design both, (a) the software development and deployment processes, and (b) the software architecture in such a way to enable continuous integration, continuous deployment, and continuous experimentation.",Containerized Development and Microservices for Self-Driving Vehicles: Experiences & Best Practices,"In this paper, experiences and best practices from using containerized software microservices for self-driving vehicles are shared. We applied the containerized software paradigm successfully to both the software development and deployment to turn our software architecture in the vehicles following the idea of microservices. Key enabling elements include onboarding of new developers, both researchers and students, traceable development and packaging, convenient and bare-bone deployment, and traceably archiving binary distributions of our quickly evolving software environment. In this paper, we share our experience from working one year with containerized development and deployment for our self-driving vehicles highlighting our reflections and application-specific shortcomings, our approach uses several components from the widely used Docker ecosystem, but the discussion in this paper generalizes these concepts. We conclude that the growingly complex automotive software systems in combination with their computational platforms should be rather understood as data centers on wheels to design both, (a) the software development and deployment processes, and (b) the software architecture in such a way to enable continuous integration, continuous deployment, and continuous experimentation.",IEEE conference,no,"['containerized', 'development', 'vehicle', 'experience', 'best', 'practice', 'paper', 'experience', 'best', 'practice', 'using', 'containerized', 'vehicle', 'shared', 'applied', 'containerized', 'paradigm', 'successfully', 'development', 'deployment', 'turn', 'vehicle', 'following', 'idea', 'key', 'enabling', 'element', 'include', 'new', 'developer', 'researcher', 'student', 'development', 'packaging', 'convenient', 'deployment', 'binary', 'distribution', 'quickly', 'evolving', 'environment', 'paper', 'share', 'experience', 'working', 'one', 'year', 'containerized', 'development', 'deployment', 'vehicle', 'highlighting', 'shortcoming', 'us', 'several', 'widely', 'used', 'docker', 'ecosystem', 'discussion', 'paper', 'concept', 'conclude', 'complex', 'automotive', 'combination', 'computational', 'platform', 'rather', 'center', 'design', 'development', 'deployment', 'process', 'b', 'way', 'enable', 'continuous', 'integration', 'continuous', 'deployment', 'continuous']"
"Digital Twinning for Microservice Architectures Digital twins have been designed and implemented for diverse applications like smart manufacturing, healthcare, supply chain and retail management. They provide monitoring, remote prognostics and health management capabilities for the various physical assets used in these domains. Many of these capabilities would be beneficial to microservice architectures as well, given the need for lightweight monitoring solutions in multitenant environments. In particular, twins can provide operators with real-time resource usage metrics which help with operational objectives such as resource planning, anomaly detection, rewind and replay and so on. In this work, we propose a design for building digital twins for microservice architectures. As a proof of concept, we focus on modelling the resource utilization as that is a key requirement for monitoring system reliability and security. In general, digital twins require a real world counterpart, a virtual model and a mechanism for consistently keeping both synchronized. We focus on the two latter aspects of the digital twin. Our approach involves converting a formal model of a microservice architecture into a digital twin that can capture and execute an actual cluster's state. We present an extensible architecture connecting the various components of the system and the twin and evaluate the twin's ability to capture the real-time state of a real Kubernetes cluster. We also discuss future extensions which can enhance the system's security by detecting a broad range of attacks.",Digital Twinning for Microservice Architectures,"Digital twins have been designed and implemented for diverse applications like smart manufacturing, healthcare, supply chain and retail management. They provide monitoring, remote prognostics and health management capabilities for the various physical assets used in these domains. Many of these capabilities would be beneficial to microservice architectures as well, given the need for lightweight monitoring solutions in multitenant environments. In particular, twins can provide operators with real-time resource usage metrics which help with operational objectives such as resource planning, anomaly detection, rewind and replay and so on. In this work, we propose a design for building digital twins for microservice architectures. As a proof of concept, we focus on modelling the resource utilization as that is a key requirement for monitoring system reliability and security. In general, digital twins require a real world counterpart, a virtual model and a mechanism for consistently keeping both synchronized. We focus on the two latter aspects of the digital twin. Our approach involves converting a formal model of a microservice architecture into a digital twin that can capture and execute an actual cluster's state. We present an extensible architecture connecting the various components of the system and the twin and evaluate the twin's ability to capture the real-time state of a real Kubernetes cluster. We also discuss future extensions which can enhance the system's security by detecting a broad range of attacks.",IEEE conference,no,"['digital', 'digital', 'twin', 'designed', 'implemented', 'diverse', 'like', 'smart', 'manufacturing', 'healthcare', 'supply', 'chain', 'retail', 'management', 'provide', 'monitoring', 'remote', 'health', 'management', 'capability', 'various', 'physical', 'asset', 'used', 'domain', 'many', 'capability', 'would', 'beneficial', 'well', 'given', 'need', 'lightweight', 'monitoring', 'solution', 'environment', 'particular', 'twin', 'provide', 'operator', 'resource', 'usage', 'metric', 'help', 'operational', 'objective', 'resource', 'planning', 'anomaly', 'detection', 'work', 'propose', 'design', 'building', 'digital', 'twin', 'proof', 'concept', 'focus', 'modelling', 'resource', 'utilization', 'key', 'requirement', 'monitoring', 'reliability', 'security', 'general', 'digital', 'twin', 'require', 'real', 'world', 'counterpart', 'virtual', 'model', 'mechanism', 'consistently', 'keeping', 'synchronized', 'focus', 'two', 'latter', 'aspect', 'digital', 'twin', 'involves', 'converting', 'formal', 'model', 'digital', 'twin', 'capture', 'execute', 'actual', 'cluster', 'state', 'present', 'extensible', 'connecting', 'various', 'twin', 'evaluate', 'twin', 'ability', 'capture', 'state', 'real', 'kubernetes', 'cluster', 'also', 'discus', 'future', 'extension', 'enhance', 'security', 'detecting', 'broad', 'range', 'attack']"
"Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes,"The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",IEEE conference,no,"['istio', 'api', 'gateway', 'impact', 'reduce', 'latency', 'resource', 'usage', 'kubernetes', 'rapid', 'development', 'technology', 'caused', 'massive', 'demand', 'technology', 'infrastructure', 'play', 'vital', 'role', 'performance', 'unreliable', 'infrastructure', 'performance', 'resulting', 'poor', 'customer', 'experience', 'one', 'core', 'enables', 'accessibility', 'proxy', 'server', 'route', 'incoming', 'traffic', 'executing', 'business', 'logic', 'proxy', 'server', 'run', 'kubernetes', 'environment', 'abstraction', 'operating', 'virtualization', 'gateway', 'running', 'kubernetes', 'environment', 'gateway', 'server', 'excellent', 'performance', 'reliability', 'efficient', 'resource', 'utilization', 'required', 'selection', 'proxy', 'server', 'gateway', 'backend', 'significantly', 'impact', 'performance', 'user', 'perspective', 'experiment', 'conducted', 'research', 'test', 'backend', 'accessed', 'two', 'different', 'proxy', 'server', 'kubernetes', 'environment', 'research', 'compare', 'response', 'time', 'hardware', 'resource', 'utilization', 'proxy', 'server', 'common', 'mesh', 'paper', 'proposed', 'modern', 'evaluate', 'performance', 'common', 'paradigm', 'enterprise', 'high', 'request', 'rate', 'serve', 'client', 'lowest', 'latency', 'efficient', 'resource', 'result', 'show', 'istio', 'ingres', 'proxy', 'suitable', 'high', 'request', 'rate', 'nginx', 'ingres', 'proxy', 'performs', 'better', 'serving', 'lower', 'request']"
"Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes,"The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",IEEE conference,yes,"['istio', 'api', 'gateway', 'impact', 'reduce', 'latency', 'resource', 'usage', 'kubernetes', 'rapid', 'development', 'technology', 'caused', 'massive', 'demand', 'technology', 'infrastructure', 'play', 'vital', 'role', 'performance', 'unreliable', 'infrastructure', 'performance', 'resulting', 'poor', 'customer', 'experience', 'one', 'core', 'enables', 'accessibility', 'proxy', 'server', 'route', 'incoming', 'traffic', 'executing', 'business', 'logic', 'proxy', 'server', 'run', 'kubernetes', 'environment', 'abstraction', 'operating', 'virtualization', 'gateway', 'running', 'kubernetes', 'environment', 'gateway', 'server', 'excellent', 'performance', 'reliability', 'efficient', 'resource', 'utilization', 'required', 'selection', 'proxy', 'server', 'gateway', 'backend', 'significantly', 'impact', 'performance', 'user', 'perspective', 'experiment', 'conducted', 'research', 'test', 'backend', 'accessed', 'two', 'different', 'proxy', 'server', 'kubernetes', 'environment', 'research', 'compare', 'response', 'time', 'hardware', 'resource', 'utilization', 'proxy', 'server', 'common', 'mesh', 'paper', 'proposed', 'modern', 'evaluate', 'performance', 'common', 'paradigm', 'enterprise', 'high', 'request', 'rate', 'serve', 'client', 'lowest', 'latency', 'efficient', 'resource', 'result', 'show', 'istio', 'ingres', 'proxy', 'suitable', 'high', 'request', 'rate', 'nginx', 'ingres', 'proxy', 'performs', 'better', 'serving', 'lower', 'request']"
"Research on designing an integrated electric power marketing information system based on microapplications and microservices architecture With the continuous expansion of the electric power marketing business, the demand for new services is constantly increasing and the required functions continue to raise, the issues of multiple inter-system interfaces and interactions, diversified interface technologies, diverse data replication approaches, and multiple data transmission channels, are more prominent in the current electric power marketing business application system and have affected the regular operations and development of the electric power marketing services and applications. This paper proposes the microapplications and microservices architecture based on microservices framework to implement the applications and functions. Based on this architecture, it describes the design method of an integrated electric power marketing business information system and elaborates each technical layer in the overall architecture, which includes the application access layer, microapplications layer, microservices layer, data resource layer and basic platform layer, by unifying the technical architecture and implementation technologies of the information system, reducing the business interactions, interface interactions, data replication and data transmission within the system, to be suitable for the future development of the electric power marketing business. Finally the paper describes the system verification with the typical business scenario of charging and accounting in the electric power marketing business.",Research on designing an integrated electric power marketing information system based on microapplications and microservices architecture,"With the continuous expansion of the electric power marketing business, the demand for new services is constantly increasing and the required functions continue to raise, the issues of multiple inter-system interfaces and interactions, diversified interface technologies, diverse data replication approaches, and multiple data transmission channels, are more prominent in the current electric power marketing business application system and have affected the regular operations and development of the electric power marketing services and applications. This paper proposes the microapplications and microservices architecture based on microservices framework to implement the applications and functions. Based on this architecture, it describes the design method of an integrated electric power marketing business information system and elaborates each technical layer in the overall architecture, which includes the application access layer, microapplications layer, microservices layer, data resource layer and basic platform layer, by unifying the technical architecture and implementation technologies of the information system, reducing the business interactions, interface interactions, data replication and data transmission within the system, to be suitable for the future development of the electric power marketing business. Finally the paper describes the system verification with the typical business scenario of charging and accounting in the electric power marketing business.",IEEE conference,no,"['research', 'designing', 'integrated', 'electric', 'power', 'marketing', 'information', 'based', 'microapplications', 'continuous', 'expansion', 'electric', 'power', 'marketing', 'business', 'demand', 'new', 'constantly', 'increasing', 'required', 'function', 'continue', 'raise', 'issue', 'multiple', 'interface', 'interaction', 'interface', 'technology', 'diverse', 'replication', 'multiple', 'transmission', 'channel', 'prominent', 'current', 'electric', 'power', 'marketing', 'business', 'affected', 'regular', 'operation', 'development', 'electric', 'power', 'marketing', 'paper', 'proposes', 'microapplications', 'based', 'framework', 'implement', 'function', 'based', 'describes', 'design', 'method', 'integrated', 'electric', 'power', 'marketing', 'business', 'information', 'elaborates', 'technical', 'layer', 'overall', 'includes', 'access', 'layer', 'microapplications', 'layer', 'layer', 'resource', 'layer', 'basic', 'platform', 'layer', 'technical', 'implementation', 'technology', 'information', 'reducing', 'business', 'interaction', 'interface', 'interaction', 'replication', 'transmission', 'within', 'suitable', 'future', 'development', 'electric', 'power', 'marketing', 'business', 'finally', 'paper', 'describes', 'verification', 'typical', 'business', 'scenario', 'accounting', 'electric', 'power', 'marketing', 'business']"
"BLOC: Balancing Load with Overload Control In the Microservices Architecture The microservices architecture has become ubiquitous in the cloud environment. It simplifies application development by breaking monolithic applications into manageable micro services that can be developed and deployed independently of the whole. However, the move from a monolithic or simple multi-tier architecture to a distributed microservice ""service mesh"" leads to new challenges due to the more complex application topology.A particular problem when automatically managing the performance of microservices is that since each service component scales up and down independently, it can easily create load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms were designed for centralized load balancers sitting between a group of clients and a server farm. These algorithms, however, do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this paper we propose a self managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes.We show that different service layers scaling independently can create unacceptably wide response time distributions and long tails, hurting client experience. This is because popular microservice load balancing algorithms, like Least Connection, only use a single component’s view of the backend load to guide decisions. This limited perspective leads to an unevenly balanced system and the potential for incast problems where a large number of frontend components can easily overload a shared backend. BLOC uses overload control approaches like rate limiting, active queue management and backpressure to provide feedback to the load balancers. The load balancers react to this feedback with techniques like backoff and retries. We show that this performs significantly better in solving the incast problem in microservice architectures.Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 to 4 times and the tail, 99th percentile, latency by two times.",BLOC: Balancing Load with Overload Control In the Microservices Architecture,"The microservices architecture has become ubiquitous in the cloud environment. It simplifies application development by breaking monolithic applications into manageable micro services that can be developed and deployed independently of the whole. However, the move from a monolithic or simple multi-tier architecture to a distributed microservice ""service mesh"" leads to new challenges due to the more complex application topology.A particular problem when automatically managing the performance of microservices is that since each service component scales up and down independently, it can easily create load imbalance problems on shared backend services accessed by multiple components. Traditional load balancing algorithms were designed for centralized load balancers sitting between a group of clients and a server farm. These algorithms, however, do not port over well to a distributed microservice architecture where load balancers are deployed client-side. In this paper we propose a self managing load balancing system, BLOC, which provides consistent response times to users without using a centralized metadata store or explicit messaging between nodes.We show that different service layers scaling independently can create unacceptably wide response time distributions and long tails, hurting client experience. This is because popular microservice load balancing algorithms, like Least Connection, only use a single component’s view of the backend load to guide decisions. This limited perspective leads to an unevenly balanced system and the potential for incast problems where a large number of frontend components can easily overload a shared backend. BLOC uses overload control approaches like rate limiting, active queue management and backpressure to provide feedback to the load balancers. The load balancers react to this feedback with techniques like backoff and retries. We show that this performs significantly better in solving the incast problem in microservice architectures.Evaluating this framework, we found that BLOC improves the response time distribution range, between the 10th and 90th percentiles, by 2 to 4 times and the tail, 99th percentile, latency by two times.",IEEE conference,no,"['bloc', 'balancing', 'load', 'overload', 'control', 'become', 'ubiquitous', 'environment', 'simplifies', 'development', 'breaking', 'monolithic', 'manageable', 'micro', 'developed', 'deployed', 'independently', 'whole', 'however', 'move', 'monolithic', 'simple', 'distributed', 'mesh', 'lead', 'new', 'challenge', 'due', 'complex', 'particular', 'problem', 'automatically', 'managing', 'performance', 'since', 'scale', 'independently', 'easily', 'create', 'load', 'imbalance', 'problem', 'shared', 'backend', 'accessed', 'multiple', 'traditional', 'load', 'balancing', 'algorithm', 'designed', 'centralized', 'load', 'balancer', 'group', 'client', 'server', 'farm', 'algorithm', 'however', 'port', 'well', 'distributed', 'load', 'balancer', 'deployed', 'paper', 'propose', 'self', 'managing', 'load', 'balancing', 'bloc', 'provides', 'consistent', 'response', 'time', 'user', 'without', 'using', 'centralized', 'metadata', 'store', 'explicit', 'messaging', 'show', 'different', 'layer', 'scaling', 'independently', 'create', 'wide', 'response', 'time', 'distribution', 'long', 'tail', 'client', 'experience', 'popular', 'load', 'balancing', 'algorithm', 'like', 'least', 'connection', 'use', 'single', 'view', 'backend', 'load', 'guide', 'decision', 'limited', 'perspective', 'lead', 'balanced', 'potential', 'incast', 'problem', 'large', 'number', 'frontend', 'easily', 'overload', 'shared', 'backend', 'bloc', 'us', 'overload', 'control', 'like', 'rate', 'limiting', 'active', 'queue', 'management', 'provide', 'feedback', 'load', 'balancer', 'load', 'balancer', 'react', 'feedback', 'technique', 'like', 'show', 'performs', 'significantly', 'better', 'solving', 'incast', 'problem', 'framework', 'found', 'bloc', 'improves', 'response', 'time', 'distribution', 'range', 'percentile', 'time', 'tail', 'percentile', 'latency', 'two', 'time']"
"UFCity: A Software Architecture to Create Data Ecosystem in Smart Cities Smart city platforms manage resources and information to offer services to citizens. Developing technological solutions for this scenario is not trivial, given data and resource heterogeneity and the need for platform flexibility and adaptability. This work presents UFCity, a software architecture for smart city data ecosystems using microservices, artificial intelligence, and a three-layer networking structure. As a proof of concept, we developed a prototype and tested it in usage scenarios with real data. These use cases explored how our solution solves identified problems, showing meeting essential requirements for smart city platforms. In this way, UFCity is an alternative for data management in a smart city, presenting advantages over other solutions listed in this work in meeting the platform requirements.",UFCity: A Software Architecture to Create Data Ecosystem in Smart Cities,"Smart city platforms manage resources and information to offer services to citizens. Developing technological solutions for this scenario is not trivial, given data and resource heterogeneity and the need for platform flexibility and adaptability. This work presents UFCity, a software architecture for smart city data ecosystems using microservices, artificial intelligence, and a three-layer networking structure. As a proof of concept, we developed a prototype and tested it in usage scenarios with real data. These use cases explored how our solution solves identified problems, showing meeting essential requirements for smart city platforms. In this way, UFCity is an alternative for data management in a smart city, presenting advantages over other solutions listed in this work in meeting the platform requirements.",IEEE conference,no,"['ufcity', 'create', 'ecosystem', 'smart', 'city', 'smart', 'city', 'platform', 'manage', 'resource', 'information', 'offer', 'citizen', 'developing', 'technological', 'solution', 'scenario', 'given', 'resource', 'heterogeneity', 'need', 'platform', 'flexibility', 'adaptability', 'work', 'present', 'ufcity', 'smart', 'city', 'ecosystem', 'using', 'artificial', 'intelligence', 'networking', 'structure', 'proof', 'concept', 'developed', 'prototype', 'tested', 'usage', 'scenario', 'real', 'use', 'case', 'explored', 'solution', 'solves', 'identified', 'problem', 'showing', 'meeting', 'essential', 'requirement', 'smart', 'city', 'platform', 'way', 'ufcity', 'alternative', 'management', 'smart', 'city', 'presenting', 'advantage', 'solution', 'work', 'meeting', 'platform', 'requirement']"
"gym-hpa: Efficient Auto-Scaling via Reinforcement Learning for Complex Microservice-based Applications in Kubernetes Containers have revolutionized application deployment and life-cycle management in current cloud platforms. Applications have evolved from large monoliths to complex graphs of loosely-coupled microservices aiming to improve deployment flexibility and operational efficiency. However, modern microservice-based architectures are challenging since proper allocation and scaling of microservices is a difficult task due to their complex inter-dependencies. Existing works do not consider microservice dependencies, which could lead to the application’s performance degradation when service demand increases. This paper studies the impact of microservice interdependencies in auto-scaling mechanisms by proposing a novel framework named gym-hpa that enables different auto-scaling goals via Reinforcement Learning (RL). The framework has been developed based on the OpenAI Gym library for the popular Kubernetes (K8s) platform to bridge the gap between RL and auto-scaling research by training RL agents on real cloud environments. The aim is to improve resource usage and reduce the application’s response time in future cloud platforms by considering microservice inter-dependencies in horizontal scaling. Experiments with microservice benchmark applications show that RL agents trained with the gym-hpa framework can reduce on average resource usage by 30% and reduce the application’s response time by 25% compared to default scaling mechanisms.",gym-hpa: Efficient Auto-Scaling via Reinforcement Learning for Complex Microservice-based Applications in Kubernetes,"Containers have revolutionized application deployment and life-cycle management in current cloud platforms. Applications have evolved from large monoliths to complex graphs of loosely-coupled microservices aiming to improve deployment flexibility and operational efficiency. However, modern microservice-based architectures are challenging since proper allocation and scaling of microservices is a difficult task due to their complex inter-dependencies. Existing works do not consider microservice dependencies, which could lead to the application’s performance degradation when service demand increases. This paper studies the impact of microservice interdependencies in auto-scaling mechanisms by proposing a novel framework named gym-hpa that enables different auto-scaling goals via Reinforcement Learning (RL). The framework has been developed based on the OpenAI Gym library for the popular Kubernetes (K8s) platform to bridge the gap between RL and auto-scaling research by training RL agents on real cloud environments. The aim is to improve resource usage and reduce the application’s response time in future cloud platforms by considering microservice inter-dependencies in horizontal scaling. Experiments with microservice benchmark applications show that RL agents trained with the gym-hpa framework can reduce on average resource usage by 30% and reduce the application’s response time by 25% compared to default scaling mechanisms.",IEEE conference,no,"['efficient', 'via', 'reinforcement', 'learning', 'complex', 'kubernetes', 'container', 'revolutionized', 'deployment', 'management', 'current', 'platform', 'evolved', 'large', 'monolith', 'complex', 'graph', 'aiming', 'improve', 'deployment', 'flexibility', 'operational', 'efficiency', 'however', 'modern', 'challenging', 'since', 'proper', 'allocation', 'scaling', 'difficult', 'task', 'due', 'complex', 'existing', 'work', 'consider', 'dependency', 'could', 'lead', 'performance', 'degradation', 'demand', 'increase', 'paper', 'study', 'impact', 'interdependency', 'mechanism', 'proposing', 'novel', 'framework', 'named', 'enables', 'different', 'goal', 'via', 'reinforcement', 'learning', 'rl', 'framework', 'developed', 'based', 'library', 'popular', 'kubernetes', 'platform', 'bridge', 'gap', 'rl', 'research', 'training', 'rl', 'agent', 'real', 'environment', 'aim', 'improve', 'resource', 'usage', 'reduce', 'response', 'time', 'future', 'platform', 'considering', 'horizontal', 'scaling', 'experiment', 'benchmark', 'show', 'rl', 'agent', 'trained', 'framework', 'reduce', 'average', 'resource', 'usage', 'reduce', 'response', 'time', 'compared', 'default', 'scaling', 'mechanism']"
"Software Component Update for IoT Systems Frequent updates in IoT software are crucial for fixing security vulnerabilities, correcting bugs, and adding new features. However, for systems comprising geographically distributed devices, implementing updates is challenging. Such updates must be coordinated across multiple devices, automated without end-user involvement, adaptable to weak connectivity, and minimally disruptive to end users. In this paper we introduce an update method that addresses these challenges. Our approach utilizes a versioned, component-oriented software model for light-weight microservices, alongside a domain-specific language for specifying configurations, facilitating updates of individual components. We evaluate the effectiveness of this method within a Hospital-at-Home system, including servers, tablets for nurses and patients, and medical equipment located in patients’ homes. Our results demonstrate that this method offers substantial flexibility and significantly reduces total update time, switchover time, and network load compared to previous methods.",Software Component Update for IoT Systems,"Frequent updates in IoT software are crucial for fixing security vulnerabilities, correcting bugs, and adding new features. However, for systems comprising geographically distributed devices, implementing updates is challenging. Such updates must be coordinated across multiple devices, automated without end-user involvement, adaptable to weak connectivity, and minimally disruptive to end users. In this paper we introduce an update method that addresses these challenges. Our approach utilizes a versioned, component-oriented software model for light-weight microservices, alongside a domain-specific language for specifying configurations, facilitating updates of individual components. We evaluate the effectiveness of this method within a Hospital-at-Home system, including servers, tablets for nurses and patients, and medical equipment located in patients’ homes. Our results demonstrate that this method offers substantial flexibility and significantly reduces total update time, switchover time, and network load compared to previous methods.",IEEE conference,no,"['update', 'iot', 'frequent', 'update', 'iot', 'crucial', 'security', 'vulnerability', 'bug', 'adding', 'new', 'feature', 'however', 'geographically', 'distributed', 'device', 'implementing', 'update', 'challenging', 'update', 'must', 'coordinated', 'across', 'multiple', 'device', 'automated', 'without', 'involvement', 'adaptable', 'connectivity', 'end', 'user', 'paper', 'introduce', 'update', 'method', 'address', 'challenge', 'utilizes', 'model', 'language', 'configuration', 'facilitating', 'update', 'individual', 'evaluate', 'effectiveness', 'method', 'within', 'including', 'server', 'patient', 'medical', 'equipment', 'located', 'patient', 'home', 'result', 'demonstrate', 'method', 'offer', 'substantial', 'flexibility', 'significantly', 'reduces', 'total', 'update', 'time', 'time', 'network', 'load', 'compared', 'previous', 'method']"
"A Service-Oriented Digital Twins Framework for Smart Grid Management Cyber-Physical Systems (CPS) are infrastructures with capabilities of perception, networking, and computation. They consist of physical and virtual components that interact with their environment. Their main goal is to monitor the physical system in real-time to have its behavior under control. Some of these infrastructures require well-defined security measures due to their critical processing capabilities. Digital Twins are defined as virtual structures emulating the physical components in various systems. They enable the deployment of isolated simulation environments to test changes and specify the security issues of a CPS. One of the main applications of CPS is smart grids. Microservices allow us to create fine-grained operations on automation systems. Using small, well-defined, modular processes, we can observe the states of smart grid infrastructure in many stages. In this work, we aim to design a framework that can create a reliable environment for smart grids compliant with existing industrial standards. We propose a service-oriented digital twins framework consisting of entities such as power meters, data aggregators, and system parameters for providing a scalable and highly available platform for complex energy systems.",A Service-Oriented Digital Twins Framework for Smart Grid Management,"Cyber-Physical Systems (CPS) are infrastructures with capabilities of perception, networking, and computation. They consist of physical and virtual components that interact with their environment. Their main goal is to monitor the physical system in real-time to have its behavior under control. Some of these infrastructures require well-defined security measures due to their critical processing capabilities. Digital Twins are defined as virtual structures emulating the physical components in various systems. They enable the deployment of isolated simulation environments to test changes and specify the security issues of a CPS. One of the main applications of CPS is smart grids. Microservices allow us to create fine-grained operations on automation systems. Using small, well-defined, modular processes, we can observe the states of smart grid infrastructure in many stages. In this work, we aim to design a framework that can create a reliable environment for smart grids compliant with existing industrial standards. We propose a service-oriented digital twins framework consisting of entities such as power meters, data aggregators, and system parameters for providing a scalable and highly available platform for complex energy systems.",IEEE conference,no,"['digital', 'twin', 'framework', 'smart', 'grid', 'management', 'cps', 'infrastructure', 'capability', 'networking', 'computation', 'consist', 'physical', 'virtual', 'interact', 'environment', 'main', 'goal', 'monitor', 'physical', 'behavior', 'control', 'infrastructure', 'require', 'security', 'measure', 'due', 'critical', 'processing', 'capability', 'digital', 'twin', 'defined', 'virtual', 'structure', 'physical', 'various', 'enable', 'deployment', 'isolated', 'simulation', 'environment', 'test', 'change', 'specify', 'security', 'issue', 'cps', 'one', 'main', 'cps', 'smart', 'grid', 'allow', 'u', 'create', 'operation', 'automation', 'using', 'small', 'modular', 'process', 'observe', 'state', 'smart', 'grid', 'infrastructure', 'many', 'stage', 'work', 'aim', 'design', 'framework', 'create', 'reliable', 'environment', 'smart', 'grid', 'compliant', 'existing', 'industrial', 'standard', 'propose', 'digital', 'twin', 'framework', 'consisting', 'entity', 'power', 'parameter', 'providing', 'scalable', 'highly', 'available', 'platform', 'complex', 'energy']"
"Node-RED Workflow Manager for Edge Service Orchestration Microservice Architectures have increasingly become popular in Industry 4.0 as they allow heterogeneous systems to interact, reduce the complexity in the management of individual components, and support distributed deployments. The integration of those distributed services into orchestrated production processes is performed by workflow managers. Next generation workflow managers must overcome a number of challenges when operating in microservice architectures and IoT environments. To overcome these challenges (heterogeneity, high dynamism, edge deployment or scalability), we propose a workflow manager alternative built in Node-RED. Node-RED provides instruments for the development of IoT systems and leverages the edge computing paradigm. This solution is deployable in embedded systems, is able to load and execute business processes by means of BPMN recipes and enables the integration of other frameworks and architectures.",Node-RED Workflow Manager for Edge Service Orchestration,"Microservice Architectures have increasingly become popular in Industry 4.0 as they allow heterogeneous systems to interact, reduce the complexity in the management of individual components, and support distributed deployments. The integration of those distributed services into orchestrated production processes is performed by workflow managers. Next generation workflow managers must overcome a number of challenges when operating in microservice architectures and IoT environments. To overcome these challenges (heterogeneity, high dynamism, edge deployment or scalability), we propose a workflow manager alternative built in Node-RED. Node-RED provides instruments for the development of IoT systems and leverages the edge computing paradigm. This solution is deployable in embedded systems, is able to load and execute business processes by means of BPMN recipes and enables the integration of other frameworks and architectures.",IEEE conference,no,"['workflow', 'manager', 'edge', 'orchestration', 'increasingly', 'become', 'popular', 'industry', 'allow', 'heterogeneous', 'interact', 'reduce', 'complexity', 'management', 'individual', 'support', 'distributed', 'deployment', 'integration', 'distributed', 'orchestrated', 'production', 'process', 'performed', 'workflow', 'manager', 'next', 'generation', 'workflow', 'manager', 'must', 'overcome', 'number', 'challenge', 'operating', 'iot', 'environment', 'overcome', 'challenge', 'heterogeneity', 'high', 'edge', 'deployment', 'scalability', 'propose', 'workflow', 'manager', 'alternative', 'built', 'provides', 'development', 'iot', 'leverage', 'edge', 'computing', 'paradigm', 'solution', 'deployable', 'embedded', 'able', 'load', 'execute', 'business', 'process', 'mean', 'enables', 'integration', 'framework']"
"A Task-Oriented Automatic Microservice Deployment Method For Industrial Edge Applications Flexible production and intelligent manufacturing prompt a variety of production and computing requirements of industrial edge applications. Industrial edge applications need to adopt more flexible deployment methods to meet the new requirements from the new industrial cloud and edge paradigm. However, most legacy industrial applications are still based on a fixed order deployment method. In this paper, a task-oriented automatic edge-cloud collaborative microservice method is proposed to optimize the deployment process of industrial edge applications and improve the flexibility and expandability. The proposed method was verified by a case study of the thrust ball bearing producing.",A Task-Oriented Automatic Microservice Deployment Method For Industrial Edge Applications,"Flexible production and intelligent manufacturing prompt a variety of production and computing requirements of industrial edge applications. Industrial edge applications need to adopt more flexible deployment methods to meet the new requirements from the new industrial cloud and edge paradigm. However, most legacy industrial applications are still based on a fixed order deployment method. In this paper, a task-oriented automatic edge-cloud collaborative microservice method is proposed to optimize the deployment process of industrial edge applications and improve the flexibility and expandability. The proposed method was verified by a case study of the thrust ball bearing producing.",IEEE conference,no,"['automatic', 'deployment', 'method', 'industrial', 'edge', 'flexible', 'production', 'intelligent', 'manufacturing', 'variety', 'production', 'computing', 'requirement', 'industrial', 'edge', 'industrial', 'edge', 'need', 'adopt', 'flexible', 'deployment', 'method', 'meet', 'new', 'requirement', 'new', 'industrial', 'edge', 'paradigm', 'however', 'legacy', 'industrial', 'still', 'based', 'fixed', 'order', 'deployment', 'method', 'paper', 'automatic', 'collaborative', 'method', 'proposed', 'optimize', 'deployment', 'process', 'industrial', 'edge', 'improve', 'flexibility', 'proposed', 'method', 'verified', 'case', 'study', 'producing']"
"Research on building an innovative electric power marketing business application system based on cloud computing and microservices architecture technologies The paper proposes the method of building the electric power marketing business application system based on the cloud computing and microservices architecture technologies to solve the issues of the current system with the concentrated infrastructure and the monolithic architecture which are difficult to support the business processing in very large scale and the evolution of the electric power marketing business. The paper first introduces the current electric power marketing business application system and highlights its main drawbacks. Then it proposes the method to build the innovative electric power marketing business application system with the main technical characteristics as follows: the IaaS part of the application system has the distributed infrastructures with the IT resource elastic management and powerful horizontal scalability brought by the cloud computing technology; its PaaS part is the service platform with the components of the data processing, information integration, application building, and cloud service center to provide the several cloud services, such as one-click deployment, flexible scaling, fault self-healing, gray distribution, full-link monitoring and other cloud service capabilities; and its SaaS part has the microservices architecture with the features of servitization, componentization, decentralization, independent deployment etc. Finally, the suggestions are given to design the microservices for the electric power marketing business application.",Research on building an innovative electric power marketing business application system based on cloud computing and microservices architecture technologies,"The paper proposes the method of building the electric power marketing business application system based on the cloud computing and microservices architecture technologies to solve the issues of the current system with the concentrated infrastructure and the monolithic architecture which are difficult to support the business processing in very large scale and the evolution of the electric power marketing business. The paper first introduces the current electric power marketing business application system and highlights its main drawbacks. Then it proposes the method to build the innovative electric power marketing business application system with the main technical characteristics as follows: the IaaS part of the application system has the distributed infrastructures with the IT resource elastic management and powerful horizontal scalability brought by the cloud computing technology; its PaaS part is the service platform with the components of the data processing, information integration, application building, and cloud service center to provide the several cloud services, such as one-click deployment, flexible scaling, fault self-healing, gray distribution, full-link monitoring and other cloud service capabilities; and its SaaS part has the microservices architecture with the features of servitization, componentization, decentralization, independent deployment etc. Finally, the suggestions are given to design the microservices for the electric power marketing business application.",IEEE conference,no,"['research', 'building', 'innovative', 'electric', 'power', 'marketing', 'business', 'based', 'computing', 'technology', 'paper', 'proposes', 'method', 'building', 'electric', 'power', 'marketing', 'business', 'based', 'computing', 'technology', 'solve', 'issue', 'current', 'infrastructure', 'monolithic', 'difficult', 'support', 'business', 'processing', 'large', 'scale', 'evolution', 'electric', 'power', 'marketing', 'business', 'paper', 'first', 'introduces', 'current', 'electric', 'power', 'marketing', 'business', 'highlight', 'main', 'drawback', 'proposes', 'method', 'build', 'innovative', 'electric', 'power', 'marketing', 'business', 'main', 'technical', 'characteristic', 'follows', 'iaa', 'part', 'distributed', 'infrastructure', 'resource', 'elastic', 'management', 'powerful', 'horizontal', 'scalability', 'brought', 'computing', 'technology', 'paas', 'part', 'platform', 'processing', 'information', 'integration', 'building', 'center', 'provide', 'several', 'deployment', 'flexible', 'scaling', 'fault', 'gray', 'distribution', 'monitoring', 'capability', 'saas', 'part', 'feature', 'servitization', 'componentization', 'independent', 'deployment', 'etc', 'finally', 'suggestion', 'given', 'design', 'electric', 'power', 'marketing', 'business']"
"Design and Implementation of Multi-tenant Vehicle Monitoring Architecture Based on Microservices and Spark Streaming Aiming at reducing the costs for customers and making full use of software and hardware resources, this article propose a vehicle monitoring cloud architecture for multi-tenant in this paper which could provide customers with rent-and-use cloud services. This architecture is based on the springcloud microservice framework, any microservice module could be packaged into docker images and managed by kubernetes. When the system concurrency increases or decreases, the container can be dynamically expanded and recycled to rationally allocate hardware resources to solve the bottleneck of service access load. This architecture also recommends kafka message queues to decouple front-end data collection components and back-end data processing components that could achieve greater throughput. The MapReduce programming model is used to group chaotic data sent by all tenants’ vehicle terminal by tenant id, and statistical analysis is performed on the data of each tenant.",Design and Implementation of Multi-tenant Vehicle Monitoring Architecture Based on Microservices and Spark Streaming,"Aiming at reducing the costs for customers and making full use of software and hardware resources, this article propose a vehicle monitoring cloud architecture for multi-tenant in this paper which could provide customers with rent-and-use cloud services. This architecture is based on the springcloud microservice framework, any microservice module could be packaged into docker images and managed by kubernetes. When the system concurrency increases or decreases, the container can be dynamically expanded and recycled to rationally allocate hardware resources to solve the bottleneck of service access load. This architecture also recommends kafka message queues to decouple front-end data collection components and back-end data processing components that could achieve greater throughput. The MapReduce programming model is used to group chaotic data sent by all tenants’ vehicle terminal by tenant id, and statistical analysis is performed on the data of each tenant.",IEEE conference,no,"['design', 'implementation', 'vehicle', 'monitoring', 'based', 'spark', 'streaming', 'aiming', 'reducing', 'cost', 'customer', 'making', 'full', 'use', 'hardware', 'resource', 'article', 'propose', 'vehicle', 'monitoring', 'paper', 'could', 'provide', 'customer', 'based', 'framework', 'module', 'could', 'packaged', 'docker', 'image', 'managed', 'kubernetes', 'concurrency', 'increase', 'decrease', 'container', 'dynamically', 'hardware', 'resource', 'solve', 'bottleneck', 'access', 'load', 'also', 'kafka', 'message', 'queue', 'decouple', 'collection', 'processing', 'could', 'achieve', 'greater', 'throughput', 'programming', 'model', 'used', 'group', 'tenant', 'vehicle', 'tenant', 'statistical', 'analysis', 'performed', 'tenant']"
"MTAP - A Distributed Framework for NLP Pipelines To facilitate the research, development, and testing process for clinical Natural Language Processing (NLP) platforms and associated solutions we have created the Microservice Text Analysis Platform (MTAP). MTAP provides a programming language agnostic, easy-to-use platform for users to create independently deployable, testable, and scalable pipeline components.",MTAP - A Distributed Framework for NLP Pipelines,"To facilitate the research, development, and testing process for clinical Natural Language Processing (NLP) platforms and associated solutions we have created the Microservice Text Analysis Platform (MTAP). MTAP provides a programming language agnostic, easy-to-use platform for users to create independently deployable, testable, and scalable pipeline components.",IEEE conference,no,"['mtap', 'distributed', 'framework', 'pipeline', 'facilitate', 'research', 'development', 'testing', 'process', 'clinical', 'natural', 'language', 'processing', 'platform', 'associated', 'solution', 'created', 'text', 'analysis', 'platform', 'mtap', 'mtap', 'provides', 'programming', 'language', 'platform', 'user', 'create', 'independently', 'deployable', 'scalable', 'pipeline']"
"A microservice-based platform for IoT application development Modern Internet of Things ecosystems and applications must be able to cope with an increasing demand for flexibility, efficiency, scalability, security. This paper presents CMC-IoT (CRS4 Microservice Core - IoT), an Internet of Things platform built upon a microservice-based architecture, providing tools for developing scalable and robust IoT applications. The platform aims to act as a generic middleware to connect a wide variety of devices, using a connector-driven and vendor-agnostic approach, allowing the development of applications in many domains, as shown in the provided examples. The focus is on the most prominent architectural features of CMC-IoT, that are compared with a reference architecture available in literature.",A microservice-based platform for IoT application development,"Modern Internet of Things ecosystems and applications must be able to cope with an increasing demand for flexibility, efficiency, scalability, security. This paper presents CMC-IoT (CRS4 Microservice Core - IoT), an Internet of Things platform built upon a microservice-based architecture, providing tools for developing scalable and robust IoT applications. The platform aims to act as a generic middleware to connect a wide variety of devices, using a connector-driven and vendor-agnostic approach, allowing the development of applications in many domains, as shown in the provided examples. The focus is on the most prominent architectural features of CMC-IoT, that are compared with a reference architecture available in literature.",IEEE conference,no,"['platform', 'iot', 'development', 'modern', 'internet', 'thing', 'ecosystem', 'must', 'able', 'cope', 'increasing', 'demand', 'flexibility', 'efficiency', 'scalability', 'security', 'paper', 'present', 'core', 'iot', 'internet', 'thing', 'platform', 'built', 'upon', 'providing', 'tool', 'developing', 'scalable', 'robust', 'iot', 'platform', 'aim', 'act', 'generic', 'middleware', 'connect', 'wide', 'variety', 'device', 'using', 'allowing', 'development', 'many', 'domain', 'shown', 'provided', 'example', 'focus', 'prominent', 'architectural', 'feature', 'compared', 'reference', 'available', 'literature']"
"A Novel Architectural Design for Solving Lost-Link Problems in UAV Collaboration Research in unmanned aerial vehicles (UAVs) has gained attention from various communities because of their potential usage in improving safety and efficiency in different applications. An UAV has shown promising results in dangerous conditions such as forest fires, search and rescue, medical deliveries, wildlife monitoring and geophysical scanning. Some external conditions like slow or no internet connection areas such as rural, farm, forest, ocean, etc. may affect the performance of the UAVs. These conditions can be considered as lost-link problems. Several approaches have been conducted to resolve such issues by implementing robust on-board architecture, machine learning approaches and developing knowledge based reasoning systems. However, much of software architecture research has concentrated on UAV implementation in normal network condition. Thus, we propose a model for considering lost-link problems in software architecture. In this paper, we describe two interconnected architectures for client and server. The UAV as a client is controlled by microkernel based architecture and the server is developed using microservice architecture. Both of them are connected using a synchronizer component to collect, filter, analyze, predict, and mitigate an UAV when a lost-link problem occurs. Therefore, the UAV can still find an appropriate action to complete a mission as far as the sensor and actuator are not in a critical condition. Experiment results show that our approach yields high percentage of mission accomplishment, fault tolerance and performance in a lost-link situation.",A Novel Architectural Design for Solving Lost-Link Problems in UAV Collaboration,"Research in unmanned aerial vehicles (UAVs) has gained attention from various communities because of their potential usage in improving safety and efficiency in different applications. An UAV has shown promising results in dangerous conditions such as forest fires, search and rescue, medical deliveries, wildlife monitoring and geophysical scanning. Some external conditions like slow or no internet connection areas such as rural, farm, forest, ocean, etc. may affect the performance of the UAVs. These conditions can be considered as lost-link problems. Several approaches have been conducted to resolve such issues by implementing robust on-board architecture, machine learning approaches and developing knowledge based reasoning systems. However, much of software architecture research has concentrated on UAV implementation in normal network condition. Thus, we propose a model for considering lost-link problems in software architecture. In this paper, we describe two interconnected architectures for client and server. The UAV as a client is controlled by microkernel based architecture and the server is developed using microservice architecture. Both of them are connected using a synchronizer component to collect, filter, analyze, predict, and mitigate an UAV when a lost-link problem occurs. Therefore, the UAV can still find an appropriate action to complete a mission as far as the sensor and actuator are not in a critical condition. Experiment results show that our approach yields high percentage of mission accomplishment, fault tolerance and performance in a lost-link situation.",IEEE conference,no,"['novel', 'architectural', 'design', 'solving', 'problem', 'uav', 'collaboration', 'research', 'vehicle', 'uavs', 'gained', 'attention', 'various', 'community', 'potential', 'usage', 'improving', 'safety', 'efficiency', 'different', 'uav', 'shown', 'promising', 'result', 'condition', 'forest', 'fire', 'search', 'medical', 'delivery', 'monitoring', 'external', 'condition', 'like', 'internet', 'connection', 'area', 'rural', 'farm', 'forest', 'etc', 'may', 'affect', 'performance', 'uavs', 'condition', 'considered', 'problem', 'several', 'conducted', 'resolve', 'issue', 'implementing', 'robust', 'machine', 'learning', 'developing', 'knowledge', 'based', 'reasoning', 'however', 'much', 'research', 'uav', 'implementation', 'normal', 'network', 'condition', 'thus', 'propose', 'model', 'considering', 'problem', 'paper', 'describe', 'two', 'interconnected', 'client', 'server', 'uav', 'client', 'controlled', 'based', 'server', 'developed', 'using', 'connected', 'using', 'collect', 'filter', 'analyze', 'predict', 'mitigate', 'uav', 'problem', 'occurs', 'therefore', 'uav', 'still', 'find', 'appropriate', 'action', 'complete', 'mission', 'far', 'sensor', 'actuator', 'critical', 'condition', 'experiment', 'result', 'show', 'yield', 'high', 'mission', 'fault', 'tolerance', 'performance', 'situation']"
"UCSAM: A UAV Ground Control System Architecture Supporting Cooperative Control Among Multi-form Stations based on MDA and Container Cloud Platform An open, unified and loosely coupled service oriented software architecture of UAV Ground Control System (UGCS) based on container cloud platform is proposed in this paper, which is called “UCSAM”, and can support the collaborative control of UAV among multi-form ground control stations, such as portable, mobile and fixed stations. UCSAM establishes a kind of software computing environment based on the component framework conforming to the OSGi specifications on the client side, the container cloud platform on the server side, and the middleware conforming to the DDS and other transmission protocols between the two, so that the application software components on the client side and the FDSCs(Functional Domain Service Components) on the server side only rely on standardized APIs, so as to realize the architecture features of resource allocation on demand and cross platform portability. On the other hand, UCSAM defines PSM based on Open Application Model (OAM), including message model, service model and application model, which separates FDSC development from the operation and maintenance complexity of infrastructure software. Through a set of tool chains, it realizes the conversion from PIM based on XMI format to PSM based on OAM, code generation based on template and PSM, CI/CD based on GitOps, and dynamic deployment based on AppStore, so as to achieve the architectural goal of dynamic generation of capabilities, and the agile software development mode based on MDA and DevOps. Lastly, several study cases including UAV cooperative application, development of functional domain service components, and application deployment and operation are briefly discussed to verify this architecture's correctness and feasibility.",UCSAM: A UAV Ground Control System Architecture Supporting Cooperative Control Among Multi-form Stations based on MDA and Container Cloud Platform,"An open, unified and loosely coupled service oriented software architecture of UAV Ground Control System (UGCS) based on container cloud platform is proposed in this paper, which is called “UCSAM”, and can support the collaborative control of UAV among multi-form ground control stations, such as portable, mobile and fixed stations. UCSAM establishes a kind of software computing environment based on the component framework conforming to the OSGi specifications on the client side, the container cloud platform on the server side, and the middleware conforming to the DDS and other transmission protocols between the two, so that the application software components on the client side and the FDSCs(Functional Domain Service Components) on the server side only rely on standardized APIs, so as to realize the architecture features of resource allocation on demand and cross platform portability. On the other hand, UCSAM defines PSM based on Open Application Model (OAM), including message model, service model and application model, which separates FDSC development from the operation and maintenance complexity of infrastructure software. Through a set of tool chains, it realizes the conversion from PIM based on XMI format to PSM based on OAM, code generation based on template and PSM, CI/CD based on GitOps, and dynamic deployment based on AppStore, so as to achieve the architectural goal of dynamic generation of capabilities, and the agile software development mode based on MDA and DevOps. Lastly, several study cases including UAV cooperative application, development of functional domain service components, and application deployment and operation are briefly discussed to verify this architecture's correctness and feasibility.",IEEE conference,no,"['ucsam', 'uav', 'ground', 'control', 'supporting', 'cooperative', 'control', 'among', 'station', 'based', 'container', 'platform', 'open', 'unified', 'loosely', 'coupled', 'oriented', 'uav', 'ground', 'control', 'based', 'container', 'platform', 'proposed', 'paper', 'called', 'ucsam', 'support', 'collaborative', 'control', 'uav', 'among', 'ground', 'control', 'station', 'mobile', 'fixed', 'station', 'ucsam', 'kind', 'computing', 'environment', 'based', 'framework', 'conforming', 'specification', 'client', 'side', 'container', 'platform', 'server', 'side', 'middleware', 'conforming', 'transmission', 'protocol', 'two', 'client', 'side', 'functional', 'domain', 'server', 'side', 'rely', 'standardized', 'apis', 'realize', 'feature', 'resource', 'allocation', 'demand', 'platform', 'portability', 'hand', 'ucsam', 'defines', 'psm', 'based', 'open', 'model', 'including', 'message', 'model', 'model', 'model', 'separate', 'development', 'operation', 'maintenance', 'complexity', 'infrastructure', 'set', 'tool', 'chain', 'realizes', 'conversion', 'based', 'format', 'psm', 'based', 'code', 'generation', 'based', 'template', 'psm', 'based', 'gitops', 'dynamic', 'deployment', 'based', 'achieve', 'architectural', 'goal', 'dynamic', 'generation', 'capability', 'agile', 'development', 'mode', 'based', 'devops', 'several', 'study', 'case', 'including', 'uav', 'cooperative', 'development', 'functional', 'domain', 'deployment', 'operation', 'discussed', 'verify', 'correctness', 'feasibility']"
"An Intelligent Robot Sorting System By Deep Learning On RGB-D Image Industrial robots have seen wide adoption in the field of intelligent manufacturing, and they are applied to the intelligent sorting of items on the industrial product line. However, such techniques have some limitations, such as the object being placed at a fixed height. To solve this problem and meet the needs of flexible manufacturing, this paper develops an intelligent robot sorting system by deep learning on RGBD image (RSSDR). RSSDR consists of five modules, including central host, robot, visual kit, microservices and machine visual control software. The central host provides the image of 3D printed industrial parts collected by the visual kit, and calls the deep learning algorithm on the microservice to detect the key points on the image, and then sends the position of the grasping points to the robot through the machine vision control software to control the robot to sort the 3D printed industrial parts. In RSSDR, to visually identify a total of 3 different 3D printed industrial parts in uncontrolled conditions, YOLOv5s is used as the detection model, which achieves 97.65% accuracy and 35ms runtime. The experimental results show that RSSDR is suitable for sorting electronic component.",An Intelligent Robot Sorting System By Deep Learning On RGB-D Image,"Industrial robots have seen wide adoption in the field of intelligent manufacturing, and they are applied to the intelligent sorting of items on the industrial product line. However, such techniques have some limitations, such as the object being placed at a fixed height. To solve this problem and meet the needs of flexible manufacturing, this paper develops an intelligent robot sorting system by deep learning on RGBD image (RSSDR). RSSDR consists of five modules, including central host, robot, visual kit, microservices and machine visual control software. The central host provides the image of 3D printed industrial parts collected by the visual kit, and calls the deep learning algorithm on the microservice to detect the key points on the image, and then sends the position of the grasping points to the robot through the machine vision control software to control the robot to sort the 3D printed industrial parts. In RSSDR, to visually identify a total of 3 different 3D printed industrial parts in uncontrolled conditions, YOLOv5s is used as the detection model, which achieves 97.65% accuracy and 35ms runtime. The experimental results show that RSSDR is suitable for sorting electronic component.",IEEE conference,no,"['intelligent', 'robot', 'sorting', 'deep', 'learning', 'image', 'industrial', 'robot', 'seen', 'wide', 'adoption', 'field', 'intelligent', 'manufacturing', 'applied', 'intelligent', 'sorting', 'item', 'industrial', 'product', 'line', 'however', 'technique', 'limitation', 'object', 'placed', 'fixed', 'solve', 'problem', 'meet', 'need', 'flexible', 'manufacturing', 'paper', 'develops', 'intelligent', 'robot', 'sorting', 'deep', 'learning', 'image', 'rssdr', 'rssdr', 'consists', 'five', 'module', 'including', 'central', 'host', 'robot', 'visual', 'kit', 'machine', 'visual', 'control', 'central', 'host', 'provides', 'image', 'printed', 'industrial', 'part', 'collected', 'visual', 'kit', 'call', 'deep', 'learning', 'algorithm', 'detect', 'key', 'point', 'image', 'point', 'robot', 'machine', 'vision', 'control', 'control', 'robot', 'sort', 'printed', 'industrial', 'part', 'rssdr', 'identify', 'total', 'different', 'printed', 'industrial', 'part', 'condition', 'used', 'detection', 'model', 'achieves', 'accuracy', 'runtime', 'experimental', 'result', 'show', 'rssdr', 'suitable', 'sorting', 'electronic']"
"Artificial Intelligence-Based Centralized Resource Management Application for Distributed Systems Due to the decentralized nature and emergence of new practices, tools, and platforms, microservices have become one of the most widely spread software architectures in the modern software industry. Furthermore, the advancement of software packaging tools like Docker and orchestration platforms such as Kubernetes enable developers and operation engineers to deploy and manage microservice applications more effectively and efficiently. However, establishing and managing microservice applications are still cumbersome due to the infrastructure configuration and array of disjoint tools that fail to understand the application’s dynamic behavior. As a result, developers need to configure multiple tools and platforms to automate the deployment and monitoring process to provide the optimal deployment strategy for microservices. Even though many tools are available in the industry, the fully automated product which comprises deployment, monitoring, resiliency evaluation and optimization were not developed yet. In response to this issue, we propose an artificial intelligence (AI)-based centralized resource management tool, that provides an automated low latency container management, cluster metrics gathering, resiliency evaluation and optimal deployment strategy behave in dynamic nature.",Artificial Intelligence-Based Centralized Resource Management Application for Distributed Systems,"Due to the decentralized nature and emergence of new practices, tools, and platforms, microservices have become one of the most widely spread software architectures in the modern software industry. Furthermore, the advancement of software packaging tools like Docker and orchestration platforms such as Kubernetes enable developers and operation engineers to deploy and manage microservice applications more effectively and efficiently. However, establishing and managing microservice applications are still cumbersome due to the infrastructure configuration and array of disjoint tools that fail to understand the application’s dynamic behavior. As a result, developers need to configure multiple tools and platforms to automate the deployment and monitoring process to provide the optimal deployment strategy for microservices. Even though many tools are available in the industry, the fully automated product which comprises deployment, monitoring, resiliency evaluation and optimization were not developed yet. In response to this issue, we propose an artificial intelligence (AI)-based centralized resource management tool, that provides an automated low latency container management, cluster metrics gathering, resiliency evaluation and optimal deployment strategy behave in dynamic nature.",IEEE conference,no,"['artificial', 'centralized', 'resource', 'management', 'distributed', 'due', 'decentralized', 'nature', 'emergence', 'new', 'practice', 'tool', 'platform', 'become', 'one', 'widely', 'spread', 'modern', 'industry', 'furthermore', 'advancement', 'packaging', 'tool', 'like', 'docker', 'orchestration', 'platform', 'kubernetes', 'enable', 'developer', 'operation', 'engineer', 'deploy', 'manage', 'effectively', 'efficiently', 'however', 'establishing', 'managing', 'still', 'due', 'infrastructure', 'configuration', 'tool', 'fail', 'understand', 'dynamic', 'behavior', 'result', 'developer', 'need', 'configure', 'multiple', 'tool', 'platform', 'automate', 'deployment', 'monitoring', 'process', 'provide', 'optimal', 'deployment', 'strategy', 'even', 'though', 'many', 'tool', 'available', 'industry', 'fully', 'automated', 'product', 'comprises', 'deployment', 'monitoring', 'resiliency', 'evaluation', 'optimization', 'developed', 'yet', 'response', 'issue', 'propose', 'artificial', 'intelligence', 'ai', 'centralized', 'resource', 'management', 'tool', 'provides', 'automated', 'low', 'latency', 'container', 'management', 'cluster', 'metric', 'gathering', 'resiliency', 'evaluation', 'optimal', 'deployment', 'strategy', 'behave', 'dynamic', 'nature']"
"LEAD: Latency-Efficient Application Deployment for Microservices Architecture In cloud computing, microservices architecture has become the preferred choice for many applications. Accordingly, several small and decoupled containerized services hosted on different servers communicate through the network. Therefore, communication latency significantly impacts end-to-end latency. Kubernetes, the de facto standard for container orchestration, cannot reduce this overhead due to its lack of awareness of service interactions.We present LEAD, a Latency-Efficient Application Deployment framework that integrates with Kubernetes without modifying its core components. LEAD considers the inter-service relationships and resource constraints, improving service placement to reduce end-to-end latency. The idea behind LEAD is straightforward: keep the cooperating services close to each other to exploit faster in-node communication and automate this process. The proposed idea is realized by leveraging a scoring algorithm and monitoring framework to achieve dynamic improvement of service placement. Our experimental results show an average 20% improvement in the 99th percentile latency compared to Kubernetes default scheduler.",LEAD: Latency-Efficient Application Deployment for Microservices Architecture,"In cloud computing, microservices architecture has become the preferred choice for many applications. Accordingly, several small and decoupled containerized services hosted on different servers communicate through the network. Therefore, communication latency significantly impacts end-to-end latency. Kubernetes, the de facto standard for container orchestration, cannot reduce this overhead due to its lack of awareness of service interactions.We present LEAD, a Latency-Efficient Application Deployment framework that integrates with Kubernetes without modifying its core components. LEAD considers the inter-service relationships and resource constraints, improving service placement to reduce end-to-end latency. The idea behind LEAD is straightforward: keep the cooperating services close to each other to exploit faster in-node communication and automate this process. The proposed idea is realized by leveraging a scoring algorithm and monitoring framework to achieve dynamic improvement of service placement. Our experimental results show an average 20% improvement in the 99th percentile latency compared to Kubernetes default scheduler.",IEEE conference,no,"['lead', 'deployment', 'computing', 'become', 'preferred', 'choice', 'many', 'accordingly', 'several', 'small', 'decoupled', 'containerized', 'hosted', 'different', 'server', 'communicate', 'network', 'therefore', 'communication', 'latency', 'significantly', 'impact', 'latency', 'kubernetes', 'de', 'facto', 'standard', 'container', 'orchestration', 'reduce', 'overhead', 'due', 'lack', 'present', 'lead', 'deployment', 'framework', 'integrates', 'kubernetes', 'without', 'modifying', 'core', 'lead', 'considers', 'relationship', 'resource', 'constraint', 'improving', 'placement', 'reduce', 'latency', 'idea', 'behind', 'lead', 'straightforward', 'keep', 'close', 'exploit', 'faster', 'communication', 'automate', 'process', 'proposed', 'idea', 'realized', 'leveraging', 'scoring', 'algorithm', 'monitoring', 'framework', 'achieve', 'dynamic', 'improvement', 'placement', 'experimental', 'result', 'show', 'average', 'improvement', 'percentile', 'latency', 'compared', 'kubernetes', 'default', 'scheduler']"
"Demonstration of Container-Based Microservices SDN Control platform for Open Optical Networks We demonstrate a microservices SDN control platform that provides network control plane as a service with the capabilities to instantiate, upgrade, automated and on-demand deployment of the control components such as the controller and its applications.",Demonstration of Container-Based Microservices SDN Control platform for Open Optical Networks,"We demonstrate a microservices SDN control platform that provides network control plane as a service with the capabilities to instantiate, upgrade, automated and on-demand deployment of the control components such as the controller and its applications.",IEEE conference,no,"['demonstration', 'sdn', 'control', 'platform', 'open', 'optical', 'network', 'demonstrate', 'sdn', 'control', 'platform', 'provides', 'network', 'control', 'plane', 'capability', 'instantiate', 'upgrade', 'automated', 'deployment', 'control', 'controller']"
"Kuber: Cost-Efficient Microservice Deployment Planner The microservice-based architecture - a SOA-inspired principle of dividing backend systems into indepen-dently deployed components that communicate with each other using language-agnostic APIs - has gained increased popularity in industry. Realistic microservice-based applications contain hundreds of services deployed on a cloud. As cloud providers typically offer a variety of virtual machine (VM) types, each with its own hardware specification and cost, picking a proper cloud configuration for deploying all microservices in a way that satisfies performance targets while minimizing the deployment costs becomes challenging. Existing work focuses on identifying the best VM types for recurrent (mostly high-performance computing) jobs. Yet, identifying the best VM type for the myriad of all possible service combinations and further identifying the optimal subset of combinations that minimizes deployment cost is an intractable problem for applications with a large number of services. To address this problem, we propose an approach, called Kuber, which utilizes a set of strategies to efficiently sample the neces-sary subset of service combinations and VM types to explore. Comparing Kuber with baseline approaches shows that Kuber is able to find the best deployment with the lowest search cost.",Kuber: Cost-Efficient Microservice Deployment Planner,"The microservice-based architecture - a SOA-inspired principle of dividing backend systems into indepen-dently deployed components that communicate with each other using language-agnostic APIs - has gained increased popularity in industry. Realistic microservice-based applications contain hundreds of services deployed on a cloud. As cloud providers typically offer a variety of virtual machine (VM) types, each with its own hardware specification and cost, picking a proper cloud configuration for deploying all microservices in a way that satisfies performance targets while minimizing the deployment costs becomes challenging. Existing work focuses on identifying the best VM types for recurrent (mostly high-performance computing) jobs. Yet, identifying the best VM type for the myriad of all possible service combinations and further identifying the optimal subset of combinations that minimizes deployment cost is an intractable problem for applications with a large number of services. To address this problem, we propose an approach, called Kuber, which utilizes a set of strategies to efficiently sample the neces-sary subset of service combinations and VM types to explore. Comparing Kuber with baseline approaches shows that Kuber is able to find the best deployment with the lowest search cost.",IEEE conference,no,"['kuber', 'deployment', 'principle', 'dividing', 'backend', 'deployed', 'communicate', 'using', 'apis', 'gained', 'increased', 'popularity', 'industry', 'realistic', 'contain', 'hundred', 'deployed', 'provider', 'typically', 'offer', 'variety', 'virtual', 'machine', 'vm', 'type', 'hardware', 'specification', 'cost', 'proper', 'configuration', 'deploying', 'way', 'satisfies', 'performance', 'target', 'minimizing', 'deployment', 'cost', 'becomes', 'challenging', 'existing', 'work', 'focus', 'identifying', 'best', 'vm', 'type', 'recurrent', 'computing', 'job', 'yet', 'identifying', 'best', 'vm', 'type', 'possible', 'combination', 'identifying', 'optimal', 'subset', 'combination', 'minimizes', 'deployment', 'cost', 'problem', 'large', 'number', 'address', 'problem', 'propose', 'called', 'kuber', 'utilizes', 'set', 'strategy', 'efficiently', 'sample', 'subset', 'combination', 'vm', 'type', 'explore', 'comparing', 'kuber', 'baseline', 'show', 'kuber', 'able', 'find', 'best', 'deployment', 'lowest', 'search', 'cost']"
"Research and Application of Battery Production Data Management System Based on Microservice The software system based on micro-service architecture has the advantages of good scalability and agile development, and can quickly respond to the personalized and customized requirements of manufacturing enterprises. Based on existing battery Manufacturing Execution System (MES), a supplementary extension scheme founded on microservice architecture is proposed to meet the needs of battery production data management. Without making any changes to the battery MES, the scheme realized the unified authentication of the microservice system and the existing battery MES, achieved the integration of related components of the routing, monitoring and data management of the microservice system, and accomplished the design and development of the battery production data management system.",Research and Application of Battery Production Data Management System Based on Microservice,"The software system based on micro-service architecture has the advantages of good scalability and agile development, and can quickly respond to the personalized and customized requirements of manufacturing enterprises. Based on existing battery Manufacturing Execution System (MES), a supplementary extension scheme founded on microservice architecture is proposed to meet the needs of battery production data management. Without making any changes to the battery MES, the scheme realized the unified authentication of the microservice system and the existing battery MES, achieved the integration of related components of the routing, monitoring and data management of the microservice system, and accomplished the design and development of the battery production data management system.",IEEE conference,no,"['research', 'battery', 'production', 'management', 'based', 'based', 'advantage', 'good', 'scalability', 'agile', 'development', 'quickly', 'respond', 'personalized', 'customized', 'requirement', 'manufacturing', 'enterprise', 'based', 'existing', 'battery', 'manufacturing', 'execution', 'me', 'supplementary', 'extension', 'scheme', 'proposed', 'meet', 'need', 'battery', 'production', 'management', 'without', 'making', 'change', 'battery', 'me', 'scheme', 'realized', 'unified', 'authentication', 'existing', 'battery', 'me', 'achieved', 'integration', 'related', 'routing', 'monitoring', 'management', 'design', 'development', 'battery', 'production', 'management']"
"Microservice Based Edge Computing Architecture for Internet of Things Distributed computation and AI processing at the edge has been identified as an efficient solution to deliver real-time IoT services and applications compared to cloud-based paradigms. These solutions are expected to support the delay-sensitive IoT applications, autonomic decision making, and smart service creation at the edge in comparison to traditional IoT solutions. However, existing solutions have limitations concerning distributed and simultaneous resource management for AI computation and data processing at the edge; concurrent and real-time application execution; and platform-independent deployment. Hence, first, we propose a novel three-layer architecture that facilitates the above service requirements. Then we have developed a novel platform and relevant modules with integrated AI processing and edge computer paradigms considering issues related to scalability, heterogeneity, security, and interoperability of IoT services. Further, each component is designed to handle the control signals, data flows, microservice orchestration, and resource composition to match with the IoT application requirements. Finally, the effectiveness of the proposed platform is tested and have been verified.",Microservice Based Edge Computing Architecture for Internet of Things,"Distributed computation and AI processing at the edge has been identified as an efficient solution to deliver real-time IoT services and applications compared to cloud-based paradigms. These solutions are expected to support the delay-sensitive IoT applications, autonomic decision making, and smart service creation at the edge in comparison to traditional IoT solutions. However, existing solutions have limitations concerning distributed and simultaneous resource management for AI computation and data processing at the edge; concurrent and real-time application execution; and platform-independent deployment. Hence, first, we propose a novel three-layer architecture that facilitates the above service requirements. Then we have developed a novel platform and relevant modules with integrated AI processing and edge computer paradigms considering issues related to scalability, heterogeneity, security, and interoperability of IoT services. Further, each component is designed to handle the control signals, data flows, microservice orchestration, and resource composition to match with the IoT application requirements. Finally, the effectiveness of the proposed platform is tested and have been verified.",IEEE conference,no,"['based', 'edge', 'computing', 'internet', 'thing', 'distributed', 'computation', 'ai', 'processing', 'edge', 'identified', 'efficient', 'solution', 'deliver', 'iot', 'compared', 'paradigm', 'solution', 'expected', 'support', 'iot', 'autonomic', 'decision', 'making', 'smart', 'creation', 'edge', 'comparison', 'traditional', 'iot', 'solution', 'however', 'existing', 'solution', 'limitation', 'concerning', 'distributed', 'resource', 'management', 'ai', 'computation', 'processing', 'edge', 'concurrent', 'execution', 'deployment', 'hence', 'first', 'propose', 'novel', 'facilitates', 'requirement', 'developed', 'novel', 'platform', 'relevant', 'module', 'integrated', 'ai', 'processing', 'edge', 'computer', 'paradigm', 'considering', 'issue', 'related', 'scalability', 'heterogeneity', 'security', 'interoperability', 'iot', 'designed', 'handle', 'control', 'signal', 'flow', 'orchestration', 'resource', 'composition', 'match', 'iot', 'requirement', 'finally', 'effectiveness', 'proposed', 'platform', 'tested', 'verified']"
"Predicting the End-to-End Tail Latency of Containerized Microservices in the Cloud Large-scale web services are increasingly adopting cloud-native principles of application design to better utilize the advantages of cloud computing. This involves building an application using many loosely coupled service-specific components (microservices) that communicate via lightweight APIs, and utilizing containerization technologies to deploy, update, and scale these microservices quickly and independently. However, managing the end-to-end tail latency of requests flowing through the microservices is challenging in the absence of accurate performance models that can capture the complex interplay of microservice workflows with cloudinduced performance variability and inter-service performance dependencies. In this paper, we present performance characterization and modeling of containerized microservices in the cloud. Our modeling approach aims at enabling cloud platforms to combine resource usage metrics collected from multiple layers of the cloud environment, and apply machine learning techniques to predict the end-to-end tail latency of microservice workflows. We implemented and evaluated our modeling approach on NSF Cloud's Chameleon testbed using KVM for virtualization, Docker Engine for containerization and Kubernetes for container orchestration. Experimental results with an open-source microservices benchmark, Sock Shop, show that our modeling approach achieves high prediction accuracy even in the presence of multi-tenant performance interference.",Predicting the End-to-End Tail Latency of Containerized Microservices in the Cloud,"Large-scale web services are increasingly adopting cloud-native principles of application design to better utilize the advantages of cloud computing. This involves building an application using many loosely coupled service-specific components (microservices) that communicate via lightweight APIs, and utilizing containerization technologies to deploy, update, and scale these microservices quickly and independently. However, managing the end-to-end tail latency of requests flowing through the microservices is challenging in the absence of accurate performance models that can capture the complex interplay of microservice workflows with cloudinduced performance variability and inter-service performance dependencies. In this paper, we present performance characterization and modeling of containerized microservices in the cloud. Our modeling approach aims at enabling cloud platforms to combine resource usage metrics collected from multiple layers of the cloud environment, and apply machine learning techniques to predict the end-to-end tail latency of microservice workflows. We implemented and evaluated our modeling approach on NSF Cloud's Chameleon testbed using KVM for virtualization, Docker Engine for containerization and Kubernetes for container orchestration. Experimental results with an open-source microservices benchmark, Sock Shop, show that our modeling approach achieves high prediction accuracy even in the presence of multi-tenant performance interference.",IEEE conference,no,"['predicting', 'tail', 'latency', 'containerized', 'web', 'increasingly', 'adopting', 'principle', 'design', 'better', 'utilize', 'advantage', 'computing', 'involves', 'building', 'using', 'many', 'loosely', 'coupled', 'communicate', 'via', 'lightweight', 'apis', 'utilizing', 'containerization', 'technology', 'deploy', 'update', 'scale', 'quickly', 'independently', 'however', 'managing', 'tail', 'latency', 'request', 'challenging', 'accurate', 'performance', 'model', 'capture', 'complex', 'interplay', 'workflow', 'performance', 'variability', 'performance', 'dependency', 'paper', 'present', 'performance', 'characterization', 'modeling', 'containerized', 'modeling', 'aim', 'enabling', 'platform', 'combine', 'resource', 'usage', 'metric', 'collected', 'multiple', 'layer', 'environment', 'apply', 'machine', 'learning', 'technique', 'predict', 'tail', 'latency', 'workflow', 'implemented', 'evaluated', 'modeling', 'testbed', 'using', 'virtualization', 'docker', 'engine', 'containerization', 'kubernetes', 'container', 'orchestration', 'experimental', 'result', 'benchmark', 'sock', 'shop', 'show', 'modeling', 'achieves', 'high', 'prediction', 'accuracy', 'even', 'presence', 'performance', 'interference']"
"Wandering and getting lost: the architecture of an app activating local communities on dementia issues We describe the architecture of Sammen Om Demens (SOD), an application for portable devices aiming at helping persons with dementia when wandering and getting lost through the involvement of caregivers, family members, and ordinary citizens who volunteer.To enable the real-time detection of a person with dementia that has lost orientation, we transfer location data at high frequency from a frontend on the smartphone of a person with dementia to a backend system. The backend system must be able to cope with the high throughput data and carry out possibly heavy computations for the detection of anomalous behavior via artificial intelligence techniques. This sets certain performance and architectural requirements on the design of the backend.In the paper, we discuss our design and implementation choices for the backend of SOD that involve microservices and serverless services to achieve efficiency and scalability. We give evidence of the achieved goals by deploying the SOD backend on a public cloud and measuring the performance on simulated load tests.",Wandering and getting lost: the architecture of an app activating local communities on dementia issues,"We describe the architecture of Sammen Om Demens (SOD), an application for portable devices aiming at helping persons with dementia when wandering and getting lost through the involvement of caregivers, family members, and ordinary citizens who volunteer.To enable the real-time detection of a person with dementia that has lost orientation, we transfer location data at high frequency from a frontend on the smartphone of a person with dementia to a backend system. The backend system must be able to cope with the high throughput data and carry out possibly heavy computations for the detection of anomalous behavior via artificial intelligence techniques. This sets certain performance and architectural requirements on the design of the backend.In the paper, we discuss our design and implementation choices for the backend of SOD that involve microservices and serverless services to achieve efficiency and scalability. We give evidence of the achieved goals by deploying the SOD backend on a public cloud and measuring the performance on simulated load tests.",IEEE conference,no,"['getting', 'lost', 'app', 'local', 'community', 'dementia', 'issue', 'describe', 'sod', 'device', 'aiming', 'helping', 'person', 'dementia', 'getting', 'lost', 'involvement', 'caregiver', 'citizen', 'enable', 'detection', 'person', 'dementia', 'lost', 'transfer', 'location', 'high', 'frequency', 'frontend', 'person', 'dementia', 'backend', 'backend', 'must', 'able', 'cope', 'high', 'throughput', 'carry', 'possibly', 'computation', 'detection', 'behavior', 'via', 'artificial', 'intelligence', 'technique', 'set', 'certain', 'performance', 'architectural', 'requirement', 'design', 'paper', 'discus', 'design', 'implementation', 'choice', 'backend', 'sod', 'involve', 'serverless', 'achieve', 'efficiency', 'scalability', 'give', 'evidence', 'achieved', 'goal', 'deploying', 'sod', 'backend', 'public', 'measuring', 'performance', 'simulated', 'load', 'test']"
"Preproduction Deploys: Cloud-Native Integration Testing The microservice architecture for cloud-based systems is extended to not only require each loosely coupled component to be independently deployable, but also to provide independent routing for each component. This supports canary deployments, green/blue deployments and roll-back. Both ad hoc and system integration test traffic can be directed to components before they are released to production traffic. Front-end code is included in this architecture by using server-side rendering of JS bundles. Environments for integration testing are created with preproduction deploys side by side with production deploys using appropriate levels of isolation. After a successful integration test run, preproduction components are known to work with production precisely as it is. For isolation, test traffic uses staging databases that are copied daily from the production databases, omitting sensitive data. Safety and security concerns are dealt with in a targeted fashion, not monolithically. This architecture scales well with organization size; is more effective for integration testing; and is better aligned with agile business practices than traditional approaches.",Preproduction Deploys: Cloud-Native Integration Testing,"The microservice architecture for cloud-based systems is extended to not only require each loosely coupled component to be independently deployable, but also to provide independent routing for each component. This supports canary deployments, green/blue deployments and roll-back. Both ad hoc and system integration test traffic can be directed to components before they are released to production traffic. Front-end code is included in this architecture by using server-side rendering of JS bundles. Environments for integration testing are created with preproduction deploys side by side with production deploys using appropriate levels of isolation. After a successful integration test run, preproduction components are known to work with production precisely as it is. For isolation, test traffic uses staging databases that are copied daily from the production databases, omitting sensitive data. Safety and security concerns are dealt with in a targeted fashion, not monolithically. This architecture scales well with organization size; is more effective for integration testing; and is better aligned with agile business practices than traditional approaches.",IEEE conference,no,"['preproduction', 'deploys', 'integration', 'testing', 'extended', 'require', 'loosely', 'coupled', 'independently', 'deployable', 'also', 'provide', 'independent', 'routing', 'support', 'deployment', 'deployment', 'ad', 'integration', 'test', 'traffic', 'released', 'production', 'traffic', 'code', 'included', 'using', 'rendering', 'environment', 'integration', 'testing', 'created', 'preproduction', 'deploys', 'side', 'side', 'production', 'deploys', 'using', 'appropriate', 'level', 'isolation', 'successful', 'integration', 'test', 'run', 'preproduction', 'known', 'work', 'production', 'precisely', 'isolation', 'test', 'traffic', 'us', 'database', 'daily', 'production', 'database', 'sensitive', 'safety', 'security', 'concern', 'fashion', 'scale', 'well', 'organization', 'size', 'effective', 'integration', 'testing', 'better', 'aligned', 'agile', 'business', 'practice', 'traditional']"
"MI-FIWARE: A web component development method for FIWARE using microservices Within the smart solutions (Smart Industry, Smart Home and Smart City), the concept of Smart Cities is the one having the greatest growth, inducing an increment in the number of developers focused on carrying out this idea, and growth in the number of companies that contribute with solutions like IoT platforms. These platforms help in the development of smart solutions providing tools and services that automate some of the development processes and ease the creation of software for smart environments. To support the growth of smart cities, we propose MI-FIWARE, an architecture based on microservices (MI) and the IoT platform FIWARE. MI-FIWARE reduces developers from the back-end workload and the processing of context information, in a way that only the front-side has to be developed using a web component template proposed. MI- FIWARE architecture is based on microservices, taking advantage of FIWARE as a modular platform and establishing the possibility of using microservices to add extra functionality to the IoT platform, helping the developers in other development processes like security.",MI-FIWARE: A web component development method for FIWARE using microservices,"Within the smart solutions (Smart Industry, Smart Home and Smart City), the concept of Smart Cities is the one having the greatest growth, inducing an increment in the number of developers focused on carrying out this idea, and growth in the number of companies that contribute with solutions like IoT platforms. These platforms help in the development of smart solutions providing tools and services that automate some of the development processes and ease the creation of software for smart environments. To support the growth of smart cities, we propose MI-FIWARE, an architecture based on microservices (MI) and the IoT platform FIWARE. MI-FIWARE reduces developers from the back-end workload and the processing of context information, in a way that only the front-side has to be developed using a web component template proposed. MI- FIWARE architecture is based on microservices, taking advantage of FIWARE as a modular platform and establishing the possibility of using microservices to add extra functionality to the IoT platform, helping the developers in other development processes like security.",IEEE conference,no,"['web', 'development', 'method', 'fiware', 'using', 'within', 'smart', 'solution', 'smart', 'industry', 'smart', 'home', 'smart', 'city', 'concept', 'smart', 'city', 'one', 'growth', 'number', 'developer', 'focused', 'idea', 'growth', 'number', 'company', 'contribute', 'solution', 'like', 'iot', 'platform', 'platform', 'help', 'development', 'smart', 'solution', 'providing', 'tool', 'automate', 'development', 'process', 'ease', 'creation', 'smart', 'environment', 'support', 'growth', 'smart', 'city', 'propose', 'based', 'iot', 'platform', 'fiware', 'reduces', 'developer', 'workload', 'processing', 'context', 'information', 'way', 'developed', 'using', 'web', 'template', 'proposed', 'fiware', 'based', 'taking', 'advantage', 'fiware', 'modular', 'platform', 'establishing', 'possibility', 'using', 'add', 'extra', 'functionality', 'iot', 'platform', 'helping', 'developer', 'development', 'process', 'like', 'security']"
"A Microservice-Based Approach for Increasing Software Reusability in Health Applications Traditionally, legacy Health applications use software architecture models that make it difficult to reuse components. Reusability is an essential attribute in the software lifecycle, as it improves the quality of applications and reduces maintenance and development costs. This paper proposes the cloud tool Microservice4EHR, which dynamically generates reusable components from existing software artifacts (e.g., graphical interfaces), while conforming to the standards used in the healthcare domain. A software architecture based on Connectors and Microservice components is specified and made tangible by means of three algorithms. The use of both components is applied to a real-world scenario (a Brazilian blood donation center) and serves as an example. As a result, it is possible to notice that Health applications achieve greater reusability when they employ the microservice architecture. Thus, Microservice4EHR enables the use of reusable components in Health application architectures (for both new and legacy systems), increasing software reusability in this context.",A Microservice-Based Approach for Increasing Software Reusability in Health Applications,"Traditionally, legacy Health applications use software architecture models that make it difficult to reuse components. Reusability is an essential attribute in the software lifecycle, as it improves the quality of applications and reduces maintenance and development costs. This paper proposes the cloud tool Microservice4EHR, which dynamically generates reusable components from existing software artifacts (e.g., graphical interfaces), while conforming to the standards used in the healthcare domain. A software architecture based on Connectors and Microservice components is specified and made tangible by means of three algorithms. The use of both components is applied to a real-world scenario (a Brazilian blood donation center) and serves as an example. As a result, it is possible to notice that Health applications achieve greater reusability when they employ the microservice architecture. Thus, Microservice4EHR enables the use of reusable components in Health application architectures (for both new and legacy systems), increasing software reusability in this context.",IEEE conference,no,"['increasing', 'reusability', 'health', 'traditionally', 'legacy', 'health', 'use', 'model', 'make', 'difficult', 'reuse', 'reusability', 'essential', 'attribute', 'lifecycle', 'improves', 'quality', 'reduces', 'maintenance', 'development', 'cost', 'paper', 'proposes', 'tool', 'dynamically', 'generates', 'reusable', 'existing', 'artifact', 'interface', 'conforming', 'standard', 'used', 'healthcare', 'domain', 'based', 'specified', 'made', 'tangible', 'mean', 'three', 'algorithm', 'use', 'applied', 'scenario', 'blood', 'center', 'serf', 'example', 'result', 'possible', 'health', 'achieve', 'greater', 'reusability', 'employ', 'thus', 'enables', 'use', 'reusable', 'health', 'new', 'legacy', 'increasing', 'reusability', 'context']"
"Legacy applications model integration to support scientific experiment The article describes the implementation of the integration model of legacy applications to support a scientific computer experiment. The model is based on the microservices architecture using the Docker containerization technology. The information system was developed on the basis of the proposed integration model. It integrates legacy applications through a unified interface that works on HTTP protocol and allows you to remotely manage complex, time-consuming and resource-based computer experiments remotely using a mobile application. The article also describes infrastructure services, in particular, a load forecasting service that automatically adds processing power in the case of a large number of requests from users, which makes the developed system stable and increases its fault tolerance.",Legacy applications model integration to support scientific experiment,"The article describes the implementation of the integration model of legacy applications to support a scientific computer experiment. The model is based on the microservices architecture using the Docker containerization technology. The information system was developed on the basis of the proposed integration model. It integrates legacy applications through a unified interface that works on HTTP protocol and allows you to remotely manage complex, time-consuming and resource-based computer experiments remotely using a mobile application. The article also describes infrastructure services, in particular, a load forecasting service that automatically adds processing power in the case of a large number of requests from users, which makes the developed system stable and increases its fault tolerance.",IEEE conference,no,"['legacy', 'model', 'integration', 'support', 'scientific', 'experiment', 'article', 'describes', 'implementation', 'integration', 'model', 'legacy', 'support', 'scientific', 'computer', 'experiment', 'model', 'based', 'using', 'docker', 'containerization', 'technology', 'information', 'developed', 'basis', 'proposed', 'integration', 'model', 'integrates', 'legacy', 'unified', 'interface', 'work', 'http', 'protocol', 'allows', 'remotely', 'manage', 'complex', 'computer', 'experiment', 'remotely', 'using', 'mobile', 'article', 'also', 'describes', 'infrastructure', 'particular', 'load', 'forecasting', 'automatically', 'add', 'processing', 'power', 'case', 'large', 'number', 'request', 'user', 'make', 'developed', 'stable', 'increase', 'fault', 'tolerance']"
"COCOS: A Scalable Architecture for Containerized Heterogeneous Systems Nowadays software systems are organized around several and heterogeneous components. For example, a modern application can be composed of different microservices, along with dedicated components for machine learning analytics and recurring batch processing jobs. While containers offer a means to deploy the system and tackle heterogeneity, these components have different execution models, can exploit different resource types (e.g., CPUs and GPUs) and result in completely different execution times (milliseconds vs hours). This complexity calls for a new, scalable architecture to allow the systems to operate efficiently. This paper presents COCOS, an architecture, based on containers and control-theory, that is able to manage large and heterogeneous software systems. The architecture is based on a three-level hierarchy of controllers that cooperatively enforce user-defined requirements on execution times and consumed resources. The paper also shows a prototype implementation of COCOS based on Kubernetes, a well-known container orchestrator. The evaluation shows the efficiency of COCOS when dealing with microservices, Spark jobs and machine learning applications.",COCOS: A Scalable Architecture for Containerized Heterogeneous Systems,"Nowadays software systems are organized around several and heterogeneous components. For example, a modern application can be composed of different microservices, along with dedicated components for machine learning analytics and recurring batch processing jobs. While containers offer a means to deploy the system and tackle heterogeneity, these components have different execution models, can exploit different resource types (e.g., CPUs and GPUs) and result in completely different execution times (milliseconds vs hours). This complexity calls for a new, scalable architecture to allow the systems to operate efficiently. This paper presents COCOS, an architecture, based on containers and control-theory, that is able to manage large and heterogeneous software systems. The architecture is based on a three-level hierarchy of controllers that cooperatively enforce user-defined requirements on execution times and consumed resources. The paper also shows a prototype implementation of COCOS based on Kubernetes, a well-known container orchestrator. The evaluation shows the efficiency of COCOS when dealing with microservices, Spark jobs and machine learning applications.",IEEE conference,no,"['coco', 'scalable', 'containerized', 'heterogeneous', 'nowadays', 'organized', 'around', 'several', 'heterogeneous', 'example', 'modern', 'composed', 'different', 'along', 'dedicated', 'machine', 'learning', 'analytics', 'recurring', 'batch', 'processing', 'job', 'container', 'offer', 'mean', 'deploy', 'tackle', 'heterogeneity', 'different', 'execution', 'model', 'exploit', 'different', 'resource', 'type', 'cpu', 'result', 'completely', 'different', 'execution', 'time', 'v', 'hour', 'complexity', 'call', 'new', 'scalable', 'allow', 'operate', 'efficiently', 'paper', 'present', 'coco', 'based', 'container', 'able', 'manage', 'large', 'heterogeneous', 'based', 'hierarchy', 'controller', 'requirement', 'execution', 'time', 'consumed', 'resource', 'paper', 'also', 'show', 'prototype', 'implementation', 'coco', 'based', 'kubernetes', 'container', 'orchestrator', 'evaluation', 'show', 'efficiency', 'coco', 'dealing', 'spark', 'job', 'machine', 'learning']"
"Moving Toward the Obsolescence of Obsolescence: A Walk in the Clouds Many software methodologies advanced by cloud computing can be applied to automatic test software to mitigate test system obsolescence challenges. Classical monolithic test, measurement, and automation systems have traditional challenges when test system components become out-of-date. Distributed computing technologies provide incremental and modular updates that can be applied proactively or reactively to handle equipment failures, shifts in hardware (HW) and software (SW) dependencies, and improved HW and SW components. Advances in cloud computing have driven software technologies that can unlock scaling across hardware systems, introduce service architectures, open new performance possibilities, and decouple historically interlocked components. This paper will address aerospace and defense software challenges, introduce cloud computing, both service-oriented architecture (SOA) and cloud-native microservices, explain underlying principles and tenets, and craft a practical path forward. The cloud will be brought down to earth by showing how some underlying principles and tenets can be used today. These include the single responsibility principle (SRP), DevOps, and gRPC, “a modern open-source high-performance Remote Procedure Call (RPC) framework that can run in any environment.” [1] These practices enable architectures that make obsolescence issues smaller and more approachable. For instance, test hardware and software evolve as available hardware, and the development team's skillsets change over the lifetime of the tester and Unit Under Test (UUT). Using SRP allows individual engineers to troubleshoot isolated portions of complex systems without understanding the entire technology stack. Additionally, partitioning measurement tasks into measurement and analysis subtasks can maximize CPU power by offloading analysis from point-of-use test stations to higher-performance computers. This enables cost savings by removing number crunching from expensive testers and leveraging optimized high-performance computing for analysis tasks. We also show how gRPC natively incorporates low latency, security, and cross-platform interoperability between services. While it is impossible to prevent software, parts, and skills from becoming obsolete, it is possible to mitigate the risks to production through planning, procurement, and system design, including system software design. By walking in the footsteps of cloud computing, we can apply various techniques and technologies from the cloud to automated systems.",Moving Toward the Obsolescence of Obsolescence: A Walk in the Clouds,"Many software methodologies advanced by cloud computing can be applied to automatic test software to mitigate test system obsolescence challenges. Classical monolithic test, measurement, and automation systems have traditional challenges when test system components become out-of-date. Distributed computing technologies provide incremental and modular updates that can be applied proactively or reactively to handle equipment failures, shifts in hardware (HW) and software (SW) dependencies, and improved HW and SW components. Advances in cloud computing have driven software technologies that can unlock scaling across hardware systems, introduce service architectures, open new performance possibilities, and decouple historically interlocked components. This paper will address aerospace and defense software challenges, introduce cloud computing, both service-oriented architecture (SOA) and cloud-native microservices, explain underlying principles and tenets, and craft a practical path forward. The cloud will be brought down to earth by showing how some underlying principles and tenets can be used today. These include the single responsibility principle (SRP), DevOps, and gRPC, “a modern open-source high-performance Remote Procedure Call (RPC) framework that can run in any environment.” [1] These practices enable architectures that make obsolescence issues smaller and more approachable. For instance, test hardware and software evolve as available hardware, and the development team's skillsets change over the lifetime of the tester and Unit Under Test (UUT). Using SRP allows individual engineers to troubleshoot isolated portions of complex systems without understanding the entire technology stack. Additionally, partitioning measurement tasks into measurement and analysis subtasks can maximize CPU power by offloading analysis from point-of-use test stations to higher-performance computers. This enables cost savings by removing number crunching from expensive testers and leveraging optimized high-performance computing for analysis tasks. We also show how gRPC natively incorporates low latency, security, and cross-platform interoperability between services. While it is impossible to prevent software, parts, and skills from becoming obsolete, it is possible to mitigate the risks to production through planning, procurement, and system design, including system software design. By walking in the footsteps of cloud computing, we can apply various techniques and technologies from the cloud to automated systems.",IEEE conference,no,"['moving', 'toward', 'obsolescence', 'obsolescence', 'many', 'methodology', 'advanced', 'computing', 'applied', 'automatic', 'test', 'mitigate', 'test', 'obsolescence', 'challenge', 'classical', 'monolithic', 'test', 'measurement', 'automation', 'traditional', 'challenge', 'test', 'become', 'distributed', 'computing', 'technology', 'provide', 'modular', 'update', 'applied', 'handle', 'equipment', 'failure', 'shift', 'hardware', 'dependency', 'improved', 'advance', 'computing', 'driven', 'technology', 'scaling', 'across', 'hardware', 'introduce', 'open', 'new', 'performance', 'possibility', 'decouple', 'paper', 'address', 'aerospace', 'defense', 'challenge', 'introduce', 'computing', 'soa', 'explain', 'underlying', 'principle', 'tenet', 'practical', 'path', 'forward', 'brought', 'showing', 'underlying', 'principle', 'tenet', 'used', 'today', 'include', 'single', 'responsibility', 'principle', 'devops', 'grpc', 'modern', 'remote', 'procedure', 'call', 'rpc', 'framework', 'run', 'practice', 'enable', 'make', 'obsolescence', 'issue', 'smaller', 'instance', 'test', 'hardware', 'evolve', 'available', 'hardware', 'development', 'team', 'change', 'lifetime', 'unit', 'test', 'using', 'allows', 'individual', 'engineer', 'isolated', 'portion', 'complex', 'without', 'understanding', 'entire', 'technology', 'stack', 'additionally', 'partitioning', 'measurement', 'task', 'measurement', 'analysis', 'maximize', 'cpu', 'power', 'offloading', 'analysis', 'test', 'station', 'computer', 'enables', 'cost', 'saving', 'number', 'expensive', 'leveraging', 'optimized', 'computing', 'analysis', 'task', 'also', 'show', 'grpc', 'low', 'latency', 'security', 'interoperability', 'impossible', 'prevent', 'part', 'skill', 'becoming', 'possible', 'mitigate', 'risk', 'production', 'planning', 'design', 'including', 'design', 'computing', 'apply', 'various', 'technique', 'technology', 'automated']"
"Placing Multi-Component Applications in the Multi-Access Edge Computing Multi-access Edge Computing resolves the problem of the transportation of enormous amounts of data via the Internet by localizing the computation and the storage of data near the user. The offloading of applications to the MEC Servers must be done in a way that minimizes the latency while considering their limited resources. Additionally, there is a trend for applications to be separated into microservices that run on different MEC Servers. This adds complexity to the placement of applications on the MEC Servers because it has to take into consideration the dependencies of its microservices. This problem is defined in the literature as the multi-component application placement problem. This paper proposes an algorithm to tackle this problem efficiently, while the dependencies of services are linear, i.e., a chain of microservices. The algorithm starts with the allocation of the 1-medlan of the backhaul network. Then, the proposed algorithm extends outwards and moves around until the score function stops improving. In order to evaluate the proposed algorithm a Mixed-integer Quadratic Program is formulated. The results showed that the proposed algorithm achieved great performance in various cases.",Placing Multi-Component Applications in the Multi-Access Edge Computing,"Multi-access Edge Computing resolves the problem of the transportation of enormous amounts of data via the Internet by localizing the computation and the storage of data near the user. The offloading of applications to the MEC Servers must be done in a way that minimizes the latency while considering their limited resources. Additionally, there is a trend for applications to be separated into microservices that run on different MEC Servers. This adds complexity to the placement of applications on the MEC Servers because it has to take into consideration the dependencies of its microservices. This problem is defined in the literature as the multi-component application placement problem. This paper proposes an algorithm to tackle this problem efficiently, while the dependencies of services are linear, i.e., a chain of microservices. The algorithm starts with the allocation of the 1-medlan of the backhaul network. Then, the proposed algorithm extends outwards and moves around until the score function stops improving. In order to evaluate the proposed algorithm a Mixed-integer Quadratic Program is formulated. The results showed that the proposed algorithm achieved great performance in various cases.",IEEE conference,no,"['placing', 'edge', 'computing', 'edge', 'computing', 'resolve', 'problem', 'transportation', 'amount', 'via', 'internet', 'computation', 'storage', 'near', 'user', 'offloading', 'mec', 'server', 'must', 'done', 'way', 'minimizes', 'latency', 'considering', 'limited', 'resource', 'additionally', 'trend', 'run', 'different', 'mec', 'server', 'add', 'complexity', 'placement', 'mec', 'server', 'take', 'consideration', 'dependency', 'problem', 'defined', 'literature', 'placement', 'problem', 'paper', 'proposes', 'algorithm', 'tackle', 'problem', 'efficiently', 'dependency', 'linear', 'chain', 'algorithm', 'start', 'allocation', 'network', 'proposed', 'algorithm', 'move', 'around', 'score', 'function', 'stop', 'improving', 'order', 'evaluate', 'proposed', 'algorithm', 'program', 'formulated', 'result', 'showed', 'proposed', 'algorithm', 'achieved', 'great', 'performance', 'various', 'case']"
"A Microservice-based Approach to Facilitate Multi-Services in Smart Space Smart space refers to the integration of compute resources, information equipment and multi-modal sensing devices in the work or living space so that diverse services can be shared between different users in a smart way. Such a form of pervasive computing can not only make compute resources and information services universally available in such a way suitable for people's special uses but also bridge the digital information space and the physical world. However, given the properties of the smart space-each service with different resource requirements and goals, the smart space could cause resource conflicts in uses between different services and participants, and thus lead to the resource contention. In order to solve this problem while fulfilling the requirements of variable services in the smart space, in this paper, we leverage the concept of microservice to propose a universal resource scheduling and a process-evolution method for multiple concurrent services in the smart space. At the same time, to ensure that the microservice-based method is easy to implement, we also design a corresponding scheduling architecture based on the docker engine, which can functionally decompose application into a set of collaborative services. Finally, we carry out a experiment to show that the proposed architecture can achieve reasonable resource allocation, resolve resource contention, and exhibit the virtue of closed loop within different scenes and cross-scenario linkages.",A Microservice-based Approach to Facilitate Multi-Services in Smart Space,"Smart space refers to the integration of compute resources, information equipment and multi-modal sensing devices in the work or living space so that diverse services can be shared between different users in a smart way. Such a form of pervasive computing can not only make compute resources and information services universally available in such a way suitable for people's special uses but also bridge the digital information space and the physical world. However, given the properties of the smart space-each service with different resource requirements and goals, the smart space could cause resource conflicts in uses between different services and participants, and thus lead to the resource contention. In order to solve this problem while fulfilling the requirements of variable services in the smart space, in this paper, we leverage the concept of microservice to propose a universal resource scheduling and a process-evolution method for multiple concurrent services in the smart space. At the same time, to ensure that the microservice-based method is easy to implement, we also design a corresponding scheduling architecture based on the docker engine, which can functionally decompose application into a set of collaborative services. Finally, we carry out a experiment to show that the proposed architecture can achieve reasonable resource allocation, resolve resource contention, and exhibit the virtue of closed loop within different scenes and cross-scenario linkages.",IEEE conference,no,"['facilitate', 'smart', 'space', 'smart', 'space', 'refers', 'integration', 'compute', 'resource', 'information', 'equipment', 'sensing', 'device', 'work', 'living', 'space', 'diverse', 'shared', 'different', 'user', 'smart', 'way', 'form', 'pervasive', 'computing', 'make', 'compute', 'resource', 'information', 'available', 'way', 'suitable', 'people', 'special', 'us', 'also', 'bridge', 'digital', 'information', 'space', 'physical', 'world', 'however', 'given', 'property', 'smart', 'different', 'resource', 'requirement', 'goal', 'smart', 'space', 'could', 'cause', 'resource', 'us', 'different', 'participant', 'thus', 'lead', 'resource', 'contention', 'order', 'solve', 'problem', 'fulfilling', 'requirement', 'variable', 'smart', 'space', 'paper', 'leverage', 'concept', 'propose', 'universal', 'resource', 'scheduling', 'method', 'multiple', 'concurrent', 'smart', 'space', 'time', 'ensure', 'method', 'easy', 'implement', 'also', 'design', 'corresponding', 'scheduling', 'based', 'docker', 'engine', 'decompose', 'set', 'collaborative', 'finally', 'carry', 'experiment', 'show', 'proposed', 'achieve', 'reasonable', 'resource', 'allocation', 'resolve', 'resource', 'contention', 'exhibit', 'loop', 'within', 'different']"
"Evaluation of an SDN-based Microservice Architecture Microservice architectures decompose applications into individual components for enhanced maintainability and horizontal scaling, but also comes with an increased cost for orchestrating the services. Software-Defined Networks (SDNs) enables the dynamic configuration of network switches using controllers. In this paper we propose a microservice architecture that leverages SDN to orchestrate the microservices with the goal of reducing the orchestration latency cost. We perform a set of experiments using Mininet in which we implement a tailor-made microservice application that uses SDN for orchestration in combination with a set of different controllers and load balancers. Our results show that our proposed architecture performs in the same order of magnitude as a corresponding monolithic system.",Evaluation of an SDN-based Microservice Architecture,"Microservice architectures decompose applications into individual components for enhanced maintainability and horizontal scaling, but also comes with an increased cost for orchestrating the services. Software-Defined Networks (SDNs) enables the dynamic configuration of network switches using controllers. In this paper we propose a microservice architecture that leverages SDN to orchestrate the microservices with the goal of reducing the orchestration latency cost. We perform a set of experiments using Mininet in which we implement a tailor-made microservice application that uses SDN for orchestration in combination with a set of different controllers and load balancers. Our results show that our proposed architecture performs in the same order of magnitude as a corresponding monolithic system.",IEEE conference,no,"['evaluation', 'decompose', 'individual', 'enhanced', 'maintainability', 'horizontal', 'scaling', 'also', 'come', 'increased', 'cost', 'orchestrating', 'network', 'enables', 'dynamic', 'configuration', 'network', 'using', 'controller', 'paper', 'propose', 'leverage', 'sdn', 'orchestrate', 'goal', 'reducing', 'orchestration', 'latency', 'cost', 'perform', 'set', 'experiment', 'using', 'implement', 'us', 'sdn', 'orchestration', 'combination', 'set', 'different', 'controller', 'load', 'balancer', 'result', 'show', 'proposed', 'performs', 'order', 'magnitude', 'corresponding', 'monolithic']"
"Edge-Based Microservices Architecture for Internet of Things: Mobility Analysis Case Study In this paper, we describe how the microservices paradigm can be used to design and implement distributed edge services for Internet of Things applications. As a case study, traditionally monolithic user mobility analysis service is developed, with distributed and extendable microservices, for the standardized ETSI MEC system reference architecture. In each of the edge system three tiers, microservices implement the service logic with components for movement trace analysis, movement prediction and visualization of the results. The distributed service is implemented with Docker containers and evaluated on real-world settings with low capacity edge servers and real user mobility data. The results show that the edge promise of low latency can be met in such as implementation. The integration of a software development technology with a standardized edge system provides solid background for further development.",Edge-Based Microservices Architecture for Internet of Things: Mobility Analysis Case Study,"In this paper, we describe how the microservices paradigm can be used to design and implement distributed edge services for Internet of Things applications. As a case study, traditionally monolithic user mobility analysis service is developed, with distributed and extendable microservices, for the standardized ETSI MEC system reference architecture. In each of the edge system three tiers, microservices implement the service logic with components for movement trace analysis, movement prediction and visualization of the results. The distributed service is implemented with Docker containers and evaluated on real-world settings with low capacity edge servers and real user mobility data. The results show that the edge promise of low latency can be met in such as implementation. The integration of a software development technology with a standardized edge system provides solid background for further development.",IEEE conference,no,"['internet', 'thing', 'mobility', 'analysis', 'case', 'study', 'paper', 'describe', 'paradigm', 'used', 'design', 'implement', 'distributed', 'edge', 'internet', 'thing', 'case', 'study', 'traditionally', 'monolithic', 'user', 'mobility', 'analysis', 'developed', 'distributed', 'extendable', 'standardized', 'etsi', 'mec', 'reference', 'edge', 'three', 'implement', 'logic', 'movement', 'trace', 'analysis', 'movement', 'prediction', 'visualization', 'result', 'distributed', 'implemented', 'docker', 'container', 'evaluated', 'setting', 'low', 'capacity', 'edge', 'server', 'real', 'user', 'mobility', 'result', 'show', 'edge', 'promise', 'low', 'latency', 'met', 'implementation', 'integration', 'development', 'technology', 'standardized', 'edge', 'provides', 'solid', 'background', 'development']"
"Derm: SLA-aware Resource Management for Highly Dynamic Microservices Ensuring efficient resource allocation while providing service level agreement (SLA) guarantees for end-to-end (E2E) latency is crucial for microservice applications. Although existing studies have made significant contributions towards achieving this objective, they primarily concentrate on static graphs. However, microservice graphs are inherently dynamic during runtime in production environments, necessitating more effective and scalable resource management solutions.In this paper, we present Derm, a new resource management system designed for microservice applications with highly dynamic graphs. Our principal finding is that prioritizing different microservice graphs can lead to a substantial reduction in resource allocation. To take advantage of this opportunity, we develop three main components. The first is a performance model that describes uncertainties of microservice latency through a conditional exponential distribution. The second is a probabilistic quantification of the dynamics of microservice graphs. The third is an optimization method for adjusting the resource allocation of microservices to minimize resource usage. We evaluate Derm in our cluster using real microservice benchmarks and production traces. The results highlight that Derm reduces the resource usage by $68.4 \%$ and lowers SLA violation probability by $6.7 \times$, compared to existing approaches.",Derm: SLA-aware Resource Management for Highly Dynamic Microservices,"Ensuring efficient resource allocation while providing service level agreement (SLA) guarantees for end-to-end (E2E) latency is crucial for microservice applications. Although existing studies have made significant contributions towards achieving this objective, they primarily concentrate on static graphs. However, microservice graphs are inherently dynamic during runtime in production environments, necessitating more effective and scalable resource management solutions.In this paper, we present Derm, a new resource management system designed for microservice applications with highly dynamic graphs. Our principal finding is that prioritizing different microservice graphs can lead to a substantial reduction in resource allocation. To take advantage of this opportunity, we develop three main components. The first is a performance model that describes uncertainties of microservice latency through a conditional exponential distribution. The second is a probabilistic quantification of the dynamics of microservice graphs. The third is an optimization method for adjusting the resource allocation of microservices to minimize resource usage. We evaluate Derm in our cluster using real microservice benchmarks and production traces. The results highlight that Derm reduces the resource usage by $68.4 \%$ and lowers SLA violation probability by $6.7 \times$, compared to existing approaches.",IEEE conference,no,"['derm', 'resource', 'management', 'highly', 'dynamic', 'ensuring', 'efficient', 'resource', 'allocation', 'providing', 'level', 'agreement', 'sla', 'guarantee', 'latency', 'crucial', 'although', 'existing', 'study', 'made', 'significant', 'contribution', 'towards', 'achieving', 'objective', 'primarily', 'concentrate', 'static', 'graph', 'however', 'graph', 'inherently', 'dynamic', 'runtime', 'production', 'environment', 'necessitating', 'effective', 'scalable', 'resource', 'management', 'paper', 'present', 'derm', 'new', 'resource', 'management', 'designed', 'highly', 'dynamic', 'graph', 'principal', 'finding', 'different', 'graph', 'lead', 'substantial', 'reduction', 'resource', 'allocation', 'take', 'advantage', 'opportunity', 'develop', 'three', 'main', 'first', 'performance', 'model', 'describes', 'uncertainty', 'latency', 'distribution', 'second', 'probabilistic', 'dynamic', 'graph', 'third', 'optimization', 'method', 'resource', 'allocation', 'minimize', 'resource', 'usage', 'evaluate', 'derm', 'cluster', 'using', 'real', 'benchmark', 'production', 'trace', 'result', 'highlight', 'derm', 'reduces', 'resource', 'usage', 'lower', 'sla', 'violation', 'probability', 'compared', 'existing']"
"Smart Agent Edge Microservices Deployment Approach Smart agents are essential elements in an Internet of Things ecosystem. Smart agents are made up of sensors and actuators linked to a micro-controller. These smart agents are programmed through applications that are deployed at the microcontroller level. These programs can be modified or replaced or deleted. Also, the components of these smart agents may change, ie it may be that we add or remove a sensor and actuator. These smart agents must continue to operate without having to stop them and deploy new programs. In this case, we need tools that ensure deployment and continuous integration. Devops offers a continuous deployment chain. We offer an automation solution for the deployment and continuous integration of Edge microservice in microcontrollers. These systems are based on microservices. The microservices deployed are in the form of a Docker container. This solution allows the Internet of Things developer to install, modify, and delete containers at the microcontroller level and to control microcontrollers remotely while maintaining the continuity of operation of the microcontrollers.",Smart Agent Edge Microservices Deployment Approach,"Smart agents are essential elements in an Internet of Things ecosystem. Smart agents are made up of sensors and actuators linked to a micro-controller. These smart agents are programmed through applications that are deployed at the microcontroller level. These programs can be modified or replaced or deleted. Also, the components of these smart agents may change, ie it may be that we add or remove a sensor and actuator. These smart agents must continue to operate without having to stop them and deploy new programs. In this case, we need tools that ensure deployment and continuous integration. Devops offers a continuous deployment chain. We offer an automation solution for the deployment and continuous integration of Edge microservice in microcontrollers. These systems are based on microservices. The microservices deployed are in the form of a Docker container. This solution allows the Internet of Things developer to install, modify, and delete containers at the microcontroller level and to control microcontrollers remotely while maintaining the continuity of operation of the microcontrollers.",IEEE conference,no,"['smart', 'agent', 'edge', 'deployment', 'smart', 'agent', 'essential', 'element', 'internet', 'thing', 'ecosystem', 'smart', 'agent', 'made', 'sensor', 'actuator', 'linked', 'smart', 'agent', 'deployed', 'microcontroller', 'level', 'program', 'modified', 'replaced', 'also', 'smart', 'agent', 'may', 'change', 'may', 'add', 'sensor', 'actuator', 'smart', 'agent', 'must', 'continue', 'operate', 'without', 'stop', 'deploy', 'new', 'program', 'case', 'need', 'tool', 'ensure', 'deployment', 'continuous', 'integration', 'devops', 'offer', 'continuous', 'deployment', 'chain', 'offer', 'automation', 'solution', 'deployment', 'continuous', 'integration', 'edge', 'microcontrollers', 'based', 'deployed', 'form', 'docker', 'container', 'solution', 'allows', 'internet', 'thing', 'developer', 'modify', 'container', 'microcontroller', 'level', 'control', 'microcontrollers', 'remotely', 'maintaining', 'continuity', 'operation', 'microcontrollers']"
"Optimal Resource Provisioning for Data-intensive Microservices With the continuous progress of cloud computing, many microservices and complex multi-component applications arise for which resource planning is a great challenge. For example, when it comes to data-intensive cloud-native applications, the tenant might be eager to provision cloud resources in an economical manner while ensuring that the application performance meets the requirements in terms of data throughput. However, due to the complexity of the interplay between the building blocks, adequately setting resource limits of the components separately for various data rates is nearly impossible. In this paper, we propose a comprehensive approach that consists of measuring the resource footprint and data throughput performance of such a microservices-based application, analyzing the measurement results by data mining techniques, and finally formulating an optimization problem that aims to minimize the allocated resources given the performance constraints. We illustrate the benefits of the proposed approach on Cortex, an extension to Prometheus for storing monitored metrics data. The data-intensive nature of this illustrative example stems from real-time monitoring of metrics exposed by a multitude of applications running in a data center and the continuous analysis performed on the collected data that can be fetched from Cortex. We present Cortex’s performance vs resource footprint trade-off, and then we build regression models to predict the microservices’ resource consumption and draw a mathematical programming formulation to optimize the most important configuration parameters. Our most important finding is the linear relationship between resource consumption and application performance, which allows for applying linear regression and linear programming models. After the optimization, we compare our results to Cortex’s recommendation, leading to a CPU reservation reduced by 50-80%.",Optimal Resource Provisioning for Data-intensive Microservices,"With the continuous progress of cloud computing, many microservices and complex multi-component applications arise for which resource planning is a great challenge. For example, when it comes to data-intensive cloud-native applications, the tenant might be eager to provision cloud resources in an economical manner while ensuring that the application performance meets the requirements in terms of data throughput. However, due to the complexity of the interplay between the building blocks, adequately setting resource limits of the components separately for various data rates is nearly impossible. In this paper, we propose a comprehensive approach that consists of measuring the resource footprint and data throughput performance of such a microservices-based application, analyzing the measurement results by data mining techniques, and finally formulating an optimization problem that aims to minimize the allocated resources given the performance constraints. We illustrate the benefits of the proposed approach on Cortex, an extension to Prometheus for storing monitored metrics data. The data-intensive nature of this illustrative example stems from real-time monitoring of metrics exposed by a multitude of applications running in a data center and the continuous analysis performed on the collected data that can be fetched from Cortex. We present Cortex’s performance vs resource footprint trade-off, and then we build regression models to predict the microservices’ resource consumption and draw a mathematical programming formulation to optimize the most important configuration parameters. Our most important finding is the linear relationship between resource consumption and application performance, which allows for applying linear regression and linear programming models. After the optimization, we compare our results to Cortex’s recommendation, leading to a CPU reservation reduced by 50-80%.",IEEE conference,no,"['optimal', 'resource', 'provisioning', 'continuous', 'progress', 'computing', 'many', 'complex', 'arise', 'resource', 'planning', 'great', 'challenge', 'example', 'come', 'tenant', 'might', 'provision', 'resource', 'manner', 'ensuring', 'performance', 'meet', 'requirement', 'term', 'throughput', 'however', 'due', 'complexity', 'interplay', 'building', 'block', 'setting', 'resource', 'limit', 'separately', 'various', 'rate', 'nearly', 'impossible', 'paper', 'propose', 'comprehensive', 'consists', 'measuring', 'resource', 'footprint', 'throughput', 'performance', 'analyzing', 'measurement', 'result', 'mining', 'technique', 'finally', 'optimization', 'problem', 'aim', 'minimize', 'allocated', 'resource', 'given', 'performance', 'constraint', 'illustrate', 'benefit', 'proposed', 'cortex', 'extension', 'prometheus', 'storing', 'monitored', 'metric', 'nature', 'illustrative', 'example', 'monitoring', 'metric', 'exposed', 'running', 'center', 'continuous', 'analysis', 'performed', 'collected', 'cortex', 'present', 'cortex', 'performance', 'v', 'resource', 'footprint', 'build', 'regression', 'model', 'predict', 'resource', 'consumption', 'mathematical', 'programming', 'formulation', 'optimize', 'important', 'configuration', 'parameter', 'important', 'finding', 'linear', 'relationship', 'resource', 'consumption', 'performance', 'allows', 'applying', 'linear', 'regression', 'linear', 'programming', 'model', 'optimization', 'compare', 'result', 'cortex', 'recommendation', 'leading', 'cpu', 'reduced']"
"Flexible, scalable, and robust architecture for industrial automation applications with real-time requirements With the outbreak of the fourth industrial revolution, industrial automation applications demand a more flexible, scalable, and robust architecture than the one proposed in the ISA-95 standard. In addition, the high availability demanded by industrial processes hinders maintenance and updating tasks that go against information security policies. To address these challenges, some frameworks supported by virtualization and information distribution technologies have been generated for industrial automation systems. However, these proposals lack the means to implement real-time task scheduling algorithms, so it is difficult to migrate developments already implemented under these requirements. In this work, we present a microservices-based architecture that integrates virtualization technologies, a robust middleware for the communication layer, and a priority-based task scheduler. It also describes the implementation of this architecture using low-cost hardware components and free software, to be subsequently tested on a pressure control process in a laboratory setting. The obtained results confirm the effectiveness of the proposed architecture and meeting of real-time requirements, which enables supporting the diverse current industrial automation applications requirements, addressing the challenges of migration to Industry 4.0, and incorporating new functionalities.","Flexible, scalable, and robust architecture for industrial automation applications with real-time requirements","With the outbreak of the fourth industrial revolution, industrial automation applications demand a more flexible, scalable, and robust architecture than the one proposed in the ISA-95 standard. In addition, the high availability demanded by industrial processes hinders maintenance and updating tasks that go against information security policies. To address these challenges, some frameworks supported by virtualization and information distribution technologies have been generated for industrial automation systems. However, these proposals lack the means to implement real-time task scheduling algorithms, so it is difficult to migrate developments already implemented under these requirements. In this work, we present a microservices-based architecture that integrates virtualization technologies, a robust middleware for the communication layer, and a priority-based task scheduler. It also describes the implementation of this architecture using low-cost hardware components and free software, to be subsequently tested on a pressure control process in a laboratory setting. The obtained results confirm the effectiveness of the proposed architecture and meeting of real-time requirements, which enables supporting the diverse current industrial automation applications requirements, addressing the challenges of migration to Industry 4.0, and incorporating new functionalities.",IEEE conference,no,"['flexible', 'scalable', 'robust', 'industrial', 'automation', 'requirement', 'industrial', 'industrial', 'automation', 'demand', 'flexible', 'scalable', 'robust', 'one', 'proposed', 'standard', 'addition', 'high', 'availability', 'industrial', 'process', 'hinders', 'maintenance', 'updating', 'task', 'go', 'information', 'security', 'policy', 'address', 'challenge', 'framework', 'supported', 'virtualization', 'information', 'distribution', 'technology', 'generated', 'industrial', 'automation', 'however', 'proposal', 'lack', 'mean', 'implement', 'task', 'scheduling', 'algorithm', 'difficult', 'migrate', 'development', 'already', 'implemented', 'requirement', 'work', 'present', 'integrates', 'virtualization', 'technology', 'robust', 'middleware', 'communication', 'layer', 'task', 'scheduler', 'also', 'describes', 'implementation', 'using', 'hardware', 'free', 'subsequently', 'tested', 'control', 'process', 'setting', 'obtained', 'result', 'effectiveness', 'proposed', 'meeting', 'requirement', 'enables', 'supporting', 'diverse', 'current', 'industrial', 'automation', 'requirement', 'addressing', 'challenge', 'migration', 'industry', 'incorporating', 'new', 'functionality']"
"Design and Implementation of High-availability PaaS Platform Based on Virtualization Platform In view of the problems of the virtual platform failure migration mechanism under the cloud model and microservice architecture platform, the demand for redundant resources is large, and the failure recovery time is long. The key components such as the operating components, resource pools and middleware of the PaaS platform are analyzed, combined with virtualization The platform deployment application is practical. The PaaS platform high-availability architecture is designed to realize the PaaS platform high-availability deployment mode, which effectively improves the robustness of the PaaS platform and guarantees the business continuity of the application system running on the PaaS platform. This paper first analyzes the functional architecture and role positioning of the PaaS platform; secondly, it proposes high-availability design of operating components, resource pools and middleware, and at the same time maps the design to the physical deployment mode, the experimental results show that the design is feasible and effective.",Design and Implementation of High-availability PaaS Platform Based on Virtualization Platform,"In view of the problems of the virtual platform failure migration mechanism under the cloud model and microservice architecture platform, the demand for redundant resources is large, and the failure recovery time is long. The key components such as the operating components, resource pools and middleware of the PaaS platform are analyzed, combined with virtualization The platform deployment application is practical. The PaaS platform high-availability architecture is designed to realize the PaaS platform high-availability deployment mode, which effectively improves the robustness of the PaaS platform and guarantees the business continuity of the application system running on the PaaS platform. This paper first analyzes the functional architecture and role positioning of the PaaS platform; secondly, it proposes high-availability design of operating components, resource pools and middleware, and at the same time maps the design to the physical deployment mode, the experimental results show that the design is feasible and effective.",IEEE conference,no,"['design', 'implementation', 'paas', 'platform', 'based', 'virtualization', 'platform', 'view', 'problem', 'virtual', 'platform', 'failure', 'migration', 'mechanism', 'model', 'platform', 'demand', 'redundant', 'resource', 'large', 'failure', 'recovery', 'time', 'long', 'key', 'operating', 'resource', 'pool', 'middleware', 'paas', 'platform', 'analyzed', 'combined', 'virtualization', 'platform', 'deployment', 'practical', 'paas', 'platform', 'designed', 'realize', 'paas', 'platform', 'deployment', 'mode', 'effectively', 'improves', 'robustness', 'paas', 'platform', 'guarantee', 'business', 'continuity', 'running', 'paas', 'platform', 'paper', 'first', 'analyzes', 'functional', 'role', 'paas', 'platform', 'secondly', 'proposes', 'design', 'operating', 'resource', 'pool', 'middleware', 'time', 'map', 'design', 'physical', 'deployment', 'mode', 'experimental', 'result', 'show', 'design', 'feasible', 'effective']"
"TDD4Fog: A Test-Driven Software Development Platform for Fog Computing Systems As an ideal infrastructure for smart services, Fog Computing is becoming the next wave of IT investment harnessing the successful models of Cloud Computing and latest technologies such as 5G and Internet of Things (IoT). However, the development of Fog Computing systems is a big challenge due to its complex, heterogeneous and distributed nature. Currently, there are a few SDKs released by some public Cloud service providers to support the development of Fog services in a top-down fashion as the key motive is to leverage their business Cloud services. However, Fog Computing systems are usually designed in a bottom-up fashion as the major functionalities are centred around the Edge Nodes and the End Devices. Meanwhile, significant efforts are required to verify the conformance of software behaviours as the collaboration between the End Devices, Edge Nodes and Cloud Servers is vital to the success of a Fog Computing System. Therefore, a holistically designed software development platform is urgently required. In this paper, we propose TDD4Fog, a test-driven software development platform for Fog Computing systems. Following the Test-Driven Development (TDD) methodology and a bottom-up design fashion, TDD4Fog supports the microservice architecture and provides the Test-Driven utilities such as metamorphic testing, mutation testing and random testing for the whole software development lifecycle of Fog Computing systems. To demonstrate the feasibility of TDD4Fog, we have presented some preliminary results on the key components of TDD4Fog and discussed some important future research directions.",TDD4Fog: A Test-Driven Software Development Platform for Fog Computing Systems,"As an ideal infrastructure for smart services, Fog Computing is becoming the next wave of IT investment harnessing the successful models of Cloud Computing and latest technologies such as 5G and Internet of Things (IoT). However, the development of Fog Computing systems is a big challenge due to its complex, heterogeneous and distributed nature. Currently, there are a few SDKs released by some public Cloud service providers to support the development of Fog services in a top-down fashion as the key motive is to leverage their business Cloud services. However, Fog Computing systems are usually designed in a bottom-up fashion as the major functionalities are centred around the Edge Nodes and the End Devices. Meanwhile, significant efforts are required to verify the conformance of software behaviours as the collaboration between the End Devices, Edge Nodes and Cloud Servers is vital to the success of a Fog Computing System. Therefore, a holistically designed software development platform is urgently required. In this paper, we propose TDD4Fog, a test-driven software development platform for Fog Computing systems. Following the Test-Driven Development (TDD) methodology and a bottom-up design fashion, TDD4Fog supports the microservice architecture and provides the Test-Driven utilities such as metamorphic testing, mutation testing and random testing for the whole software development lifecycle of Fog Computing systems. To demonstrate the feasibility of TDD4Fog, we have presented some preliminary results on the key components of TDD4Fog and discussed some important future research directions.",IEEE conference,no,"['development', 'platform', 'fog', 'computing', 'infrastructure', 'smart', 'fog', 'computing', 'becoming', 'next', 'investment', 'harnessing', 'successful', 'model', 'computing', 'latest', 'technology', 'internet', 'thing', 'iot', 'however', 'development', 'fog', 'computing', 'big', 'challenge', 'due', 'complex', 'heterogeneous', 'distributed', 'nature', 'currently', 'released', 'public', 'provider', 'support', 'development', 'fog', 'fashion', 'key', 'leverage', 'business', 'however', 'fog', 'computing', 'usually', 'designed', 'fashion', 'major', 'functionality', 'around', 'edge', 'node', 'end', 'device', 'significant', 'effort', 'required', 'verify', 'conformance', 'behaviour', 'collaboration', 'end', 'device', 'edge', 'node', 'server', 'vital', 'success', 'fog', 'computing', 'therefore', 'designed', 'development', 'platform', 'required', 'paper', 'propose', 'development', 'platform', 'fog', 'computing', 'following', 'development', 'methodology', 'design', 'fashion', 'support', 'provides', 'testing', 'testing', 'random', 'testing', 'whole', 'development', 'lifecycle', 'fog', 'computing', 'demonstrate', 'feasibility', 'presented', 'preliminary', 'result', 'key', 'discussed', 'important', 'future', 'research', 'direction']"
"A Deployment Management of High-Availability Microservices for Edge Computing In recent years, due to the emergence of Edge Computing architecture, the microservice computing originally located in the cloud has been extended to the edge to reduce network latency. At the same time, the data transmission bandwidth is dispersed on the edge, reducing the usage of network bandwidth, and thus shortening the response time of the service. In addition, the way which the cloud manages the edge is called cloud-edge collaboration. This is the unified management of microservice operations at the edge by the cloud. And, cloud-edge collaboration can also reduce maintenance costs. KubeEdge is an open source cloud-edge collaboration architecture by using the cloud to manage multiple edge microservices. However, if there are multiple microservices that need to be continuously deployed and updated, a more complete solution is still needed in microservice management. Hence, this study proposes an edge computing architecture that apply additional component to KubeEdge to deploy and manage containerized microservices automatically.",A Deployment Management of High-Availability Microservices for Edge Computing,"In recent years, due to the emergence of Edge Computing architecture, the microservice computing originally located in the cloud has been extended to the edge to reduce network latency. At the same time, the data transmission bandwidth is dispersed on the edge, reducing the usage of network bandwidth, and thus shortening the response time of the service. In addition, the way which the cloud manages the edge is called cloud-edge collaboration. This is the unified management of microservice operations at the edge by the cloud. And, cloud-edge collaboration can also reduce maintenance costs. KubeEdge is an open source cloud-edge collaboration architecture by using the cloud to manage multiple edge microservices. However, if there are multiple microservices that need to be continuously deployed and updated, a more complete solution is still needed in microservice management. Hence, this study proposes an edge computing architecture that apply additional component to KubeEdge to deploy and manage containerized microservices automatically.",IEEE conference,no,"['deployment', 'management', 'edge', 'computing', 'recent', 'year', 'due', 'emergence', 'edge', 'computing', 'computing', 'located', 'extended', 'edge', 'reduce', 'network', 'latency', 'time', 'transmission', 'bandwidth', 'edge', 'reducing', 'usage', 'network', 'bandwidth', 'thus', 'response', 'time', 'addition', 'way', 'manages', 'edge', 'called', 'collaboration', 'unified', 'management', 'operation', 'edge', 'collaboration', 'also', 'reduce', 'maintenance', 'cost', 'kubeedge', 'open', 'source', 'collaboration', 'using', 'manage', 'multiple', 'edge', 'however', 'multiple', 'need', 'continuously', 'deployed', 'updated', 'complete', 'solution', 'still', 'needed', 'management', 'hence', 'study', 'proposes', 'edge', 'computing', 'apply', 'additional', 'kubeedge', 'deploy', 'manage', 'containerized', 'automatically']"
"A microservices-based iterative development approach for usable, reliable and explainable A.I.-infused medical applications using R.U.P The Rational Unified Process (R.U.P.) is an iterative Software Engineering Process, that ensures alignment between engineers and stakeholders through optimized and detailed partinionalised steps within predefined constraints [1]. The independently deployable services that are components of distributed systems are called microservices. They are part of applications and are easier to manage and scale. Each microservice serve different purpose and has a unique responsibility making it easier to understand manage and collaborate on. Medical applications are created to assist in treatment, disease prevention and health optimization. However patients' needs and abilities vary and patients' requirements and definitions on the usability aspect of an application are different and should be acknowledged for the application to be successful and for the patients/users to benefit from it. In this study the development of an A.I.-infused medical application, is outlined through the R.U.P. methodology and built using microservices, where the patients' needs and requirements are at the center of continuous development process focused on improvements.","A microservices-based iterative development approach for usable, reliable and explainable A.I.-infused medical applications using R.U.P","The Rational Unified Process (R.U.P.) is an iterative Software Engineering Process, that ensures alignment between engineers and stakeholders through optimized and detailed partinionalised steps within predefined constraints [1]. The independently deployable services that are components of distributed systems are called microservices. They are part of applications and are easier to manage and scale. Each microservice serve different purpose and has a unique responsibility making it easier to understand manage and collaborate on. Medical applications are created to assist in treatment, disease prevention and health optimization. However patients' needs and abilities vary and patients' requirements and definitions on the usability aspect of an application are different and should be acknowledged for the application to be successful and for the patients/users to benefit from it. In this study the development of an A.I.-infused medical application, is outlined through the R.U.P. methodology and built using microservices, where the patients' needs and requirements are at the center of continuous development process focused on improvements.",IEEE conference,no,"['iterative', 'development', 'reliable', 'medical', 'using', 'unified', 'process', 'iterative', 'engineering', 'process', 'ensures', 'alignment', 'engineer', 'stakeholder', 'optimized', 'detailed', 'step', 'within', 'predefined', 'constraint', 'independently', 'deployable', 'distributed', 'called', 'part', 'easier', 'manage', 'scale', 'serve', 'different', 'purpose', 'unique', 'responsibility', 'making', 'easier', 'understand', 'manage', 'collaborate', 'medical', 'created', 'assist', 'treatment', 'disease', 'prevention', 'health', 'optimization', 'however', 'patient', 'need', 'ability', 'vary', 'patient', 'requirement', 'definition', 'usability', 'aspect', 'different', 'successful', 'benefit', 'study', 'development', 'medical', 'outlined', 'methodology', 'built', 'using', 'patient', 'need', 'requirement', 'center', 'continuous', 'development', 'process', 'focused', 'improvement']"
"CrowdSensing for smart mobility through a service-oriented architecture Crowdsensing is a powerful approach to build representations of specific aspects of reality which are of interest for citizens in smart cities, and in particular for people with special needs. In this work, we present an application of the microservice paradigm to create a mobility services platform. By exposing each part of the process as a microservice, we achieve the ability of developing applications as orchestration of available components. Moreover, we leverage the possibility of sharing data between different applications in a controlled environment.",CrowdSensing for smart mobility through a service-oriented architecture,"Crowdsensing is a powerful approach to build representations of specific aspects of reality which are of interest for citizens in smart cities, and in particular for people with special needs. In this work, we present an application of the microservice paradigm to create a mobility services platform. By exposing each part of the process as a microservice, we achieve the ability of developing applications as orchestration of available components. Moreover, we leverage the possibility of sharing data between different applications in a controlled environment.",IEEE conference,no,"['crowdsensing', 'smart', 'mobility', 'crowdsensing', 'powerful', 'build', 'representation', 'specific', 'aspect', 'reality', 'interest', 'citizen', 'smart', 'city', 'particular', 'people', 'special', 'need', 'work', 'present', 'paradigm', 'create', 'mobility', 'platform', 'exposing', 'part', 'process', 'achieve', 'ability', 'developing', 'orchestration', 'available', 'moreover', 'leverage', 'possibility', 'sharing', 'different', 'controlled', 'environment']"
"On Monolithic and Microservice Deployment of Network Functions Network Function Virtualization (NFV) has recently attracted telecom operators to migrate network functionalities from expensive bespoke hardware systems to virtualized IT infrastructures where they are deployed as software components. Scalability, up-gradation, fault tolerance and simplified testing are important challenges in the field of NFV. In order to overcome these challenges, there is significant interest from research communities to scale or decompose network functions using the monolithic and microservice approach. In this paper, we compare the performance of both approaches using an analytic model and implementing test-bed experiments. In addition, we calculate the number of instances of monoliths or microservices in which a network function could be scaled or decomposed in order to get the maximum or required performance. Single and multiple CPU core scenarios are considered. Experimentation is performed by using an open source network function, SNORT and running monoliths and microservices of SNORT as Docker containers on bare metal machines. The experimental results compare the performance of monolith and microservice approaches and are used to estimate the validity of the analytic model. The results also show the effectiveness of our approach in finding the number of instances (monoliths or microservices) required to maximize performance.",On Monolithic and Microservice Deployment of Network Functions,"Network Function Virtualization (NFV) has recently attracted telecom operators to migrate network functionalities from expensive bespoke hardware systems to virtualized IT infrastructures where they are deployed as software components. Scalability, up-gradation, fault tolerance and simplified testing are important challenges in the field of NFV. In order to overcome these challenges, there is significant interest from research communities to scale or decompose network functions using the monolithic and microservice approach. In this paper, we compare the performance of both approaches using an analytic model and implementing test-bed experiments. In addition, we calculate the number of instances of monoliths or microservices in which a network function could be scaled or decomposed in order to get the maximum or required performance. Single and multiple CPU core scenarios are considered. Experimentation is performed by using an open source network function, SNORT and running monoliths and microservices of SNORT as Docker containers on bare metal machines. The experimental results compare the performance of monolith and microservice approaches and are used to estimate the validity of the analytic model. The results also show the effectiveness of our approach in finding the number of instances (monoliths or microservices) required to maximize performance.",IEEE conference,no,"['monolithic', 'deployment', 'network', 'function', 'network', 'function', 'virtualization', 'nfv', 'recently', 'attracted', 'telecom', 'operator', 'migrate', 'network', 'functionality', 'expensive', 'hardware', 'virtualized', 'infrastructure', 'deployed', 'scalability', 'fault', 'tolerance', 'simplified', 'testing', 'important', 'challenge', 'field', 'nfv', 'order', 'overcome', 'challenge', 'significant', 'interest', 'research', 'community', 'scale', 'decompose', 'network', 'function', 'using', 'monolithic', 'paper', 'compare', 'performance', 'using', 'analytic', 'model', 'implementing', 'experiment', 'addition', 'calculate', 'number', 'instance', 'monolith', 'network', 'function', 'could', 'scaled', 'decomposed', 'order', 'get', 'maximum', 'required', 'performance', 'single', 'multiple', 'cpu', 'core', 'scenario', 'considered', 'performed', 'using', 'open', 'source', 'network', 'function', 'running', 'monolith', 'docker', 'container', 'bare', 'metal', 'machine', 'experimental', 'result', 'compare', 'performance', 'monolith', 'used', 'estimate', 'validity', 'analytic', 'model', 'result', 'also', 'show', 'effectiveness', 'finding', 'number', 'instance', 'monolith', 'required', 'maximize', 'performance']"
"A service orchestration architecture for Fog-enabled infrastructures The development of Fog Computing technology is crucial to address the challenges to come with the mass adoption of Internet Of Things technology, where the generation of data tends to grow at an unprecedented pace. The technology brings computing power to the surrounds of devices, to offer local processing, filtering, storage and analysis of data and control over actuators. Orchestration is a requirement of Fog Computing technology to deliver services, based on the composition of microservices. It must take into consideration the heterogeneity of the IoT environment and device's capabilities and constraints. This heterogeneity requires a different approach for orchestration, be it regarding infrastructure management, node selection and/or service placement. Orchestrations shall be manually or automatically started through event triggers. Also, the Orchestrator must be flexible enough to work in a centralized or distributed fashion. Orchestration is still a hot topic and can be seen in different areas, especially in the Service Oriented Architectures, hardware virtualization, in the Cloud, and in Network Virtualization Function. However, the architecture of these solutions is not enough to handle Fog Requirements, specially Fog's heterogeneity, and dynamics. In this paper, we propose an architecture for Orchestration for the Fog Computing environment. We developed a prototype to prof some concepts. We discuss in this paper the implementation, and the tools chose, and their roles. We end the paper with a discussion on performance indicators and future direction on the evaluation of non-functional aspects of the Architecture.",A service orchestration architecture for Fog-enabled infrastructures,"The development of Fog Computing technology is crucial to address the challenges to come with the mass adoption of Internet Of Things technology, where the generation of data tends to grow at an unprecedented pace. The technology brings computing power to the surrounds of devices, to offer local processing, filtering, storage and analysis of data and control over actuators. Orchestration is a requirement of Fog Computing technology to deliver services, based on the composition of microservices. It must take into consideration the heterogeneity of the IoT environment and device's capabilities and constraints. This heterogeneity requires a different approach for orchestration, be it regarding infrastructure management, node selection and/or service placement. Orchestrations shall be manually or automatically started through event triggers. Also, the Orchestrator must be flexible enough to work in a centralized or distributed fashion. Orchestration is still a hot topic and can be seen in different areas, especially in the Service Oriented Architectures, hardware virtualization, in the Cloud, and in Network Virtualization Function. However, the architecture of these solutions is not enough to handle Fog Requirements, specially Fog's heterogeneity, and dynamics. In this paper, we propose an architecture for Orchestration for the Fog Computing environment. We developed a prototype to prof some concepts. We discuss in this paper the implementation, and the tools chose, and their roles. We end the paper with a discussion on performance indicators and future direction on the evaluation of non-functional aspects of the Architecture.",IEEE conference,no,"['orchestration', 'infrastructure', 'development', 'fog', 'computing', 'technology', 'crucial', 'address', 'challenge', 'come', 'adoption', 'internet', 'thing', 'technology', 'generation', 'grow', 'unprecedented', 'pace', 'technology', 'brings', 'computing', 'power', 'device', 'offer', 'local', 'processing', 'storage', 'analysis', 'control', 'actuator', 'orchestration', 'requirement', 'fog', 'computing', 'technology', 'deliver', 'based', 'composition', 'must', 'take', 'consideration', 'heterogeneity', 'iot', 'environment', 'device', 'capability', 'constraint', 'heterogeneity', 'requires', 'different', 'orchestration', 'regarding', 'infrastructure', 'management', 'node', 'selection', 'placement', 'orchestration', 'shall', 'manually', 'automatically', 'started', 'event', 'also', 'orchestrator', 'must', 'flexible', 'enough', 'work', 'centralized', 'distributed', 'fashion', 'orchestration', 'still', 'topic', 'seen', 'different', 'area', 'especially', 'oriented', 'hardware', 'virtualization', 'network', 'virtualization', 'function', 'however', 'solution', 'enough', 'handle', 'fog', 'requirement', 'fog', 'heterogeneity', 'dynamic', 'paper', 'propose', 'orchestration', 'fog', 'computing', 'environment', 'developed', 'prototype', 'concept', 'discus', 'paper', 'implementation', 'tool', 'role', 'end', 'paper', 'discussion', 'performance', 'indicator', 'future', 'direction', 'evaluation', 'aspect']"
"Cache Replacement Algorithm Based on Dynamic Constraints in Microservice Platform Distributed cache is one of the most important components in cloud computing and microservice systems. Adding cache components to the microservice system can significantly improve the concurrency and throughput of the whole microservice framework, and effectively promote the overall performance of the microservice system. However, traditional caching algorithm can easily lead to the imbalance of cache resource allocation in microservice system, thus affecting the performance of the microservice system. We study the basic architecture of cache in microservice platform, and propose the mathematical model of distributed cache and the probability distribution model of data access in microservice platform. Next, based on the above mathematical model, we design a cache replacement algorithm based on dynamic constraints in microservice platform. The algorithm can effectively solve a series of problems of traditional algorithms in microservice platform, making the algorithm more suitable for the environment of microservice architecture. We evaluate and verify the algorithm on the simulation software platform. Experimental results show that the algorithm meets the performance requirements of high cache hit rate, high concurrency and high throughput in the microservice platform, and is significantly better than the traditional cache algorithms.",Cache Replacement Algorithm Based on Dynamic Constraints in Microservice Platform,"Distributed cache is one of the most important components in cloud computing and microservice systems. Adding cache components to the microservice system can significantly improve the concurrency and throughput of the whole microservice framework, and effectively promote the overall performance of the microservice system. However, traditional caching algorithm can easily lead to the imbalance of cache resource allocation in microservice system, thus affecting the performance of the microservice system. We study the basic architecture of cache in microservice platform, and propose the mathematical model of distributed cache and the probability distribution model of data access in microservice platform. Next, based on the above mathematical model, we design a cache replacement algorithm based on dynamic constraints in microservice platform. The algorithm can effectively solve a series of problems of traditional algorithms in microservice platform, making the algorithm more suitable for the environment of microservice architecture. We evaluate and verify the algorithm on the simulation software platform. Experimental results show that the algorithm meets the performance requirements of high cache hit rate, high concurrency and high throughput in the microservice platform, and is significantly better than the traditional cache algorithms.",IEEE conference,no,"['cache', 'algorithm', 'based', 'dynamic', 'constraint', 'platform', 'distributed', 'cache', 'one', 'important', 'computing', 'adding', 'cache', 'significantly', 'improve', 'concurrency', 'throughput', 'whole', 'framework', 'effectively', 'promote', 'overall', 'performance', 'however', 'traditional', 'caching', 'algorithm', 'easily', 'lead', 'imbalance', 'cache', 'resource', 'allocation', 'thus', 'affecting', 'performance', 'study', 'basic', 'cache', 'platform', 'propose', 'mathematical', 'model', 'distributed', 'cache', 'probability', 'distribution', 'model', 'access', 'platform', 'next', 'based', 'mathematical', 'model', 'design', 'cache', 'algorithm', 'based', 'dynamic', 'constraint', 'platform', 'algorithm', 'effectively', 'solve', 'series', 'problem', 'traditional', 'algorithm', 'platform', 'making', 'algorithm', 'suitable', 'environment', 'evaluate', 'verify', 'algorithm', 'simulation', 'platform', 'experimental', 'result', 'show', 'algorithm', 'meet', 'performance', 'requirement', 'high', 'cache', 'hit', 'rate', 'high', 'concurrency', 'high', 'throughput', 'platform', 'significantly', 'better', 'traditional', 'cache', 'algorithm']"
"Runtime Orchestration of Distributed Control System Services with TOSCA, Kubernetes, and GitOps Runtime orchestration of cloud-native microservice systems remains a challenge with many technology options, such as Kubernetes for managing containers and OASIS TOSCA, a standard for orchestrating cloud services. Researchers and practitioners have proposed different approaches on how the benefits of Kubernetes and TOSCA could be combined, but followed different design philosophies and did not reach consensus. An analysis of the different combinations is missing and existing TOSCA and Kubernetes combinations do not consider the integration of real-time embedded controllers without container workloads, nor a safeguarding of configuration changes, which can be achieved using the GitOps paradigm. In this experience report, we describe and analyze different options for combining TOSCA and Kubernetes. Following one of the options, we have enhanced an existing TOSCA orchestrator to demonstrate software updates across heterogeneous software services as well as a GitOps extension for TOSCA service models. We found that TOSCA workflows are well suited for updates spanning different technologies and that a custom GitOps operator is needed in case not all components are orchestrated by Kubernetes.","Runtime Orchestration of Distributed Control System Services with TOSCA, Kubernetes, and GitOps","Runtime orchestration of cloud-native microservice systems remains a challenge with many technology options, such as Kubernetes for managing containers and OASIS TOSCA, a standard for orchestrating cloud services. Researchers and practitioners have proposed different approaches on how the benefits of Kubernetes and TOSCA could be combined, but followed different design philosophies and did not reach consensus. An analysis of the different combinations is missing and existing TOSCA and Kubernetes combinations do not consider the integration of real-time embedded controllers without container workloads, nor a safeguarding of configuration changes, which can be achieved using the GitOps paradigm. In this experience report, we describe and analyze different options for combining TOSCA and Kubernetes. Following one of the options, we have enhanced an existing TOSCA orchestrator to demonstrate software updates across heterogeneous software services as well as a GitOps extension for TOSCA service models. We found that TOSCA workflows are well suited for updates spanning different technologies and that a custom GitOps operator is needed in case not all components are orchestrated by Kubernetes.",IEEE conference,no,"['runtime', 'orchestration', 'distributed', 'control', 'tosca', 'kubernetes', 'gitops', 'runtime', 'orchestration', 'remains', 'challenge', 'many', 'technology', 'option', 'kubernetes', 'managing', 'container', 'tosca', 'standard', 'orchestrating', 'researcher', 'practitioner', 'proposed', 'different', 'benefit', 'kubernetes', 'tosca', 'could', 'combined', 'followed', 'different', 'design', 'philosophy', 'reach', 'analysis', 'different', 'combination', 'missing', 'existing', 'tosca', 'kubernetes', 'combination', 'consider', 'integration', 'embedded', 'controller', 'without', 'container', 'workload', 'configuration', 'change', 'achieved', 'using', 'gitops', 'paradigm', 'experience', 'report', 'describe', 'analyze', 'different', 'option', 'combining', 'tosca', 'kubernetes', 'following', 'one', 'option', 'enhanced', 'existing', 'tosca', 'orchestrator', 'demonstrate', 'update', 'across', 'heterogeneous', 'well', 'gitops', 'extension', 'tosca', 'model', 'found', 'tosca', 'workflow', 'well', 'suited', 'update', 'different', 'technology', 'custom', 'gitops', 'operator', 'needed', 'case', 'orchestrated', 'kubernetes']"
"Latency-Optimal Network Microservice Architecture Deployment in SDN The Software-Defined Networking (SDN) paradigm enables network administrators to manage the behavior of the network thanks to a centralized control plane. By programming network-level applications, it is possible to determine how traffic flows must be handled programmatically. In the same manner that computing applications have evolved from monolithic to microservice-based architectures, network-level applications are expected to evolve into microservice-based SDN controllers, implementing each application as a replicable and individually deployable component of the SDN controller. In such a scenario, the Quality of Service (QoS) experienced by traffic flows depends on how these microservices are placed and deployed through the network topology. In this work, we provide a system to optimize the QoS of the traffic in microservice-based SDN networks by optimally placing and replicating the network-level microservices. Experimental results show the effectiveness of the proposed solution over a real network topology with varying traffic loads.",Latency-Optimal Network Microservice Architecture Deployment in SDN,"The Software-Defined Networking (SDN) paradigm enables network administrators to manage the behavior of the network thanks to a centralized control plane. By programming network-level applications, it is possible to determine how traffic flows must be handled programmatically. In the same manner that computing applications have evolved from monolithic to microservice-based architectures, network-level applications are expected to evolve into microservice-based SDN controllers, implementing each application as a replicable and individually deployable component of the SDN controller. In such a scenario, the Quality of Service (QoS) experienced by traffic flows depends on how these microservices are placed and deployed through the network topology. In this work, we provide a system to optimize the QoS of the traffic in microservice-based SDN networks by optimally placing and replicating the network-level microservices. Experimental results show the effectiveness of the proposed solution over a real network topology with varying traffic loads.",IEEE conference,no,"['network', 'deployment', 'sdn', 'networking', 'sdn', 'paradigm', 'enables', 'network', 'administrator', 'manage', 'behavior', 'network', 'thanks', 'centralized', 'control', 'plane', 'programming', 'possible', 'determine', 'traffic', 'flow', 'must', 'handled', 'manner', 'computing', 'evolved', 'monolithic', 'expected', 'evolve', 'sdn', 'controller', 'implementing', 'individually', 'deployable', 'sdn', 'controller', 'scenario', 'quality', 'qos', 'experienced', 'traffic', 'flow', 'depends', 'placed', 'deployed', 'network', 'topology', 'work', 'provide', 'optimize', 'qos', 'traffic', 'sdn', 'network', 'optimally', 'placing', 'replicating', 'experimental', 'result', 'show', 'effectiveness', 'proposed', 'solution', 'real', 'network', 'topology', 'varying', 'traffic', 'load']"
"Transition of Cloud Computing from Traditional Applications to the Cloud Native Approach A cloud-native application hosted in the cloud and are designed to capitalize on the inherent characteristics of a cloud computing software delivery model. Applications that are cloud-native use a microservice design, which effectively distributes resources to each service they require, allowing them to be flexible and adaptive to a cloud architecture. This paper discusses cloud native approach in detail and compares with the traditional approach to designing, building and deploying applications and highlights the need to switch to cloud native. The four major components of Cloud native architecture are microservices, devops, continuous delivery and containers. The various factors to consider during the design of a cloud native application and the development stack required to do so, are also discussed in detail. Tools such as the 12 factor application, API based design, and microservices, which are essential for cloud native applications, are examined. Some of the other challenges that come with cloud computing are resolved by cloud-native applications. Cloud native solutions, from an economic perspective, provide the full value of the cloud by enabling applications to scale and change more quickly than before. This scalability gives the company new chances to grow their revenue, become more efficient, or provide better customer service.",Transition of Cloud Computing from Traditional Applications to the Cloud Native Approach,"A cloud-native application hosted in the cloud and are designed to capitalize on the inherent characteristics of a cloud computing software delivery model. Applications that are cloud-native use a microservice design, which effectively distributes resources to each service they require, allowing them to be flexible and adaptive to a cloud architecture. This paper discusses cloud native approach in detail and compares with the traditional approach to designing, building and deploying applications and highlights the need to switch to cloud native. The four major components of Cloud native architecture are microservices, devops, continuous delivery and containers. The various factors to consider during the design of a cloud native application and the development stack required to do so, are also discussed in detail. Tools such as the 12 factor application, API based design, and microservices, which are essential for cloud native applications, are examined. Some of the other challenges that come with cloud computing are resolved by cloud-native applications. Cloud native solutions, from an economic perspective, provide the full value of the cloud by enabling applications to scale and change more quickly than before. This scalability gives the company new chances to grow their revenue, become more efficient, or provide better customer service.",IEEE conference,no,"['transition', 'computing', 'traditional', 'native', 'hosted', 'designed', 'inherent', 'characteristic', 'computing', 'delivery', 'model', 'use', 'design', 'effectively', 'resource', 'require', 'allowing', 'flexible', 'adaptive', 'paper', 'discusses', 'native', 'detail', 'compare', 'traditional', 'designing', 'building', 'deploying', 'highlight', 'need', 'native', 'four', 'major', 'native', 'devops', 'continuous', 'delivery', 'container', 'various', 'factor', 'consider', 'design', 'native', 'development', 'stack', 'required', 'also', 'discussed', 'detail', 'tool', 'factor', 'api', 'based', 'design', 'essential', 'native', 'examined', 'challenge', 'come', 'computing', 'native', 'solution', 'economic', 'perspective', 'provide', 'full', 'value', 'enabling', 'scale', 'change', 'quickly', 'scalability', 'give', 'company', 'new', 'chance', 'grow', 'become', 'efficient', 'provide', 'better', 'customer']"
"A Multi-Agent Deep-Reinforcement Learning Approach for Application-Agnostic Microservice Scaling Efficient cloud-based microservice scaling needs to take into account the inter-dependencies between application components to avoid bottlenecks and to swiftly adapt to dynamically changing environments or user demands. Most of today's solutions are not adaptive enough especially to handle large-scale microservices. In this paper, we propose a novel solution leveraging Multi-Agent Deep Reinforcement Learning (MADRL). First, we define our model for horizontal scaling of microservices and formalize the problem. Second, we propose an algorithm based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG) to solve it. Third, a dedicated simulation environment is presented, where arbitrary microservices can be created for testing purposes, and we carry out a comprehensive evaluation. We analyze the performance of the model for microservices of different sizes, investigating its ability to optimize scaling while considering efficient resource utilization and application stability. Results show that our MADDPG-based RL algorithm outperforms the industry standard approach provided by Kubernetes' HPA by at least 14% in terms of resource usage cost.",A Multi-Agent Deep-Reinforcement Learning Approach for Application-Agnostic Microservice Scaling,"Efficient cloud-based microservice scaling needs to take into account the inter-dependencies between application components to avoid bottlenecks and to swiftly adapt to dynamically changing environments or user demands. Most of today's solutions are not adaptive enough especially to handle large-scale microservices. In this paper, we propose a novel solution leveraging Multi-Agent Deep Reinforcement Learning (MADRL). First, we define our model for horizontal scaling of microservices and formalize the problem. Second, we propose an algorithm based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG) to solve it. Third, a dedicated simulation environment is presented, where arbitrary microservices can be created for testing purposes, and we carry out a comprehensive evaluation. We analyze the performance of the model for microservices of different sizes, investigating its ability to optimize scaling while considering efficient resource utilization and application stability. Results show that our MADDPG-based RL algorithm outperforms the industry standard approach provided by Kubernetes' HPA by at least 14% in terms of resource usage cost.",IEEE conference,no,"['learning', 'scaling', 'efficient', 'scaling', 'need', 'take', 'account', 'avoid', 'bottleneck', 'adapt', 'dynamically', 'changing', 'environment', 'user', 'demand', 'today', 'solution', 'adaptive', 'enough', 'especially', 'handle', 'paper', 'propose', 'novel', 'solution', 'leveraging', 'deep', 'reinforcement', 'learning', 'first', 'define', 'model', 'horizontal', 'scaling', 'problem', 'second', 'propose', 'algorithm', 'based', 'deep', 'policy', 'gradient', 'solve', 'third', 'dedicated', 'simulation', 'environment', 'presented', 'created', 'testing', 'purpose', 'carry', 'comprehensive', 'evaluation', 'analyze', 'performance', 'model', 'different', 'size', 'investigating', 'ability', 'optimize', 'scaling', 'considering', 'efficient', 'resource', 'utilization', 'stability', 'result', 'show', 'rl', 'algorithm', 'outperforms', 'industry', 'standard', 'provided', 'kubernetes', 'least', 'term', 'resource', 'usage', 'cost']"
"Performance and Scalability Testing Strategy Based on Kubemark The technology of container orchestration dramatically speeds up the extension of applications architected on microservices. As the complexity of those applications continues to increase, the orchestration system needs to resolve performance challenge to deploy thousands of coexisting applications to work cooperatively, and to reach the requirements of efficiency and scalability of microservices architectures. The Kubernetes is an open source project to implement container orchestration, and more popular than the others. For performance testing, the Kubernetes provides Kubemark as a deployment tool, which can simulate a large- scale Kubenetes clusters. Kubemark supports the performance evaluation of cluster scale much larger than the real cluster scale. This paper addresses performance issues of microservices structure, describes the architecture of Kubernetes to implement schedule of resource, and finally proposes a method of performance testing with Kubemark.",Performance and Scalability Testing Strategy Based on Kubemark,"The technology of container orchestration dramatically speeds up the extension of applications architected on microservices. As the complexity of those applications continues to increase, the orchestration system needs to resolve performance challenge to deploy thousands of coexisting applications to work cooperatively, and to reach the requirements of efficiency and scalability of microservices architectures. The Kubernetes is an open source project to implement container orchestration, and more popular than the others. For performance testing, the Kubernetes provides Kubemark as a deployment tool, which can simulate a large- scale Kubenetes clusters. Kubemark supports the performance evaluation of cluster scale much larger than the real cluster scale. This paper addresses performance issues of microservices structure, describes the architecture of Kubernetes to implement schedule of resource, and finally proposes a method of performance testing with Kubemark.",IEEE conference,no,"['performance', 'scalability', 'testing', 'strategy', 'based', 'kubemark', 'technology', 'container', 'orchestration', 'dramatically', 'speed', 'extension', 'complexity', 'increase', 'orchestration', 'need', 'resolve', 'performance', 'challenge', 'deploy', 'thousand', 'work', 'reach', 'requirement', 'efficiency', 'scalability', 'kubernetes', 'open', 'source', 'project', 'implement', 'container', 'orchestration', 'popular', 'others', 'performance', 'testing', 'kubernetes', 'provides', 'kubemark', 'deployment', 'tool', 'simulate', 'scale', 'cluster', 'kubemark', 'support', 'performance', 'evaluation', 'cluster', 'scale', 'much', 'larger', 'real', 'cluster', 'scale', 'paper', 'address', 'performance', 'issue', 'structure', 'describes', 'kubernetes', 'implement', 'schedule', 'resource', 'finally', 'proposes', 'method', 'performance', 'testing', 'kubemark']"
"A Cloud-Edge Artificial Intelligence Framework for Sensor Networks Internet of Things devices allow building increasingly large-scale sensor networks for gathering heterogeneous high-volume data streams. Artificial Intelligence (AI) applications typically collect them into centralized cloud infrastructures to run computationally intensive Machine Learning (ML) tasks. According to the emerging edge computing paradigm, instead, data preprocessing, model training and inference can be distributed among devices at the border of the local network, exploiting data locality to improve response latency, bandwidth usage and privacy, at the cost of suboptimal model accuracy due to smaller training sets. The paper proposes a cloud-edge framework for sensor-based AI applications, enabling a dynamic trade-off between edge and cloud layers by means of: (i) a novel containerized microservice architecture, allowing the execution of both model training and prediction either on edge or on cloud nodes; (ii) flexible automatic migration of tasks between the edge and the cloud, based on opportunistic management of resources and workloads. In order to facilitate implementations, a scouting of compatible device platforms for field sensing and edge computing nodes has been carried out, as well as a selection of suitable open-source off-the-shelf software tools. Early experiments validate the feasibility and core benefits of the proposal.",A Cloud-Edge Artificial Intelligence Framework for Sensor Networks,"Internet of Things devices allow building increasingly large-scale sensor networks for gathering heterogeneous high-volume data streams. Artificial Intelligence (AI) applications typically collect them into centralized cloud infrastructures to run computationally intensive Machine Learning (ML) tasks. According to the emerging edge computing paradigm, instead, data preprocessing, model training and inference can be distributed among devices at the border of the local network, exploiting data locality to improve response latency, bandwidth usage and privacy, at the cost of suboptimal model accuracy due to smaller training sets. The paper proposes a cloud-edge framework for sensor-based AI applications, enabling a dynamic trade-off between edge and cloud layers by means of: (i) a novel containerized microservice architecture, allowing the execution of both model training and prediction either on edge or on cloud nodes; (ii) flexible automatic migration of tasks between the edge and the cloud, based on opportunistic management of resources and workloads. In order to facilitate implementations, a scouting of compatible device platforms for field sensing and edge computing nodes has been carried out, as well as a selection of suitable open-source off-the-shelf software tools. Early experiments validate the feasibility and core benefits of the proposal.",IEEE conference,no,"['artificial', 'intelligence', 'framework', 'sensor', 'network', 'internet', 'thing', 'device', 'allow', 'building', 'increasingly', 'sensor', 'network', 'gathering', 'heterogeneous', 'stream', 'artificial', 'intelligence', 'ai', 'typically', 'collect', 'centralized', 'infrastructure', 'run', 'computationally', 'intensive', 'machine', 'learning', 'ml', 'task', 'according', 'emerging', 'edge', 'computing', 'paradigm', 'instead', 'model', 'training', 'inference', 'distributed', 'among', 'device', 'local', 'network', 'exploiting', 'improve', 'response', 'latency', 'bandwidth', 'usage', 'privacy', 'cost', 'model', 'accuracy', 'due', 'smaller', 'training', 'set', 'paper', 'proposes', 'framework', 'ai', 'enabling', 'dynamic', 'edge', 'layer', 'mean', 'novel', 'containerized', 'allowing', 'execution', 'model', 'training', 'prediction', 'either', 'edge', 'node', 'ii', 'flexible', 'automatic', 'migration', 'task', 'edge', 'based', 'management', 'resource', 'workload', 'order', 'facilitate', 'implementation', 'compatible', 'device', 'platform', 'field', 'sensing', 'edge', 'computing', 'node', 'carried', 'well', 'selection', 'suitable', 'tool', 'early', 'experiment', 'validate', 'feasibility', 'core', 'benefit', 'proposal']"
"Shimmy: Accelerating Inter-Container Communication for the IoT Edge Cloud-native technologies consisting of containers, microservices, and service meshes bring the traditional advantages of Cloud Computing like scalability, composability, and rapid deployability to the IoT Edge. An application built on the microservices architecture relies on a collection of individual containerized components offering modular services via REST or gRPC interfaces over the network. Compared to a monolithic application, the magnitude of data and control exchange between the components of a microservices application is several orders higher. Studies have shown that overheads caused by such inter-container communication are a significant hurdle in achieving the sub-50ms latencies required for 5G enabled network Edges comprised of a much smaller compute cluster, unlike the Cloud. In this paper, we present Shimmy - a shared memory-based communication interface for containers that is cleanly integrated into the Kubernetes orchestration architecture while offering significant acceleration for microservices. Results have shown a consistent 3-4x latency improvement over UDP and TCP, as much as 20x latency improvement over RabbitMQ, while significantly reducing memory and CPU usage in large data transfers as well as real-time video streaming.",Shimmy: Accelerating Inter-Container Communication for the IoT Edge,"Cloud-native technologies consisting of containers, microservices, and service meshes bring the traditional advantages of Cloud Computing like scalability, composability, and rapid deployability to the IoT Edge. An application built on the microservices architecture relies on a collection of individual containerized components offering modular services via REST or gRPC interfaces over the network. Compared to a monolithic application, the magnitude of data and control exchange between the components of a microservices application is several orders higher. Studies have shown that overheads caused by such inter-container communication are a significant hurdle in achieving the sub-50ms latencies required for 5G enabled network Edges comprised of a much smaller compute cluster, unlike the Cloud. In this paper, we present Shimmy - a shared memory-based communication interface for containers that is cleanly integrated into the Kubernetes orchestration architecture while offering significant acceleration for microservices. Results have shown a consistent 3-4x latency improvement over UDP and TCP, as much as 20x latency improvement over RabbitMQ, while significantly reducing memory and CPU usage in large data transfers as well as real-time video streaming.",IEEE conference,no,"['communication', 'iot', 'edge', 'technology', 'consisting', 'container', 'mesh', 'bring', 'traditional', 'advantage', 'computing', 'like', 'scalability', 'composability', 'rapid', 'iot', 'edge', 'built', 'relies', 'collection', 'individual', 'containerized', 'offering', 'modular', 'via', 'rest', 'grpc', 'interface', 'network', 'compared', 'monolithic', 'magnitude', 'control', 'exchange', 'several', 'order', 'higher', 'study', 'shown', 'overhead', 'caused', 'communication', 'significant', 'achieving', 'latency', 'required', 'enabled', 'network', 'edge', 'comprised', 'much', 'smaller', 'compute', 'cluster', 'unlike', 'paper', 'present', 'shared', 'communication', 'interface', 'container', 'integrated', 'kubernetes', 'orchestration', 'offering', 'significant', 'result', 'shown', 'consistent', 'latency', 'improvement', 'tcp', 'much', 'latency', 'improvement', 'rabbitmq', 'significantly', 'reducing', 'memory', 'cpu', 'usage', 'large', 'transfer', 'well', 'video', 'streaming']"
"A Microservice Architecture for the Design of Computer-Interpretable Guideline Processing Tools Several tools exist that are designed to process computer interpretable guidelines (CIGs), each with a distinct purpose, such as detecting interactions or patient personalisation. While it is desirable to use these tools as part of larger decision support systems (DSSs) doing so is often not straightforward, as their design does not often support external interoperability or account for the fact that other CIG tools may be running in parallel, a situation that will become increasingly more prevalent with the increased adoption of CIGs in different parts of the health system. This results in an integration overhead, system redundancy and a lack of flexibility in how these tools can be combined. To address these issues, we define a blueprint architecture to be used in the design of guideline processing tools, based on the conceptualisation of key components as RESTful microservices. In addition, we define the types of data endpoints that each component should expose, for both the communication between internal components and communication with external components that exist as a part of a DSS. To demonstrate the utility of our architecture, we show how an example guideline processing tool can be restructured according to these principles, in order to enable it to be flexibly integrated into the DSS used in the CONSULT project.",A Microservice Architecture for the Design of Computer-Interpretable Guideline Processing Tools,"Several tools exist that are designed to process computer interpretable guidelines (CIGs), each with a distinct purpose, such as detecting interactions or patient personalisation. While it is desirable to use these tools as part of larger decision support systems (DSSs) doing so is often not straightforward, as their design does not often support external interoperability or account for the fact that other CIG tools may be running in parallel, a situation that will become increasingly more prevalent with the increased adoption of CIGs in different parts of the health system. This results in an integration overhead, system redundancy and a lack of flexibility in how these tools can be combined. To address these issues, we define a blueprint architecture to be used in the design of guideline processing tools, based on the conceptualisation of key components as RESTful microservices. In addition, we define the types of data endpoints that each component should expose, for both the communication between internal components and communication with external components that exist as a part of a DSS. To demonstrate the utility of our architecture, we show how an example guideline processing tool can be restructured according to these principles, in order to enable it to be flexibly integrated into the DSS used in the CONSULT project.",IEEE conference,no,"['design', 'guideline', 'processing', 'tool', 'several', 'tool', 'exist', 'designed', 'process', 'computer', 'guideline', 'distinct', 'purpose', 'detecting', 'interaction', 'patient', 'desirable', 'use', 'tool', 'part', 'larger', 'decision', 'support', 'often', 'straightforward', 'design', 'often', 'support', 'external', 'interoperability', 'account', 'fact', 'tool', 'may', 'running', 'parallel', 'situation', 'become', 'increasingly', 'prevalent', 'increased', 'adoption', 'different', 'part', 'health', 'result', 'integration', 'overhead', 'redundancy', 'lack', 'flexibility', 'tool', 'combined', 'address', 'issue', 'define', 'blueprint', 'used', 'design', 'guideline', 'processing', 'tool', 'based', 'key', 'restful', 'addition', 'define', 'type', 'endpoint', 'expose', 'communication', 'internal', 'communication', 'external', 'exist', 'part', 'demonstrate', 'show', 'example', 'guideline', 'processing', 'tool', 'according', 'principle', 'order', 'enable', 'flexibly', 'integrated', 'used', 'project']"
"RobotKube: Orchestrating Large-Scale Cooperative Multi-Robot Systems with Kubernetes and ROS Modern cyber-physical systems (CPS) such as Cooperative Intelligent Transport Systems (C-ITS) are increasingly defined by the software which operates these systems. In practice, microservice architectures can be employed, which may consist of containerized microservices running in a cluster comprised of robots and supporting infrastructure. These microservices need to be orchestrated dynamically according to ever changing requirements posed at the system. Additionally, these systems are embedded in DevOps processes aiming at continually updating and upgrading both the capabilities of CPS components and of the system as a whole. In this paper, we present RobotKube, an approach to orchestrating containerized microservices for large-scale cooperative multi-robot CPS based on Kubernetes. We describe how to automate the orchestration of software across a CPS, and include the possibility to monitor and selectively store relevant accruing data. In this context, we present two main components of such a system: an event detector capable of, e.g., requesting the deployment of additional applications, and an application manager capable of automatically configuring the required changes in the Kubernetes cluster. By combining the widely adopted Kubernetes platform with the Robot Operating System (ROS), we enable the use of standard tools and practices for developing, deploying, scaling, and monitoring microservices in C-ITS. We demonstrate and evaluate RobotKube in an exemplary and reproducible use case that we make publicly available at github.com/ika-rwth-aachen/robotkube.",RobotKube: Orchestrating Large-Scale Cooperative Multi-Robot Systems with Kubernetes and ROS,"Modern cyber-physical systems (CPS) such as Cooperative Intelligent Transport Systems (C-ITS) are increasingly defined by the software which operates these systems. In practice, microservice architectures can be employed, which may consist of containerized microservices running in a cluster comprised of robots and supporting infrastructure. These microservices need to be orchestrated dynamically according to ever changing requirements posed at the system. Additionally, these systems are embedded in DevOps processes aiming at continually updating and upgrading both the capabilities of CPS components and of the system as a whole. In this paper, we present RobotKube, an approach to orchestrating containerized microservices for large-scale cooperative multi-robot CPS based on Kubernetes. We describe how to automate the orchestration of software across a CPS, and include the possibility to monitor and selectively store relevant accruing data. In this context, we present two main components of such a system: an event detector capable of, e.g., requesting the deployment of additional applications, and an application manager capable of automatically configuring the required changes in the Kubernetes cluster. By combining the widely adopted Kubernetes platform with the Robot Operating System (ROS), we enable the use of standard tools and practices for developing, deploying, scaling, and monitoring microservices in C-ITS. We demonstrate and evaluate RobotKube in an exemplary and reproducible use case that we make publicly available at github.com/ika-rwth-aachen/robotkube.",IEEE conference,no,"['robotkube', 'orchestrating', 'cooperative', 'kubernetes', 'ro', 'modern', 'cps', 'cooperative', 'intelligent', 'transport', 'increasingly', 'defined', 'operates', 'practice', 'employed', 'may', 'consist', 'containerized', 'running', 'cluster', 'comprised', 'robot', 'supporting', 'infrastructure', 'need', 'orchestrated', 'dynamically', 'according', 'ever', 'changing', 'requirement', 'additionally', 'embedded', 'devops', 'process', 'aiming', 'updating', 'upgrading', 'capability', 'cps', 'whole', 'paper', 'present', 'robotkube', 'orchestrating', 'containerized', 'cooperative', 'cps', 'based', 'kubernetes', 'describe', 'automate', 'orchestration', 'across', 'cps', 'include', 'possibility', 'monitor', 'selectively', 'store', 'relevant', 'context', 'present', 'two', 'main', 'event', 'capable', 'deployment', 'additional', 'manager', 'capable', 'automatically', 'configuring', 'required', 'change', 'kubernetes', 'cluster', 'combining', 'widely', 'adopted', 'kubernetes', 'platform', 'robot', 'operating', 'ro', 'enable', 'use', 'standard', 'tool', 'practice', 'developing', 'deploying', 'scaling', 'monitoring', 'demonstrate', 'evaluate', 'robotkube', 'use', 'case', 'make', 'publicly', 'available']"
"Context-aware IoT Service Recommendation: A Deep Collaborative Filtering-based Approach The advantages of Service-Oriented Architecture (SOA) combined with the emerging and diffusion of the Internet of Things (IoT) instances have given birth to a new paradigm for IoT components integration, i.e., Service-Oriented IoT. Microservices especially have been widely used to deliver IoT services due to their lightweight implementation and distributed nature. With the continuous increase in IoT services available on the Internet, the selection of services becomes difficult. Furthermore, IoT services are often featured with rich contexts and OpenAPI descriptions, which impede service recommendation approaches that are designed for WSDL-based Web services or mashup services. To address this challenging issue, we propose a context-aware IoT service recommendation approach called DFORM for proactive service provision. DFORM considers both functional features of OpenAPI descriptions and contextual features of IoT environments, and leverages a deep collaborative filtering-based recommendation model to learn the feature representations and capture the interactions between users and services. We conduct a series of experiments to evaluate the recommendation performance of DFORM and the experimental results show that DFORM is effective in IoT service recommendation and outperforms state-of-the-art techniques.",Context-aware IoT Service Recommendation: A Deep Collaborative Filtering-based Approach,"The advantages of Service-Oriented Architecture (SOA) combined with the emerging and diffusion of the Internet of Things (IoT) instances have given birth to a new paradigm for IoT components integration, i.e., Service-Oriented IoT. Microservices especially have been widely used to deliver IoT services due to their lightweight implementation and distributed nature. With the continuous increase in IoT services available on the Internet, the selection of services becomes difficult. Furthermore, IoT services are often featured with rich contexts and OpenAPI descriptions, which impede service recommendation approaches that are designed for WSDL-based Web services or mashup services. To address this challenging issue, we propose a context-aware IoT service recommendation approach called DFORM for proactive service provision. DFORM considers both functional features of OpenAPI descriptions and contextual features of IoT environments, and leverages a deep collaborative filtering-based recommendation model to learn the feature representations and capture the interactions between users and services. We conduct a series of experiments to evaluate the recommendation performance of DFORM and the experimental results show that DFORM is effective in IoT service recommendation and outperforms state-of-the-art techniques.",IEEE conference,no,"['iot', 'recommendation', 'deep', 'collaborative', 'advantage', 'soa', 'combined', 'emerging', 'internet', 'thing', 'iot', 'instance', 'given', 'new', 'paradigm', 'iot', 'integration', 'iot', 'especially', 'widely', 'used', 'deliver', 'iot', 'due', 'lightweight', 'implementation', 'distributed', 'nature', 'continuous', 'increase', 'iot', 'available', 'internet', 'selection', 'becomes', 'difficult', 'furthermore', 'iot', 'often', 'rich', 'context', 'openapi', 'description', 'recommendation', 'designed', 'web', 'address', 'challenging', 'issue', 'propose', 'iot', 'recommendation', 'called', 'dform', 'proactive', 'provision', 'dform', 'considers', 'functional', 'feature', 'openapi', 'description', 'contextual', 'feature', 'iot', 'environment', 'leverage', 'deep', 'collaborative', 'recommendation', 'model', 'learn', 'feature', 'representation', 'capture', 'interaction', 'user', 'conduct', 'series', 'experiment', 'evaluate', 'recommendation', 'performance', 'dform', 'experimental', 'result', 'show', 'dform', 'effective', 'iot', 'recommendation', 'outperforms', 'technique']"
"Hardware-Software Complex to Diagnostic and Rehabilitation the Patients with Damages of Cervical-Thoracic Spine and Hand Nerves After the damages of the cervical-thoracic spine and hand nerves, the motor and/or sensory functions are impaired, especially fine motor skills of each finger individually or a group of fingers. In the paper is proposed a method of constructing a “green zone” (optimal for patient training mode), and “red zone” (dangerous training mode), which correspond to the model of quantitative dependence the time fulfill several sequential actions patient based on the moving average. A decrease of muscle spasticity of the arm in scores following the Modified Ashworth Scale (MAS) was used as a criterion of optimality. For medical studies and training the diagnostic and training (DT) devices of the “Reflex-Txx” series were developed, which has in-built Hall sensors or touch sensors. The results of each individual's training are accumulated, displayed and analyzed on the microservices of the developed hardware and software complex and available on the user's gadgets. The electronic components and architecture of the developed devices based on a cost-effective Arduino platform hardware implementation.",Hardware-Software Complex to Diagnostic and Rehabilitation the Patients with Damages of Cervical-Thoracic Spine and Hand Nerves,"After the damages of the cervical-thoracic spine and hand nerves, the motor and/or sensory functions are impaired, especially fine motor skills of each finger individually or a group of fingers. In the paper is proposed a method of constructing a “green zone” (optimal for patient training mode), and “red zone” (dangerous training mode), which correspond to the model of quantitative dependence the time fulfill several sequential actions patient based on the moving average. A decrease of muscle spasticity of the arm in scores following the Modified Ashworth Scale (MAS) was used as a criterion of optimality. For medical studies and training the diagnostic and training (DT) devices of the “Reflex-Txx” series were developed, which has in-built Hall sensors or touch sensors. The results of each individual's training are accumulated, displayed and analyzed on the microservices of the developed hardware and software complex and available on the user's gadgets. The electronic components and architecture of the developed devices based on a cost-effective Arduino platform hardware implementation.",IEEE conference,no,"['complex', 'diagnostic', 'rehabilitation', 'patient', 'damage', 'hand', 'damage', 'hand', 'sensory', 'function', 'especially', 'fine', 'skill', 'individually', 'group', 'paper', 'proposed', 'method', 'constructing', 'zone', 'optimal', 'patient', 'training', 'mode', 'zone', 'training', 'mode', 'model', 'quantitative', 'dependence', 'time', 'fulfill', 'several', 'sequential', 'action', 'patient', 'based', 'moving', 'average', 'decrease', 'arm', 'score', 'following', 'modified', 'scale', 'used', 'criterion', 'medical', 'study', 'training', 'diagnostic', 'training', 'dt', 'device', 'series', 'developed', 'sensor', 'sensor', 'result', 'individual', 'training', 'displayed', 'analyzed', 'developed', 'hardware', 'complex', 'available', 'user', 'electronic', 'developed', 'device', 'based', 'platform', 'hardware', 'implementation']"
"Smart Healthcare Monitoring System For Healthy Driving in Public Transportation In an age where citizens are constantly moving between different places, transport demand is extremely high, and so, it is important to have sophisticated public transportation systems in place to ensure a sustainable development of urban areas and meet the needs of citizens. Public transport operators consequently need to provide reliable services in order to minimize disruption events that can affect the vehicles and their drivers, such as breakdowns, accidents or illnesses. The project here described focuses on the type of events and approaches related with the vehicle drivers and the identification of both their performance profiles and health condition while in operation. For that purpose, existing nonintrusive technologies present on the vehicle are leveraged, able to collect data related to physiological measurements taken in realtime. Such sensitive data will be processed, stored and shared in a secure manner, using blockchain-based technologies, so that only authenticated and authorized parties will be able to access the data, according to their clearance level, through an Application Programming Interface (API) designed for that purpose. The architecture of the system will be microservices-based, with components deployed at different infrastructure levels—from On Board Units (OBUs) in vehicles up to cloud-based subsystems.",Smart Healthcare Monitoring System For Healthy Driving in Public Transportation,"In an age where citizens are constantly moving between different places, transport demand is extremely high, and so, it is important to have sophisticated public transportation systems in place to ensure a sustainable development of urban areas and meet the needs of citizens. Public transport operators consequently need to provide reliable services in order to minimize disruption events that can affect the vehicles and their drivers, such as breakdowns, accidents or illnesses. The project here described focuses on the type of events and approaches related with the vehicle drivers and the identification of both their performance profiles and health condition while in operation. For that purpose, existing nonintrusive technologies present on the vehicle are leveraged, able to collect data related to physiological measurements taken in realtime. Such sensitive data will be processed, stored and shared in a secure manner, using blockchain-based technologies, so that only authenticated and authorized parties will be able to access the data, according to their clearance level, through an Application Programming Interface (API) designed for that purpose. The architecture of the system will be microservices-based, with components deployed at different infrastructure levels—from On Board Units (OBUs) in vehicles up to cloud-based subsystems.",IEEE conference,no,"['smart', 'healthcare', 'monitoring', 'driving', 'public', 'transportation', 'age', 'citizen', 'constantly', 'moving', 'different', 'place', 'transport', 'demand', 'extremely', 'high', 'important', 'sophisticated', 'public', 'transportation', 'place', 'ensure', 'sustainable', 'development', 'urban', 'area', 'meet', 'need', 'citizen', 'public', 'transport', 'operator', 'consequently', 'need', 'provide', 'reliable', 'order', 'minimize', 'event', 'affect', 'vehicle', 'driver', 'project', 'described', 'focus', 'type', 'event', 'related', 'vehicle', 'driver', 'identification', 'performance', 'profile', 'health', 'condition', 'operation', 'purpose', 'existing', 'nonintrusive', 'technology', 'present', 'vehicle', 'able', 'collect', 'related', 'measurement', 'taken', 'realtime', 'sensitive', 'processed', 'stored', 'shared', 'secure', 'manner', 'using', 'technology', 'authenticated', 'authorized', 'able', 'access', 'according', 'level', 'programming', 'interface', 'api', 'designed', 'purpose', 'deployed', 'different', 'infrastructure', 'unit', 'vehicle']"
"Architectural Vision of Cloud Computing in the Indian Government The GI (Govt. of India) cloud started in 2014 is built on the state of art technologies and rich architecture with the nationwide network infrastructure and Data Centres located across the country on National and State data centres. This paper investigates, study and analyze the cloud architecture of Govt. of India and suggests modifications that need to be adapted for sustainable development as per the global changing scenario and fulfill the future needs with improved service delivery, increased throughput, and increased efficiency to provide secured cloud services and to minimize the gap between the cloud service providers and end-users. The cloud services are designed for centralized storage and processing. The cloud data centers are generally located thousands of miles away from the end-users where the data is actually generated. The physical distance between the cloud infrastructure and the data source at edge level end-users produces latency for the real-time processing of the huge amount of data generated at the source level. In recent years the automation scenario is changing globally with various emerging technologies such as the Internet of Things (IoT), Wireless Fidelity 6 (Wi-Fi 6), Fifth Generation Mobile Network connectivity (5G), Artificial Intelligence (AI), and Machine Learning, etc. Emerging technologies like IoT, Wi-Fi 6, 5G gives large scope for boundary level computing and generates a very huge amount of data at the data source level produced by the end-users. These technologies require agile real-time processing and analysis of the data at the source level. Edge computing and Fog computing are the distributed architectures that work together, for reduced latency and speedy real-time processing where the data is actually generated by the end-user. According to the new implementation demands, various emerging cloud technologies such as Mobile Cloud Apps, Containers, Serverless, Microservices, Development and Information Technology Operations (DevOps), BlockChain, Fog computing, Edge Computing, and Software-Defined Infrastructure (SDI), etc are proposed for implementation.",Architectural Vision of Cloud Computing in the Indian Government,"The GI (Govt. of India) cloud started in 2014 is built on the state of art technologies and rich architecture with the nationwide network infrastructure and Data Centres located across the country on National and State data centres. This paper investigates, study and analyze the cloud architecture of Govt. of India and suggests modifications that need to be adapted for sustainable development as per the global changing scenario and fulfill the future needs with improved service delivery, increased throughput, and increased efficiency to provide secured cloud services and to minimize the gap between the cloud service providers and end-users. The cloud services are designed for centralized storage and processing. The cloud data centers are generally located thousands of miles away from the end-users where the data is actually generated. The physical distance between the cloud infrastructure and the data source at edge level end-users produces latency for the real-time processing of the huge amount of data generated at the source level. In recent years the automation scenario is changing globally with various emerging technologies such as the Internet of Things (IoT), Wireless Fidelity 6 (Wi-Fi 6), Fifth Generation Mobile Network connectivity (5G), Artificial Intelligence (AI), and Machine Learning, etc. Emerging technologies like IoT, Wi-Fi 6, 5G gives large scope for boundary level computing and generates a very huge amount of data at the data source level produced by the end-users. These technologies require agile real-time processing and analysis of the data at the source level. Edge computing and Fog computing are the distributed architectures that work together, for reduced latency and speedy real-time processing where the data is actually generated by the end-user. According to the new implementation demands, various emerging cloud technologies such as Mobile Cloud Apps, Containers, Serverless, Microservices, Development and Information Technology Operations (DevOps), BlockChain, Fog computing, Edge Computing, and Software-Defined Infrastructure (SDI), etc are proposed for implementation.",IEEE conference,no,"['architectural', 'vision', 'computing', 'government', 'started', 'built', 'state', 'art', 'technology', 'rich', 'network', 'infrastructure', 'located', 'across', 'state', 'paper', 'investigates', 'study', 'analyze', 'suggests', 'modification', 'need', 'adapted', 'sustainable', 'development', 'per', 'global', 'changing', 'scenario', 'fulfill', 'future', 'need', 'improved', 'delivery', 'increased', 'throughput', 'increased', 'efficiency', 'provide', 'minimize', 'gap', 'provider', 'designed', 'centralized', 'storage', 'processing', 'center', 'generally', 'located', 'thousand', 'away', 'actually', 'generated', 'physical', 'distance', 'infrastructure', 'source', 'edge', 'level', 'produce', 'latency', 'processing', 'huge', 'amount', 'generated', 'source', 'level', 'recent', 'year', 'automation', 'scenario', 'changing', 'various', 'emerging', 'technology', 'internet', 'thing', 'iot', 'wireless', 'generation', 'mobile', 'network', 'connectivity', 'artificial', 'intelligence', 'ai', 'machine', 'learning', 'etc', 'emerging', 'technology', 'like', 'iot', 'give', 'large', 'scope', 'boundary', 'level', 'computing', 'generates', 'huge', 'amount', 'source', 'level', 'produced', 'technology', 'require', 'agile', 'processing', 'analysis', 'source', 'level', 'edge', 'computing', 'fog', 'computing', 'distributed', 'work', 'together', 'reduced', 'latency', 'processing', 'actually', 'generated', 'according', 'new', 'implementation', 'demand', 'various', 'emerging', 'technology', 'mobile', 'apps', 'container', 'serverless', 'development', 'information', 'technology', 'operation', 'devops', 'fog', 'computing', 'edge', 'computing', 'infrastructure', 'etc', 'proposed', 'implementation']"
"Towards Transforming an Industrial Automation System from Monolithic to Microservices Container technology enables designers to build (micro)service-oriented systems with on-demand scalability and availability easily, provided the original system has been well-modularized to begin with. Industry automation applications, built a long time ago, aim to adopt this technology to become more flexible and ready to be a part of the internet of thing based next-generation industrial system. In this paper, we share our work-in-progress experience of transforming a complex, distributed industrial automation system to a microservice based containerized architecture. We propose a containerized architecture of the “to-be” system and observe that despite being distributed, the “as-is” system tend to follow a monolithic architecture with strong coupling among the participating components. Consequently it becomes difficult to achieve the proposed microservice based architecture without a significant change. We also discuss the workload handling, resource utilization and reliability aspects of the “to-be” architecture using a prototype implementation.",Towards Transforming an Industrial Automation System from Monolithic to Microservices,"Container technology enables designers to build (micro)service-oriented systems with on-demand scalability and availability easily, provided the original system has been well-modularized to begin with. Industry automation applications, built a long time ago, aim to adopt this technology to become more flexible and ready to be a part of the internet of thing based next-generation industrial system. In this paper, we share our work-in-progress experience of transforming a complex, distributed industrial automation system to a microservice based containerized architecture. We propose a containerized architecture of the “to-be” system and observe that despite being distributed, the “as-is” system tend to follow a monolithic architecture with strong coupling among the participating components. Consequently it becomes difficult to achieve the proposed microservice based architecture without a significant change. We also discuss the workload handling, resource utilization and reliability aspects of the “to-be” architecture using a prototype implementation.",IEEE conference,no,"['towards', 'transforming', 'industrial', 'automation', 'monolithic', 'container', 'technology', 'enables', 'designer', 'build', 'micro', 'scalability', 'availability', 'easily', 'provided', 'original', 'begin', 'industry', 'automation', 'built', 'long', 'time', 'ago', 'aim', 'adopt', 'technology', 'become', 'flexible', 'ready', 'part', 'internet', 'thing', 'based', 'industrial', 'paper', 'share', 'experience', 'transforming', 'complex', 'distributed', 'industrial', 'automation', 'based', 'containerized', 'propose', 'containerized', 'observe', 'despite', 'distributed', 'tend', 'follow', 'monolithic', 'strong', 'coupling', 'among', 'participating', 'consequently', 'becomes', 'difficult', 'achieve', 'proposed', 'based', 'without', 'significant', 'change', 'also', 'discus', 'workload', 'handling', 'resource', 'utilization', 'reliability', 'aspect', 'using', 'prototype', 'implementation']"
"Data Processing Unit (DPU) Based Network Process Offloading for Efficient Service Meshes As the software development landscape transitions from monolithic applications to agile, cloud-native microservices, new challenges in communication have emerged, necessitating the evolution of network service meshes to effectively manage these complexities. While addressing these communication challenges, implementing service mesh solutions introduces higher resource utilization, primarily due to deploying an additional per-microservice infrastructure known as the “sidecar” container. A sidecarless approach, as exemplified by Cilium, leverages extended Berkeley packet filter (eBPF) technology to minimize operational overhead and optimize resource utilization. The advent of data processing units (DPU) represents a crucial evolution in cloud accelerators, designed to offload and accelerate networking, security, and storage tasks traditionally handled by server CPUs. This demonstration integrates DPU technology with Cilium’s sidecarless architecture and proposes to offload Cilium’s control plane to DPUs. Our proposed approach improves system efficiency by deploying Cilium’s control plane components on DPUs in a separate host mode. Further, our approach conserves CPU cycles on the primary hosts and shifts network processing tasks to the DPU. This strategic offload aims to enhance throughput, reduce latency, and unburden CPUs from network processing tasks to improve the performance of service mesh architectures for cloud-native ecosystems.",Data Processing Unit (DPU) Based Network Process Offloading for Efficient Service Meshes,"As the software development landscape transitions from monolithic applications to agile, cloud-native microservices, new challenges in communication have emerged, necessitating the evolution of network service meshes to effectively manage these complexities. While addressing these communication challenges, implementing service mesh solutions introduces higher resource utilization, primarily due to deploying an additional per-microservice infrastructure known as the “sidecar” container. A sidecarless approach, as exemplified by Cilium, leverages extended Berkeley packet filter (eBPF) technology to minimize operational overhead and optimize resource utilization. The advent of data processing units (DPU) represents a crucial evolution in cloud accelerators, designed to offload and accelerate networking, security, and storage tasks traditionally handled by server CPUs. This demonstration integrates DPU technology with Cilium’s sidecarless architecture and proposes to offload Cilium’s control plane to DPUs. Our proposed approach improves system efficiency by deploying Cilium’s control plane components on DPUs in a separate host mode. Further, our approach conserves CPU cycles on the primary hosts and shifts network processing tasks to the DPU. This strategic offload aims to enhance throughput, reduce latency, and unburden CPUs from network processing tasks to improve the performance of service mesh architectures for cloud-native ecosystems.",IEEE conference,no,"['processing', 'unit', 'dpu', 'based', 'network', 'process', 'offloading', 'efficient', 'mesh', 'development', 'landscape', 'transition', 'monolithic', 'agile', 'new', 'challenge', 'communication', 'emerged', 'necessitating', 'evolution', 'network', 'mesh', 'effectively', 'manage', 'complexity', 'addressing', 'communication', 'challenge', 'implementing', 'mesh', 'solution', 'introduces', 'higher', 'resource', 'utilization', 'primarily', 'due', 'deploying', 'additional', 'infrastructure', 'known', 'container', 'cilium', 'leverage', 'extended', 'berkeley', 'packet', 'filter', 'ebpf', 'technology', 'minimize', 'operational', 'overhead', 'optimize', 'resource', 'utilization', 'advent', 'processing', 'unit', 'dpu', 'represents', 'crucial', 'evolution', 'accelerator', 'designed', 'offload', 'accelerate', 'networking', 'security', 'storage', 'task', 'traditionally', 'handled', 'server', 'cpu', 'demonstration', 'integrates', 'dpu', 'technology', 'cilium', 'proposes', 'offload', 'cilium', 'control', 'plane', 'proposed', 'improves', 'efficiency', 'deploying', 'cilium', 'control', 'plane', 'separate', 'host', 'mode', 'cpu', 'cycle', 'primary', 'host', 'shift', 'network', 'processing', 'task', 'dpu', 'offload', 'aim', 'enhance', 'throughput', 'reduce', 'latency', 'cpu', 'network', 'processing', 'task', 'improve', 'performance', 'mesh', 'ecosystem']"
"A Microservice Based Architecture Topology for Machine Learning Deployment Smart solutions that make use of machine learning and data analyses are on the rise. Big Data analysis is attracting more and more developers and researchers, and at least five requirements (Velocity, Volume, Value, Variety, and Veracity) show challenges in deploying such solutions. Across the globe, many Smart City initiatives are using Big Data Analytics as a tool for doing predictive analytics which can be helpful to human well being. This work presents a generic architecture named Machine Learning in Microservices Architecture (MLMA) that provides design patterns to transform a monolithic architecture of machine learning pipelines in microservices with separate roles. We present two case studies deployed to a Smart City initiative, where we discuss how each component of the architecture applied in specific applications that use predictions with machine learning. Among the benefits of this architecture, we argue prediction performance, scalability, code maintenance and reusability makes such transition a natural trend in Big Data and machine learning applications.",A Microservice Based Architecture Topology for Machine Learning Deployment,"Smart solutions that make use of machine learning and data analyses are on the rise. Big Data analysis is attracting more and more developers and researchers, and at least five requirements (Velocity, Volume, Value, Variety, and Veracity) show challenges in deploying such solutions. Across the globe, many Smart City initiatives are using Big Data Analytics as a tool for doing predictive analytics which can be helpful to human well being. This work presents a generic architecture named Machine Learning in Microservices Architecture (MLMA) that provides design patterns to transform a monolithic architecture of machine learning pipelines in microservices with separate roles. We present two case studies deployed to a Smart City initiative, where we discuss how each component of the architecture applied in specific applications that use predictions with machine learning. Among the benefits of this architecture, we argue prediction performance, scalability, code maintenance and reusability makes such transition a natural trend in Big Data and machine learning applications.",IEEE conference,no,"['based', 'topology', 'machine', 'learning', 'deployment', 'smart', 'solution', 'make', 'use', 'machine', 'learning', 'analysis', 'rise', 'big', 'analysis', 'developer', 'researcher', 'least', 'five', 'requirement', 'volume', 'value', 'variety', 'show', 'challenge', 'deploying', 'solution', 'across', 'many', 'smart', 'city', 'initiative', 'using', 'big', 'analytics', 'tool', 'predictive', 'analytics', 'helpful', 'human', 'well', 'work', 'present', 'generic', 'named', 'machine', 'learning', 'provides', 'design', 'pattern', 'transform', 'monolithic', 'machine', 'learning', 'pipeline', 'separate', 'role', 'present', 'two', 'case', 'study', 'deployed', 'smart', 'city', 'initiative', 'discus', 'applied', 'specific', 'use', 'prediction', 'machine', 'learning', 'among', 'benefit', 'argue', 'prediction', 'performance', 'scalability', 'code', 'maintenance', 'reusability', 'make', 'transition', 'natural', 'trend', 'big', 'machine', 'learning']"
"A Layered Architecture Enabling Metaverse Applications in Smart Manufacturing Environments The steady rollout of Industrial IoT (IIoT) technology in the manufacturing domain embodies the potential to implement smarter and more resilient production processes. To this end, it is expected that there will be a strong reliance of manufacturing processes on cloud/edge services so as to act intelligently and flexibly. While automation is necessary to handle the environment’s complexity, human-in-the-loop design approaches are paramount. In this context, Digital Twins play a crucial role by allowing human operators to inspect and monitor the environment to ensure stability and reliability. Integrating the IIoT with the Metaverse enhances the system’s capabilities even further, offering new opportunities for efficiency and collaboration while enabling integrated management of assets and processes. This article presents a layered conceptual architecture as an enabler for smart manufacturing metaverse environments, targeting real-time data collection and representations from shopfloor assets and processes. At the bottom layer, our proposal relies on middleware technology, serving differentiated Quality of Service (QoS) needs of the Operation Technology (OT) monitoring processes. The latter contributes to feeding a virtual layer where data processes reside, creating representations of the monitored phenomena at different timescales. Metaverse applications can consume data by tapping into the metaverse engine, a microservice-oriented and accelerated Platform as a Service (PaaS) layer tasked with bringing data to life. Without loss of generality, we profile different facets of our proposal by relying on two different proof-of-concept inspection applications aimed at real-time monitoring of the network fabric activity and a visual asset monitoring one.",A Layered Architecture Enabling Metaverse Applications in Smart Manufacturing Environments,"The steady rollout of Industrial IoT (IIoT) technology in the manufacturing domain embodies the potential to implement smarter and more resilient production processes. To this end, it is expected that there will be a strong reliance of manufacturing processes on cloud/edge services so as to act intelligently and flexibly. While automation is necessary to handle the environment’s complexity, human-in-the-loop design approaches are paramount. In this context, Digital Twins play a crucial role by allowing human operators to inspect and monitor the environment to ensure stability and reliability. Integrating the IIoT with the Metaverse enhances the system’s capabilities even further, offering new opportunities for efficiency and collaboration while enabling integrated management of assets and processes. This article presents a layered conceptual architecture as an enabler for smart manufacturing metaverse environments, targeting real-time data collection and representations from shopfloor assets and processes. At the bottom layer, our proposal relies on middleware technology, serving differentiated Quality of Service (QoS) needs of the Operation Technology (OT) monitoring processes. The latter contributes to feeding a virtual layer where data processes reside, creating representations of the monitored phenomena at different timescales. Metaverse applications can consume data by tapping into the metaverse engine, a microservice-oriented and accelerated Platform as a Service (PaaS) layer tasked with bringing data to life. Without loss of generality, we profile different facets of our proposal by relying on two different proof-of-concept inspection applications aimed at real-time monitoring of the network fabric activity and a visual asset monitoring one.",IEEE conference,no,"['layered', 'enabling', 'metaverse', 'smart', 'manufacturing', 'environment', 'industrial', 'iot', 'iiot', 'technology', 'manufacturing', 'domain', 'potential', 'implement', 'resilient', 'production', 'process', 'end', 'expected', 'strong', 'manufacturing', 'process', 'act', 'intelligently', 'flexibly', 'automation', 'necessary', 'handle', 'environment', 'complexity', 'design', 'paramount', 'context', 'digital', 'twin', 'play', 'crucial', 'role', 'allowing', 'human', 'operator', 'monitor', 'environment', 'ensure', 'stability', 'reliability', 'integrating', 'iiot', 'metaverse', 'enhances', 'capability', 'even', 'offering', 'new', 'opportunity', 'efficiency', 'collaboration', 'enabling', 'integrated', 'management', 'asset', 'process', 'article', 'present', 'layered', 'conceptual', 'enabler', 'smart', 'manufacturing', 'metaverse', 'environment', 'collection', 'representation', 'asset', 'process', 'layer', 'proposal', 'relies', 'middleware', 'technology', 'serving', 'differentiated', 'quality', 'qos', 'need', 'operation', 'technology', 'monitoring', 'process', 'latter', 'contributes', 'virtual', 'layer', 'process', 'creating', 'representation', 'monitored', 'phenomenon', 'different', 'metaverse', 'metaverse', 'engine', 'platform', 'paas', 'layer', 'bringing', 'life', 'without', 'loss', 'profile', 'different', 'proposal', 'relying', 'two', 'different', 'inspection', 'aimed', 'monitoring', 'network', 'activity', 'visual', 'asset', 'monitoring', 'one']"
"DESK: Distributed Observability Framework for Edge-Based Containerized Microservices Modern information technology (IT) infrastructures are becoming more complex to meet the diverse demands of emerging technology paradigms such as 5G/6G networks, edge, and internet of things (IoT). The intricacy of these infrastructures grows further when hosting containerized workloads as microservices, resulting in the challenge to detect and troubleshoot performance issues, incidents or even outages of critical use cases like industrial automation processes. Thus, fine-grained measurements and associated visualization are essential for operation observability of these IT infrastructures. However, most existing observability tools operate independently without systematically covering the entire data workflow. This paper presents an integrated design for multi-stage observability workflows, denoted as DistributEd obServability frameworK (DESK). The proposed framework aims to improve observability workflows for measurement, collection, fusion, storage, visualization, and notification. As a proof of concept, we deployed the framework in a Kubernetes-based testbed to demonstrate the successful integration of various components and usability of collected observability data. We also conducted a comprehensive study to determine the caused overhead by DESK agents at the reasonably powerful edge node hardware, which shows on average a CPU and memory overhead of around 2.5 % of total available hardware resource.",DESK: Distributed Observability Framework for Edge-Based Containerized Microservices,"Modern information technology (IT) infrastructures are becoming more complex to meet the diverse demands of emerging technology paradigms such as 5G/6G networks, edge, and internet of things (IoT). The intricacy of these infrastructures grows further when hosting containerized workloads as microservices, resulting in the challenge to detect and troubleshoot performance issues, incidents or even outages of critical use cases like industrial automation processes. Thus, fine-grained measurements and associated visualization are essential for operation observability of these IT infrastructures. However, most existing observability tools operate independently without systematically covering the entire data workflow. This paper presents an integrated design for multi-stage observability workflows, denoted as DistributEd obServability frameworK (DESK). The proposed framework aims to improve observability workflows for measurement, collection, fusion, storage, visualization, and notification. As a proof of concept, we deployed the framework in a Kubernetes-based testbed to demonstrate the successful integration of various components and usability of collected observability data. We also conducted a comprehensive study to determine the caused overhead by DESK agents at the reasonably powerful edge node hardware, which shows on average a CPU and memory overhead of around 2.5 % of total available hardware resource.",IEEE conference,no,"['desk', 'distributed', 'observability', 'framework', 'containerized', 'modern', 'information', 'technology', 'infrastructure', 'becoming', 'complex', 'meet', 'diverse', 'demand', 'emerging', 'technology', 'paradigm', 'network', 'edge', 'internet', 'thing', 'iot', 'intricacy', 'infrastructure', 'grows', 'hosting', 'containerized', 'workload', 'resulting', 'challenge', 'detect', 'performance', 'issue', 'incident', 'even', 'critical', 'use', 'case', 'like', 'industrial', 'automation', 'process', 'thus', 'measurement', 'associated', 'visualization', 'essential', 'operation', 'observability', 'infrastructure', 'however', 'existing', 'observability', 'tool', 'operate', 'independently', 'without', 'covering', 'entire', 'workflow', 'paper', 'present', 'integrated', 'design', 'observability', 'workflow', 'distributed', 'observability', 'framework', 'desk', 'proposed', 'framework', 'aim', 'improve', 'observability', 'workflow', 'measurement', 'collection', 'fusion', 'storage', 'visualization', 'proof', 'concept', 'deployed', 'framework', 'testbed', 'demonstrate', 'successful', 'integration', 'various', 'usability', 'collected', 'observability', 'also', 'conducted', 'comprehensive', 'study', 'determine', 'caused', 'overhead', 'desk', 'agent', 'powerful', 'edge', 'node', 'hardware', 'show', 'average', 'cpu', 'memory', 'overhead', 'around', 'total', 'available', 'hardware', 'resource']"
"Application of Containerized Microservice Approach to Airline Sentiment Analysis Containers are getting more popularity than the virtual machines by offering the benefits of virtualization along with the performance nearby bare metal. Standardizing support of Docker containers among various cloud providers has made them a trendy solution for developers. In this paper, we elaborate on containerized microservice, leveraging the lightweight Docker container technology. The evolution of microservice architecture allows applications to be structured into independent modular components making them easier to manage and scale. As a special case, the containerized sentiment analysis microservice is deployed using popular classification approaches. We implement and compare eight machine learning algorithms: Multinomial Naive Bayes, Decision Tree, Random Forest, K-Nearest Neighbour, AdaBoost, Support Vector Machine, Multilayer Perceptron, and Stochastic Gradient Descent to analyze and classify the tweets into positive, negative, and neutral sentiments. Experimental results procured for the Twitter US Airline Sentiment dataset show that Support Vector Machine, Multinomial Naive Bayes, Stochastic Gradient Descent, and Random Forest outperform the other algorithms. We believe that this research study will assist companies and organizations to improve their services by precisely analyzing Twitter data.",Application of Containerized Microservice Approach to Airline Sentiment Analysis,"Containers are getting more popularity than the virtual machines by offering the benefits of virtualization along with the performance nearby bare metal. Standardizing support of Docker containers among various cloud providers has made them a trendy solution for developers. In this paper, we elaborate on containerized microservice, leveraging the lightweight Docker container technology. The evolution of microservice architecture allows applications to be structured into independent modular components making them easier to manage and scale. As a special case, the containerized sentiment analysis microservice is deployed using popular classification approaches. We implement and compare eight machine learning algorithms: Multinomial Naive Bayes, Decision Tree, Random Forest, K-Nearest Neighbour, AdaBoost, Support Vector Machine, Multilayer Perceptron, and Stochastic Gradient Descent to analyze and classify the tweets into positive, negative, and neutral sentiments. Experimental results procured for the Twitter US Airline Sentiment dataset show that Support Vector Machine, Multinomial Naive Bayes, Stochastic Gradient Descent, and Random Forest outperform the other algorithms. We believe that this research study will assist companies and organizations to improve their services by precisely analyzing Twitter data.",IEEE conference,no,"['containerized', 'sentiment', 'analysis', 'container', 'getting', 'popularity', 'virtual', 'machine', 'offering', 'benefit', 'virtualization', 'along', 'performance', 'nearby', 'bare', 'metal', 'support', 'docker', 'container', 'among', 'various', 'provider', 'made', 'solution', 'developer', 'paper', 'elaborate', 'containerized', 'leveraging', 'lightweight', 'docker', 'container', 'technology', 'evolution', 'allows', 'structured', 'independent', 'modular', 'making', 'easier', 'manage', 'scale', 'special', 'case', 'containerized', 'sentiment', 'analysis', 'deployed', 'using', 'popular', 'classification', 'implement', 'compare', 'eight', 'machine', 'learning', 'algorithm', 'naive', 'decision', 'tree', 'random', 'forest', 'support', 'vector', 'machine', 'stochastic', 'gradient', 'analyze', 'classify', 'positive', 'sentiment', 'experimental', 'result', 'twitter', 'u', 'sentiment', 'dataset', 'show', 'support', 'vector', 'machine', 'naive', 'stochastic', 'gradient', 'random', 'forest', 'algorithm', 'believe', 'research', 'study', 'assist', 'company', 'organization', 'improve', 'precisely', 'analyzing', 'twitter']"
"helyOS: A customized off-the-shelf solution for autonomous driving applications in delimited areas Microservice Architectures (MSA), known to successfully handle complex software systems, are emerging as the new paradigm for automotive software. The design of an MSA requires correct subdivision of the software system and implementation of the communication between components. These tasks demand both software expertise and domain knowledge. In this context, we developed an MSA framework pre-tailored to meet the requirements of autonomous driving applications in delimited areas - the helyOS framework. The framework decomposes complex applications in predefined microservice domains and provides a communication backbone for event messages and data. This paper demonstrates how such a tailored MSA framework can accelerate the development by prompting a quick start for the integration of motion planning algorithms, device controllers, vehicles simulators and web-browser interfaces.",helyOS: A customized off-the-shelf solution for autonomous driving applications in delimited areas,"Microservice Architectures (MSA), known to successfully handle complex software systems, are emerging as the new paradigm for automotive software. The design of an MSA requires correct subdivision of the software system and implementation of the communication between components. These tasks demand both software expertise and domain knowledge. In this context, we developed an MSA framework pre-tailored to meet the requirements of autonomous driving applications in delimited areas - the helyOS framework. The framework decomposes complex applications in predefined microservice domains and provides a communication backbone for event messages and data. This paper demonstrates how such a tailored MSA framework can accelerate the development by prompting a quick start for the integration of motion planning algorithms, device controllers, vehicles simulators and web-browser interfaces.",IEEE conference,no,"['customized', 'solution', 'autonomous', 'driving', 'area', 'msa', 'known', 'successfully', 'handle', 'complex', 'emerging', 'new', 'paradigm', 'automotive', 'design', 'msa', 'requires', 'correct', 'implementation', 'communication', 'task', 'demand', 'expertise', 'domain', 'knowledge', 'context', 'developed', 'msa', 'framework', 'meet', 'requirement', 'autonomous', 'driving', 'area', 'framework', 'framework', 'decomposes', 'complex', 'predefined', 'domain', 'provides', 'communication', 'event', 'message', 'paper', 'demonstrates', 'tailored', 'msa', 'framework', 'accelerate', 'development', 'start', 'integration', 'planning', 'algorithm', 'device', 'controller', 'vehicle', 'simulator', 'interface']"
"PMDC: Programmable Mobile Device Clouds for Convenient and Efficient Service Provisioning Modern mobile devices feature ever increasing computational, sensory, and network resources, which can be shared to execute tasks on behalf of nearby devices. Mobile device clouds (MDCs) facilitate such distributed execution by exposing the collective resources of a set of nearby mobile devices through a unified programming interface. However, the true potential of MDCs remains untapped, as they fail to provide practical programming support for developers to execute distributed functionalities. To address this problem, we introduce a microservice-based Programmable MDC architecture (PMDC), highly customized for the unique features of MDC environments. PMDC conveniently provisions functionalities as microservices, which are deployed on MDC devices on demand. PMDC features a novel domain specific language that provides abstractions for concisely expressing fine-grained control over the procedures of device capability sharing and microservice execution. Furthermore, PMDC introduces a new system component-the microservice gateway, which reconciles the supply of available device capabilities and the demand for microservice execution to distribute microservices within an MDC. Our evaluation shows that MDCs, expressed by developers through the PMDC declarative programming interface, exhibit low energy consumption and high performance.",PMDC: Programmable Mobile Device Clouds for Convenient and Efficient Service Provisioning,"Modern mobile devices feature ever increasing computational, sensory, and network resources, which can be shared to execute tasks on behalf of nearby devices. Mobile device clouds (MDCs) facilitate such distributed execution by exposing the collective resources of a set of nearby mobile devices through a unified programming interface. However, the true potential of MDCs remains untapped, as they fail to provide practical programming support for developers to execute distributed functionalities. To address this problem, we introduce a microservice-based Programmable MDC architecture (PMDC), highly customized for the unique features of MDC environments. PMDC conveniently provisions functionalities as microservices, which are deployed on MDC devices on demand. PMDC features a novel domain specific language that provides abstractions for concisely expressing fine-grained control over the procedures of device capability sharing and microservice execution. Furthermore, PMDC introduces a new system component-the microservice gateway, which reconciles the supply of available device capabilities and the demand for microservice execution to distribute microservices within an MDC. Our evaluation shows that MDCs, expressed by developers through the PMDC declarative programming interface, exhibit low energy consumption and high performance.",IEEE conference,no,"['pmdc', 'programmable', 'mobile', 'device', 'convenient', 'efficient', 'provisioning', 'modern', 'mobile', 'device', 'feature', 'ever', 'increasing', 'computational', 'sensory', 'network', 'resource', 'shared', 'execute', 'task', 'nearby', 'device', 'mobile', 'device', 'mdcs', 'facilitate', 'distributed', 'execution', 'exposing', 'resource', 'set', 'nearby', 'mobile', 'device', 'unified', 'programming', 'interface', 'however', 'true', 'potential', 'mdcs', 'remains', 'fail', 'provide', 'practical', 'programming', 'support', 'developer', 'execute', 'distributed', 'functionality', 'address', 'problem', 'introduce', 'programmable', 'mdc', 'pmdc', 'highly', 'customized', 'unique', 'feature', 'mdc', 'environment', 'pmdc', 'provision', 'functionality', 'deployed', 'mdc', 'device', 'demand', 'pmdc', 'feature', 'novel', 'domain', 'specific', 'language', 'provides', 'abstraction', 'control', 'procedure', 'device', 'capability', 'sharing', 'execution', 'furthermore', 'pmdc', 'introduces', 'new', 'gateway', 'supply', 'available', 'device', 'capability', 'demand', 'execution', 'distribute', 'within', 'mdc', 'evaluation', 'show', 'mdcs', 'expressed', 'developer', 'pmdc', 'declarative', 'programming', 'interface', 'exhibit', 'low', 'energy', 'consumption', 'high', 'performance']"
"Performance Study of Kubernetes Cluster Deployed on Openstack,VMs and BareMetal Kubernetes is an OpenSource Orchestration tool used for deploying and managing containerized applications at scale. Kubernetes easily manages the cluster with a master and worker nodes in which the Pods are been hosted. Nowadays many Cloud Infrastructure Providers like AWS, GoogleCloud, and Microsoft Azure understand the importance of Kubernetes and have added these services to their products. Openstack is a novel and highly manageable OpenSource cloud IaaS platform where the components manage huge pools of compute, storage, and networking resources. In this experimental work we studied and compared the local deployment of Kubernetes Cluster using minikube with the VM deployment and OpenStack deployment. A demo application with microservices Architecture has been taken for this study in which we compare the CPU and Memory Usage on the deployment of the cluster in OpenStack, VMs and BareMetal. The preliminary results show that the bare-metal deployment outperforms the other deployments in both Computing and Memory intensive applications.","Performance Study of Kubernetes Cluster Deployed on Openstack,VMs and BareMetal","Kubernetes is an OpenSource Orchestration tool used for deploying and managing containerized applications at scale. Kubernetes easily manages the cluster with a master and worker nodes in which the Pods are been hosted. Nowadays many Cloud Infrastructure Providers like AWS, GoogleCloud, and Microsoft Azure understand the importance of Kubernetes and have added these services to their products. Openstack is a novel and highly manageable OpenSource cloud IaaS platform where the components manage huge pools of compute, storage, and networking resources. In this experimental work we studied and compared the local deployment of Kubernetes Cluster using minikube with the VM deployment and OpenStack deployment. A demo application with microservices Architecture has been taken for this study in which we compare the CPU and Memory Usage on the deployment of the cluster in OpenStack, VMs and BareMetal. The preliminary results show that the bare-metal deployment outperforms the other deployments in both Computing and Memory intensive applications.",IEEE conference,no,"['performance', 'study', 'kubernetes', 'cluster', 'deployed', 'openstack', 'vms', 'kubernetes', 'orchestration', 'tool', 'used', 'deploying', 'managing', 'containerized', 'scale', 'kubernetes', 'easily', 'manages', 'cluster', 'master', 'worker', 'node', 'pod', 'hosted', 'nowadays', 'many', 'infrastructure', 'provider', 'like', 'aws', 'microsoft', 'azure', 'understand', 'importance', 'kubernetes', 'added', 'product', 'openstack', 'novel', 'highly', 'manageable', 'iaa', 'platform', 'manage', 'huge', 'pool', 'compute', 'storage', 'networking', 'resource', 'experimental', 'work', 'studied', 'compared', 'local', 'deployment', 'kubernetes', 'cluster', 'using', 'vm', 'deployment', 'openstack', 'deployment', 'demo', 'taken', 'study', 'compare', 'cpu', 'memory', 'usage', 'deployment', 'cluster', 'openstack', 'vms', 'preliminary', 'result', 'show', 'deployment', 'outperforms', 'deployment', 'computing', 'memory', 'intensive']"
"Control as a Service Architecture to Support Cloud-Based and Event-Driven Control Application Development Taking advantage of IoT, services and Cloud technologies, the development of Cloud of Things (CoT) changes the way control applications are engineered and developed. CoT helps to move from a dedicated design and development of control applications to a Control as a Service vision, relying on Cyber Physical System (CPS) composition and orchestration. This vision requires developing a new architecture to provide a loosely coupled way to interconnect objects and control software, mixing both logical and physical constraints. To fit this challenge, we propose a Control as a Service model, extending the Cloud XaaS model to automation. To this end, we design a Control Service, gathering both functional and non functional specifications to describe each component of the system. By this way, control applications can be implemented by selecting and composing these control services associated to the different sensors, controllers and actuators. Fitting the loosely coupled principle of the service oriented architecture, we introduce a data manager component used to store the information produced and exchanged by control components (sensors and controllers) and an event manager, in charge of invoking the services when all the necessary input information are available. A prototype hosting a simulated smart city control application, distributed on different machines, is presented.",Control as a Service Architecture to Support Cloud-Based and Event-Driven Control Application Development,"Taking advantage of IoT, services and Cloud technologies, the development of Cloud of Things (CoT) changes the way control applications are engineered and developed. CoT helps to move from a dedicated design and development of control applications to a Control as a Service vision, relying on Cyber Physical System (CPS) composition and orchestration. This vision requires developing a new architecture to provide a loosely coupled way to interconnect objects and control software, mixing both logical and physical constraints. To fit this challenge, we propose a Control as a Service model, extending the Cloud XaaS model to automation. To this end, we design a Control Service, gathering both functional and non functional specifications to describe each component of the system. By this way, control applications can be implemented by selecting and composing these control services associated to the different sensors, controllers and actuators. Fitting the loosely coupled principle of the service oriented architecture, we introduce a data manager component used to store the information produced and exchanged by control components (sensors and controllers) and an event manager, in charge of invoking the services when all the necessary input information are available. A prototype hosting a simulated smart city control application, distributed on different machines, is presented.",IEEE conference,no,"['control', 'support', 'control', 'development', 'taking', 'advantage', 'iot', 'technology', 'development', 'thing', 'change', 'way', 'control', 'developed', 'help', 'move', 'dedicated', 'design', 'development', 'control', 'control', 'vision', 'relying', 'cyber', 'physical', 'cps', 'composition', 'orchestration', 'vision', 'requires', 'developing', 'new', 'provide', 'loosely', 'coupled', 'way', 'object', 'control', 'logical', 'physical', 'constraint', 'fit', 'challenge', 'propose', 'control', 'model', 'extending', 'xaas', 'model', 'automation', 'end', 'design', 'control', 'gathering', 'functional', 'functional', 'specification', 'describe', 'way', 'control', 'implemented', 'selecting', 'composing', 'control', 'associated', 'different', 'sensor', 'controller', 'actuator', 'loosely', 'coupled', 'principle', 'oriented', 'introduce', 'manager', 'used', 'store', 'information', 'produced', 'control', 'sensor', 'controller', 'event', 'manager', 'charge', 'invoking', 'necessary', 'input', 'information', 'available', 'prototype', 'hosting', 'simulated', 'smart', 'city', 'control', 'distributed', 'different', 'machine', 'presented']"
"Dev-for-Operations and Multi-sided Platform for Next Generation Platform as a Service This paper presents two new challenges for the Telco ecosystem transformation in the era of cloud-native microservice-based architectures. (1) Development-for-Operations (Dev-for-Operations) impacts not only the overall workflow for deploying a Platform as a Service (PaaS) in an open foundry environment, but also the Telco business as well as operational models to achieve an economy of scope and an economy of scale. (2) For that purpose, we construct an integrative platform business model in the form of a Multi-Sided Platform (MSP) for building Telco PaaSes. The proposed MSP based architecture enables a multi-organizational ecosystem with increased automation possibilities for Telco-grade service creation and operation. The paper describes how the Dev-for-Operations and MSP lift constraints and offers an effective way for next-generation PaaS building, while mutually reinforcing each other in the Next Generation Platform as a Service (NGPaaS) framework.",Dev-for-Operations and Multi-sided Platform for Next Generation Platform as a Service,"This paper presents two new challenges for the Telco ecosystem transformation in the era of cloud-native microservice-based architectures. (1) Development-for-Operations (Dev-for-Operations) impacts not only the overall workflow for deploying a Platform as a Service (PaaS) in an open foundry environment, but also the Telco business as well as operational models to achieve an economy of scope and an economy of scale. (2) For that purpose, we construct an integrative platform business model in the form of a Multi-Sided Platform (MSP) for building Telco PaaSes. The proposed MSP based architecture enables a multi-organizational ecosystem with increased automation possibilities for Telco-grade service creation and operation. The paper describes how the Dev-for-Operations and MSP lift constraints and offers an effective way for next-generation PaaS building, while mutually reinforcing each other in the Next Generation Platform as a Service (NGPaaS) framework.",IEEE conference,no,"['platform', 'next', 'generation', 'platform', 'paper', 'present', 'two', 'new', 'challenge', 'telco', 'ecosystem', 'transformation', 'era', 'impact', 'overall', 'workflow', 'deploying', 'platform', 'paas', 'open', 'foundry', 'environment', 'also', 'telco', 'business', 'well', 'operational', 'model', 'achieve', 'economy', 'scope', 'economy', 'scale', 'purpose', 'construct', 'platform', 'business', 'model', 'form', 'platform', 'msp', 'building', 'telco', 'proposed', 'msp', 'based', 'enables', 'ecosystem', 'increased', 'automation', 'possibility', 'creation', 'operation', 'paper', 'describes', 'msp', 'constraint', 'offer', 'effective', 'way', 'paas', 'building', 'next', 'generation', 'platform', 'framework']"
"Heterogeneity-aware Load Balancing in Serverless Computing Environments The current trend in cloud computing is the growing adoption of microservices. Microservices are specialized components of an application that can be deployed and scaled individually, potentially replacing monolithic applications. To meet this demand, cloud providers offer function-as-a-service (FaaS) or serverless computing. Additionally, there is an increasing use of containerized environments in Internet of Things (IoT) applications. It has been shown that implementing serverless computing at the edge of an IoT network for executing tasks can reduce the overall execution time of these tasks. In cloud and edge computing environments, the infrastructure typically exhibits heterogeneity, and this heterogeneity can significantly impact the performance of different invocations of a FaaS function. This paper addresses infrastructure heterogeneity by partitioning a heterogeneous cluster into homogeneous pools based on similar resource characteristics and profiling function performance. The key contributions of this paper are: (1) a serverless architecture design that extends the function placement and load-balancing capabilities of current serverless platforms to effectively manage infrastructure heterogeneity, (2) a load-balancing approach that efficiently utilizes multi-core hardware, and (3) an implementation of the architecture, thereby evaluating the impact of the extended capabilities.",Heterogeneity-aware Load Balancing in Serverless Computing Environments,"The current trend in cloud computing is the growing adoption of microservices. Microservices are specialized components of an application that can be deployed and scaled individually, potentially replacing monolithic applications. To meet this demand, cloud providers offer function-as-a-service (FaaS) or serverless computing. Additionally, there is an increasing use of containerized environments in Internet of Things (IoT) applications. It has been shown that implementing serverless computing at the edge of an IoT network for executing tasks can reduce the overall execution time of these tasks. In cloud and edge computing environments, the infrastructure typically exhibits heterogeneity, and this heterogeneity can significantly impact the performance of different invocations of a FaaS function. This paper addresses infrastructure heterogeneity by partitioning a heterogeneous cluster into homogeneous pools based on similar resource characteristics and profiling function performance. The key contributions of this paper are: (1) a serverless architecture design that extends the function placement and load-balancing capabilities of current serverless platforms to effectively manage infrastructure heterogeneity, (2) a load-balancing approach that efficiently utilizes multi-core hardware, and (3) an implementation of the architecture, thereby evaluating the impact of the extended capabilities.",IEEE conference,no,"['load', 'balancing', 'serverless', 'computing', 'environment', 'current', 'trend', 'computing', 'growing', 'adoption', 'specialized', 'deployed', 'scaled', 'individually', 'potentially', 'replacing', 'monolithic', 'meet', 'demand', 'provider', 'offer', 'faa', 'serverless', 'computing', 'additionally', 'increasing', 'use', 'containerized', 'environment', 'internet', 'thing', 'iot', 'shown', 'implementing', 'serverless', 'computing', 'edge', 'iot', 'network', 'executing', 'task', 'reduce', 'overall', 'execution', 'time', 'task', 'edge', 'computing', 'environment', 'infrastructure', 'typically', 'exhibit', 'heterogeneity', 'heterogeneity', 'significantly', 'impact', 'performance', 'different', 'invocation', 'faa', 'function', 'paper', 'address', 'infrastructure', 'heterogeneity', 'partitioning', 'heterogeneous', 'cluster', 'homogeneous', 'pool', 'based', 'similar', 'resource', 'characteristic', 'profiling', 'function', 'performance', 'key', 'contribution', 'paper', 'serverless', 'design', 'function', 'placement', 'capability', 'current', 'serverless', 'platform', 'effectively', 'manage', 'infrastructure', 'heterogeneity', 'efficiently', 'utilizes', 'hardware', 'implementation', 'thereby', 'evaluating', 'impact', 'extended', 'capability']"
"Graph Attention Networks and Deep Q-Learning for Service Mesh Optimization: A Digital Twinning Approach In the realm of cloud native environments, Ku-bernetes has emerged as the de facto orchestration system for containers, and the service mesh architecture, with its interconnected microservices, has become increasingly prominent. Efficient scheduling and resource allocation for these microservices play a pivotal role in achieving high performance and maintaining system reliability. In this paper, we introduce a novel approach for container scheduling within Kubernetes clusters, leveraging Graph Attention Networks (GATs) for representation learning. Our proposed method captures the intricate dependencies among containers and services by constructing a representation graph. The deep Q-learning algorithm is then employed to optimize scheduling decisions, focusing on container-to-node placements, CPU request-response allocation, and adherence to node affinity and anti-affinity rules. Our experiments demonstrate that our GATs-based method outperforms traditional scheduling strategies, leading to enhanced resource utilization, reduced service latency, and improved overall system throughput. The insights gleaned from this study pave the way for a new frontier in cloud native performance optimization and offer tangible benefits to industries adopting microservice-based architectures.",Graph Attention Networks and Deep Q-Learning for Service Mesh Optimization: A Digital Twinning Approach,"In the realm of cloud native environments, Ku-bernetes has emerged as the de facto orchestration system for containers, and the service mesh architecture, with its interconnected microservices, has become increasingly prominent. Efficient scheduling and resource allocation for these microservices play a pivotal role in achieving high performance and maintaining system reliability. In this paper, we introduce a novel approach for container scheduling within Kubernetes clusters, leveraging Graph Attention Networks (GATs) for representation learning. Our proposed method captures the intricate dependencies among containers and services by constructing a representation graph. The deep Q-learning algorithm is then employed to optimize scheduling decisions, focusing on container-to-node placements, CPU request-response allocation, and adherence to node affinity and anti-affinity rules. Our experiments demonstrate that our GATs-based method outperforms traditional scheduling strategies, leading to enhanced resource utilization, reduced service latency, and improved overall system throughput. The insights gleaned from this study pave the way for a new frontier in cloud native performance optimization and offer tangible benefits to industries adopting microservice-based architectures.",IEEE conference,no,"['graph', 'attention', 'network', 'deep', 'mesh', 'optimization', 'digital', 'realm', 'native', 'environment', 'emerged', 'de', 'facto', 'orchestration', 'container', 'mesh', 'interconnected', 'become', 'increasingly', 'prominent', 'efficient', 'scheduling', 'resource', 'allocation', 'play', 'pivotal', 'role', 'achieving', 'high', 'performance', 'maintaining', 'reliability', 'paper', 'introduce', 'novel', 'container', 'scheduling', 'within', 'kubernetes', 'cluster', 'leveraging', 'graph', 'attention', 'network', 'representation', 'learning', 'proposed', 'method', 'capture', 'intricate', 'dependency', 'among', 'container', 'constructing', 'representation', 'graph', 'deep', 'algorithm', 'employed', 'optimize', 'scheduling', 'decision', 'focusing', 'placement', 'cpu', 'allocation', 'node', 'affinity', 'rule', 'experiment', 'demonstrate', 'method', 'outperforms', 'traditional', 'scheduling', 'strategy', 'leading', 'enhanced', 'resource', 'utilization', 'reduced', 'latency', 'improved', 'overall', 'throughput', 'insight', 'study', 'pave', 'way', 'new', 'native', 'performance', 'optimization', 'offer', 'tangible', 'benefit', 'industry', 'adopting']"
"A Microservice-Based Architecture for the Development of Accessible, Crowdsensing-Based Mobility Platforms Crowdsensing is a powerful approach to collaboratively build representations of specific aspects of reality which are of great interest for people with special needs. In this paper, we present an evolution of the classical, vertical approach to detect urban barriers and other features to later exploit this knowledge in accessible route planning. By exposing every single part of the process as a microservice, we achieve the ability to develop novel applications as orchestration of available components. Moreover, in the resulting platform, we leverage the possibility to share data between different applications in a controlled environment.","A Microservice-Based Architecture for the Development of Accessible, Crowdsensing-Based Mobility Platforms","Crowdsensing is a powerful approach to collaboratively build representations of specific aspects of reality which are of great interest for people with special needs. In this paper, we present an evolution of the classical, vertical approach to detect urban barriers and other features to later exploit this knowledge in accessible route planning. By exposing every single part of the process as a microservice, we achieve the ability to develop novel applications as orchestration of available components. Moreover, in the resulting platform, we leverage the possibility to share data between different applications in a controlled environment.",IEEE conference,no,"['development', 'accessible', 'mobility', 'platform', 'crowdsensing', 'powerful', 'build', 'representation', 'specific', 'aspect', 'reality', 'great', 'interest', 'people', 'special', 'need', 'paper', 'present', 'evolution', 'classical', 'vertical', 'detect', 'urban', 'barrier', 'feature', 'later', 'exploit', 'knowledge', 'accessible', 'route', 'planning', 'exposing', 'every', 'single', 'part', 'process', 'achieve', 'ability', 'develop', 'novel', 'orchestration', 'available', 'moreover', 'resulting', 'platform', 'leverage', 'possibility', 'share', 'different', 'controlled', 'environment']"
"A Petri Net-based Formal Modeling for Microservices Auto-scaling Microservices auto-scaling is an attracking research domain focusing on the dynamic definition of strategies to optimize system efficiency and performance while minimizing service cost. An efficient auto-scaling strikes a balance between individual microservices quality requirements while collaborating to uphold the overall system quality. Maintaining such balance needs a formal method to establish relevant parameters to ensure quality attributes and enable prior accuracy and efficiency verification. To that end, we extend the Petri net model to define Hierarchical Parallel Petri nets (HPPNs) to correctly model both autonomy and cooperation of microservices. Unlike conventional Petri nets, HPPNs consider trade-offs between tokens during transitions, enabling the computation of compromises between multiple quality dimensions and the adaptation strategies. Additionally, preliminary testing mechanisms define the qualities of individual components, providing a solid foundation for assessing microservices’ performance and quality attributes.",A Petri Net-based Formal Modeling for Microservices Auto-scaling,"Microservices auto-scaling is an attracking research domain focusing on the dynamic definition of strategies to optimize system efficiency and performance while minimizing service cost. An efficient auto-scaling strikes a balance between individual microservices quality requirements while collaborating to uphold the overall system quality. Maintaining such balance needs a formal method to establish relevant parameters to ensure quality attributes and enable prior accuracy and efficiency verification. To that end, we extend the Petri net model to define Hierarchical Parallel Petri nets (HPPNs) to correctly model both autonomy and cooperation of microservices. Unlike conventional Petri nets, HPPNs consider trade-offs between tokens during transitions, enabling the computation of compromises between multiple quality dimensions and the adaptation strategies. Additionally, preliminary testing mechanisms define the qualities of individual components, providing a solid foundation for assessing microservices’ performance and quality attributes.",IEEE conference,no,"['petri', 'formal', 'modeling', 'research', 'domain', 'focusing', 'dynamic', 'definition', 'strategy', 'optimize', 'efficiency', 'performance', 'minimizing', 'cost', 'efficient', 'balance', 'individual', 'quality', 'requirement', 'overall', 'quality', 'maintaining', 'balance', 'need', 'formal', 'method', 'establish', 'relevant', 'parameter', 'ensure', 'quality', 'attribute', 'enable', 'prior', 'accuracy', 'efficiency', 'verification', 'end', 'extend', 'petri', 'net', 'model', 'define', 'hierarchical', 'parallel', 'petri', 'net', 'correctly', 'model', 'autonomy', 'unlike', 'conventional', 'petri', 'net', 'consider', 'transition', 'enabling', 'computation', 'compromise', 'multiple', 'quality', 'dimension', 'adaptation', 'strategy', 'additionally', 'preliminary', 'testing', 'mechanism', 'define', 'quality', 'individual', 'providing', 'solid', 'foundation', 'assessing', 'performance', 'quality', 'attribute']"
"An Approach to Designing Intelligent RAN Controller Services The concept of Open Radio Access Network (O-RAN) drives the RAN transition toward intellectualization, virtualization, and interoperability. The core of O-RAN is the separation of near real time intelligent control from the non-real time one, and the open interfaces between virtualized components. While some O-RAN trails are being deployed, there are some open issues related to the design of intelligent solutions and extension of open interfaces. The paper presents an approach to software design of virtualized O-RAN functionality following the principles of Service Based Architecture. The RAN Intelligent Controller (RIC) functions are designed as microservices which enable application management and Machine Learning (ML) model management. To illustrate the approach feasibility, the ML model lifecycle is modelled from ML model designer and O-RAN point of views. Models are formally described and verified.",An Approach to Designing Intelligent RAN Controller Services,"The concept of Open Radio Access Network (O-RAN) drives the RAN transition toward intellectualization, virtualization, and interoperability. The core of O-RAN is the separation of near real time intelligent control from the non-real time one, and the open interfaces between virtualized components. While some O-RAN trails are being deployed, there are some open issues related to the design of intelligent solutions and extension of open interfaces. The paper presents an approach to software design of virtualized O-RAN functionality following the principles of Service Based Architecture. The RAN Intelligent Controller (RIC) functions are designed as microservices which enable application management and Machine Learning (ML) model management. To illustrate the approach feasibility, the ML model lifecycle is modelled from ML model designer and O-RAN point of views. Models are formally described and verified.",IEEE conference,no,"['designing', 'intelligent', 'ran', 'controller', 'concept', 'open', 'radio', 'access', 'network', 'drive', 'ran', 'transition', 'toward', 'virtualization', 'interoperability', 'core', 'separation', 'near', 'real', 'time', 'intelligent', 'control', 'time', 'one', 'open', 'interface', 'virtualized', 'deployed', 'open', 'issue', 'related', 'design', 'intelligent', 'solution', 'extension', 'open', 'interface', 'paper', 'present', 'design', 'virtualized', 'functionality', 'following', 'principle', 'based', 'ran', 'intelligent', 'controller', 'ric', 'function', 'designed', 'enable', 'management', 'machine', 'learning', 'ml', 'model', 'management', 'illustrate', 'feasibility', 'ml', 'model', 'lifecycle', 'ml', 'model', 'designer', 'point', 'view', 'model', 'described', 'verified']"
"SmartOClock: Workload- and Risk-Aware Overclocking in the Cloud Operating server components beyond their voltage and power design limit (i.e., overclocking) enables improving performance and lowering cost for cloud workloads. However, overclocking can significantly degrade component lifetime, increase power draw, and cause power capping events, eventually diminishing the performance benefits. In this paper, we characterize the impact of overclocking on cloud workloads by studying their profiles from production deployments. Based on the characterization insights, we propose SmartOClock, the first distributed overclocking management platform specifically designed for cloud environments. SmartOClock is a workload-aware scheme that relies on power predictions to heterogeneously distribute the power budgets across its servers based on their needs and then enforce budget compliance locally, per-server, in a decentralized manner. SmartOClock reduces the tail latency by 9%, application cost by 30% and total energy consumption by 10% for latencysensitive microservices on a 36-server deployment. Simulation analysis using production traces show that SmartOClock reduces the number of power capping events by up to 95% while increasing the overclocking success rate by up to 62%. We also describe lessons from building a first-of-its-kind overclockable cluster in Microsoft Azure for production experiments.",SmartOClock: Workload- and Risk-Aware Overclocking in the Cloud,"Operating server components beyond their voltage and power design limit (i.e., overclocking) enables improving performance and lowering cost for cloud workloads. However, overclocking can significantly degrade component lifetime, increase power draw, and cause power capping events, eventually diminishing the performance benefits. In this paper, we characterize the impact of overclocking on cloud workloads by studying their profiles from production deployments. Based on the characterization insights, we propose SmartOClock, the first distributed overclocking management platform specifically designed for cloud environments. SmartOClock is a workload-aware scheme that relies on power predictions to heterogeneously distribute the power budgets across its servers based on their needs and then enforce budget compliance locally, per-server, in a decentralized manner. SmartOClock reduces the tail latency by 9%, application cost by 30% and total energy consumption by 10% for latencysensitive microservices on a 36-server deployment. Simulation analysis using production traces show that SmartOClock reduces the number of power capping events by up to 95% while increasing the overclocking success rate by up to 62%. We also describe lessons from building a first-of-its-kind overclockable cluster in Microsoft Azure for production experiments.",IEEE conference,no,"['smartoclock', 'overclocking', 'operating', 'server', 'beyond', 'voltage', 'power', 'design', 'limit', 'overclocking', 'enables', 'improving', 'performance', 'lowering', 'cost', 'workload', 'however', 'overclocking', 'significantly', 'lifetime', 'increase', 'power', 'cause', 'power', 'event', 'eventually', 'performance', 'benefit', 'paper', 'characterize', 'impact', 'overclocking', 'workload', 'profile', 'production', 'deployment', 'based', 'characterization', 'insight', 'propose', 'smartoclock', 'first', 'distributed', 'overclocking', 'management', 'platform', 'specifically', 'designed', 'environment', 'smartoclock', 'scheme', 'relies', 'power', 'prediction', 'distribute', 'power', 'across', 'server', 'based', 'need', 'locally', 'decentralized', 'manner', 'smartoclock', 'reduces', 'tail', 'latency', 'cost', 'total', 'energy', 'consumption', 'deployment', 'simulation', 'analysis', 'using', 'production', 'trace', 'show', 'smartoclock', 'reduces', 'number', 'power', 'event', 'increasing', 'overclocking', 'success', 'rate', 'also', 'describe', 'lesson', 'building', 'cluster', 'microsoft', 'azure', 'production', 'experiment']"
"Auto Scaling Infrastructure with Monitoring Tools using Linux Server on Cloud Cloud computing is the term that has gained widespread usage over these last few years. Due to the rapid increase in the use of information in the digital age of the 21st century, it is increasingly becoming a more attractive option for individuals and organizations to manage all their essential data, projects, and collaborations, rather than relying solely on in-house computers. The user's requirement for hardware and software is reduced via cloud computing. The interface software of cloud computing systems, typically as simple as a web browser, is the only thing the user must operate, and the Cloud network handles the rest. To decrease operational costs, both business and government organizations are adopting cloud computing, seeking a flexible and adaptable solution for the supply and delivery of their product services. Microservices and decoupled apps are becoming more popular. These container-based architectures make it easier to build sophisticated SaaS apps quickly, but managing and creating microservices can be a daunting task. Managing and creating microservices that involve a wide range of diverse functions, including handling and storing information, and performing predictive and prescriptive analysis, can be a challenging undertaking. Establishing auto scaling infrastructure on doud can be challenging due to several reasons, some of which are: understanding the application architecture, setting up monitoring, scaling policies, cost optimization and implementation complexity. Server farms include the tremendous and heterogeneous virtualized frameworks, which are continually extending and broadening after sometime are the essential starting point for registering specialized organizations. These solutions also need to be integrated into existing systems while adhering to Quality of Service (QoS) requirements. The principal objective of this work is to propose an on-premise design to leverage Kubernetes and Docker containers to improve the quality of service based on resource usage and Service Level Objectives (SLOs). The Prometheus Administrator set up is used to perform namespace checking. Normally, doud providers enable their own monitoring tools (like CloudWatch) for monitoring CPU, storage and network usage, service component, however these tools cannot monitor the service component. Additionally, the advancements have restricted the capacity to follow QoS highlights at the application level (like security and execution) since the main focus will be dedicated towards the equipment assets. These types of node-level monitoring make it difficult to scale requests and deploy pods to match the demand. Infrastructure monitoring should enable runtime changes to monitor the requirements or metric operationalization should be done on those criteria without modifying the underlying infrastructure.",Auto Scaling Infrastructure with Monitoring Tools using Linux Server on Cloud,"Cloud computing is the term that has gained widespread usage over these last few years. Due to the rapid increase in the use of information in the digital age of the 21st century, it is increasingly becoming a more attractive option for individuals and organizations to manage all their essential data, projects, and collaborations, rather than relying solely on in-house computers. The user's requirement for hardware and software is reduced via cloud computing. The interface software of cloud computing systems, typically as simple as a web browser, is the only thing the user must operate, and the Cloud network handles the rest. To decrease operational costs, both business and government organizations are adopting cloud computing, seeking a flexible and adaptable solution for the supply and delivery of their product services. Microservices and decoupled apps are becoming more popular. These container-based architectures make it easier to build sophisticated SaaS apps quickly, but managing and creating microservices can be a daunting task. Managing and creating microservices that involve a wide range of diverse functions, including handling and storing information, and performing predictive and prescriptive analysis, can be a challenging undertaking. Establishing auto scaling infrastructure on doud can be challenging due to several reasons, some of which are: understanding the application architecture, setting up monitoring, scaling policies, cost optimization and implementation complexity. Server farms include the tremendous and heterogeneous virtualized frameworks, which are continually extending and broadening after sometime are the essential starting point for registering specialized organizations. These solutions also need to be integrated into existing systems while adhering to Quality of Service (QoS) requirements. The principal objective of this work is to propose an on-premise design to leverage Kubernetes and Docker containers to improve the quality of service based on resource usage and Service Level Objectives (SLOs). The Prometheus Administrator set up is used to perform namespace checking. Normally, doud providers enable their own monitoring tools (like CloudWatch) for monitoring CPU, storage and network usage, service component, however these tools cannot monitor the service component. Additionally, the advancements have restricted the capacity to follow QoS highlights at the application level (like security and execution) since the main focus will be dedicated towards the equipment assets. These types of node-level monitoring make it difficult to scale requests and deploy pods to match the demand. Infrastructure monitoring should enable runtime changes to monitor the requirements or metric operationalization should be done on those criteria without modifying the underlying infrastructure.",IEEE conference,no,"['auto', 'scaling', 'infrastructure', 'monitoring', 'tool', 'using', 'server', 'computing', 'term', 'gained', 'widespread', 'usage', 'last', 'year', 'due', 'rapid', 'increase', 'use', 'information', 'digital', 'age', 'increasingly', 'becoming', 'option', 'individual', 'organization', 'manage', 'essential', 'project', 'collaboration', 'rather', 'relying', 'solely', 'computer', 'user', 'requirement', 'hardware', 'reduced', 'via', 'computing', 'interface', 'computing', 'typically', 'simple', 'web', 'thing', 'user', 'must', 'operate', 'network', 'handle', 'rest', 'decrease', 'operational', 'cost', 'business', 'government', 'organization', 'adopting', 'computing', 'seeking', 'flexible', 'adaptable', 'solution', 'supply', 'delivery', 'product', 'decoupled', 'apps', 'becoming', 'popular', 'make', 'easier', 'build', 'sophisticated', 'saas', 'apps', 'quickly', 'managing', 'creating', 'daunting', 'task', 'managing', 'creating', 'involve', 'wide', 'range', 'diverse', 'function', 'including', 'handling', 'storing', 'information', 'performing', 'predictive', 'analysis', 'challenging', 'establishing', 'auto', 'scaling', 'infrastructure', 'challenging', 'due', 'several', 'reason', 'understanding', 'setting', 'monitoring', 'scaling', 'policy', 'cost', 'optimization', 'implementation', 'complexity', 'server', 'farm', 'include', 'heterogeneous', 'virtualized', 'framework', 'extending', 'essential', 'starting', 'point', 'specialized', 'organization', 'solution', 'also', 'need', 'integrated', 'existing', 'quality', 'qos', 'requirement', 'principal', 'objective', 'work', 'propose', 'design', 'leverage', 'kubernetes', 'docker', 'container', 'improve', 'quality', 'based', 'resource', 'usage', 'level', 'objective', 'slos', 'prometheus', 'administrator', 'set', 'used', 'perform', 'checking', 'provider', 'enable', 'monitoring', 'tool', 'like', 'monitoring', 'cpu', 'storage', 'network', 'usage', 'however', 'tool', 'monitor', 'additionally', 'advancement', 'capacity', 'follow', 'qos', 'highlight', 'level', 'like', 'security', 'execution', 'since', 'main', 'focus', 'dedicated', 'towards', 'equipment', 'asset', 'type', 'monitoring', 'make', 'difficult', 'scale', 'request', 'deploy', 'pod', 'match', 'demand', 'infrastructure', 'monitoring', 'enable', 'runtime', 'change', 'monitor', 'requirement', 'metric', 'done', 'criterion', 'without', 'modifying', 'underlying', 'infrastructure']"
"Demonstration of Closed Loop AI-Driven RAN Controllers Using O-RAN SDR Testbed Open Radio Access Network (O-RAN), a virtualized, modular, and disaggregated design paradigm for 5G/NextG cellular RANs, aims to integrate intelligence into cellular networks, enabling advanced deployment, operation and maintenance of the network. A key component of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the controllability of RAN elements through data-driven, closed-loop, intelligent control using software microservices called extended Applications (xApps). In this demonstration, we showcase intelligent RAN control using a custom interference classification xApp hosted within the near-real-time RIC. This demonstration provides a tangible example for designing, testing, and experimenting with AI-driven RAN controllers and exploring various possibilities within the O-RAN system.",Demonstration of Closed Loop AI-Driven RAN Controllers Using O-RAN SDR Testbed,"Open Radio Access Network (O-RAN), a virtualized, modular, and disaggregated design paradigm for 5G/NextG cellular RANs, aims to integrate intelligence into cellular networks, enabling advanced deployment, operation and maintenance of the network. A key component of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the controllability of RAN elements through data-driven, closed-loop, intelligent control using software microservices called extended Applications (xApps). In this demonstration, we showcase intelligent RAN control using a custom interference classification xApp hosted within the near-real-time RIC. This demonstration provides a tangible example for designing, testing, and experimenting with AI-driven RAN controllers and exploring various possibilities within the O-RAN system.",IEEE conference,no,"['demonstration', 'loop', 'ran', 'controller', 'using', 'testbed', 'open', 'radio', 'access', 'network', 'virtualized', 'modular', 'disaggregated', 'design', 'paradigm', 'cellular', 'aim', 'integrate', 'intelligence', 'cellular', 'network', 'enabling', 'advanced', 'deployment', 'operation', 'maintenance', 'network', 'key', 'ran', 'intelligent', 'controller', 'ric', 'facilitates', 'ran', 'element', 'intelligent', 'control', 'using', 'called', 'extended', 'demonstration', 'showcase', 'intelligent', 'ran', 'control', 'using', 'custom', 'interference', 'classification', 'hosted', 'within', 'ric', 'demonstration', 'provides', 'tangible', 'example', 'designing', 'testing', 'ran', 'controller', 'exploring', 'various', 'possibility', 'within']"
"MLOps: Creating powerful AI pipelines by stitching together heterogeneous Machine Learning models As Machine Learning and Deep Learning are being widely adopted in different industries, in many cases there is also a need to use these models together to have a system or a solution that is capable of performing much better on a task than the individual models can. Or even perform tasks that the individual models cannot perform by themselves in isolation. We developed the Acumos AI Platform that provides capabilities to create powerful AI pipelines by stitching together heterogeneous Machine Learning models. We will demonstrate with multiple examples how we can create such powerful AI pipelines which we will call ‘composite ML solutions’ using the Acumos Design Studio. We will use some other components we developed i.e. Acumos Runtime Orchestrator, proto viewer, Data broker, Splitter, and Collator to deploy the aforementioned composite ML solutions as a set of communicating microservices. The individual ML models that are part of the composite solution may not necessarily have been created by the same person or same group or the same organization or have been created in a particular language or use a particular ML framework. But still, Acumos allows these models created in disparate languages and frameworks by different individuals or organizations to be stitched (subject to their compatibility) together to create composite solutions using a simple intuitive GUI and deploy them very easily to various cloud targets. In this paper, we will describe these novel capabilities that we have developed in Acumos and we will show how to create and deploy a composite ML solution or an AI pipeline using this toolset.",MLOps: Creating powerful AI pipelines by stitching together heterogeneous Machine Learning models,"As Machine Learning and Deep Learning are being widely adopted in different industries, in many cases there is also a need to use these models together to have a system or a solution that is capable of performing much better on a task than the individual models can. Or even perform tasks that the individual models cannot perform by themselves in isolation. We developed the Acumos AI Platform that provides capabilities to create powerful AI pipelines by stitching together heterogeneous Machine Learning models. We will demonstrate with multiple examples how we can create such powerful AI pipelines which we will call ‘composite ML solutions’ using the Acumos Design Studio. We will use some other components we developed i.e. Acumos Runtime Orchestrator, proto viewer, Data broker, Splitter, and Collator to deploy the aforementioned composite ML solutions as a set of communicating microservices. The individual ML models that are part of the composite solution may not necessarily have been created by the same person or same group or the same organization or have been created in a particular language or use a particular ML framework. But still, Acumos allows these models created in disparate languages and frameworks by different individuals or organizations to be stitched (subject to their compatibility) together to create composite solutions using a simple intuitive GUI and deploy them very easily to various cloud targets. In this paper, we will describe these novel capabilities that we have developed in Acumos and we will show how to create and deploy a composite ML solution or an AI pipeline using this toolset.",IEEE conference,no,"['mlops', 'creating', 'powerful', 'ai', 'pipeline', 'together', 'heterogeneous', 'machine', 'learning', 'model', 'machine', 'learning', 'deep', 'learning', 'widely', 'adopted', 'different', 'industry', 'many', 'case', 'also', 'need', 'use', 'model', 'together', 'solution', 'capable', 'performing', 'much', 'better', 'task', 'individual', 'model', 'even', 'perform', 'task', 'individual', 'model', 'perform', 'isolation', 'developed', 'acumos', 'ai', 'platform', 'provides', 'capability', 'create', 'powerful', 'ai', 'pipeline', 'together', 'heterogeneous', 'machine', 'learning', 'model', 'demonstrate', 'multiple', 'example', 'create', 'powerful', 'ai', 'pipeline', 'call', 'composite', 'ml', 'solution', 'using', 'acumos', 'design', 'use', 'developed', 'acumos', 'runtime', 'orchestrator', 'broker', 'deploy', 'aforementioned', 'composite', 'ml', 'solution', 'set', 'communicating', 'individual', 'ml', 'model', 'part', 'composite', 'solution', 'may', 'created', 'person', 'group', 'organization', 'created', 'particular', 'language', 'use', 'particular', 'ml', 'framework', 'still', 'acumos', 'allows', 'model', 'created', 'language', 'framework', 'different', 'individual', 'organization', 'subject', 'compatibility', 'together', 'create', 'composite', 'solution', 'using', 'simple', 'intuitive', 'deploy', 'easily', 'various', 'target', 'paper', 'describe', 'novel', 'capability', 'developed', 'acumos', 'show', 'create', 'deploy', 'composite', 'ml', 'solution', 'ai', 'pipeline', 'using']"
"Poster: (Re)-Configuration Framework for Mission-Critical Applications in Edge Environments Mission-critical applications, which must adhere to processing deadlines, can benefit from low latencies offered by the edge. Adapting the Quality of Result allows for targeted processing times by selecting various approximations. Feature models can be employed to manage the resulting multitude of possible configurations. However, the deployment and (re)-configuration process is very time-consuming, making it impractical for mission-critical applications. In this work, we introduce a processing pipeline with components that significantly accelerate online (re)-configuration based on changing latencies compared to the state-of-the-art. Additionally, we address the edge-specific discovery of potential microservice chains capable of executing the application.",Poster: (Re)-Configuration Framework for Mission-Critical Applications in Edge Environments,"Mission-critical applications, which must adhere to processing deadlines, can benefit from low latencies offered by the edge. Adapting the Quality of Result allows for targeted processing times by selecting various approximations. Feature models can be employed to manage the resulting multitude of possible configurations. However, the deployment and (re)-configuration process is very time-consuming, making it impractical for mission-critical applications. In this work, we introduce a processing pipeline with components that significantly accelerate online (re)-configuration based on changing latencies compared to the state-of-the-art. Additionally, we address the edge-specific discovery of potential microservice chains capable of executing the application.",IEEE conference,no,"['framework', 'edge', 'environment', 'must', 'processing', 'benefit', 'low', 'latency', 'offered', 'edge', 'adapting', 'quality', 'result', 'allows', 'processing', 'time', 'selecting', 'various', 'feature', 'model', 'employed', 'manage', 'resulting', 'possible', 'configuration', 'however', 'deployment', 'process', 'making', 'work', 'introduce', 'processing', 'pipeline', 'significantly', 'accelerate', 'online', 'based', 'changing', 'latency', 'compared', 'additionally', 'address', 'discovery', 'potential', 'chain', 'capable', 'executing']"
"Research on Kubernetes Scheduler Optimization Based on real Load With the development of microservices, more and more companies are deploying applications in a containerized manner. Kubernetes is one of the most popular container orchestration tools currently available, which can effectively simplify container management. The scheduler is a core component of Kubernetes, which is responsible for scheduling containers to run on the most appropriate node. The main reference metric for the Kubernetes default scheduler is the resource request rate of the node. It prioritizes scheduling containers to run on nodes with low resource request rates. The resource application rate of a node is calculated by dividing the amount of resources requested by the node by the total amount of resources of the node. This scheduling strategy can meet business requirements in most cases, but it may have problems in certain scenarios. In a certain business scenario, the data processing module of the system needs to run multiple processing tasks on multiple nodes, and the computing resources required for these processing tasks vary. When using the default scheduler, it is difficult for users to predict their actual needs when applying for resources. Therefore, there may be significant differences between the resource application rate of nodes and the actual resource utilization rate of nodes, resulting in significant waste of cluster resources and uneven load. Therefore, this article refers to the relevant research results in the field of Kubernetes scheduling and implements a scheduler based on real load, aiming to solve the possible resource waste and load imbalance when scheduling Pods by default schedulers.",Research on Kubernetes Scheduler Optimization Based on real Load,"With the development of microservices, more and more companies are deploying applications in a containerized manner. Kubernetes is one of the most popular container orchestration tools currently available, which can effectively simplify container management. The scheduler is a core component of Kubernetes, which is responsible for scheduling containers to run on the most appropriate node. The main reference metric for the Kubernetes default scheduler is the resource request rate of the node. It prioritizes scheduling containers to run on nodes with low resource request rates. The resource application rate of a node is calculated by dividing the amount of resources requested by the node by the total amount of resources of the node. This scheduling strategy can meet business requirements in most cases, but it may have problems in certain scenarios. In a certain business scenario, the data processing module of the system needs to run multiple processing tasks on multiple nodes, and the computing resources required for these processing tasks vary. When using the default scheduler, it is difficult for users to predict their actual needs when applying for resources. Therefore, there may be significant differences between the resource application rate of nodes and the actual resource utilization rate of nodes, resulting in significant waste of cluster resources and uneven load. Therefore, this article refers to the relevant research results in the field of Kubernetes scheduling and implements a scheduler based on real load, aiming to solve the possible resource waste and load imbalance when scheduling Pods by default schedulers.",IEEE conference,no,"['research', 'kubernetes', 'scheduler', 'optimization', 'based', 'real', 'load', 'development', 'company', 'deploying', 'containerized', 'manner', 'kubernetes', 'one', 'popular', 'container', 'orchestration', 'tool', 'currently', 'available', 'effectively', 'simplify', 'container', 'management', 'scheduler', 'core', 'kubernetes', 'responsible', 'scheduling', 'container', 'run', 'appropriate', 'node', 'main', 'reference', 'metric', 'kubernetes', 'default', 'scheduler', 'resource', 'request', 'rate', 'node', 'scheduling', 'container', 'run', 'node', 'low', 'resource', 'request', 'rate', 'resource', 'rate', 'node', 'calculated', 'dividing', 'amount', 'resource', 'requested', 'node', 'total', 'amount', 'resource', 'node', 'scheduling', 'strategy', 'meet', 'business', 'requirement', 'case', 'may', 'problem', 'certain', 'scenario', 'certain', 'business', 'scenario', 'processing', 'module', 'need', 'run', 'multiple', 'processing', 'task', 'multiple', 'node', 'computing', 'resource', 'required', 'processing', 'task', 'vary', 'using', 'default', 'scheduler', 'difficult', 'user', 'predict', 'actual', 'need', 'applying', 'resource', 'therefore', 'may', 'significant', 'difference', 'resource', 'rate', 'node', 'actual', 'resource', 'utilization', 'rate', 'node', 'resulting', 'significant', 'waste', 'cluster', 'resource', 'load', 'therefore', 'article', 'refers', 'relevant', 'research', 'result', 'field', 'kubernetes', 'scheduling', 'implement', 'scheduler', 'based', 'real', 'load', 'aiming', 'solve', 'possible', 'resource', 'waste', 'load', 'imbalance', 'scheduling', 'pod', 'default', 'scheduler']"
"Data-Driven Edge Resource Provisioning for Inter-Dependent Microservices with Dynamic Load This paper studies how to provision edge computing and network resources for complex microservice-based applications (MSAs) in face of uncertain and dynamic geo-distributed demands. The complex inter-dependencies between distributed microservice components make load balancing for MSAs extremely challenging, and the dynamic geo-distributed demands exacerbate load imbalance and consequently congestion and performance loss. In this paper, we develop an edge resource provisioning model that accurately captures the inter-dependencies between microservices and their impact on load balancing across both computation and communication resources. We also propose a robust formulation that employs explicit risk estimation and optimization to hedge against potential worst-case load fluctuations, with controlled robustness-resource trade-off. Utilizing a data-driven approach, we provide a solution that provides risk estimation with measurement data of past load geo-distributions. Simulations with real-world datasets have validated that our solution provides the important robustness crucially needed in MSAs, and performs superiorly compared to baselines that neglect either network or inter-dependency constraints.",Data-Driven Edge Resource Provisioning for Inter-Dependent Microservices with Dynamic Load,"This paper studies how to provision edge computing and network resources for complex microservice-based applications (MSAs) in face of uncertain and dynamic geo-distributed demands. The complex inter-dependencies between distributed microservice components make load balancing for MSAs extremely challenging, and the dynamic geo-distributed demands exacerbate load imbalance and consequently congestion and performance loss. In this paper, we develop an edge resource provisioning model that accurately captures the inter-dependencies between microservices and their impact on load balancing across both computation and communication resources. We also propose a robust formulation that employs explicit risk estimation and optimization to hedge against potential worst-case load fluctuations, with controlled robustness-resource trade-off. Utilizing a data-driven approach, we provide a solution that provides risk estimation with measurement data of past load geo-distributions. Simulations with real-world datasets have validated that our solution provides the important robustness crucially needed in MSAs, and performs superiorly compared to baselines that neglect either network or inter-dependency constraints.",IEEE conference,no,"['edge', 'resource', 'provisioning', 'dynamic', 'load', 'paper', 'study', 'provision', 'edge', 'computing', 'network', 'resource', 'complex', 'msas', 'face', 'dynamic', 'demand', 'complex', 'distributed', 'make', 'load', 'balancing', 'msas', 'extremely', 'challenging', 'dynamic', 'demand', 'load', 'imbalance', 'consequently', 'congestion', 'performance', 'loss', 'paper', 'develop', 'edge', 'resource', 'provisioning', 'model', 'accurately', 'capture', 'impact', 'load', 'balancing', 'across', 'computation', 'communication', 'resource', 'also', 'propose', 'robust', 'formulation', 'employ', 'explicit', 'risk', 'estimation', 'optimization', 'potential', 'load', 'controlled', 'utilizing', 'provide', 'solution', 'provides', 'risk', 'estimation', 'measurement', 'past', 'load', 'simulation', 'datasets', 'validated', 'solution', 'provides', 'important', 'robustness', 'needed', 'msas', 'performs', 'compared', 'baseline', 'either', 'network', 'constraint']"
"Workload Management for Power Efficiency in Heterogeneous Data Centers The cloud computing paradigm has recently emerged as a convenient solution for running different workloads on highly parallel and scalable infrastructures. One major appeal of cloud computing is its capability of abstracting hardware resources and making them easy to use. Conversely, one of the major challenges for cloud providers is the energy efficiency improvement of their infrastructures. Aimed at overcoming this challenge, heterogeneous architectures have started to become part of the standard equipment used in data centers. Despite this effort, heterogeneous systems remain difficult to program and manage, while their effectiveness has been proven only in the HPC domain. Cloud workloads are different in nature and a way to exploit heterogeneity effectively is still lacking. This paper takes a first step towards an effective use of heterogeneous architectures in cloud infrastructures. It presents an in-depth analysis of cloud workloads, highlighting where energy efficiency can be obtained. The microservices paradigm is then presented as a way of intelligently partitioning applications in such a way that different components can take advantage of the heterogeneous hardware, thus providing energy efficiency. Finally, the integration of microservices and heterogeneous architectures, as well as the challenge of managing legacy applications, is presented in the context of the OPERA project.",Workload Management for Power Efficiency in Heterogeneous Data Centers,"The cloud computing paradigm has recently emerged as a convenient solution for running different workloads on highly parallel and scalable infrastructures. One major appeal of cloud computing is its capability of abstracting hardware resources and making them easy to use. Conversely, one of the major challenges for cloud providers is the energy efficiency improvement of their infrastructures. Aimed at overcoming this challenge, heterogeneous architectures have started to become part of the standard equipment used in data centers. Despite this effort, heterogeneous systems remain difficult to program and manage, while their effectiveness has been proven only in the HPC domain. Cloud workloads are different in nature and a way to exploit heterogeneity effectively is still lacking. This paper takes a first step towards an effective use of heterogeneous architectures in cloud infrastructures. It presents an in-depth analysis of cloud workloads, highlighting where energy efficiency can be obtained. The microservices paradigm is then presented as a way of intelligently partitioning applications in such a way that different components can take advantage of the heterogeneous hardware, thus providing energy efficiency. Finally, the integration of microservices and heterogeneous architectures, as well as the challenge of managing legacy applications, is presented in the context of the OPERA project.",IEEE conference,no,"['workload', 'management', 'power', 'efficiency', 'heterogeneous', 'center', 'computing', 'paradigm', 'recently', 'emerged', 'convenient', 'solution', 'running', 'different', 'workload', 'highly', 'parallel', 'scalable', 'infrastructure', 'one', 'major', 'computing', 'capability', 'abstracting', 'hardware', 'resource', 'making', 'easy', 'use', 'one', 'major', 'challenge', 'provider', 'energy', 'efficiency', 'improvement', 'infrastructure', 'aimed', 'overcoming', 'challenge', 'heterogeneous', 'started', 'become', 'part', 'standard', 'equipment', 'used', 'center', 'despite', 'effort', 'heterogeneous', 'remain', 'difficult', 'program', 'manage', 'effectiveness', 'proven', 'hpc', 'domain', 'workload', 'different', 'nature', 'way', 'exploit', 'heterogeneity', 'effectively', 'still', 'paper', 'take', 'first', 'step', 'towards', 'effective', 'use', 'heterogeneous', 'infrastructure', 'present', 'analysis', 'workload', 'highlighting', 'energy', 'efficiency', 'obtained', 'paradigm', 'presented', 'way', 'intelligently', 'partitioning', 'way', 'different', 'take', 'advantage', 'heterogeneous', 'hardware', 'thus', 'providing', 'energy', 'efficiency', 'finally', 'integration', 'heterogeneous', 'well', 'challenge', 'managing', 'legacy', 'presented', 'context', 'project']"
"Hybrid Robot-as-a-Service (RaaS) Platform (Using MQTT and CoAP) Robots are evolving from factory work-horses to be the robot-companions. The future of robots will be as companions in the workplace functioning as interactive salespeople. In order to support this transition, it is important to combine service-oriented architecture and robotics. Service-oriented architecture and cloud computing have become dominant computing paradigms and adding a RaaS (Robot as a Service) unit as a part of this system will help the companies manage and develop robots more efficiently. The major components of RaaS will be the integration of RMS (Robot Management System) and ROC (Robot Operation Center). As more and more robots are increasing in the service industry, the inter-robot communication is very critical. This communication can be achieved by ROC and the robots can be monitored remotely or locally via RMS. The RaaS platform will comply with all the standards of SOA (Service Oriented Architecture) like the development platform and execution unit, thereby creating a flexible and more development-friendly process.",Hybrid Robot-as-a-Service (RaaS) Platform (Using MQTT and CoAP),"Robots are evolving from factory work-horses to be the robot-companions. The future of robots will be as companions in the workplace functioning as interactive salespeople. In order to support this transition, it is important to combine service-oriented architecture and robotics. Service-oriented architecture and cloud computing have become dominant computing paradigms and adding a RaaS (Robot as a Service) unit as a part of this system will help the companies manage and develop robots more efficiently. The major components of RaaS will be the integration of RMS (Robot Management System) and ROC (Robot Operation Center). As more and more robots are increasing in the service industry, the inter-robot communication is very critical. This communication can be achieved by ROC and the robots can be monitored remotely or locally via RMS. The RaaS platform will comply with all the standards of SOA (Service Oriented Architecture) like the development platform and execution unit, thereby creating a flexible and more development-friendly process.",IEEE conference,no,"['hybrid', 'raas', 'platform', 'using', 'mqtt', 'robot', 'evolving', 'factory', 'future', 'robot', 'functioning', 'interactive', 'order', 'support', 'transition', 'important', 'combine', 'robotics', 'computing', 'become', 'dominant', 'computing', 'paradigm', 'adding', 'raas', 'robot', 'unit', 'part', 'help', 'company', 'manage', 'develop', 'robot', 'efficiently', 'major', 'raas', 'integration', 'robot', 'management', 'robot', 'operation', 'center', 'robot', 'increasing', 'industry', 'communication', 'critical', 'communication', 'achieved', 'robot', 'monitored', 'remotely', 'locally', 'via', 'raas', 'platform', 'standard', 'soa', 'oriented', 'like', 'development', 'platform', 'execution', 'unit', 'thereby', 'creating', 'flexible', 'process']"
"nuhealthsoft: A Nutritional and Health Data Processing Software Tool from a patient’s perspective To ensure the alignment between developers, engineers and other stakeholders in the various steps of a production cycle, the utilisation of one of the many development methodologies is imperative for the successful transition from an idea to a useful and reliable end product. The rational unified process is the most precise and most cited iterative software engineering process that can ensure that alignment, trough comprehensive and simple separate steps, taken within pre - set restrictions [1].Modern software has to be easily managed and easy to scale. By scalable software, we refer to software, where functionality and processing power can be easily attached. For that reason more often than not, the software is divided in smaller components that serve different purposes and are tasked with different responsibilities. Those components can be deployed independently or as as part of greater distributed systems and are easier to collaborate on and to manage. This development framework is referred to as a microservices architecture.Medical applications for treatment, disease prevention and health optimisation can greatly benefit from machine learning and artificial intelligence. The success of such applications is better defined by its ability to include patients and users with different requirements and abilities.In this study we look into the development process of nuhealthsoft, an A.I infused medical application, that uses blood analysis and dieatary variables and other health data for the identification of health states and risk factors, such us metabolic syndrome and high blood pressure. The main characteristics of the application will be outlined through the R.U.P methodology. The microservices architecture and the patients’ needs and requirements are the key issues to be addressed in this paper, through the aforementioned methodology and are the forefront, all through the development cycle.",nuhealthsoft: A Nutritional and Health Data Processing Software Tool from a patient’s perspective,"To ensure the alignment between developers, engineers and other stakeholders in the various steps of a production cycle, the utilisation of one of the many development methodologies is imperative for the successful transition from an idea to a useful and reliable end product. The rational unified process is the most precise and most cited iterative software engineering process that can ensure that alignment, trough comprehensive and simple separate steps, taken within pre - set restrictions [1].Modern software has to be easily managed and easy to scale. By scalable software, we refer to software, where functionality and processing power can be easily attached. For that reason more often than not, the software is divided in smaller components that serve different purposes and are tasked with different responsibilities. Those components can be deployed independently or as as part of greater distributed systems and are easier to collaborate on and to manage. This development framework is referred to as a microservices architecture.Medical applications for treatment, disease prevention and health optimisation can greatly benefit from machine learning and artificial intelligence. The success of such applications is better defined by its ability to include patients and users with different requirements and abilities.In this study we look into the development process of nuhealthsoft, an A.I infused medical application, that uses blood analysis and dieatary variables and other health data for the identification of health states and risk factors, such us metabolic syndrome and high blood pressure. The main characteristics of the application will be outlined through the R.U.P methodology. The microservices architecture and the patients’ needs and requirements are the key issues to be addressed in this paper, through the aforementioned methodology and are the forefront, all through the development cycle.",IEEE conference,no,"['health', 'processing', 'tool', 'patient', 'perspective', 'ensure', 'alignment', 'developer', 'engineer', 'stakeholder', 'various', 'step', 'production', 'cycle', 'one', 'many', 'development', 'methodology', 'imperative', 'successful', 'transition', 'idea', 'useful', 'reliable', 'end', 'product', 'unified', 'process', 'precise', 'iterative', 'engineering', 'process', 'ensure', 'alignment', 'comprehensive', 'simple', 'separate', 'step', 'taken', 'within', 'set', 'restriction', 'easily', 'managed', 'easy', 'scale', 'scalable', 'functionality', 'processing', 'power', 'easily', 'reason', 'often', 'divided', 'smaller', 'serve', 'different', 'purpose', 'different', 'responsibility', 'deployed', 'independently', 'part', 'greater', 'distributed', 'easier', 'collaborate', 'manage', 'development', 'framework', 'referred', 'treatment', 'disease', 'prevention', 'health', 'greatly', 'benefit', 'machine', 'learning', 'artificial', 'intelligence', 'success', 'better', 'defined', 'ability', 'include', 'patient', 'user', 'different', 'requirement', 'study', 'look', 'development', 'process', 'medical', 'us', 'blood', 'analysis', 'variable', 'health', 'identification', 'health', 'state', 'risk', 'factor', 'u', 'high', 'blood', 'main', 'characteristic', 'outlined', 'methodology', 'patient', 'need', 'requirement', 'key', 'issue', 'addressed', 'paper', 'aforementioned', 'methodology', 'development', 'cycle']"
"A VNF-as-a-service design through micro-services disassembling the IMS With the evolution of Telco systems towards 5G, new requirements emerge for delivering services. Network services are expected to be designed to allow greater flexibility. In order to cope with the new users' requirements, Telcos should rethink their complex and monolithic network architectures into more agile architectures. Adoption of NFV as well as micro-services patterns are opportunities promising such an evolution. However, to gain in flexibility, it is crucial to satisfy structural requirements for the design of VNFs as services. We present in this paper an approach for designing VNF-asa-Service. With this approach, we define design requirements for the service architecture and the service logic of VNFs. As Telcos have adopted IMS as the de facto platform for service delivery in 3G and even 4G systems, it is interesting to study its evolution for 5G towards a microservices-based architecture with an optimal design. Therefore, we consider IMS as a case of study to illustrate the proposed approach. We present new functional entities for IMS-as-a-Service through a functional decomposition of legacy network functions. We have developed and implemented IMS-as-a-Service with respect to the proposed requirements. We consider a service scenario where we focus on authentication and authorization procedures. We evaluate the involved microservices comparing to the state-of-the-art. Finally, we discuss our results and highlight the advantages of our approach.",A VNF-as-a-service design through micro-services disassembling the IMS,"With the evolution of Telco systems towards 5G, new requirements emerge for delivering services. Network services are expected to be designed to allow greater flexibility. In order to cope with the new users' requirements, Telcos should rethink their complex and monolithic network architectures into more agile architectures. Adoption of NFV as well as micro-services patterns are opportunities promising such an evolution. However, to gain in flexibility, it is crucial to satisfy structural requirements for the design of VNFs as services. We present in this paper an approach for designing VNF-asa-Service. With this approach, we define design requirements for the service architecture and the service logic of VNFs. As Telcos have adopted IMS as the de facto platform for service delivery in 3G and even 4G systems, it is interesting to study its evolution for 5G towards a microservices-based architecture with an optimal design. Therefore, we consider IMS as a case of study to illustrate the proposed approach. We present new functional entities for IMS-as-a-Service through a functional decomposition of legacy network functions. We have developed and implemented IMS-as-a-Service with respect to the proposed requirements. We consider a service scenario where we focus on authentication and authorization procedures. We evaluate the involved microservices comparing to the state-of-the-art. Finally, we discuss our results and highlight the advantages of our approach.",IEEE conference,no,"['design', 'ims', 'evolution', 'telco', 'towards', 'new', 'requirement', 'emerge', 'delivering', 'network', 'expected', 'designed', 'allow', 'greater', 'flexibility', 'order', 'cope', 'new', 'user', 'requirement', 'telco', 'complex', 'monolithic', 'network', 'agile', 'adoption', 'nfv', 'well', 'pattern', 'opportunity', 'promising', 'evolution', 'however', 'gain', 'flexibility', 'crucial', 'satisfy', 'structural', 'requirement', 'design', 'vnfs', 'present', 'paper', 'designing', 'define', 'design', 'requirement', 'logic', 'vnfs', 'telco', 'adopted', 'ims', 'de', 'facto', 'platform', 'delivery', 'even', 'study', 'evolution', 'towards', 'optimal', 'design', 'therefore', 'consider', 'ims', 'case', 'study', 'illustrate', 'proposed', 'present', 'new', 'functional', 'entity', 'functional', 'decomposition', 'legacy', 'network', 'function', 'developed', 'implemented', 'respect', 'proposed', 'requirement', 'consider', 'scenario', 'focus', 'authentication', 'authorization', 'procedure', 'evaluate', 'involved', 'comparing', 'finally', 'discus', 'result', 'highlight', 'advantage']"
"Synchronous Reconfiguration of Distributed Embedded Applications During Operation Speed of adaptation to changing demand is a critical success factor in factory automation. The key to speed is to enable agile development by independent engineer offices and equipment producers with industrial-grade microservice architectures. The expensive drawback is: While software components evolve over time, manufacturers have to integrate and deploy more and more updates during costly production stops. To avoid production stops as much as possible, we propose reconfiguration extensions to a real-time container architecture proposed earlier. The original container approach addresses both the functional and nonfunctional aspects of integrating embedded software components in late engineering phases. The extended approach allows modifications of the running distributed embedded application even during operation, while continuously ensuring reactivity of the system. The agents running on each node prepare the reconfiguration in background and then synchronously perform the required modifications according to a detailed reconfiguration plan. We demonstrate our concept by describing a synchronous API change between two distributed software components of a running gesture recognition system. An evaluation shows the feasibility of the concepts, but also calls for further research.",Synchronous Reconfiguration of Distributed Embedded Applications During Operation,"Speed of adaptation to changing demand is a critical success factor in factory automation. The key to speed is to enable agile development by independent engineer offices and equipment producers with industrial-grade microservice architectures. The expensive drawback is: While software components evolve over time, manufacturers have to integrate and deploy more and more updates during costly production stops. To avoid production stops as much as possible, we propose reconfiguration extensions to a real-time container architecture proposed earlier. The original container approach addresses both the functional and nonfunctional aspects of integrating embedded software components in late engineering phases. The extended approach allows modifications of the running distributed embedded application even during operation, while continuously ensuring reactivity of the system. The agents running on each node prepare the reconfiguration in background and then synchronously perform the required modifications according to a detailed reconfiguration plan. We demonstrate our concept by describing a synchronous API change between two distributed software components of a running gesture recognition system. An evaluation shows the feasibility of the concepts, but also calls for further research.",IEEE conference,no,"['synchronous', 'reconfiguration', 'distributed', 'embedded', 'operation', 'speed', 'adaptation', 'changing', 'demand', 'critical', 'success', 'factor', 'factory', 'automation', 'key', 'speed', 'enable', 'agile', 'development', 'independent', 'engineer', 'office', 'equipment', 'producer', 'expensive', 'drawback', 'evolve', 'time', 'integrate', 'deploy', 'update', 'costly', 'production', 'stop', 'avoid', 'production', 'stop', 'much', 'possible', 'propose', 'reconfiguration', 'extension', 'container', 'proposed', 'earlier', 'original', 'container', 'address', 'functional', 'aspect', 'integrating', 'embedded', 'engineering', 'phase', 'extended', 'allows', 'modification', 'running', 'distributed', 'embedded', 'even', 'operation', 'continuously', 'ensuring', 'agent', 'running', 'node', 'reconfiguration', 'background', 'perform', 'required', 'modification', 'according', 'detailed', 'reconfiguration', 'plan', 'demonstrate', 'concept', 'describing', 'synchronous', 'api', 'change', 'two', 'distributed', 'running', 'recognition', 'evaluation', 'show', 'feasibility', 'concept', 'also', 'call', 'research']"
"Hierarchical Scaling of Microservices in Kubernetes In the last years, we have seen the increasing adoption of the microservice architectural style where applications satisfy user requests by invoking a set of independently deployable services. Software containers and orchestration tools, such as Kubernetes, have simplified the development and management of microservices. To manage containers' horizontal elasticity, Kubernetes uses a decentralized threshold-based policy that requires to set thresholds on system-oriented metrics (i.e., CPU utilization). This might not be well-suited to scale latency-sensitive applications, which need to express requirements in terms of response time. Moreover, being a fully decentralized solution, it may lead to frequent and uncoordinated application reconfigurations. In this paper, we present me-kube (Multi-level Elastic Kubernetes), a Kubernetes extension that introduces a hierarchical architecture for controlling the elasticity of microservice-based applications. At higher level, a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of each microservice. Then, we propose novel proactive and reactive hierarchical control policies, based on queuing theory. To show that me-kube provides general mechanisms, we also integrate reinforcement learning-based scaling policies. Using me-kube, we perform a large set of experiments, aimed to show the advantages of a hierarchical control over the default Kubernetes autoscaler.",Hierarchical Scaling of Microservices in Kubernetes,"In the last years, we have seen the increasing adoption of the microservice architectural style where applications satisfy user requests by invoking a set of independently deployable services. Software containers and orchestration tools, such as Kubernetes, have simplified the development and management of microservices. To manage containers' horizontal elasticity, Kubernetes uses a decentralized threshold-based policy that requires to set thresholds on system-oriented metrics (i.e., CPU utilization). This might not be well-suited to scale latency-sensitive applications, which need to express requirements in terms of response time. Moreover, being a fully decentralized solution, it may lead to frequent and uncoordinated application reconfigurations. In this paper, we present me-kube (Multi-level Elastic Kubernetes), a Kubernetes extension that introduces a hierarchical architecture for controlling the elasticity of microservice-based applications. At higher level, a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of each microservice. Then, we propose novel proactive and reactive hierarchical control policies, based on queuing theory. To show that me-kube provides general mechanisms, we also integrate reinforcement learning-based scaling policies. Using me-kube, we perform a large set of experiments, aimed to show the advantages of a hierarchical control over the default Kubernetes autoscaler.",IEEE conference,no,"['hierarchical', 'scaling', 'kubernetes', 'last', 'year', 'seen', 'increasing', 'adoption', 'architectural', 'style', 'satisfy', 'user', 'request', 'invoking', 'set', 'independently', 'deployable', 'container', 'orchestration', 'tool', 'kubernetes', 'simplified', 'development', 'management', 'manage', 'container', 'horizontal', 'elasticity', 'kubernetes', 'us', 'decentralized', 'policy', 'requires', 'set', 'threshold', 'metric', 'cpu', 'utilization', 'might', 'scale', 'need', 'requirement', 'term', 'response', 'time', 'moreover', 'fully', 'decentralized', 'solution', 'may', 'lead', 'frequent', 'paper', 'present', 'elastic', 'kubernetes', 'kubernetes', 'extension', 'introduces', 'hierarchical', 'controlling', 'elasticity', 'higher', 'level', 'centralized', 'coordinate', 'adaptation', 'distributed', 'turn', 'locally', 'control', 'adaptation', 'propose', 'novel', 'proactive', 'reactive', 'hierarchical', 'control', 'policy', 'based', 'queuing', 'show', 'provides', 'general', 'mechanism', 'also', 'integrate', 'reinforcement', 'scaling', 'policy', 'using', 'perform', 'large', 'set', 'experiment', 'aimed', 'show', 'advantage', 'hierarchical', 'control', 'default', 'kubernetes', 'autoscaler']"
"Characterization of Microservice Response Time in Kubernetes: A Mixture Density Network Approach The use of microservice-based applications is becoming more prominent also in the telecommunication field. The current 5G core network, for instance, is already built around the concept of a “Service Based Architecture”, and it is foreseeable that 6G will push even further this concept to enable more flexible and pervasive deployments. However, the increasing complexity of future networks calls for sophisticated platforms that could help network providers with their deployments design. In this framework, a central research trend is the development of digital twins of the physical infrastructures. These digital representations should closely mimic the behavior of the managed system, allowing the operators to test new configurations, analyze what-if scenarios, or train their reinforcement learning algorithms in safe environments. Considering that Kubernetes is becoming the de-facto standard platform for container orchestration and microservice-based application lifecycle management, the implementation of a Kubernetes digital twin requires an accurate characterization of the microservice response time, possibly leveraging suitable Machine Learning techniques trained with measurement data collected in the field. In this paper we introduce a new methodology, based on Mixture Density Networks, to accurately estimate the statistical distribution of the response time of microservice-based applications. We show the improvement in performance with respect to simulation-based inference procedures proposed in literature.",Characterization of Microservice Response Time in Kubernetes: A Mixture Density Network Approach,"The use of microservice-based applications is becoming more prominent also in the telecommunication field. The current 5G core network, for instance, is already built around the concept of a “Service Based Architecture”, and it is foreseeable that 6G will push even further this concept to enable more flexible and pervasive deployments. However, the increasing complexity of future networks calls for sophisticated platforms that could help network providers with their deployments design. In this framework, a central research trend is the development of digital twins of the physical infrastructures. These digital representations should closely mimic the behavior of the managed system, allowing the operators to test new configurations, analyze what-if scenarios, or train their reinforcement learning algorithms in safe environments. Considering that Kubernetes is becoming the de-facto standard platform for container orchestration and microservice-based application lifecycle management, the implementation of a Kubernetes digital twin requires an accurate characterization of the microservice response time, possibly leveraging suitable Machine Learning techniques trained with measurement data collected in the field. In this paper we introduce a new methodology, based on Mixture Density Networks, to accurately estimate the statistical distribution of the response time of microservice-based applications. We show the improvement in performance with respect to simulation-based inference procedures proposed in literature.",IEEE conference,no,"['characterization', 'response', 'time', 'kubernetes', 'network', 'use', 'becoming', 'prominent', 'also', 'telecommunication', 'field', 'current', 'core', 'network', 'instance', 'already', 'built', 'around', 'concept', 'based', 'push', 'even', 'concept', 'enable', 'flexible', 'pervasive', 'deployment', 'however', 'increasing', 'complexity', 'future', 'network', 'call', 'sophisticated', 'platform', 'could', 'help', 'network', 'provider', 'deployment', 'design', 'framework', 'central', 'research', 'trend', 'development', 'digital', 'twin', 'physical', 'infrastructure', 'digital', 'representation', 'closely', 'behavior', 'managed', 'allowing', 'operator', 'test', 'new', 'configuration', 'analyze', 'scenario', 'reinforcement', 'learning', 'algorithm', 'safe', 'environment', 'considering', 'kubernetes', 'becoming', 'standard', 'platform', 'container', 'orchestration', 'lifecycle', 'management', 'implementation', 'kubernetes', 'digital', 'twin', 'requires', 'accurate', 'characterization', 'response', 'time', 'possibly', 'leveraging', 'suitable', 'machine', 'learning', 'technique', 'trained', 'measurement', 'collected', 'field', 'paper', 'introduce', 'new', 'methodology', 'based', 'network', 'accurately', 'estimate', 'statistical', 'distribution', 'response', 'time', 'show', 'improvement', 'performance', 'respect', 'inference', 'procedure', 'proposed', 'literature']"
"A Secure Microservice Framework for IoT The Internet of Things (IoT) has connected an incredible diversity of devices in novel ways, which has enabled exciting new services and opportunities. Unfortunately, IoT systems also present several important challenges to developers. This paper proposes a vision for how we may build IoT systems in the future by reconceiving IoT's fundamental unit of construction not as a ""thing"", but rather as a widely and finely distributed ""microservice"" already familiar to web service engineering circles. Since IoT systems are quite different from more established uses of microservice architectures, success of the approach depends on adaptations that enable them to met the key challenges that IoT systems present. We argue that a microservice approach to building IoT systems can combine in a mutually enforcing way with patterns for microservices, API gateways, distribution of services, uniform service discovery, containers, and access control. The approach is illustrated using two case studies of IoT systems in personal health management and connected autonomous vehicles. Our hope is that the vision of a microservices approach will help focus research that can fill in current gaps preventing more effective, interoperable, and secure IoT services and solutions in a wide variety of contexts.",A Secure Microservice Framework for IoT,"The Internet of Things (IoT) has connected an incredible diversity of devices in novel ways, which has enabled exciting new services and opportunities. Unfortunately, IoT systems also present several important challenges to developers. This paper proposes a vision for how we may build IoT systems in the future by reconceiving IoT's fundamental unit of construction not as a ""thing"", but rather as a widely and finely distributed ""microservice"" already familiar to web service engineering circles. Since IoT systems are quite different from more established uses of microservice architectures, success of the approach depends on adaptations that enable them to met the key challenges that IoT systems present. We argue that a microservice approach to building IoT systems can combine in a mutually enforcing way with patterns for microservices, API gateways, distribution of services, uniform service discovery, containers, and access control. The approach is illustrated using two case studies of IoT systems in personal health management and connected autonomous vehicles. Our hope is that the vision of a microservices approach will help focus research that can fill in current gaps preventing more effective, interoperable, and secure IoT services and solutions in a wide variety of contexts.",IEEE conference,no,"['secure', 'framework', 'iot', 'internet', 'thing', 'iot', 'connected', 'diversity', 'device', 'novel', 'way', 'enabled', 'new', 'opportunity', 'iot', 'also', 'present', 'several', 'important', 'challenge', 'developer', 'paper', 'proposes', 'vision', 'may', 'build', 'iot', 'future', 'iot', 'fundamental', 'unit', 'construction', 'thing', 'rather', 'widely', 'distributed', 'already', 'web', 'engineering', 'since', 'iot', 'quite', 'different', 'established', 'us', 'success', 'depends', 'adaptation', 'enable', 'met', 'key', 'challenge', 'iot', 'present', 'argue', 'building', 'iot', 'combine', 'enforcing', 'way', 'pattern', 'api', 'gateway', 'distribution', 'uniform', 'discovery', 'container', 'access', 'control', 'illustrated', 'using', 'two', 'case', 'study', 'iot', 'health', 'management', 'connected', 'autonomous', 'vehicle', 'vision', 'help', 'focus', 'research', 'fill', 'current', 'gap', 'effective', 'interoperable', 'secure', 'iot', 'solution', 'wide', 'variety', 'context']"
"Towards a Unified Description Language for Simulations in Cloud Environments In recent years, cloud infrastructures evolved into a flexible technology for computationally intense simulations. Container-based clouds provide scalability to scientists who develop large-scale distributed simulations, particularly when developed in microservice architecture. Automating configuration and deployment of such distributed simulations on container as a service (CaaS) platforms is a challenge that we approach in this paper. We will present a novel concept for a descriptive language, that allows for convenient execution and management of distributed simulations on cloud infrastructures using heterogeneous models run on CaaS platforms, supplemented by a software framework to implement functionality. This language will allow for reproducible simulation results and a reduction of necessary configuration for structural layers by providing an interface towards the simulation system. The configuration, deployment, conduction, roll-back and result collection of the simulation will be handled by the framework provided for this domain specific language (DSL).",Towards a Unified Description Language for Simulations in Cloud Environments,"In recent years, cloud infrastructures evolved into a flexible technology for computationally intense simulations. Container-based clouds provide scalability to scientists who develop large-scale distributed simulations, particularly when developed in microservice architecture. Automating configuration and deployment of such distributed simulations on container as a service (CaaS) platforms is a challenge that we approach in this paper. We will present a novel concept for a descriptive language, that allows for convenient execution and management of distributed simulations on cloud infrastructures using heterogeneous models run on CaaS platforms, supplemented by a software framework to implement functionality. This language will allow for reproducible simulation results and a reduction of necessary configuration for structural layers by providing an interface towards the simulation system. The configuration, deployment, conduction, roll-back and result collection of the simulation will be handled by the framework provided for this domain specific language (DSL).",IEEE conference,no,"['towards', 'unified', 'description', 'language', 'simulation', 'environment', 'recent', 'year', 'infrastructure', 'evolved', 'flexible', 'technology', 'computationally', 'simulation', 'provide', 'scalability', 'scientist', 'develop', 'distributed', 'simulation', 'particularly', 'developed', 'automating', 'configuration', 'deployment', 'distributed', 'simulation', 'container', 'caas', 'platform', 'challenge', 'paper', 'present', 'novel', 'concept', 'language', 'allows', 'convenient', 'execution', 'management', 'distributed', 'simulation', 'infrastructure', 'using', 'heterogeneous', 'model', 'run', 'caas', 'platform', 'framework', 'implement', 'functionality', 'language', 'allow', 'simulation', 'result', 'reduction', 'necessary', 'configuration', 'structural', 'layer', 'providing', 'interface', 'towards', 'simulation', 'configuration', 'deployment', 'result', 'collection', 'simulation', 'handled', 'framework', 'provided', 'domain', 'specific', 'language', 'dsl']"
"An Architecture for Social Robot-assisted Subjective and Objective Health Monitoring The population of the world is growing older, while the shortage of required health care workers is growing increasingly severe. To combat this situation and support the independence and autonomy of older adults while reducing the burden of their caregivers, we propose a health monitoring system to assess and monitor subjective and objective health data. The system includes a conversational AI integrated into a social robot, various connected health devices, and a mobile and web app to visualize all acquired data. We formulated system requirements based on expert interviews with health care workers and literature findings. In this paper, we present the resulting system architecture integrating all health monitoring components using individual microservices, along with an app designed for our target group of older adults and their caregivers. Our goal with the proposed system is to support older adults in taking charge of their own health, thus promoting healthy and positive aging.",An Architecture for Social Robot-assisted Subjective and Objective Health Monitoring,"The population of the world is growing older, while the shortage of required health care workers is growing increasingly severe. To combat this situation and support the independence and autonomy of older adults while reducing the burden of their caregivers, we propose a health monitoring system to assess and monitor subjective and objective health data. The system includes a conversational AI integrated into a social robot, various connected health devices, and a mobile and web app to visualize all acquired data. We formulated system requirements based on expert interviews with health care workers and literature findings. In this paper, we present the resulting system architecture integrating all health monitoring components using individual microservices, along with an app designed for our target group of older adults and their caregivers. Our goal with the proposed system is to support older adults in taking charge of their own health, thus promoting healthy and positive aging.",IEEE conference,no,"['social', 'objective', 'health', 'monitoring', 'world', 'growing', 'older', 'required', 'health', 'care', 'worker', 'growing', 'increasingly', 'severe', 'combat', 'situation', 'support', 'independence', 'autonomy', 'older', 'adult', 'reducing', 'burden', 'caregiver', 'propose', 'health', 'monitoring', 'assess', 'monitor', 'objective', 'health', 'includes', 'ai', 'integrated', 'social', 'robot', 'various', 'connected', 'health', 'device', 'mobile', 'web', 'app', 'visualize', 'acquired', 'formulated', 'requirement', 'based', 'expert', 'health', 'care', 'worker', 'literature', 'finding', 'paper', 'present', 'resulting', 'integrating', 'health', 'monitoring', 'using', 'individual', 'along', 'app', 'designed', 'target', 'group', 'older', 'adult', 'caregiver', 'goal', 'proposed', 'support', 'older', 'adult', 'taking', 'charge', 'health', 'thus', 'promoting', 'positive', 'aging']"
"An Architecture of a Web Application for Deploying Machine Learning Models in Healthcare Domain An important gap exists between advanced Deep Learning (DL) models developed for medical imaging and their insitu implementation in clinical environments. Our research proposes a scalable software architecture for a web-based computer-aided diagnosis (CAD) application that bridges the gap between research and production. The main goals are: (1) creating an intuitive interface for non-technical users and, (2) designating a feedback mechanism to enhance the Machine Learning (ML) models with medical expertise. The architecture assumes a microservices approach, containerization, and current web technologies to enable seamless deployment and user-friendliness. Key components include ONNX-based model conversion for cross- platform compatibility, asynchronous processing based on FastAPI, and a modular data handling system using PostgreSQL. Our proof-of-concept (PoC) shows the system's feasibility by deploying it on the Google Cloud Platform (GCP) and getting initial feedback from clinicians. Early findings indicate potential improvements in model usability and stakeholder engagement. Future actions will involve implementing continuous integration and delivery pipelines to improve system resilience and scalability.",An Architecture of a Web Application for Deploying Machine Learning Models in Healthcare Domain,"An important gap exists between advanced Deep Learning (DL) models developed for medical imaging and their insitu implementation in clinical environments. Our research proposes a scalable software architecture for a web-based computer-aided diagnosis (CAD) application that bridges the gap between research and production. The main goals are: (1) creating an intuitive interface for non-technical users and, (2) designating a feedback mechanism to enhance the Machine Learning (ML) models with medical expertise. The architecture assumes a microservices approach, containerization, and current web technologies to enable seamless deployment and user-friendliness. Key components include ONNX-based model conversion for cross- platform compatibility, asynchronous processing based on FastAPI, and a modular data handling system using PostgreSQL. Our proof-of-concept (PoC) shows the system's feasibility by deploying it on the Google Cloud Platform (GCP) and getting initial feedback from clinicians. Early findings indicate potential improvements in model usability and stakeholder engagement. Future actions will involve implementing continuous integration and delivery pipelines to improve system resilience and scalability.",IEEE conference,no,"['web', 'deploying', 'machine', 'learning', 'model', 'healthcare', 'domain', 'important', 'gap', 'exists', 'advanced', 'deep', 'learning', 'model', 'developed', 'medical', 'implementation', 'clinical', 'environment', 'research', 'proposes', 'scalable', 'diagnosis', 'bridge', 'gap', 'research', 'production', 'main', 'goal', 'creating', 'intuitive', 'interface', 'user', 'feedback', 'mechanism', 'enhance', 'machine', 'learning', 'ml', 'model', 'medical', 'expertise', 'containerization', 'current', 'web', 'technology', 'enable', 'seamless', 'deployment', 'key', 'include', 'model', 'conversion', 'platform', 'compatibility', 'asynchronous', 'processing', 'based', 'modular', 'handling', 'using', 'show', 'feasibility', 'deploying', 'google', 'platform', 'getting', 'initial', 'feedback', 'early', 'finding', 'indicate', 'potential', 'improvement', 'model', 'usability', 'stakeholder', 'future', 'action', 'involve', 'implementing', 'continuous', 'integration', 'delivery', 'pipeline', 'improve', 'resilience', 'scalability']"
"Architecture of an interoperable IoT platform based on microservices The vision of the Internet of Things enabled the development of a wide spectrum of services, applications and ecosystems, deemed infeasible not long ago. However, lack of standardization poses a number of questions still requiring proper addressing. Due to the need of supporting large number of users and significant data processing throughput, Internet of Things requires a specific approach towards the problem of providing sufficient scalability and performance, clearly pointing towards the distribution of effort among a large number of small and specialized services. Reflecting on the importance of coexistence of heterogeneous systems supporting the Internet of Things, this paper presents an overview of the specific problems inherent to the IoT and the proposal of an architecture of a microservice based middleware aimed at connecting heterogeneous IoT devices. The middleware functionality is achieved irrespective of the size and complexity of a given device network, both from the data model aspect and from the aspect of connecting existing and newly created middleware components.",Architecture of an interoperable IoT platform based on microservices,"The vision of the Internet of Things enabled the development of a wide spectrum of services, applications and ecosystems, deemed infeasible not long ago. However, lack of standardization poses a number of questions still requiring proper addressing. Due to the need of supporting large number of users and significant data processing throughput, Internet of Things requires a specific approach towards the problem of providing sufficient scalability and performance, clearly pointing towards the distribution of effort among a large number of small and specialized services. Reflecting on the importance of coexistence of heterogeneous systems supporting the Internet of Things, this paper presents an overview of the specific problems inherent to the IoT and the proposal of an architecture of a microservice based middleware aimed at connecting heterogeneous IoT devices. The middleware functionality is achieved irrespective of the size and complexity of a given device network, both from the data model aspect and from the aspect of connecting existing and newly created middleware components.",IEEE conference,no,"['interoperable', 'iot', 'platform', 'based', 'vision', 'internet', 'thing', 'enabled', 'development', 'wide', 'ecosystem', 'long', 'ago', 'however', 'lack', 'pose', 'number', 'question', 'still', 'requiring', 'proper', 'addressing', 'due', 'need', 'supporting', 'large', 'number', 'user', 'significant', 'processing', 'throughput', 'internet', 'thing', 'requires', 'specific', 'towards', 'problem', 'providing', 'scalability', 'performance', 'clearly', 'towards', 'distribution', 'effort', 'among', 'large', 'number', 'small', 'specialized', 'importance', 'heterogeneous', 'supporting', 'internet', 'thing', 'paper', 'present', 'overview', 'specific', 'problem', 'inherent', 'iot', 'proposal', 'based', 'middleware', 'aimed', 'connecting', 'heterogeneous', 'iot', 'device', 'middleware', 'functionality', 'achieved', 'size', 'complexity', 'given', 'device', 'network', 'model', 'aspect', 'aspect', 'connecting', 'existing', 'newly', 'created', 'middleware']"
"Socially Assistive Robot’s Behaviors using Microservices In this work, we introduce a set of robot's behavior aimed at being used for monitoring and interaction with elderly people affected by Alzheimer disease. Robot's behaviors for a low cost robotic device rely on the use of microservices running on a local server. A microservice is an independent, self-contained, self-scope, and self-responsibility component of the robotic system proposed for decoupling the implemented functions needed to obtain the proper robot behaviors. The developed robotic behaviors include navigation, interaction, and monitoring capabilities. The requests and the signals of the patients are handled and managed relying on event-based communications between the system components. The use of design patterns like this one increases the overall reliability of a service composition. The system is currently operating in a private house with an elderly couple.",Socially Assistive Robot’s Behaviors using Microservices,"In this work, we introduce a set of robot's behavior aimed at being used for monitoring and interaction with elderly people affected by Alzheimer disease. Robot's behaviors for a low cost robotic device rely on the use of microservices running on a local server. A microservice is an independent, self-contained, self-scope, and self-responsibility component of the robotic system proposed for decoupling the implemented functions needed to obtain the proper robot behaviors. The developed robotic behaviors include navigation, interaction, and monitoring capabilities. The requests and the signals of the patients are handled and managed relying on event-based communications between the system components. The use of design patterns like this one increases the overall reliability of a service composition. The system is currently operating in a private house with an elderly couple.",IEEE conference,no,"['robot', 'behavior', 'using', 'work', 'introduce', 'set', 'robot', 'behavior', 'aimed', 'used', 'monitoring', 'interaction', 'elderly', 'people', 'affected', 'disease', 'robot', 'behavior', 'low', 'cost', 'robotic', 'device', 'rely', 'use', 'running', 'local', 'server', 'independent', 'robotic', 'proposed', 'decoupling', 'implemented', 'function', 'needed', 'obtain', 'proper', 'robot', 'behavior', 'developed', 'robotic', 'behavior', 'include', 'interaction', 'monitoring', 'capability', 'request', 'signal', 'patient', 'handled', 'managed', 'relying', 'communication', 'use', 'design', 'pattern', 'like', 'one', 'increase', 'overall', 'reliability', 'composition', 'currently', 'operating', 'private', 'elderly']"
"Extending ETSI MEC Towards Stateful Application Relocation Based on Container Migration Edge computing allows to run microservices in close proximity to end user devices. This proximity lets edge computing support emerging 5G application scenarios that need low latency and high bandwidth (e.g., augmented reality, autonomous vehicles). Given its interest, edge computing is fastly gaining momentum and is currently being standardised by the European Telecommunications Standards Institute (ETSI) as Multi-Access Edge Computing (MEC). Notwithstanding its strengths, edge computing is significantly challenged by device mobility, as this can reduce proximity to the edge microservice, putting edge computing benefits at risk. A way to solve this problem is to migrate the edge microservice across edge servers, to let it follow the application component running on the mobile device. Besides, if the microservice is stateful (i.e., it maintains a state associated to the user), its state needs to be migrated as well. Within ETSI MEC, this concept is expressed as stateful application relocation. The standard identifies three different high-level ways to transfer the application state. However, all of them assume that it is up to the application to actually relocate the state. In this work, we assume that applications at the edge run as containers, and we extend ETSI MEC to let it support stateful application relocation by leveraging container migration techniques. This approach allows to transfer the application state in a transparent way to the application itself. We implemented our solution and tested it over a small-scale edge computing testbed to extract initial results.",Extending ETSI MEC Towards Stateful Application Relocation Based on Container Migration,"Edge computing allows to run microservices in close proximity to end user devices. This proximity lets edge computing support emerging 5G application scenarios that need low latency and high bandwidth (e.g., augmented reality, autonomous vehicles). Given its interest, edge computing is fastly gaining momentum and is currently being standardised by the European Telecommunications Standards Institute (ETSI) as Multi-Access Edge Computing (MEC). Notwithstanding its strengths, edge computing is significantly challenged by device mobility, as this can reduce proximity to the edge microservice, putting edge computing benefits at risk. A way to solve this problem is to migrate the edge microservice across edge servers, to let it follow the application component running on the mobile device. Besides, if the microservice is stateful (i.e., it maintains a state associated to the user), its state needs to be migrated as well. Within ETSI MEC, this concept is expressed as stateful application relocation. The standard identifies three different high-level ways to transfer the application state. However, all of them assume that it is up to the application to actually relocate the state. In this work, we assume that applications at the edge run as containers, and we extend ETSI MEC to let it support stateful application relocation by leveraging container migration techniques. This approach allows to transfer the application state in a transparent way to the application itself. We implemented our solution and tested it over a small-scale edge computing testbed to extract initial results.",IEEE conference,no,"['extending', 'etsi', 'mec', 'towards', 'stateful', 'relocation', 'based', 'container', 'migration', 'edge', 'computing', 'allows', 'run', 'close', 'proximity', 'end', 'user', 'device', 'proximity', 'let', 'edge', 'computing', 'support', 'emerging', 'scenario', 'need', 'low', 'latency', 'high', 'bandwidth', 'reality', 'autonomous', 'vehicle', 'given', 'interest', 'edge', 'computing', 'gaining', 'momentum', 'currently', 'european', 'telecommunication', 'standard', 'etsi', 'edge', 'computing', 'mec', 'strength', 'edge', 'computing', 'significantly', 'device', 'mobility', 'reduce', 'proximity', 'edge', 'edge', 'computing', 'benefit', 'risk', 'way', 'solve', 'problem', 'migrate', 'edge', 'across', 'edge', 'server', 'let', 'follow', 'running', 'mobile', 'device', 'besides', 'stateful', 'maintains', 'state', 'associated', 'user', 'state', 'need', 'well', 'within', 'etsi', 'mec', 'concept', 'expressed', 'stateful', 'relocation', 'standard', 'identifies', 'three', 'different', 'way', 'transfer', 'state', 'however', 'actually', 'state', 'work', 'edge', 'run', 'container', 'extend', 'etsi', 'mec', 'let', 'support', 'stateful', 'relocation', 'leveraging', 'container', 'migration', 'technique', 'allows', 'transfer', 'state', 'way', 'implemented', 'solution', 'tested', 'edge', 'computing', 'testbed', 'extract', 'initial', 'result']"
"A Microservices-based IoT Monitoring System to Improve the Safety in Public Building Safety of public buildings' users is an important issue, especially in buildings frequented by a great number of people. In such places, in case of an emergency, like a fire, rescue workers must intervene in a timely manner, directing their efforts towards places where there are people to be saved. This work presents an Internet of Things(IoT)-based framework, aiming at monitoring environmental parameters in order to support rescuers during emergencies. The microservices paradigm allows a pattern-based specification of system components that are refined and adapted on-the-fly depending on the specific execution context, based on the changing aspects such as, user's need and requirements, context variables, user's behavior, sensor data. First results related to the validation of the proposed system mainly concerning non-functional requirements of the implemented system through a proof-of-concept are reported and discussed.",A Microservices-based IoT Monitoring System to Improve the Safety in Public Building,"Safety of public buildings' users is an important issue, especially in buildings frequented by a great number of people. In such places, in case of an emergency, like a fire, rescue workers must intervene in a timely manner, directing their efforts towards places where there are people to be saved. This work presents an Internet of Things(IoT)-based framework, aiming at monitoring environmental parameters in order to support rescuers during emergencies. The microservices paradigm allows a pattern-based specification of system components that are refined and adapted on-the-fly depending on the specific execution context, based on the changing aspects such as, user's need and requirements, context variables, user's behavior, sensor data. First results related to the validation of the proposed system mainly concerning non-functional requirements of the implemented system through a proof-of-concept are reported and discussed.",IEEE conference,no,"['iot', 'monitoring', 'improve', 'safety', 'public', 'building', 'safety', 'public', 'building', 'user', 'important', 'issue', 'especially', 'building', 'great', 'number', 'people', 'place', 'case', 'like', 'fire', 'worker', 'must', 'timely', 'manner', 'effort', 'towards', 'place', 'people', 'work', 'present', 'internet', 'thing', 'iot', 'framework', 'aiming', 'monitoring', 'environmental', 'parameter', 'order', 'support', 'paradigm', 'allows', 'specification', 'adapted', 'depending', 'specific', 'execution', 'context', 'based', 'changing', 'aspect', 'user', 'need', 'requirement', 'context', 'variable', 'user', 'behavior', 'sensor', 'first', 'result', 'related', 'validation', 'proposed', 'mainly', 'concerning', 'requirement', 'implemented', 'reported', 'discussed']"
"Internet of Things Services Orchestration Framework Based on Kubernetes and Edge Computing Presented work is fanalysis of how the microservices paradigm can be used to design and implement distributed edge services for Internet of Things (IoT) applications. Basically, IoT is a platform where integrated services are associated with the common network, thus all devices are able to gather and exchange data among each other. Typically, monolithic user mobility research services are developed for the unified ETSI MEC system reference architecture centers. ETSI MEC considers microservices as a tool for breaking monolithic applications into a set of loosely coupled distributed components. It is expected that this architecture will facilitate the dynamic adaptation during the application execution. However, increased modularity can also increase the burden on orchestration and system management. In MEC, user hardware is connected through gateways to microservices running on the edge host.There are three levels in each of the edge systems: 1) microservices perform a logical operation with components for motion track analysis, 2) movement foresight and 3) outcome visualization. The distributed service is realized with Docker containers and calculated on actual world adjustment with low capacity edge servers and real user mobility information. The results demonstrate the fact that the edge perspective of low latency may be encountered in this sort of implementation. The integration of a software creation technology with a standardized edge system supplies respectable basis for subsequent development. The paper considers the application of the boundary computing architecture and Kubernetes as an orchestration and management of network applications.",Internet of Things Services Orchestration Framework Based on Kubernetes and Edge Computing,"Presented work is fanalysis of how the microservices paradigm can be used to design and implement distributed edge services for Internet of Things (IoT) applications. Basically, IoT is a platform where integrated services are associated with the common network, thus all devices are able to gather and exchange data among each other. Typically, monolithic user mobility research services are developed for the unified ETSI MEC system reference architecture centers. ETSI MEC considers microservices as a tool for breaking monolithic applications into a set of loosely coupled distributed components. It is expected that this architecture will facilitate the dynamic adaptation during the application execution. However, increased modularity can also increase the burden on orchestration and system management. In MEC, user hardware is connected through gateways to microservices running on the edge host.There are three levels in each of the edge systems: 1) microservices perform a logical operation with components for motion track analysis, 2) movement foresight and 3) outcome visualization. The distributed service is realized with Docker containers and calculated on actual world adjustment with low capacity edge servers and real user mobility information. The results demonstrate the fact that the edge perspective of low latency may be encountered in this sort of implementation. The integration of a software creation technology with a standardized edge system supplies respectable basis for subsequent development. The paper considers the application of the boundary computing architecture and Kubernetes as an orchestration and management of network applications.",IEEE conference,no,"['internet', 'thing', 'orchestration', 'framework', 'based', 'kubernetes', 'edge', 'computing', 'presented', 'work', 'paradigm', 'used', 'design', 'implement', 'distributed', 'edge', 'internet', 'thing', 'iot', 'iot', 'platform', 'integrated', 'associated', 'common', 'network', 'thus', 'device', 'able', 'exchange', 'among', 'typically', 'monolithic', 'user', 'mobility', 'research', 'developed', 'unified', 'etsi', 'mec', 'reference', 'center', 'etsi', 'mec', 'considers', 'tool', 'breaking', 'monolithic', 'set', 'loosely', 'coupled', 'distributed', 'expected', 'facilitate', 'dynamic', 'adaptation', 'execution', 'however', 'increased', 'modularity', 'also', 'increase', 'burden', 'orchestration', 'management', 'mec', 'user', 'hardware', 'connected', 'gateway', 'running', 'edge', 'three', 'level', 'edge', 'perform', 'logical', 'operation', 'track', 'analysis', 'movement', 'outcome', 'visualization', 'distributed', 'realized', 'docker', 'container', 'calculated', 'actual', 'world', 'low', 'capacity', 'edge', 'server', 'real', 'user', 'mobility', 'information', 'result', 'demonstrate', 'fact', 'edge', 'perspective', 'low', 'latency', 'may', 'sort', 'implementation', 'integration', 'creation', 'technology', 'standardized', 'edge', 'supply', 'basis', 'development', 'paper', 'considers', 'boundary', 'computing', 'kubernetes', 'orchestration', 'management', 'network']"
"Communication Infrastructure and Cloud Computing in Robotic Vessel as-a-Service Application The current trend in robotics is to enhance robustness against uncertainties through complex modern control methods, applied to either single- or multi-agent systems. While indeed these approaches provide noticeable performance improvements, their implementation requires, among others, enclosed environments, where the operator has full access to the control systems of the robot during execution. With constant improvement of the Internet of Things, real-time systems needed different software architectures and communication systems to enable their remote operation, with the same degree of flexibility when operating the systems in-situ. The paradigm shifted towards cloud computing, which moved the computational effort needed for various real-time processing algorithms on the server-sided application and focused on having lightweight interfaces between components, with the exception of the lower-level control loops, which remained within the processing units of the robotic systems. This paper describes the software architecture and communication system for a state-of-the-art service to enable, support and monitor different robotics applications. The case study presents all features of a small-sized but scalable robotics application, having multiple challenges when building a service oriented platform: autonomous robots, over- and underwater long-distance communication, sensor fusion and command and control of multiple users. The goal is to create a centralised cloud computing environment, with decentralised microservices and redundant resources, supporting a plethora of port-specific operations conducted with the help of waterborne robotics with different equipment configurations, that can be remotely operated and monitored in real time over cross-platform, lightweight applications.",Communication Infrastructure and Cloud Computing in Robotic Vessel as-a-Service Application,"The current trend in robotics is to enhance robustness against uncertainties through complex modern control methods, applied to either single- or multi-agent systems. While indeed these approaches provide noticeable performance improvements, their implementation requires, among others, enclosed environments, where the operator has full access to the control systems of the robot during execution. With constant improvement of the Internet of Things, real-time systems needed different software architectures and communication systems to enable their remote operation, with the same degree of flexibility when operating the systems in-situ. The paradigm shifted towards cloud computing, which moved the computational effort needed for various real-time processing algorithms on the server-sided application and focused on having lightweight interfaces between components, with the exception of the lower-level control loops, which remained within the processing units of the robotic systems. This paper describes the software architecture and communication system for a state-of-the-art service to enable, support and monitor different robotics applications. The case study presents all features of a small-sized but scalable robotics application, having multiple challenges when building a service oriented platform: autonomous robots, over- and underwater long-distance communication, sensor fusion and command and control of multiple users. The goal is to create a centralised cloud computing environment, with decentralised microservices and redundant resources, supporting a plethora of port-specific operations conducted with the help of waterborne robotics with different equipment configurations, that can be remotely operated and monitored in real time over cross-platform, lightweight applications.",IEEE conference,no,"['communication', 'infrastructure', 'computing', 'robotic', 'current', 'trend', 'robotics', 'enhance', 'robustness', 'uncertainty', 'complex', 'modern', 'control', 'method', 'applied', 'either', 'indeed', 'provide', 'noticeable', 'performance', 'improvement', 'implementation', 'requires', 'among', 'others', 'environment', 'operator', 'full', 'access', 'control', 'robot', 'execution', 'constant', 'improvement', 'internet', 'thing', 'needed', 'different', 'communication', 'enable', 'remote', 'operation', 'degree', 'flexibility', 'operating', 'paradigm', 'shifted', 'towards', 'computing', 'moved', 'computational', 'effort', 'needed', 'various', 'processing', 'algorithm', 'focused', 'lightweight', 'interface', 'control', 'loop', 'within', 'processing', 'unit', 'robotic', 'paper', 'describes', 'communication', 'enable', 'support', 'monitor', 'different', 'robotics', 'case', 'study', 'present', 'feature', 'scalable', 'robotics', 'multiple', 'challenge', 'building', 'oriented', 'platform', 'autonomous', 'robot', 'communication', 'sensor', 'fusion', 'command', 'control', 'multiple', 'user', 'goal', 'create', 'computing', 'environment', 'redundant', 'resource', 'supporting', 'plethora', 'operation', 'conducted', 'help', 'robotics', 'different', 'equipment', 'configuration', 'remotely', 'operated', 'monitored', 'real', 'time', 'lightweight']"
"Optimization of MQTT Communication Between Microservices in the IoT Cloud Number of devices being connected to the Internet is increasing at an incredible rate forming the Internet of things (IoT). IoT cloud gathers data from physical devices, processes it and shares among IoT system components. To ensure the quality of service, cloud architecture has to be fault-tolerant. In this paper, we presented a highly available, fault-tolerant microservice cloud architecture, where services are communicating via MQTT (Message Queuing Telemetry Transport) protocol. The main focus of the research is to optimize communication between cloud microservices. Thus, we propose the message routing algorithm, which we will refer to as a balanced forwarding algorithm.",Optimization of MQTT Communication Between Microservices in the IoT Cloud,"Number of devices being connected to the Internet is increasing at an incredible rate forming the Internet of things (IoT). IoT cloud gathers data from physical devices, processes it and shares among IoT system components. To ensure the quality of service, cloud architecture has to be fault-tolerant. In this paper, we presented a highly available, fault-tolerant microservice cloud architecture, where services are communicating via MQTT (Message Queuing Telemetry Transport) protocol. The main focus of the research is to optimize communication between cloud microservices. Thus, we propose the message routing algorithm, which we will refer to as a balanced forwarding algorithm.",IEEE conference,no,"['optimization', 'mqtt', 'communication', 'iot', 'number', 'device', 'connected', 'internet', 'increasing', 'rate', 'internet', 'thing', 'iot', 'iot', 'physical', 'device', 'process', 'share', 'among', 'iot', 'ensure', 'quality', 'paper', 'presented', 'highly', 'available', 'communicating', 'via', 'mqtt', 'message', 'queuing', 'telemetry', 'transport', 'protocol', 'main', 'focus', 'research', 'optimize', 'communication', 'thus', 'propose', 'message', 'routing', 'algorithm', 'balanced', 'algorithm']"
"A 5G PaaS Collaborative Management and Control Platform Technology Based on Cloud Edge Collaboration Based on Particle Swarm Optimization Algorithm The traditional cloud computing technology architecture mainly adopts the way of full set for the construction and operation of cloud computing center. With the development of cloud computing technology and the popularization of application, this all centralized mode of cloud computing may not be the optimal solution. Especially in the case of 5g derived vertical business scenarios, for the cloud collaborative management and control platform of power system, the system efficiency of full centralized mode is relatively low. In this paper, through the research on the key technologies of multi form PAAS platform in 5g scenario, the cloud side collaborative 5gpaas power system cloud collaborative management and control platform is realized. The platform is based on cloud native flexible microservice technology components and elastic resource computing, and integrates big data and artificial intelligence platform. This paper constructs a new generation of PAAS platform which is universal in the whole domain, unified in standard, multi cloud collaboration and flexible expansion, and realizes the global cloud, global virtualization and global perceptual construction of cloud collaborative management and control platform under 5g scenario. The platform has a certain reference value for the upgrading of cloud collaborative management and control of power system.",A 5G PaaS Collaborative Management and Control Platform Technology Based on Cloud Edge Collaboration Based on Particle Swarm Optimization Algorithm,"The traditional cloud computing technology architecture mainly adopts the way of full set for the construction and operation of cloud computing center. With the development of cloud computing technology and the popularization of application, this all centralized mode of cloud computing may not be the optimal solution. Especially in the case of 5g derived vertical business scenarios, for the cloud collaborative management and control platform of power system, the system efficiency of full centralized mode is relatively low. In this paper, through the research on the key technologies of multi form PAAS platform in 5g scenario, the cloud side collaborative 5gpaas power system cloud collaborative management and control platform is realized. The platform is based on cloud native flexible microservice technology components and elastic resource computing, and integrates big data and artificial intelligence platform. This paper constructs a new generation of PAAS platform which is universal in the whole domain, unified in standard, multi cloud collaboration and flexible expansion, and realizes the global cloud, global virtualization and global perceptual construction of cloud collaborative management and control platform under 5g scenario. The platform has a certain reference value for the upgrading of cloud collaborative management and control of power system.",IEEE conference,no,"['paas', 'collaborative', 'management', 'control', 'platform', 'technology', 'based', 'edge', 'collaboration', 'based', 'optimization', 'algorithm', 'traditional', 'computing', 'technology', 'mainly', 'adopts', 'way', 'full', 'set', 'construction', 'operation', 'computing', 'center', 'development', 'computing', 'technology', 'centralized', 'mode', 'computing', 'may', 'optimal', 'solution', 'especially', 'case', 'derived', 'vertical', 'business', 'scenario', 'collaborative', 'management', 'control', 'platform', 'power', 'efficiency', 'full', 'centralized', 'mode', 'relatively', 'low', 'paper', 'research', 'key', 'technology', 'form', 'paas', 'platform', 'scenario', 'side', 'collaborative', 'power', 'collaborative', 'management', 'control', 'platform', 'realized', 'platform', 'based', 'native', 'flexible', 'technology', 'elastic', 'resource', 'computing', 'integrates', 'big', 'artificial', 'intelligence', 'platform', 'paper', 'construct', 'new', 'generation', 'paas', 'platform', 'universal', 'whole', 'domain', 'unified', 'standard', 'collaboration', 'flexible', 'expansion', 'realizes', 'global', 'global', 'virtualization', 'global', 'construction', 'collaborative', 'management', 'control', 'platform', 'scenario', 'platform', 'certain', 'reference', 'value', 'upgrading', 'collaborative', 'management', 'control', 'power']"
"Self-adaptive Threshold-based Policy for Microservices Elasticity The microservice architecture structures an application as a collection of loosely coupled and distributed services. Since application workloads usually change over time, the number of replicas per microservice should be accordingly scaled at run-time. The most widely adopted scaling policy relies on statically defined thresholds, expressed in terms of system-oriented metrics. This policy might not be well-suited to scale multi-component and latency-sensitive applications, which express requirements in terms of response time. In this paper, we present a two-layered hierarchical solution for controlling the elasticity of microservice-based applications. The higher-level controller estimates the microservice contribution to the application performance, and informs the lower-level components. The latter accordingly scale the single microservices using a dynamic threshold-based policy. So, we propose MB Threshold and QL Threshold, two policies that employ respectively model-based and model-free reinforcement learning approaches to learn threshold update strategies. These policies can compute different thresholds for the different application components, according to the desired deployment objectives. A wide set of simulation results shows the benefits and flexibility of the proposed solution, emphasizing the advantages of using dynamic thresholds over the most adopted policy that uses static thresholds.",Self-adaptive Threshold-based Policy for Microservices Elasticity,"The microservice architecture structures an application as a collection of loosely coupled and distributed services. Since application workloads usually change over time, the number of replicas per microservice should be accordingly scaled at run-time. The most widely adopted scaling policy relies on statically defined thresholds, expressed in terms of system-oriented metrics. This policy might not be well-suited to scale multi-component and latency-sensitive applications, which express requirements in terms of response time. In this paper, we present a two-layered hierarchical solution for controlling the elasticity of microservice-based applications. The higher-level controller estimates the microservice contribution to the application performance, and informs the lower-level components. The latter accordingly scale the single microservices using a dynamic threshold-based policy. So, we propose MB Threshold and QL Threshold, two policies that employ respectively model-based and model-free reinforcement learning approaches to learn threshold update strategies. These policies can compute different thresholds for the different application components, according to the desired deployment objectives. A wide set of simulation results shows the benefits and flexibility of the proposed solution, emphasizing the advantages of using dynamic thresholds over the most adopted policy that uses static thresholds.",IEEE conference,no,"['policy', 'elasticity', 'structure', 'collection', 'loosely', 'coupled', 'distributed', 'since', 'workload', 'usually', 'change', 'time', 'number', 'per', 'accordingly', 'scaled', 'widely', 'adopted', 'scaling', 'policy', 'relies', 'defined', 'threshold', 'expressed', 'term', 'metric', 'policy', 'might', 'scale', 'requirement', 'term', 'response', 'time', 'paper', 'present', 'hierarchical', 'solution', 'controlling', 'elasticity', 'controller', 'estimate', 'contribution', 'performance', 'latter', 'accordingly', 'scale', 'single', 'using', 'dynamic', 'policy', 'propose', 'mb', 'threshold', 'threshold', 'two', 'policy', 'employ', 'respectively', 'reinforcement', 'learning', 'learn', 'threshold', 'update', 'strategy', 'policy', 'compute', 'different', 'threshold', 'different', 'according', 'desired', 'deployment', 'objective', 'wide', 'set', 'simulation', 'result', 'show', 'benefit', 'flexibility', 'proposed', 'solution', 'emphasizing', 'advantage', 'using', 'dynamic', 'threshold', 'adopted', 'policy', 'us', 'static', 'threshold']"
"The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer.",The Architecture of Kaligreen V2: A Middleware Aware of Hardware Opportunities to Save Energy,"Nowadays, energy saving in the use of information technologies is a very important issue both from the economic and sustainability point of view. Many scientists investigate methods to save energy at different application levels (cloud: i.e., architectures, grid: i.e., middlewares and frameworks and hardware management: i.e., operating systems) and many of them agree on the strategy of executing programs, processes or virtual machines only using the time and resources that are strictly necessary. For this, it is necessary to plan strategies for deployment and relocation of processes; but always taking into account hardware repercussions and the knowledge of the architecture and applications behavior. On the other hand, it has already been demonstrated that the use of microservices brings numerous advantages in availability and efficiency; but we do not find many jobs that exploit this technique on the energy level. In this article, we present the architecture of a middleware for distributed microservices-based applications, which allows any negotiation-based scheduling algorithm to duplicate or move microservices from one device to another in a non-centralized way for energy savings, taking into account the consumption characteristics of the microservices and the capabilities that the hardware components offer.",IEEE conference,no,"['middleware', 'aware', 'hardware', 'opportunity', 'save', 'energy', 'nowadays', 'energy', 'saving', 'use', 'information', 'technology', 'important', 'issue', 'economic', 'sustainability', 'point', 'view', 'many', 'scientist', 'investigate', 'method', 'save', 'energy', 'different', 'level', 'grid', 'framework', 'hardware', 'management', 'operating', 'many', 'strategy', 'executing', 'program', 'process', 'virtual', 'machine', 'using', 'time', 'resource', 'strictly', 'necessary', 'necessary', 'plan', 'strategy', 'deployment', 'relocation', 'process', 'always', 'taking', 'account', 'hardware', 'knowledge', 'behavior', 'hand', 'already', 'demonstrated', 'use', 'brings', 'numerous', 'advantage', 'availability', 'efficiency', 'find', 'many', 'job', 'exploit', 'technique', 'energy', 'level', 'article', 'present', 'middleware', 'distributed', 'allows', 'scheduling', 'algorithm', 'duplicate', 'move', 'one', 'device', 'another', 'way', 'energy', 'saving', 'taking', 'account', 'consumption', 'characteristic', 'capability', 'hardware', 'offer']"
"Latency-Aware Load Distribution Algorithm for Microservice Deployment in UAV Networks The Internet has become the driving wheel of social interactions as well as economic growth, but its access is vastly unequal. Noticeably, such rural dwellers experience a digital service access gap, due to the inadequate broadband infrastructure available to them when compared with their urban counterparts. This paper is aimed at overcoming the gap between rural communities with no reliable Internet service by deploying the service through the use of Unmanned Aerial Vehicles (UAVs). By decomposing the Internet of Things (IoT) applications into elements (namely microservices) and deploying them through UAVs, latency is shortened, distributing workload among UAVs while limiting microservice deployment to optimize request execution, making the quality service improvement, essential for such applications. Simulations revealed the effectiveness of our approach, as it indicated the lowest latency when a UAV connected to several users in scenarios where there was no Internet connectivity.",Latency-Aware Load Distribution Algorithm for Microservice Deployment in UAV Networks,"The Internet has become the driving wheel of social interactions as well as economic growth, but its access is vastly unequal. Noticeably, such rural dwellers experience a digital service access gap, due to the inadequate broadband infrastructure available to them when compared with their urban counterparts. This paper is aimed at overcoming the gap between rural communities with no reliable Internet service by deploying the service through the use of Unmanned Aerial Vehicles (UAVs). By decomposing the Internet of Things (IoT) applications into elements (namely microservices) and deploying them through UAVs, latency is shortened, distributing workload among UAVs while limiting microservice deployment to optimize request execution, making the quality service improvement, essential for such applications. Simulations revealed the effectiveness of our approach, as it indicated the lowest latency when a UAV connected to several users in scenarios where there was no Internet connectivity.",IEEE conference,no,"['load', 'distribution', 'algorithm', 'deployment', 'uav', 'network', 'internet', 'become', 'driving', 'social', 'interaction', 'well', 'economic', 'growth', 'access', 'rural', 'experience', 'digital', 'access', 'gap', 'due', 'infrastructure', 'available', 'compared', 'urban', 'counterpart', 'paper', 'aimed', 'overcoming', 'gap', 'rural', 'community', 'reliable', 'internet', 'deploying', 'use', 'vehicle', 'uavs', 'decomposing', 'internet', 'thing', 'iot', 'element', 'namely', 'deploying', 'uavs', 'latency', 'distributing', 'workload', 'among', 'uavs', 'limiting', 'deployment', 'optimize', 'request', 'execution', 'making', 'quality', 'improvement', 'essential', 'simulation', 'revealed', 'effectiveness', 'lowest', 'latency', 'uav', 'connected', 'several', 'user', 'scenario', 'internet', 'connectivity']"
A Kubernetes Algorithm for scaling Virtual Objects Continuous scalability of applications operating in the Internet of Things domain has raised the issue of continuous integration. Users and operators involved in the establishment of connected object ecosystems want to have the ability to change and modify their applications without having to stop the function of the physical components in order to deploy their various applications that run these objects. In this article we propose a continuous integration and scaling approach based on Kubernetes and object virtualization to deploy different types of appropriate microservices.,A Kubernetes Algorithm for scaling Virtual Objects,Continuous scalability of applications operating in the Internet of Things domain has raised the issue of continuous integration. Users and operators involved in the establishment of connected object ecosystems want to have the ability to change and modify their applications without having to stop the function of the physical components in order to deploy their various applications that run these objects. In this article we propose a continuous integration and scaling approach based on Kubernetes and object virtualization to deploy different types of appropriate microservices.,IEEE conference,no,"['kubernetes', 'algorithm', 'scaling', 'virtual', 'object', 'continuous', 'scalability', 'operating', 'internet', 'thing', 'domain', 'issue', 'continuous', 'integration', 'user', 'operator', 'involved', 'connected', 'object', 'ecosystem', 'ability', 'change', 'modify', 'without', 'stop', 'function', 'physical', 'order', 'deploy', 'various', 'run', 'object', 'article', 'propose', 'continuous', 'integration', 'scaling', 'based', 'kubernetes', 'object', 'virtualization', 'deploy', 'different', 'type', 'appropriate']"
"Pushing the Boundaries of Scalable 5G Core Networks: Cloud-Native NEF and CAPIF Interplay The advent of Cloud-Native principles and architectures stands as a beacon of innovation in the design of the next generation of 5G/6G, promising a transition from initial (disaggregated) 5G Service-Based Architectures (SBA) implemen-tations capable of running on the cloud to a truly distributed and scalable Cloud-Native 6G. However, the incumbent components and interfaces of the 5G core (5GC) pose barriers to the seamless integration of microservices. Challenges range from intricate Network Functions (NF) interdependencies to interface constraints and the absence of an event-driven architecture, which might jeopardise efficient microservice communication. This paper discusses the interplay between the Network Exposure Function (NEF) and the Common API Framework (CAPIF), defined by 3GPP, to standardize how network APIs are discovered, exposed, and consumed. First, we propose a novel microservice and eventdriven Network Exposure Function (MNEF) built upon the NEF standard, featuring CAPIF support and various purpose-built microservices. Then, we evaluate its interoperability with reference implementations of 5GC (Open5GS) and CAPIF (OpenCAPIF) and its performance and scalability for handling large-scale request scenarios. The results attest to the MNEF's benefits and interoperability with compliant 5GC implementations. Despite an additional (expected) overhead in communications, MNEF fulfils its purpose as defined in the 5G architecture, acting as an interface between external parties and 5G core NFs. Moreover, the proposed MNEF presents itself as a design solution that pushes the scalable 5G communication boundaries.",Pushing the Boundaries of Scalable 5G Core Networks: Cloud-Native NEF and CAPIF Interplay,"The advent of Cloud-Native principles and architectures stands as a beacon of innovation in the design of the next generation of 5G/6G, promising a transition from initial (disaggregated) 5G Service-Based Architectures (SBA) implemen-tations capable of running on the cloud to a truly distributed and scalable Cloud-Native 6G. However, the incumbent components and interfaces of the 5G core (5GC) pose barriers to the seamless integration of microservices. Challenges range from intricate Network Functions (NF) interdependencies to interface constraints and the absence of an event-driven architecture, which might jeopardise efficient microservice communication. This paper discusses the interplay between the Network Exposure Function (NEF) and the Common API Framework (CAPIF), defined by 3GPP, to standardize how network APIs are discovered, exposed, and consumed. First, we propose a novel microservice and eventdriven Network Exposure Function (MNEF) built upon the NEF standard, featuring CAPIF support and various purpose-built microservices. Then, we evaluate its interoperability with reference implementations of 5GC (Open5GS) and CAPIF (OpenCAPIF) and its performance and scalability for handling large-scale request scenarios. The results attest to the MNEF's benefits and interoperability with compliant 5GC implementations. Despite an additional (expected) overhead in communications, MNEF fulfils its purpose as defined in the 5G architecture, acting as an interface between external parties and 5G core NFs. Moreover, the proposed MNEF presents itself as a design solution that pushes the scalable 5G communication boundaries.",IEEE conference,no,"['pushing', 'boundary', 'scalable', 'core', 'network', 'nef', 'capif', 'interplay', 'advent', 'principle', 'stand', 'design', 'next', 'generation', 'promising', 'transition', 'initial', 'disaggregated', 'sba', 'capable', 'running', 'distributed', 'scalable', 'however', 'interface', 'core', 'pose', 'barrier', 'seamless', 'integration', 'challenge', 'range', 'intricate', 'network', 'function', 'interdependency', 'interface', 'constraint', 'might', 'efficient', 'communication', 'paper', 'discusses', 'interplay', 'network', 'exposure', 'function', 'nef', 'common', 'api', 'framework', 'capif', 'defined', 'network', 'apis', 'exposed', 'consumed', 'first', 'propose', 'novel', 'network', 'exposure', 'function', 'mnef', 'built', 'upon', 'nef', 'standard', 'capif', 'support', 'various', 'evaluate', 'interoperability', 'reference', 'implementation', 'capif', 'performance', 'scalability', 'handling', 'request', 'scenario', 'result', 'mnef', 'benefit', 'interoperability', 'compliant', 'implementation', 'despite', 'additional', 'expected', 'overhead', 'communication', 'mnef', 'purpose', 'defined', 'interface', 'external', 'core', 'moreover', 'proposed', 'mnef', 'present', 'design', 'solution', 'push', 'scalable', 'communication', 'boundary']"
"Towards Osmotic Computing: Analyzing Overlay Network Solutions to Optimize the Deployment of Container-Based Microservices in Fog, Edge and IoT Environments In recent years, the rapid growth of new Cloud technologies acted as an enabling factor for the adoption of microservices based architecture that leverages container virtualization in order to build modular and robust systems. As the number of containers running on hosts increases, it becomes essential to have tools to manage them in a simple, straightforward manner and with a high level of abstraction. Osmotic Computing is an emerging research field that studies the migration, deployment and optimization of microservices from the Cloud to Fog, Edge, and Internet of Things (IoT) environments. However, in order to achieve Osmotic Computing environments, connectivity issues have to be addressed. This paper investigates these connectivity issues leveraging different network overlays. In particular, we analyze the performance of four network overlays that are OVN, Calico, Weave, and Flannel. Our results give a concrete overview in terms of overhead and performances for each proposed overlay solution, helping us to understand which the best overlay solution is. Specifically, we deployed CoAP and FTP microservices which helped us to carry out these benchmarks and collect the results in terms of transfer times.","Towards Osmotic Computing: Analyzing Overlay Network Solutions to Optimize the Deployment of Container-Based Microservices in Fog, Edge and IoT Environments","In recent years, the rapid growth of new Cloud technologies acted as an enabling factor for the adoption of microservices based architecture that leverages container virtualization in order to build modular and robust systems. As the number of containers running on hosts increases, it becomes essential to have tools to manage them in a simple, straightforward manner and with a high level of abstraction. Osmotic Computing is an emerging research field that studies the migration, deployment and optimization of microservices from the Cloud to Fog, Edge, and Internet of Things (IoT) environments. However, in order to achieve Osmotic Computing environments, connectivity issues have to be addressed. This paper investigates these connectivity issues leveraging different network overlays. In particular, we analyze the performance of four network overlays that are OVN, Calico, Weave, and Flannel. Our results give a concrete overview in terms of overhead and performances for each proposed overlay solution, helping us to understand which the best overlay solution is. Specifically, we deployed CoAP and FTP microservices which helped us to carry out these benchmarks and collect the results in terms of transfer times.",IEEE conference,no,"['towards', 'osmotic', 'computing', 'analyzing', 'overlay', 'network', 'solution', 'optimize', 'deployment', 'fog', 'edge', 'iot', 'environment', 'recent', 'year', 'rapid', 'growth', 'new', 'technology', 'enabling', 'factor', 'adoption', 'based', 'leverage', 'container', 'virtualization', 'order', 'build', 'modular', 'robust', 'number', 'container', 'running', 'host', 'increase', 'becomes', 'essential', 'tool', 'manage', 'simple', 'straightforward', 'manner', 'high', 'level', 'abstraction', 'osmotic', 'computing', 'emerging', 'research', 'field', 'study', 'migration', 'deployment', 'optimization', 'fog', 'edge', 'internet', 'thing', 'iot', 'environment', 'however', 'order', 'achieve', 'osmotic', 'computing', 'environment', 'connectivity', 'issue', 'addressed', 'paper', 'investigates', 'connectivity', 'issue', 'leveraging', 'different', 'network', 'overlay', 'particular', 'analyze', 'performance', 'four', 'network', 'overlay', 'result', 'give', 'overview', 'term', 'overhead', 'performance', 'proposed', 'overlay', 'solution', 'helping', 'u', 'understand', 'best', 'overlay', 'solution', 'specifically', 'deployed', 'helped', 'u', 'carry', 'benchmark', 'collect', 'result', 'term', 'transfer', 'time']"
"Container and Microservice Driven Design for Cloud Infrastructure DevOps Emerging container technologies, such as Docker, offer unprecedented agility in developing and running applications in cloud environment especially when combined with a microservice-style architecture. However, it is often difficult to use containers to manage the cloud infrastructure, without sacrificing many benefits container offers. This paper identifies the key challenges that impede realizing the full promise of containerizing infrastructure services. Using OpenStack as a case study, we explore solutions to these challenges. Specifically, we redesign OpenStack deployment architecture to enable dynamic service registration and discovery, explore different ways to manage service state in containers, and enable containers to access the host kernel and devices. We quantify the efficiency of the container-based microservice-style DevOps compared to the VM-based approach, and study the scalability of the stateless and stateful containerized components. We also discuss limitations in our current design, and highlight open research problems that, if solved, can lead to wider adoption of containers in cloud infrastructure management.",Container and Microservice Driven Design for Cloud Infrastructure DevOps,"Emerging container technologies, such as Docker, offer unprecedented agility in developing and running applications in cloud environment especially when combined with a microservice-style architecture. However, it is often difficult to use containers to manage the cloud infrastructure, without sacrificing many benefits container offers. This paper identifies the key challenges that impede realizing the full promise of containerizing infrastructure services. Using OpenStack as a case study, we explore solutions to these challenges. Specifically, we redesign OpenStack deployment architecture to enable dynamic service registration and discovery, explore different ways to manage service state in containers, and enable containers to access the host kernel and devices. We quantify the efficiency of the container-based microservice-style DevOps compared to the VM-based approach, and study the scalability of the stateless and stateful containerized components. We also discuss limitations in our current design, and highlight open research problems that, if solved, can lead to wider adoption of containers in cloud infrastructure management.",IEEE conference,no,"['container', 'driven', 'design', 'infrastructure', 'devops', 'emerging', 'container', 'technology', 'docker', 'offer', 'unprecedented', 'agility', 'developing', 'running', 'environment', 'especially', 'combined', 'however', 'often', 'difficult', 'use', 'container', 'manage', 'infrastructure', 'without', 'many', 'benefit', 'container', 'offer', 'paper', 'identifies', 'key', 'challenge', 'realizing', 'full', 'promise', 'infrastructure', 'using', 'openstack', 'case', 'study', 'explore', 'solution', 'challenge', 'specifically', 'redesign', 'openstack', 'deployment', 'enable', 'dynamic', 'registration', 'discovery', 'explore', 'different', 'way', 'manage', 'state', 'container', 'enable', 'container', 'access', 'host', 'kernel', 'device', 'quantify', 'efficiency', 'devops', 'compared', 'study', 'scalability', 'stateless', 'stateful', 'containerized', 'also', 'discus', 'limitation', 'current', 'design', 'highlight', 'open', 'research', 'problem', 'solved', 'lead', 'adoption', 'container', 'infrastructure', 'management']"
"Architecture Blueprints to Enable Scalable Vertical Integration of Assets with Digital Twins Many Industry 4.0 use cases require the integration of live data, e.g., from sensors and devices. However, the large number of legacy fieldbus protocols and proprietary data formats turns this integration into an effort-consuming task. As the number of digital twins in a factory increases rapidly, data source integration has to scale well. Until now, little guidance is available on how to implement this integration in a scalable and reusable manner for Industry 4.0. To close this gap, we define five architecture blueprints based on our experience in various Industry 4.0 projects. These blueprints detail various integration scenarios differentiated by key attributes like frequency of data consumption and data production. In these architecture blueprints, two core components, the Updater and the Delegator, are identified. By providing and evaluating our open-source implementation of these two components, we show the feasibility of the defined blueprints. Utilizing the provided open-source components and the defined architecture blueprints will benefit practitioners as well as researchers when it comes to data integration with digital twins.",Architecture Blueprints to Enable Scalable Vertical Integration of Assets with Digital Twins,"Many Industry 4.0 use cases require the integration of live data, e.g., from sensors and devices. However, the large number of legacy fieldbus protocols and proprietary data formats turns this integration into an effort-consuming task. As the number of digital twins in a factory increases rapidly, data source integration has to scale well. Until now, little guidance is available on how to implement this integration in a scalable and reusable manner for Industry 4.0. To close this gap, we define five architecture blueprints based on our experience in various Industry 4.0 projects. These blueprints detail various integration scenarios differentiated by key attributes like frequency of data consumption and data production. In these architecture blueprints, two core components, the Updater and the Delegator, are identified. By providing and evaluating our open-source implementation of these two components, we show the feasibility of the defined blueprints. Utilizing the provided open-source components and the defined architecture blueprints will benefit practitioners as well as researchers when it comes to data integration with digital twins.",IEEE conference,no,"['blueprint', 'enable', 'scalable', 'vertical', 'integration', 'asset', 'digital', 'twin', 'many', 'industry', 'use', 'case', 'require', 'integration', 'live', 'sensor', 'device', 'however', 'large', 'number', 'legacy', 'protocol', 'format', 'turn', 'integration', 'task', 'number', 'digital', 'twin', 'factory', 'increase', 'rapidly', 'source', 'integration', 'scale', 'well', 'little', 'guidance', 'available', 'implement', 'integration', 'scalable', 'reusable', 'manner', 'industry', 'close', 'gap', 'define', 'five', 'blueprint', 'based', 'experience', 'various', 'industry', 'project', 'blueprint', 'detail', 'various', 'integration', 'scenario', 'differentiated', 'key', 'attribute', 'like', 'frequency', 'consumption', 'production', 'blueprint', 'two', 'core', 'identified', 'providing', 'evaluating', 'implementation', 'two', 'show', 'feasibility', 'defined', 'blueprint', 'utilizing', 'provided', 'defined', 'blueprint', 'benefit', 'practitioner', 'well', 'researcher', 'come', 'integration', 'digital', 'twin']"
"AnB: Application-in-a-Box to Rapidly Deploy and Self-optimize 5G Apps We present ""Application in a Box"" (AnB) product concept aimed at simplifying the deployment and operation of remote 5G applications. AnB comes pre-configured with all necessary hardware and software components, including sensors like cameras, hardware and software components for a local 5G wireless network, and 5G-ready apps. Enterprises can easily download additional apps from an App Store. Setting up a 5G infrastructure and running applications on it is a significant challenge, but AnB is designed to make it fast, convenient, and easy, even for those without extensive knowledge of software, computers, wireless networks, or AI-based analytics. With AnB, customers only need to open the box, set up the sensors, turn on the 5G networking and edge computing devices, and start running their applications. Our system software automatically deploys and optimizes the pipeline of microservices in the application on a tiered computing infrastructure that includes device, edge, and cloud computing. Application scalability, dynamic resource management, placement of critical tasks for low-latency response, and dynamic network bandwidth allocation for efficient 5G network usage are all automatically orchestrated.AnB offers cost savings, simplified setup and management, and increased reliability and security. We’ve implemented several real-world applications, such as collision prediction at busy traffic light intersections and remote construction site monitoring using video analytics. With AnB, deployment and optimization effort can be reduced from several months to just a few minutes. This is the first-of-its-kind approach to easing deployment effort and automating self-optimization of the application during system operation.",AnB: Application-in-a-Box to Rapidly Deploy and Self-optimize 5G Apps,"We present ""Application in a Box"" (AnB) product concept aimed at simplifying the deployment and operation of remote 5G applications. AnB comes pre-configured with all necessary hardware and software components, including sensors like cameras, hardware and software components for a local 5G wireless network, and 5G-ready apps. Enterprises can easily download additional apps from an App Store. Setting up a 5G infrastructure and running applications on it is a significant challenge, but AnB is designed to make it fast, convenient, and easy, even for those without extensive knowledge of software, computers, wireless networks, or AI-based analytics. With AnB, customers only need to open the box, set up the sensors, turn on the 5G networking and edge computing devices, and start running their applications. Our system software automatically deploys and optimizes the pipeline of microservices in the application on a tiered computing infrastructure that includes device, edge, and cloud computing. Application scalability, dynamic resource management, placement of critical tasks for low-latency response, and dynamic network bandwidth allocation for efficient 5G network usage are all automatically orchestrated.AnB offers cost savings, simplified setup and management, and increased reliability and security. We’ve implemented several real-world applications, such as collision prediction at busy traffic light intersections and remote construction site monitoring using video analytics. With AnB, deployment and optimization effort can be reduced from several months to just a few minutes. This is the first-of-its-kind approach to easing deployment effort and automating self-optimization of the application during system operation.",IEEE conference,no,"['anb', 'rapidly', 'deploy', 'apps', 'present', 'box', 'anb', 'product', 'concept', 'aimed', 'deployment', 'operation', 'remote', 'anb', 'come', 'necessary', 'hardware', 'including', 'sensor', 'like', 'camera', 'hardware', 'local', 'wireless', 'network', 'apps', 'enterprise', 'easily', 'additional', 'apps', 'app', 'store', 'setting', 'infrastructure', 'running', 'significant', 'challenge', 'anb', 'designed', 'make', 'fast', 'convenient', 'easy', 'even', 'without', 'extensive', 'knowledge', 'computer', 'wireless', 'network', 'analytics', 'anb', 'customer', 'need', 'open', 'box', 'set', 'sensor', 'turn', 'networking', 'edge', 'computing', 'device', 'start', 'running', 'automatically', 'deploys', 'pipeline', 'computing', 'infrastructure', 'includes', 'device', 'edge', 'computing', 'scalability', 'dynamic', 'resource', 'management', 'placement', 'critical', 'task', 'response', 'dynamic', 'network', 'bandwidth', 'allocation', 'efficient', 'network', 'usage', 'automatically', 'offer', 'cost', 'saving', 'simplified', 'setup', 'management', 'increased', 'reliability', 'security', 'implemented', 'several', 'prediction', 'traffic', 'light', 'remote', 'construction', 'site', 'monitoring', 'using', 'video', 'analytics', 'anb', 'deployment', 'optimization', 'effort', 'reduced', 'several', 'deployment', 'effort', 'automating', 'operation']"
"A Platform-Based Approach to Implementation of Future Smart Distributed Energy Control Systems Smart distributed energy is a combination of advanced power supply and information technologies that enable reliable and efficient management of circuit segments with distributed energy sources, electric energy storage systems, and controlled load. Smooth operation of such segments requires high level of unification and automation for such procedures as information exchange between participants of management cycles, optimal operation planning for equipment included into physical or virtual groups, clearing of financial transactions, etc. In order to meet such requirements with minimal time and labor, it is proposed to compose and operate applied control systems within the framework of a digital platform. The platform architecture is presented based upon reusable functional microservices, viz. small, loosely connected, and easily modifiable components with an open application program interface, that interact in a unified information environment. Applications that automate various business functions of smart distributed energy control are composed from calls to microservices, adapters of interaction with external systems, and graphical user interface. Algorithms are presented as part of the platform's mathematical means, for automatic modeling and analyzing electrical modes, forecasting load/generation and external technical and economic factors, and solving optimization control problems. Procedures for composing and operating digital twins of smart distributed energy objects over the platform's data bases and streams are described.",A Platform-Based Approach to Implementation of Future Smart Distributed Energy Control Systems,"Smart distributed energy is a combination of advanced power supply and information technologies that enable reliable and efficient management of circuit segments with distributed energy sources, electric energy storage systems, and controlled load. Smooth operation of such segments requires high level of unification and automation for such procedures as information exchange between participants of management cycles, optimal operation planning for equipment included into physical or virtual groups, clearing of financial transactions, etc. In order to meet such requirements with minimal time and labor, it is proposed to compose and operate applied control systems within the framework of a digital platform. The platform architecture is presented based upon reusable functional microservices, viz. small, loosely connected, and easily modifiable components with an open application program interface, that interact in a unified information environment. Applications that automate various business functions of smart distributed energy control are composed from calls to microservices, adapters of interaction with external systems, and graphical user interface. Algorithms are presented as part of the platform's mathematical means, for automatic modeling and analyzing electrical modes, forecasting load/generation and external technical and economic factors, and solving optimization control problems. Procedures for composing and operating digital twins of smart distributed energy objects over the platform's data bases and streams are described.",IEEE conference,no,"['implementation', 'future', 'smart', 'distributed', 'energy', 'control', 'smart', 'distributed', 'energy', 'combination', 'advanced', 'power', 'supply', 'information', 'technology', 'enable', 'reliable', 'efficient', 'management', 'circuit', 'segment', 'distributed', 'energy', 'source', 'electric', 'energy', 'storage', 'controlled', 'load', 'operation', 'segment', 'requires', 'high', 'level', 'automation', 'procedure', 'information', 'exchange', 'participant', 'management', 'cycle', 'optimal', 'operation', 'planning', 'equipment', 'included', 'physical', 'virtual', 'group', 'financial', 'transaction', 'etc', 'order', 'meet', 'requirement', 'minimal', 'time', 'proposed', 'compose', 'operate', 'applied', 'control', 'within', 'framework', 'digital', 'platform', 'platform', 'presented', 'based', 'upon', 'reusable', 'functional', 'viz', 'small', 'loosely', 'connected', 'easily', 'open', 'program', 'interface', 'interact', 'unified', 'information', 'environment', 'automate', 'various', 'business', 'function', 'smart', 'distributed', 'energy', 'control', 'composed', 'call', 'adapter', 'interaction', 'external', 'user', 'interface', 'algorithm', 'presented', 'part', 'platform', 'mathematical', 'mean', 'automatic', 'modeling', 'analyzing', 'mode', 'forecasting', 'external', 'technical', 'economic', 'factor', 'solving', 'optimization', 'control', 'problem', 'procedure', 'composing', 'operating', 'digital', 'twin', 'smart', 'distributed', 'energy', 'object', 'platform', 'base', 'stream', 'described']"
"Towards Optimal Load Balancing in Multi-Zone Kubernetes Clusters via Reinforcement Learning With the advent of container technology, companies have been developing microservice-based applications, converting the old monolithic software into a group of loosely coupled containers, with the aim of offering greater flexibility and improving operational efficiency. When users access microservices, their initial point of contact is typically a load balancer. This component is responsible for distributing incoming traffic or requests between multiple instances of microservices. Traditional load balancing approaches mainly rely on round-robin, or weighted roundrobin algorithms which are inadequate to maintain the overall performance and scalability of microservice-based applications. Microservices are often deployed in dynamic environments needing a more adaptive and efficient load balancing strategy to optimize resources and reduce the overall latency for end users. This paper presents a dynamic load balancer for Kubernetes (K8s) clusters based on Reinforcement Learning (RL). It aims to minimize the overall latency while promoting fair distribution of requests. To achieve this goal, the load balancer considers both current network delays and processing loads in the cluster. The evaluation shows that our solution is effective even in environments where both the network traffic and the processing loads in the cluster change dynamically over time. In addition, this study highlights the flexibility of DeepSets neural networks in solving the load balancing challenge in diverse setups without retraining. The results show that the DeepSets algorithms can solve the microservice load balancing problem even in scenarios up to 30 times larger than the trained setup.",Towards Optimal Load Balancing in Multi-Zone Kubernetes Clusters via Reinforcement Learning,"With the advent of container technology, companies have been developing microservice-based applications, converting the old monolithic software into a group of loosely coupled containers, with the aim of offering greater flexibility and improving operational efficiency. When users access microservices, their initial point of contact is typically a load balancer. This component is responsible for distributing incoming traffic or requests between multiple instances of microservices. Traditional load balancing approaches mainly rely on round-robin, or weighted roundrobin algorithms which are inadequate to maintain the overall performance and scalability of microservice-based applications. Microservices are often deployed in dynamic environments needing a more adaptive and efficient load balancing strategy to optimize resources and reduce the overall latency for end users. This paper presents a dynamic load balancer for Kubernetes (K8s) clusters based on Reinforcement Learning (RL). It aims to minimize the overall latency while promoting fair distribution of requests. To achieve this goal, the load balancer considers both current network delays and processing loads in the cluster. The evaluation shows that our solution is effective even in environments where both the network traffic and the processing loads in the cluster change dynamically over time. In addition, this study highlights the flexibility of DeepSets neural networks in solving the load balancing challenge in diverse setups without retraining. The results show that the DeepSets algorithms can solve the microservice load balancing problem even in scenarios up to 30 times larger than the trained setup.",IEEE conference,no,"['towards', 'optimal', 'load', 'balancing', 'kubernetes', 'cluster', 'via', 'reinforcement', 'learning', 'advent', 'container', 'technology', 'company', 'developing', 'converting', 'monolithic', 'group', 'loosely', 'coupled', 'container', 'aim', 'offering', 'greater', 'flexibility', 'improving', 'operational', 'efficiency', 'user', 'access', 'initial', 'point', 'typically', 'load', 'balancer', 'responsible', 'distributing', 'incoming', 'traffic', 'request', 'multiple', 'instance', 'traditional', 'load', 'balancing', 'mainly', 'rely', 'algorithm', 'maintain', 'overall', 'performance', 'scalability', 'often', 'deployed', 'dynamic', 'environment', 'adaptive', 'efficient', 'load', 'balancing', 'strategy', 'optimize', 'resource', 'reduce', 'overall', 'latency', 'end', 'user', 'paper', 'present', 'dynamic', 'load', 'balancer', 'kubernetes', 'cluster', 'based', 'reinforcement', 'learning', 'rl', 'aim', 'minimize', 'overall', 'latency', 'promoting', 'fair', 'distribution', 'request', 'achieve', 'goal', 'load', 'balancer', 'considers', 'current', 'network', 'delay', 'processing', 'load', 'cluster', 'evaluation', 'show', 'solution', 'effective', 'even', 'environment', 'network', 'traffic', 'processing', 'load', 'cluster', 'change', 'dynamically', 'time', 'addition', 'study', 'highlight', 'flexibility', 'neural', 'network', 'solving', 'load', 'balancing', 'challenge', 'diverse', 'setup', 'without', 'result', 'show', 'algorithm', 'solve', 'load', 'balancing', 'problem', 'even', 'scenario', 'time', 'larger', 'trained', 'setup']"
"Protagoras: A Service for Tagging E-Commerce Products at Scale Despite widespread adoption of machine learning to solve real world problems, the implementation of ML solutions in production environment is more complicated than it seems. It is quite straightforward to write machine learning codes these days but they are not designed to be deployed in production scale where millions of requests per day is a norm. In this paper, we describe our implementation of a ML service for large scale product tagging in e-commerce called Protagoras. The problem of tagging products can be seen as multi-label classification where the labels are product tags. By performing the classification within each product category, the precision can be increased and the inference can be performed faster. Protagoras combined the scalability and speed of microservice implementation in Golang and robust machine learning implementation in Python. We present the architecture of the system with all its components including API endpoints, job queue, database, and monitoring. The benchmark shows that, even with 1000 classifiers in one category, the average latency for online inference is below 300 millisecond. The throughput can be further maximized by replicating the service into multiple servers.",Protagoras: A Service for Tagging E-Commerce Products at Scale,"Despite widespread adoption of machine learning to solve real world problems, the implementation of ML solutions in production environment is more complicated than it seems. It is quite straightforward to write machine learning codes these days but they are not designed to be deployed in production scale where millions of requests per day is a norm. In this paper, we describe our implementation of a ML service for large scale product tagging in e-commerce called Protagoras. The problem of tagging products can be seen as multi-label classification where the labels are product tags. By performing the classification within each product category, the precision can be increased and the inference can be performed faster. Protagoras combined the scalability and speed of microservice implementation in Golang and robust machine learning implementation in Python. We present the architecture of the system with all its components including API endpoints, job queue, database, and monitoring. The benchmark shows that, even with 1000 classifiers in one category, the average latency for online inference is below 300 millisecond. The throughput can be further maximized by replicating the service into multiple servers.",IEEE conference,no,"['protagoras', 'tagging', 'product', 'scale', 'despite', 'widespread', 'adoption', 'machine', 'learning', 'solve', 'real', 'world', 'problem', 'implementation', 'ml', 'solution', 'production', 'environment', 'complicated', 'seems', 'quite', 'straightforward', 'write', 'machine', 'learning', 'code', 'day', 'designed', 'deployed', 'production', 'scale', 'million', 'request', 'per', 'day', 'paper', 'describe', 'implementation', 'ml', 'large', 'scale', 'product', 'tagging', 'called', 'protagoras', 'problem', 'tagging', 'product', 'seen', 'classification', 'product', 'tag', 'performing', 'classification', 'within', 'product', 'category', 'precision', 'increased', 'inference', 'performed', 'faster', 'protagoras', 'combined', 'scalability', 'speed', 'implementation', 'golang', 'robust', 'machine', 'learning', 'implementation', 'python', 'present', 'including', 'api', 'endpoint', 'job', 'queue', 'database', 'monitoring', 'benchmark', 'show', 'even', 'one', 'category', 'average', 'latency', 'online', 'inference', 'throughput', 'replicating', 'multiple', 'server']"
"Application and Infrastructure-Aware Orchestration in the Cloud-to-Edge Continuum Defining a scheduling and orchestration strategy for modern distributed microservices-based applications is a complex problem to deal with, especially if they are deployed on geo-distributed Cloud-to-Edge environments. Kubernetes is today the de-facto standard for container orchestration on Cloud data centers. However, its static container scheduling strategy is not suitable for the placement of complex and distributed microservices-based applications on Edge environments. Current infrastructure network conditions and resource availability neither run time application state are taken into account when scheduling microservices. To deal with these limitations in this work we present an extension of the Kubernetes platform in order to implement an effective application and infrastructure-aware container scheduling and orchestration strategy. In particular, we propose an extension of the default Kubernetes scheduler that considers application and infrastructure telemetry data when taking scheduling decisions. Furthermore, a descheduler component is also proposed that continuously tunes the application placement based on the ever changing application and infrastructure states. An evaluation of the proposed approach is presented by comparing it with the default Kubernetes scheduling strategy.",Application and Infrastructure-Aware Orchestration in the Cloud-to-Edge Continuum,"Defining a scheduling and orchestration strategy for modern distributed microservices-based applications is a complex problem to deal with, especially if they are deployed on geo-distributed Cloud-to-Edge environments. Kubernetes is today the de-facto standard for container orchestration on Cloud data centers. However, its static container scheduling strategy is not suitable for the placement of complex and distributed microservices-based applications on Edge environments. Current infrastructure network conditions and resource availability neither run time application state are taken into account when scheduling microservices. To deal with these limitations in this work we present an extension of the Kubernetes platform in order to implement an effective application and infrastructure-aware container scheduling and orchestration strategy. In particular, we propose an extension of the default Kubernetes scheduler that considers application and infrastructure telemetry data when taking scheduling decisions. Furthermore, a descheduler component is also proposed that continuously tunes the application placement based on the ever changing application and infrastructure states. An evaluation of the proposed approach is presented by comparing it with the default Kubernetes scheduling strategy.",IEEE conference,no,"['orchestration', 'continuum', 'defining', 'scheduling', 'orchestration', 'strategy', 'modern', 'distributed', 'complex', 'problem', 'deal', 'especially', 'deployed', 'environment', 'kubernetes', 'today', 'standard', 'container', 'orchestration', 'center', 'however', 'static', 'container', 'scheduling', 'strategy', 'suitable', 'placement', 'complex', 'distributed', 'edge', 'environment', 'current', 'infrastructure', 'network', 'condition', 'resource', 'availability', 'run', 'time', 'state', 'taken', 'account', 'scheduling', 'deal', 'limitation', 'work', 'present', 'extension', 'kubernetes', 'platform', 'order', 'implement', 'effective', 'container', 'scheduling', 'orchestration', 'strategy', 'particular', 'propose', 'extension', 'default', 'kubernetes', 'scheduler', 'considers', 'infrastructure', 'telemetry', 'taking', 'scheduling', 'decision', 'furthermore', 'also', 'proposed', 'continuously', 'placement', 'based', 'ever', 'changing', 'infrastructure', 'state', 'evaluation', 'proposed', 'presented', 'comparing', 'default', 'kubernetes', 'scheduling', 'strategy']"
"Towards a Digital Twin Platform for Industrie 4.0 In an Industrie 4.0 (I4.0), rigid structures and architectures applied in manufacturing and industrial information technologies today, should be replaced by highly dynamic and self-organizing networks. Today's proprietary technical systems lead to strictly defined engineering processes and value chains. Interacting Digital Twins (DT) are considered an enabling technology that could help to increase flexibility based on semantically enriched information. Nevertheless, for interacting digital twins to become a reality, their implementation should be based on existing standards like the Asset Administration Shell (AAS). Additionally, DT Platforms could accelerate development, deployment and ensure resilient operation of DT. This paper presents such a platform based on a microservices architecture and offering solutions for continuous deployment, data infrastructure and I4.0 business services. The platform is evaluated in the use case scenarios platform-based manufacturing and collaborative condition monitoring. As a result, implemented AAS-based mi-croservices organize manufacturing, and submodels of the AAS enable cross-company data sharing for collaborative condition monitoring. Future work should focus on fault-management and service recovery, as well as integration of the AAS into lower platform layers, e.g. for improving data usage control.",Towards a Digital Twin Platform for Industrie 4.0,"In an Industrie 4.0 (I4.0), rigid structures and architectures applied in manufacturing and industrial information technologies today, should be replaced by highly dynamic and self-organizing networks. Today's proprietary technical systems lead to strictly defined engineering processes and value chains. Interacting Digital Twins (DT) are considered an enabling technology that could help to increase flexibility based on semantically enriched information. Nevertheless, for interacting digital twins to become a reality, their implementation should be based on existing standards like the Asset Administration Shell (AAS). Additionally, DT Platforms could accelerate development, deployment and ensure resilient operation of DT. This paper presents such a platform based on a microservices architecture and offering solutions for continuous deployment, data infrastructure and I4.0 business services. The platform is evaluated in the use case scenarios platform-based manufacturing and collaborative condition monitoring. As a result, implemented AAS-based mi-croservices organize manufacturing, and submodels of the AAS enable cross-company data sharing for collaborative condition monitoring. Future work should focus on fault-management and service recovery, as well as integration of the AAS into lower platform layers, e.g. for improving data usage control.",IEEE conference,no,"['towards', 'digital', 'twin', 'platform', 'structure', 'applied', 'manufacturing', 'industrial', 'information', 'technology', 'today', 'replaced', 'highly', 'dynamic', 'network', 'today', 'technical', 'lead', 'strictly', 'defined', 'engineering', 'process', 'value', 'chain', 'interacting', 'digital', 'twin', 'dt', 'considered', 'enabling', 'technology', 'could', 'help', 'increase', 'flexibility', 'based', 'information', 'nevertheless', 'interacting', 'digital', 'twin', 'become', 'reality', 'implementation', 'based', 'existing', 'standard', 'like', 'asset', 'administration', 'aa', 'additionally', 'dt', 'platform', 'could', 'accelerate', 'development', 'deployment', 'ensure', 'resilient', 'operation', 'dt', 'paper', 'present', 'platform', 'based', 'offering', 'solution', 'continuous', 'deployment', 'infrastructure', 'business', 'platform', 'evaluated', 'use', 'case', 'scenario', 'manufacturing', 'collaborative', 'condition', 'monitoring', 'result', 'implemented', 'manufacturing', 'aa', 'enable', 'sharing', 'collaborative', 'condition', 'monitoring', 'future', 'work', 'focus', 'recovery', 'well', 'integration', 'aa', 'lower', 'platform', 'layer', 'improving', 'usage', 'control']"
"Intelligent Distribution Grid Reliability Simplification Algorithm and Online Computing Implementation At present, reliability calculation is mostly focused on the calculation of current years, and the calculation of planned years is often carried out using fuzzy estimation. The calculation methods are complex and diverse, which is not conducive to the modular implementation of software. This article deeply analyzes the essence of reliability calculation based on the topology simplification model of power flow, and summarizes the parts related to the grid structure in its classic calculation formula as the grid coefficient. The parts related to the equipment are collected as the equipment coefficient, and the calculation is simplified based on practical applications. Then, based on the power grid resource business center and data center, a centralized (microservices/micro applications) + collaborative computing framework construction model is adopted, integrating basic data such as reliability facility ledger, topology, and graph model, and obtaining important parameters such as fault rate and average recovery time of distribution grid equipment components. Based on simplified calculation formulas, a power supply reliability online calculation micro application module is developed, The calculation results can provide convenient data support for reliability management and decision-making of distribution grids. This article also provides the specific implementation of software architecture design and interface display effects.",Intelligent Distribution Grid Reliability Simplification Algorithm and Online Computing Implementation,"At present, reliability calculation is mostly focused on the calculation of current years, and the calculation of planned years is often carried out using fuzzy estimation. The calculation methods are complex and diverse, which is not conducive to the modular implementation of software. This article deeply analyzes the essence of reliability calculation based on the topology simplification model of power flow, and summarizes the parts related to the grid structure in its classic calculation formula as the grid coefficient. The parts related to the equipment are collected as the equipment coefficient, and the calculation is simplified based on practical applications. Then, based on the power grid resource business center and data center, a centralized (microservices/micro applications) + collaborative computing framework construction model is adopted, integrating basic data such as reliability facility ledger, topology, and graph model, and obtaining important parameters such as fault rate and average recovery time of distribution grid equipment components. Based on simplified calculation formulas, a power supply reliability online calculation micro application module is developed, The calculation results can provide convenient data support for reliability management and decision-making of distribution grids. This article also provides the specific implementation of software architecture design and interface display effects.",IEEE conference,no,"['intelligent', 'distribution', 'grid', 'reliability', 'simplification', 'algorithm', 'online', 'computing', 'implementation', 'present', 'reliability', 'calculation', 'focused', 'calculation', 'current', 'year', 'calculation', 'planned', 'year', 'often', 'carried', 'using', 'fuzzy', 'estimation', 'calculation', 'method', 'complex', 'diverse', 'modular', 'implementation', 'article', 'analyzes', 'reliability', 'calculation', 'based', 'topology', 'simplification', 'model', 'power', 'flow', 'part', 'related', 'grid', 'structure', 'classic', 'calculation', 'grid', 'part', 'related', 'equipment', 'collected', 'equipment', 'calculation', 'simplified', 'based', 'practical', 'based', 'power', 'grid', 'resource', 'business', 'center', 'center', 'centralized', 'collaborative', 'computing', 'framework', 'construction', 'model', 'adopted', 'integrating', 'basic', 'reliability', 'facility', 'topology', 'graph', 'model', 'important', 'parameter', 'fault', 'rate', 'average', 'recovery', 'time', 'distribution', 'grid', 'equipment', 'based', 'simplified', 'calculation', 'power', 'supply', 'reliability', 'online', 'calculation', 'micro', 'module', 'developed', 'calculation', 'result', 'provide', 'convenient', 'support', 'reliability', 'management', 'distribution', 'grid', 'article', 'also', 'provides', 'specific', 'implementation', 'design', 'interface', 'effect']"
"Resulting Artifacts and Application Scenarios of the Communication Intermediate Layer SFCS With A Focus on Usability for the Automation Industry Due to the influence of the digital transformation, communication between individual shop-floor components is changing more dynamic communication patterns and development platforms such as messaging and microservices. For a user-friendly entry into these interaction patterns, the research project SFSC (Shop-Floor Service Connector) created a communication layer that makes it easier for users from the automation sector and abstracts infrastructure aspects, such as a service registry, from the user in order to simplify the transformation for employees from the automation sector. For this purpose, the integrated messaging pattern publish/subscribe, request/reply, and channels with the SFSC architecture have already been presented in previous work. This architecture has now been extended with a microcontroller implementation and a deployment environment and finally validated with a demonstrator, which is presented with its performance results. The validation environment consists of a CNC milling cell, supplying assembly lines, a camera-based quality control, and a handling robot arm. Application scenario-typical data flow for data integration is mapped with the help of the PLC4X project and data-driven value-added services are realized and connected based on these.",Resulting Artifacts and Application Scenarios of the Communication Intermediate Layer SFCS With A Focus on Usability for the Automation Industry,"Due to the influence of the digital transformation, communication between individual shop-floor components is changing more dynamic communication patterns and development platforms such as messaging and microservices. For a user-friendly entry into these interaction patterns, the research project SFSC (Shop-Floor Service Connector) created a communication layer that makes it easier for users from the automation sector and abstracts infrastructure aspects, such as a service registry, from the user in order to simplify the transformation for employees from the automation sector. For this purpose, the integrated messaging pattern publish/subscribe, request/reply, and channels with the SFSC architecture have already been presented in previous work. This architecture has now been extended with a microcontroller implementation and a deployment environment and finally validated with a demonstrator, which is presented with its performance results. The validation environment consists of a CNC milling cell, supplying assembly lines, a camera-based quality control, and a handling robot arm. Application scenario-typical data flow for data integration is mapped with the help of the PLC4X project and data-driven value-added services are realized and connected based on these.",IEEE conference,no,"['resulting', 'artifact', 'scenario', 'communication', 'intermediate', 'layer', 'focus', 'usability', 'automation', 'industry', 'due', 'influence', 'digital', 'transformation', 'communication', 'individual', 'changing', 'dynamic', 'communication', 'pattern', 'development', 'platform', 'messaging', 'entry', 'interaction', 'pattern', 'research', 'project', 'created', 'communication', 'layer', 'make', 'easier', 'user', 'automation', 'sector', 'infrastructure', 'aspect', 'registry', 'user', 'order', 'simplify', 'transformation', 'automation', 'sector', 'purpose', 'integrated', 'messaging', 'pattern', 'channel', 'already', 'presented', 'previous', 'work', 'extended', 'microcontroller', 'implementation', 'deployment', 'environment', 'finally', 'validated', 'presented', 'performance', 'result', 'validation', 'environment', 'consists', 'cell', 'assembly', 'line', 'quality', 'control', 'handling', 'robot', 'arm', 'flow', 'integration', 'mapped', 'help', 'project', 'realized', 'connected', 'based']"
"TeleRobot: Design and Implementation of a Live Remote Interaction Platform for Robots* With the increasing number and variety of robots, there are still many problems and limitations in the offline communication and cooperation platform. To address this issue, we present a universal ROS robot live interaction platform named TeleRobot that offers features such as multi-angle robot live streaming, remote interaction with robots, and online discussions. The implementation of TeleRobot is based on microservice architecture and WebRTC technology, utilizing Kurento Media Server (KMS) for streaming media transmission, and rosbridge for real-time remote interaction with robots. To enable compatibility with the communication between web, WeChat mini-programs and robots, we have developed a custom high-level text transfer protocol called HIKER based on WebSocket. We also provide web UI for users to manage robots, allowing them to quickly configure the IP, port, and control commands to remotely access their robots for interaction in the live broadcast room. This paper describes in detail the architecture design, system components and system implementation of TeleRobot. Finally, we applied the TeleRobot platform on the XBot-ARM robotic arm and the human-robot interaction (HRI) performance evaluation system of the platform was established using Analytic Hierarchy Process (AHP).",TeleRobot: Design and Implementation of a Live Remote Interaction Platform for Robots*,"With the increasing number and variety of robots, there are still many problems and limitations in the offline communication and cooperation platform. To address this issue, we present a universal ROS robot live interaction platform named TeleRobot that offers features such as multi-angle robot live streaming, remote interaction with robots, and online discussions. The implementation of TeleRobot is based on microservice architecture and WebRTC technology, utilizing Kurento Media Server (KMS) for streaming media transmission, and rosbridge for real-time remote interaction with robots. To enable compatibility with the communication between web, WeChat mini-programs and robots, we have developed a custom high-level text transfer protocol called HIKER based on WebSocket. We also provide web UI for users to manage robots, allowing them to quickly configure the IP, port, and control commands to remotely access their robots for interaction in the live broadcast room. This paper describes in detail the architecture design, system components and system implementation of TeleRobot. Finally, we applied the TeleRobot platform on the XBot-ARM robotic arm and the human-robot interaction (HRI) performance evaluation system of the platform was established using Analytic Hierarchy Process (AHP).",IEEE conference,no,"['telerobot', 'design', 'implementation', 'live', 'remote', 'interaction', 'platform', 'robot', 'increasing', 'number', 'variety', 'robot', 'still', 'many', 'problem', 'limitation', 'communication', 'platform', 'address', 'issue', 'present', 'universal', 'ro', 'robot', 'live', 'interaction', 'platform', 'named', 'telerobot', 'offer', 'feature', 'robot', 'live', 'streaming', 'remote', 'interaction', 'robot', 'online', 'discussion', 'implementation', 'telerobot', 'based', 'technology', 'utilizing', 'medium', 'server', 'streaming', 'medium', 'transmission', 'remote', 'interaction', 'robot', 'enable', 'compatibility', 'communication', 'web', 'robot', 'developed', 'custom', 'text', 'transfer', 'protocol', 'called', 'based', 'also', 'provide', 'web', 'ui', 'user', 'manage', 'robot', 'allowing', 'quickly', 'configure', 'port', 'control', 'command', 'remotely', 'access', 'robot', 'interaction', 'live', 'room', 'paper', 'describes', 'detail', 'design', 'implementation', 'telerobot', 'finally', 'applied', 'telerobot', 'platform', 'robotic', 'arm', 'interaction', 'performance', 'evaluation', 'platform', 'established', 'using', 'analytic', 'hierarchy', 'process']"
Method of development and deployment of reconfigurable FPGA-based projects in cloud infrastructure The detailed research of usage reconfigurable FPGA-based systems in cloud technologies is done. Disadvantages of existing FPGA projects within the cloud infrastructure are discussed. Existing approaches of remote services deployment are being improved along with the development of information technology. Docker containerization approach with Docker Compose tool is applied to deploy FaaS infrastructure. The advantages of proposed FaaS deployment approach and microservice architecture are discussed.,Method of development and deployment of reconfigurable FPGA-based projects in cloud infrastructure,The detailed research of usage reconfigurable FPGA-based systems in cloud technologies is done. Disadvantages of existing FPGA projects within the cloud infrastructure are discussed. Existing approaches of remote services deployment are being improved along with the development of information technology. Docker containerization approach with Docker Compose tool is applied to deploy FaaS infrastructure. The advantages of proposed FaaS deployment approach and microservice architecture are discussed.,IEEE conference,no,"['method', 'development', 'deployment', 'project', 'infrastructure', 'detailed', 'research', 'usage', 'technology', 'done', 'disadvantage', 'existing', 'project', 'within', 'infrastructure', 'discussed', 'existing', 'remote', 'deployment', 'improved', 'along', 'development', 'information', 'technology', 'docker', 'containerization', 'docker', 'compose', 'tool', 'applied', 'deploy', 'faa', 'infrastructure', 'advantage', 'proposed', 'faa', 'deployment', 'discussed']"
"Implementations of Data Analysis Tools Into the Biomedical Modular System Systems that utilise biological data typically necessitate a module with the capacity to analyse biomedical data. This article investigates the problem at the specified location and outlines the potential solutions at our disposal. Then, we'll examine the deployed technology. This technology has the potential to be bundled as a container for a microservice architecture and provides data analysis capabilities. Each module must contain fundamental features and capabilities, such as fuzzy machine learning and data mining methods. Similarly, this component must be able to generate an accurate report based on the results of its analysis. The primary benefit of such a system is its modularity, which enables quick adaptations and expansion for various data types, as well as simpler adaptation of the modules to newer technologies.",Implementations of Data Analysis Tools Into the Biomedical Modular System,"Systems that utilise biological data typically necessitate a module with the capacity to analyse biomedical data. This article investigates the problem at the specified location and outlines the potential solutions at our disposal. Then, we'll examine the deployed technology. This technology has the potential to be bundled as a container for a microservice architecture and provides data analysis capabilities. Each module must contain fundamental features and capabilities, such as fuzzy machine learning and data mining methods. Similarly, this component must be able to generate an accurate report based on the results of its analysis. The primary benefit of such a system is its modularity, which enables quick adaptations and expansion for various data types, as well as simpler adaptation of the modules to newer technologies.",IEEE conference,no,"['implementation', 'analysis', 'tool', 'modular', 'utilise', 'biological', 'typically', 'necessitate', 'module', 'capacity', 'analyse', 'article', 'investigates', 'problem', 'specified', 'location', 'outline', 'potential', 'solution', 'examine', 'deployed', 'technology', 'technology', 'potential', 'container', 'provides', 'analysis', 'capability', 'module', 'must', 'contain', 'fundamental', 'feature', 'capability', 'fuzzy', 'machine', 'learning', 'mining', 'method', 'must', 'able', 'generate', 'accurate', 'report', 'based', 'result', 'analysis', 'primary', 'benefit', 'modularity', 'enables', 'adaptation', 'expansion', 'various', 'type', 'well', 'simpler', 'adaptation', 'module', 'technology']"
"Use What You Know: Network and Service Coordination Beyond Certainty Modern services often comprise several components, such as chained virtual network functions, microservices, or machine learning functions. Providing such services requires to decide how often to instantiate each component, where to place these instances in the network, how to chain them and route traffic through them. To overcome limitations of conventional, hardwired heuristics, deep reinforcement learning (DRL) approaches for self-learning network and service management have emerged recently. These model-free DRL approaches are more flexible but typically learn tabula rasa, i.e., disregard existing understanding of networks, services, and their coordination.Instead, we propose FutureCoord, a novel model-based AI approach that leverages existing understanding of networks and services for more efficient and effective coordination without time-intensive training. FutureCoord combines Monte Carlo Tree Search with a stochastic traffic model. This allows FutureCoord to estimate the impact of future incoming traffic and effectively optimize long-term effects, taking fluctuating demand and Quality of Service (QoS) requirements into account. Our extensive evaluation based on real-world network topologies, services, and traffic traces indicates that FutureCoord clearly outperforms state-of-the-art model-free and model-based approaches with up to 51% higher flow success ratios.",Use What You Know: Network and Service Coordination Beyond Certainty,"Modern services often comprise several components, such as chained virtual network functions, microservices, or machine learning functions. Providing such services requires to decide how often to instantiate each component, where to place these instances in the network, how to chain them and route traffic through them. To overcome limitations of conventional, hardwired heuristics, deep reinforcement learning (DRL) approaches for self-learning network and service management have emerged recently. These model-free DRL approaches are more flexible but typically learn tabula rasa, i.e., disregard existing understanding of networks, services, and their coordination.Instead, we propose FutureCoord, a novel model-based AI approach that leverages existing understanding of networks and services for more efficient and effective coordination without time-intensive training. FutureCoord combines Monte Carlo Tree Search with a stochastic traffic model. This allows FutureCoord to estimate the impact of future incoming traffic and effectively optimize long-term effects, taking fluctuating demand and Quality of Service (QoS) requirements into account. Our extensive evaluation based on real-world network topologies, services, and traffic traces indicates that FutureCoord clearly outperforms state-of-the-art model-free and model-based approaches with up to 51% higher flow success ratios.",IEEE conference,no,"['use', 'know', 'network', 'coordination', 'beyond', 'modern', 'often', 'comprise', 'several', 'virtual', 'network', 'function', 'machine', 'learning', 'function', 'providing', 'requires', 'often', 'instantiate', 'place', 'instance', 'network', 'chain', 'route', 'traffic', 'overcome', 'limitation', 'conventional', 'heuristic', 'deep', 'reinforcement', 'learning', 'network', 'management', 'emerged', 'recently', 'flexible', 'typically', 'learn', 'existing', 'understanding', 'network', 'propose', 'futurecoord', 'novel', 'ai', 'leverage', 'existing', 'understanding', 'network', 'efficient', 'effective', 'coordination', 'without', 'training', 'futurecoord', 'combine', 'tree', 'search', 'stochastic', 'traffic', 'model', 'allows', 'futurecoord', 'estimate', 'impact', 'future', 'incoming', 'traffic', 'effectively', 'optimize', 'effect', 'taking', 'fluctuating', 'demand', 'quality', 'qos', 'requirement', 'account', 'extensive', 'evaluation', 'based', 'network', 'topology', 'traffic', 'trace', 'futurecoord', 'clearly', 'outperforms', 'higher', 'flow', 'success', 'ratio']"
"An Approach to Support Automated Deployment of Applications on Heterogeneous Cloud-HPC Infrastructures Complex applications, which include microservices, computationally intensive batch jobs, and sophisticated interaction with the external environment, demand for heterogeneous computational infrastructures that range from cloud to HPC and edge computing. In this context, a crucial problem is to facilitate the work of DevOps teams in i. the conception of the right operational architecture for the application, ii. its transformation into infrastructural code that automates its deployment, taking into account the peculiarities of each of the diverse infrastructures involved in this, and iii. its operation. The SODALITE framework aims at addressing this scenario. This paper presents the main features offered by the first version of the framework, currently focusing on managing cloud and HPC clusters, and shows them in practice through a relevant case study.",An Approach to Support Automated Deployment of Applications on Heterogeneous Cloud-HPC Infrastructures,"Complex applications, which include microservices, computationally intensive batch jobs, and sophisticated interaction with the external environment, demand for heterogeneous computational infrastructures that range from cloud to HPC and edge computing. In this context, a crucial problem is to facilitate the work of DevOps teams in i. the conception of the right operational architecture for the application, ii. its transformation into infrastructural code that automates its deployment, taking into account the peculiarities of each of the diverse infrastructures involved in this, and iii. its operation. The SODALITE framework aims at addressing this scenario. This paper presents the main features offered by the first version of the framework, currently focusing on managing cloud and HPC clusters, and shows them in practice through a relevant case study.",IEEE conference,no,"['support', 'automated', 'deployment', 'heterogeneous', 'infrastructure', 'complex', 'include', 'computationally', 'intensive', 'batch', 'job', 'sophisticated', 'interaction', 'external', 'environment', 'demand', 'heterogeneous', 'computational', 'infrastructure', 'range', 'hpc', 'edge', 'computing', 'context', 'crucial', 'problem', 'facilitate', 'work', 'devops', 'team', 'right', 'operational', 'ii', 'transformation', 'code', 'automates', 'deployment', 'taking', 'account', 'diverse', 'infrastructure', 'involved', 'iii', 'operation', 'framework', 'aim', 'addressing', 'scenario', 'paper', 'present', 'main', 'feature', 'offered', 'first', 'version', 'framework', 'currently', 'focusing', 'managing', 'hpc', 'cluster', 'show', 'practice', 'relevant', 'case', 'study']"
"BigVM: A Multi-Layer-Microservice-Based Platform for Deploying SaaS With the advent of Software-as-a-Service (SaaS), SaaS developers are facing many more challenges associated with multi-tenancy and dramatically increased number of users, e.g., scalability, availability, increased cost of development/testing/deployment, high cost of customization. As most of them are highly common, it is becoming very desirable if a generic and powerful deployment platform can be designed. For such a purpose, in this paper, a new platform namely BigVM is proposed to isolate SaaS developers from deployments and bridge the gap between the best practices and the real-world adoptions.BigVM provides microservice-oriented deployment kits to enable SaaS developer to create, customize, and deploy SaaS solutions in a multi-layer-microservice-based manner, which can utilize fault tolerance, optimize the resources, and scale in/out the underlying resources not only based on resource utilization but also on the non-functional requirements from the system, e.g., timing constraint. A set of experiments are implemented in sysbench to test one of BigVM's core components-Docker containers. The results show that Docker containers can achieve desirable performance in terms of CPU workload and file I/O, thus laying a solid foundation for our future work.",BigVM: A Multi-Layer-Microservice-Based Platform for Deploying SaaS,"With the advent of Software-as-a-Service (SaaS), SaaS developers are facing many more challenges associated with multi-tenancy and dramatically increased number of users, e.g., scalability, availability, increased cost of development/testing/deployment, high cost of customization. As most of them are highly common, it is becoming very desirable if a generic and powerful deployment platform can be designed. For such a purpose, in this paper, a new platform namely BigVM is proposed to isolate SaaS developers from deployments and bridge the gap between the best practices and the real-world adoptions.BigVM provides microservice-oriented deployment kits to enable SaaS developer to create, customize, and deploy SaaS solutions in a multi-layer-microservice-based manner, which can utilize fault tolerance, optimize the resources, and scale in/out the underlying resources not only based on resource utilization but also on the non-functional requirements from the system, e.g., timing constraint. A set of experiments are implemented in sysbench to test one of BigVM's core components-Docker containers. The results show that Docker containers can achieve desirable performance in terms of CPU workload and file I/O, thus laying a solid foundation for our future work.",IEEE conference,no,"['bigvm', 'platform', 'deploying', 'saas', 'advent', 'saas', 'saas', 'developer', 'facing', 'many', 'challenge', 'associated', 'dramatically', 'increased', 'number', 'user', 'scalability', 'availability', 'increased', 'cost', 'high', 'cost', 'customization', 'highly', 'common', 'becoming', 'desirable', 'generic', 'powerful', 'deployment', 'platform', 'designed', 'purpose', 'paper', 'new', 'platform', 'namely', 'bigvm', 'proposed', 'isolate', 'saas', 'developer', 'deployment', 'bridge', 'gap', 'best', 'practice', 'provides', 'deployment', 'kit', 'enable', 'saas', 'developer', 'create', 'customize', 'deploy', 'saas', 'solution', 'manner', 'utilize', 'fault', 'tolerance', 'optimize', 'resource', 'scale', 'underlying', 'resource', 'based', 'resource', 'utilization', 'also', 'requirement', 'constraint', 'set', 'experiment', 'implemented', 'test', 'one', 'bigvm', 'core', 'container', 'result', 'show', 'docker', 'container', 'achieve', 'desirable', 'performance', 'term', 'cpu', 'workload', 'file', 'thus', 'solid', 'foundation', 'future', 'work']"
"A Provenance-Aware Approach to Big Data Workflow Management in Heterogeneous Cloud Environments Big data workflows have emerged as a powerful paradigm that enables researchers and practitioners to run complex multi-step computational processes in the cloud to gain insight into their large datasets. To create a workflow, a user logs on to a specialized software, called Big Data Workflow Management System, or simply BDW system, to select and connect together various components, or tasks, into a workflow. The workflow is then mapped onto a set of distributed compute resources, such as Virtual Machines (VMs), and storage resources, such as S3 buckets and EBS volumes. It is then executed, with different branches and tasks of the workflow running in parallel on different nodes. During execution, the BDW system captures provenance, which is the history of data derivation that describes data processing steps that yielded each output result. Workflow management, including workflow composition and schedule refinement, is a challenging problem. This problem is further exacerbated by the growing number and heterogeneity of workflow tasks and cloud resources, as well as by the growing size and complexity of workflow structures. Few efforts were made to leverage provenance for facilitating workflow composition and schedule refinement. To address these issues, we 1) produce a comprehensive conceptual model for big data workflow provenance that captures the complexity and heterogeneity of cloud-based workflow execution, 2) propose a scalable Cassandra database schema for provenance-aware workflow composition and schedule refinement, 3) outline a four-step provenance-based schedule refinement process for balancing workflow execution time and cost, and 4) present a scalable and highly available microservices-based reference architecture for big data workflow management in the cloud. Our proposed loosely coupled architecture ensures superior scalability, as well as operational and technological independence of each module within the BDW system.",A Provenance-Aware Approach to Big Data Workflow Management in Heterogeneous Cloud Environments,"Big data workflows have emerged as a powerful paradigm that enables researchers and practitioners to run complex multi-step computational processes in the cloud to gain insight into their large datasets. To create a workflow, a user logs on to a specialized software, called Big Data Workflow Management System, or simply BDW system, to select and connect together various components, or tasks, into a workflow. The workflow is then mapped onto a set of distributed compute resources, such as Virtual Machines (VMs), and storage resources, such as S3 buckets and EBS volumes. It is then executed, with different branches and tasks of the workflow running in parallel on different nodes. During execution, the BDW system captures provenance, which is the history of data derivation that describes data processing steps that yielded each output result. Workflow management, including workflow composition and schedule refinement, is a challenging problem. This problem is further exacerbated by the growing number and heterogeneity of workflow tasks and cloud resources, as well as by the growing size and complexity of workflow structures. Few efforts were made to leverage provenance for facilitating workflow composition and schedule refinement. To address these issues, we 1) produce a comprehensive conceptual model for big data workflow provenance that captures the complexity and heterogeneity of cloud-based workflow execution, 2) propose a scalable Cassandra database schema for provenance-aware workflow composition and schedule refinement, 3) outline a four-step provenance-based schedule refinement process for balancing workflow execution time and cost, and 4) present a scalable and highly available microservices-based reference architecture for big data workflow management in the cloud. Our proposed loosely coupled architecture ensures superior scalability, as well as operational and technological independence of each module within the BDW system.",IEEE conference,no,"['big', 'workflow', 'management', 'heterogeneous', 'environment', 'big', 'workflow', 'emerged', 'powerful', 'paradigm', 'enables', 'researcher', 'practitioner', 'run', 'complex', 'computational', 'process', 'gain', 'insight', 'large', 'datasets', 'create', 'workflow', 'user', 'log', 'specialized', 'called', 'big', 'workflow', 'management', 'simply', 'bdw', 'select', 'connect', 'together', 'various', 'task', 'workflow', 'workflow', 'mapped', 'onto', 'set', 'distributed', 'compute', 'resource', 'virtual', 'machine', 'vms', 'storage', 'resource', 'volume', 'executed', 'different', 'task', 'workflow', 'running', 'parallel', 'different', 'node', 'execution', 'bdw', 'capture', 'provenance', 'history', 'describes', 'processing', 'step', 'output', 'result', 'workflow', 'management', 'including', 'workflow', 'composition', 'schedule', 'refinement', 'challenging', 'problem', 'problem', 'growing', 'number', 'heterogeneity', 'workflow', 'task', 'resource', 'well', 'growing', 'size', 'complexity', 'workflow', 'structure', 'effort', 'made', 'leverage', 'provenance', 'facilitating', 'workflow', 'composition', 'schedule', 'refinement', 'address', 'issue', 'produce', 'comprehensive', 'conceptual', 'model', 'big', 'workflow', 'provenance', 'capture', 'complexity', 'heterogeneity', 'workflow', 'execution', 'propose', 'scalable', 'database', 'schema', 'workflow', 'composition', 'schedule', 'refinement', 'outline', 'schedule', 'refinement', 'process', 'balancing', 'workflow', 'execution', 'time', 'cost', 'present', 'scalable', 'highly', 'available', 'reference', 'big', 'workflow', 'management', 'proposed', 'loosely', 'coupled', 'ensures', 'superior', 'scalability', 'well', 'operational', 'technological', 'independence', 'module', 'within', 'bdw']"
"Analysis, Evaluation, and Assessment for Containerizing an Industry Automation Software Container-based virtualization is becoming a preferred choice to deploy services since it is lightweight and supports on-demand scalability as well as availability. The Process Automation Industry has accepted this technology to make their applications service oriented. However, container-based microservice architecture is effective only when the original software strictly followed modularity principles during its design. In this article, we share our learning of converting a distributed software to a microservice-based architecture using containers. Though the existing system has a modular design and deployed as distributed components, analysis of the current architecture shows that the application is monolithic (though modularized) and the components are strongly coupled in an indirect manner. As a result, it turns to be impossible to attain microservice-based architecture without changing the architecture. Next, we propose a microservice-based containerized TO-BE architecture of the application, and demonstrate that this TO-BE architecture does not incur any significant overhead. Finally, we propose a set of recommendations that the practitioners can follow to convert a monolithic application to a containerized architecture.","Analysis, Evaluation, and Assessment for Containerizing an Industry Automation Software","Container-based virtualization is becoming a preferred choice to deploy services since it is lightweight and supports on-demand scalability as well as availability. The Process Automation Industry has accepted this technology to make their applications service oriented. However, container-based microservice architecture is effective only when the original software strictly followed modularity principles during its design. In this article, we share our learning of converting a distributed software to a microservice-based architecture using containers. Though the existing system has a modular design and deployed as distributed components, analysis of the current architecture shows that the application is monolithic (though modularized) and the components are strongly coupled in an indirect manner. As a result, it turns to be impossible to attain microservice-based architecture without changing the architecture. Next, we propose a microservice-based containerized TO-BE architecture of the application, and demonstrate that this TO-BE architecture does not incur any significant overhead. Finally, we propose a set of recommendations that the practitioners can follow to convert a monolithic application to a containerized architecture.",IEEE conference,no,"['analysis', 'evaluation', 'assessment', 'industry', 'automation', 'virtualization', 'becoming', 'preferred', 'choice', 'deploy', 'since', 'lightweight', 'support', 'scalability', 'well', 'availability', 'process', 'automation', 'industry', 'technology', 'make', 'oriented', 'however', 'effective', 'original', 'strictly', 'followed', 'modularity', 'principle', 'design', 'article', 'share', 'learning', 'converting', 'distributed', 'using', 'container', 'though', 'existing', 'modular', 'design', 'deployed', 'distributed', 'analysis', 'current', 'show', 'monolithic', 'though', 'strongly', 'coupled', 'manner', 'result', 'turn', 'impossible', 'without', 'changing', 'next', 'propose', 'containerized', 'demonstrate', 'significant', 'overhead', 'finally', 'propose', 'set', 'recommendation', 'practitioner', 'follow', 'convert', 'monolithic', 'containerized']"
"On the Resilience of the NFV-MANO: An Availability Model of a Cloud-native Architecture With Network Function Virtualization (NFV), the management and orchestration of network services require a new set of functionalities to be added on top of legacy models of operation. Due to the introduction of the virtualization layer and the decoupling of the network functions and their running infrastructure, the operation models need to include new elements like virtual network functions (VNFs) and a new set of relationships between them and the NFV Infrastructure (NFVI). The NFV Management and Orchestration (MANO) framework plays the key role in managing and orchestrating the NFV infrastructure, network services and the associated VNFs. Failures of the MANO hinders the network ability to react to new service requests or events related to the normal lifecycle operation of network services. Thus, it becomes extremely important to ensure a high level of availability for the MANO architecture. The goal of this work is to model, analyze, and evaluate the impact that different failure modes have on the MANO availability. A model based on Stochastic Activity Networks (SANs), derived from current standard-compliant microservice-based implementations, is proposed as a case study. The case study is used to quantitatively evaluate the steady-state availability and identify the most important parameters influencing the system availability for different deployment configurations.",On the Resilience of the NFV-MANO: An Availability Model of a Cloud-native Architecture,"With Network Function Virtualization (NFV), the management and orchestration of network services require a new set of functionalities to be added on top of legacy models of operation. Due to the introduction of the virtualization layer and the decoupling of the network functions and their running infrastructure, the operation models need to include new elements like virtual network functions (VNFs) and a new set of relationships between them and the NFV Infrastructure (NFVI). The NFV Management and Orchestration (MANO) framework plays the key role in managing and orchestrating the NFV infrastructure, network services and the associated VNFs. Failures of the MANO hinders the network ability to react to new service requests or events related to the normal lifecycle operation of network services. Thus, it becomes extremely important to ensure a high level of availability for the MANO architecture. The goal of this work is to model, analyze, and evaluate the impact that different failure modes have on the MANO availability. A model based on Stochastic Activity Networks (SANs), derived from current standard-compliant microservice-based implementations, is proposed as a case study. The case study is used to quantitatively evaluate the steady-state availability and identify the most important parameters influencing the system availability for different deployment configurations.",IEEE conference,no,"['resilience', 'availability', 'model', 'network', 'function', 'virtualization', 'nfv', 'management', 'orchestration', 'network', 'require', 'new', 'set', 'functionality', 'added', 'top', 'legacy', 'model', 'operation', 'due', 'introduction', 'virtualization', 'layer', 'decoupling', 'network', 'function', 'running', 'infrastructure', 'operation', 'model', 'need', 'include', 'new', 'element', 'like', 'virtual', 'network', 'function', 'vnfs', 'new', 'set', 'relationship', 'nfv', 'infrastructure', 'nfv', 'management', 'orchestration', 'mano', 'framework', 'play', 'key', 'role', 'managing', 'orchestrating', 'nfv', 'infrastructure', 'network', 'associated', 'vnfs', 'failure', 'mano', 'hinders', 'network', 'ability', 'react', 'new', 'request', 'event', 'related', 'normal', 'lifecycle', 'operation', 'network', 'thus', 'becomes', 'extremely', 'important', 'ensure', 'high', 'level', 'availability', 'mano', 'goal', 'work', 'model', 'analyze', 'evaluate', 'impact', 'different', 'failure', 'mode', 'mano', 'availability', 'model', 'based', 'stochastic', 'activity', 'network', 'derived', 'current', 'implementation', 'proposed', 'case', 'study', 'case', 'study', 'used', 'quantitatively', 'evaluate', 'availability', 'identify', 'important', 'parameter', 'availability', 'different', 'deployment', 'configuration']"
"DCE: A High-Performance, Scalable, Enterprise-Level Cloud Native Operating System The cloud native methodology has emerged as an optimized solution to meet the ever-evolving demands and challenges of modern application deployment and management. DaoCloud Enterprise (DCE), a high-performance and scalable cloud native operating system, developed by Shanghai DaoCloud Network Technology Co., Ltd. has experienced rapid growth over the past eight years, gaining valuable experience in various fields, including finance, automotive, manufacturing, smart city and retail etc. The company has been dedicated to exploring cloud native technology, such efforts fruited as eight major components offered by DCE, including multi-cloud orchestration, microservice governance, container management, insight, global management, storage, service mesh, and workbench. DCE empowers the enterprises to embrace their digital future, and efficiently increase their business agility.","DCE: A High-Performance, Scalable, Enterprise-Level Cloud Native Operating System","The cloud native methodology has emerged as an optimized solution to meet the ever-evolving demands and challenges of modern application deployment and management. DaoCloud Enterprise (DCE), a high-performance and scalable cloud native operating system, developed by Shanghai DaoCloud Network Technology Co., Ltd. has experienced rapid growth over the past eight years, gaining valuable experience in various fields, including finance, automotive, manufacturing, smart city and retail etc. The company has been dedicated to exploring cloud native technology, such efforts fruited as eight major components offered by DCE, including multi-cloud orchestration, microservice governance, container management, insight, global management, storage, service mesh, and workbench. DCE empowers the enterprises to embrace their digital future, and efficiently increase their business agility.",IEEE conference,no,"['dce', 'scalable', 'native', 'operating', 'native', 'methodology', 'emerged', 'optimized', 'solution', 'meet', 'demand', 'challenge', 'modern', 'deployment', 'management', 'enterprise', 'dce', 'scalable', 'native', 'operating', 'developed', 'network', 'technology', 'experienced', 'rapid', 'growth', 'past', 'eight', 'year', 'gaining', 'valuable', 'experience', 'various', 'field', 'including', 'finance', 'automotive', 'manufacturing', 'smart', 'city', 'retail', 'etc', 'company', 'dedicated', 'exploring', 'native', 'technology', 'effort', 'eight', 'major', 'offered', 'dce', 'including', 'orchestration', 'governance', 'container', 'management', 'insight', 'global', 'management', 'storage', 'mesh', 'dce', 'enterprise', 'digital', 'future', 'efficiently', 'increase', 'business', 'agility']"
"A Generic and Highly Scalable Framework for the Automation and Execution of Scientific Data Processing and Simulation Workflows In order to perform complex data processing and co-simulation workflows for research on data driven energy systems, a generic, modular and highly scalable process operation framework is presented in this article. This framework consistently applies web technologies to build up a microservices architecture. It automates the startup, synchronization, and management of scientific data processing and simulation tools (e.g. Python, Matlab, OpenModelica) as part of larger transdisciplinary, multi-domain data processing and co-simulation workflows. It uses container virtualization on the underlying cluster computing environment to control and manage different simulation nodes.Within the framework's processing workflow, software executables can be distributed to different nodes on the cluster, easily access data and communicate with other components via communication adapters and a high-performance messaging channel infrastructure. By integrating Apache NiFi, the framework also provides an easy-to-use web user interface to allow users to model, perform and operate workflows for future energy system solutions. As soon as a complex workflow is set up in the process operation framework, researchers can use the workflow without any setup or configuration on their local workstations and without knowing any details of the underlying infrastructure or software environment.",A Generic and Highly Scalable Framework for the Automation and Execution of Scientific Data Processing and Simulation Workflows,"In order to perform complex data processing and co-simulation workflows for research on data driven energy systems, a generic, modular and highly scalable process operation framework is presented in this article. This framework consistently applies web technologies to build up a microservices architecture. It automates the startup, synchronization, and management of scientific data processing and simulation tools (e.g. Python, Matlab, OpenModelica) as part of larger transdisciplinary, multi-domain data processing and co-simulation workflows. It uses container virtualization on the underlying cluster computing environment to control and manage different simulation nodes.Within the framework's processing workflow, software executables can be distributed to different nodes on the cluster, easily access data and communicate with other components via communication adapters and a high-performance messaging channel infrastructure. By integrating Apache NiFi, the framework also provides an easy-to-use web user interface to allow users to model, perform and operate workflows for future energy system solutions. As soon as a complex workflow is set up in the process operation framework, researchers can use the workflow without any setup or configuration on their local workstations and without knowing any details of the underlying infrastructure or software environment.",IEEE conference,no,"['generic', 'highly', 'scalable', 'framework', 'automation', 'execution', 'scientific', 'processing', 'simulation', 'workflow', 'order', 'perform', 'complex', 'processing', 'workflow', 'research', 'driven', 'energy', 'generic', 'modular', 'highly', 'scalable', 'process', 'operation', 'framework', 'presented', 'article', 'framework', 'consistently', 'applies', 'web', 'technology', 'build', 'automates', 'startup', 'synchronization', 'management', 'scientific', 'processing', 'simulation', 'tool', 'python', 'part', 'larger', 'processing', 'workflow', 'us', 'container', 'virtualization', 'underlying', 'cluster', 'computing', 'environment', 'control', 'manage', 'different', 'simulation', 'framework', 'processing', 'workflow', 'distributed', 'different', 'node', 'cluster', 'easily', 'access', 'communicate', 'via', 'communication', 'adapter', 'messaging', 'channel', 'infrastructure', 'integrating', 'apache', 'framework', 'also', 'provides', 'web', 'user', 'interface', 'allow', 'user', 'model', 'perform', 'operate', 'workflow', 'future', 'energy', 'solution', 'soon', 'complex', 'workflow', 'set', 'process', 'operation', 'framework', 'researcher', 'use', 'workflow', 'without', 'setup', 'configuration', 'local', 'without', 'detail', 'underlying', 'infrastructure', 'environment']"
"Organic 6G Networks: Ultra-Flexibility Through Extensive Stateless Functional Split With the increase in hardware performance, the 5G mobile network architecture shifted from physical components to software-only micro-services. The very modular network functions can be deployed flexibly on commodity hardware. However, the extensive modularity of these network functions is increasing the number of managed entities, and the core network request latency. Also, it requires extensive procedures to be able to re-select the components for specific devices, a fundamental condition for a potential system scale down. In this paper, we propose a new organic 6G network architecture that handles these challenges through a new functionality split based on the experience of IT software services. Furthermore, we provide an analysis based on main 5G procedures, showing that the newly proposed architecture is handling the re-selection of functionality significantly better. Which is a cornerstone of high-speed scaling (especially scaling-out), as well as migration of functionality and users.",Organic 6G Networks: Ultra-Flexibility Through Extensive Stateless Functional Split,"With the increase in hardware performance, the 5G mobile network architecture shifted from physical components to software-only micro-services. The very modular network functions can be deployed flexibly on commodity hardware. However, the extensive modularity of these network functions is increasing the number of managed entities, and the core network request latency. Also, it requires extensive procedures to be able to re-select the components for specific devices, a fundamental condition for a potential system scale down. In this paper, we propose a new organic 6G network architecture that handles these challenges through a new functionality split based on the experience of IT software services. Furthermore, we provide an analysis based on main 5G procedures, showing that the newly proposed architecture is handling the re-selection of functionality significantly better. Which is a cornerstone of high-speed scaling (especially scaling-out), as well as migration of functionality and users.",IEEE conference,no,"['network', 'extensive', 'stateless', 'functional', 'split', 'increase', 'hardware', 'performance', 'mobile', 'network', 'shifted', 'physical', 'modular', 'network', 'function', 'deployed', 'flexibly', 'commodity', 'hardware', 'however', 'extensive', 'modularity', 'network', 'function', 'increasing', 'number', 'managed', 'entity', 'core', 'network', 'request', 'latency', 'also', 'requires', 'extensive', 'procedure', 'able', 'specific', 'device', 'fundamental', 'condition', 'potential', 'scale', 'paper', 'propose', 'new', 'network', 'handle', 'challenge', 'new', 'functionality', 'split', 'based', 'experience', 'furthermore', 'provide', 'analysis', 'based', 'main', 'procedure', 'showing', 'newly', 'proposed', 'handling', 'functionality', 'significantly', 'better', 'scaling', 'especially', 'well', 'migration', 'functionality', 'user']"
"An Integrated Edge, Fog, and Cloud Computing Reference Architecture for Developing Data Ecosystems in Smart Cities A smart city provides services to citizens through technological tools that analyze data from several subdomains for decision-making. Thus, a smart city platform is essential for integrating the city's subdomains and using data efficiently. To effectively tackle challenges in a multi-domain scenario, a flexible and adaptable solution that harnesses artificial intelligence efficiently is essential. This work presents UFCity, a data-centric, microservices-based solution with a three-layer network structure (edge, fog, and cloud computing). UFCity processes, stores, and analyzes data at each level, using an approach based on AI-equipped microservices on the cloud to extend services to citizens. A prototype was developed and tested in various usage scenarios to serve as a proof of concept. Furthermore, an experimental design $n=3^{k=3}$ was employed to evaluate the solution's performance based on message quantity, message group size for data fusion, and outlier removal methods for some elements. We conclude that UFCity is an advantageous alternative for managing city data, meeting requirements, and demonstrating high workload capacity.","An Integrated Edge, Fog, and Cloud Computing Reference Architecture for Developing Data Ecosystems in Smart Cities","A smart city provides services to citizens through technological tools that analyze data from several subdomains for decision-making. Thus, a smart city platform is essential for integrating the city's subdomains and using data efficiently. To effectively tackle challenges in a multi-domain scenario, a flexible and adaptable solution that harnesses artificial intelligence efficiently is essential. This work presents UFCity, a data-centric, microservices-based solution with a three-layer network structure (edge, fog, and cloud computing). UFCity processes, stores, and analyzes data at each level, using an approach based on AI-equipped microservices on the cloud to extend services to citizens. A prototype was developed and tested in various usage scenarios to serve as a proof of concept. Furthermore, an experimental design $n=3^{k=3}$ was employed to evaluate the solution's performance based on message quantity, message group size for data fusion, and outlier removal methods for some elements. We conclude that UFCity is an advantageous alternative for managing city data, meeting requirements, and demonstrating high workload capacity.",IEEE conference,no,"['integrated', 'edge', 'fog', 'computing', 'reference', 'developing', 'ecosystem', 'smart', 'city', 'smart', 'city', 'provides', 'citizen', 'technological', 'tool', 'analyze', 'several', 'thus', 'smart', 'city', 'platform', 'essential', 'integrating', 'city', 'using', 'efficiently', 'effectively', 'tackle', 'challenge', 'scenario', 'flexible', 'adaptable', 'solution', 'harness', 'artificial', 'intelligence', 'efficiently', 'essential', 'work', 'present', 'ufcity', 'solution', 'network', 'structure', 'edge', 'fog', 'computing', 'ufcity', 'process', 'store', 'analyzes', 'level', 'using', 'based', 'extend', 'citizen', 'prototype', 'developed', 'tested', 'various', 'usage', 'scenario', 'serve', 'proof', 'concept', 'furthermore', 'experimental', 'design', 'employed', 'evaluate', 'solution', 'performance', 'based', 'message', 'message', 'group', 'size', 'fusion', 'outlier', 'method', 'element', 'conclude', 'ufcity', 'alternative', 'managing', 'city', 'meeting', 'requirement', 'demonstrating', 'high', 'workload', 'capacity']"
"Mobilytics- An Extensible, Modular and Resilient Mobility Platform Transportation management platforms provide communities the ability to integrate the available mobility options and localized transportation demand management policies. A central component of a transportation management platform is the mobility planning application. Given the societal relevance of these platforms, it is necessary to ensure that they operate resiliently. Modularity and extensibility are also critical properties that are required for manageability. Modularity allows to isolate faults easily. Extensibility enables update of policies and integration of new mobility modes or new routing algorithms. However, state of the art mobility planning applications like open trip planner, are monolithic applications, which makes it difficult to scale and modify them dynamically. This paper describes a microservices based modular multi-modal mobility platform Mobilytics, that integrates mobility providers, commuters, and community stakeholders. We describe our requirements, architecture, and discuss the resilience challenges, and how our platform functions properly in presence of failure. Conceivably, the patterns and principles manifested in our system can serve as guidelines for current and future practitioners in this field.","Mobilytics- An Extensible, Modular and Resilient Mobility Platform","Transportation management platforms provide communities the ability to integrate the available mobility options and localized transportation demand management policies. A central component of a transportation management platform is the mobility planning application. Given the societal relevance of these platforms, it is necessary to ensure that they operate resiliently. Modularity and extensibility are also critical properties that are required for manageability. Modularity allows to isolate faults easily. Extensibility enables update of policies and integration of new mobility modes or new routing algorithms. However, state of the art mobility planning applications like open trip planner, are monolithic applications, which makes it difficult to scale and modify them dynamically. This paper describes a microservices based modular multi-modal mobility platform Mobilytics, that integrates mobility providers, commuters, and community stakeholders. We describe our requirements, architecture, and discuss the resilience challenges, and how our platform functions properly in presence of failure. Conceivably, the patterns and principles manifested in our system can serve as guidelines for current and future practitioners in this field.",IEEE conference,no,"['extensible', 'modular', 'resilient', 'mobility', 'platform', 'transportation', 'management', 'platform', 'provide', 'community', 'ability', 'integrate', 'available', 'mobility', 'option', 'transportation', 'demand', 'management', 'policy', 'central', 'transportation', 'management', 'platform', 'mobility', 'planning', 'given', 'relevance', 'platform', 'necessary', 'ensure', 'operate', 'modularity', 'extensibility', 'also', 'critical', 'property', 'required', 'manageability', 'modularity', 'allows', 'isolate', 'fault', 'easily', 'extensibility', 'enables', 'update', 'policy', 'integration', 'new', 'mobility', 'mode', 'new', 'routing', 'algorithm', 'however', 'state', 'art', 'mobility', 'planning', 'like', 'open', 'monolithic', 'make', 'difficult', 'scale', 'modify', 'dynamically', 'paper', 'describes', 'based', 'modular', 'mobility', 'platform', 'integrates', 'mobility', 'provider', 'community', 'stakeholder', 'describe', 'requirement', 'discus', 'resilience', 'challenge', 'platform', 'function', 'properly', 'presence', 'failure', 'pattern', 'principle', 'serve', 'guideline', 'current', 'future', 'practitioner', 'field']"
"P-SaaS: knowledge service oriented manufacturing workflow model for knowledge collaboration and reuse Orders come and go, but for manufacturing enterprises product-related production problems (PPs) are forever. To realize the aim of CTQS (i.e., lower cost, faster time to market, higher quality and better service), a serviced oriented model of product related problem-solving as a service (P-SaaS) is presented with three-layer structure: task bade workflow layer, knowledge based solution layer and cognition based decision-making layer in this paper. For knowledge oriented PPs solving, each task/subtask is taken as a piece of knowledge service, and the function based task decision for problem solving is organized and implemented by independent Web applications on the microservice architecture platform. Each application is considered as an agent based on its independent function. To support this model, operational models and key enable technologies are detailed. Firstly, a task based workflow is constructed to orchestrate the meta-events of specific task through coordination agent. Secondly, focused on a specific task of PPs query, a similarity based knowledge flow is proposed to retrieve the finite set of alternative solutions with a set threshold value through core agent. Thirdly, a cognition based decision synthesis flow is used to improve the quality of alternative solutions by knowledge collaboration and fusion through the individual agent. Finally, a case on high voltage apparatus in XD company is introduced to validate the proposed models.",P-SaaS: knowledge service oriented manufacturing workflow model for knowledge collaboration and reuse,"Orders come and go, but for manufacturing enterprises product-related production problems (PPs) are forever. To realize the aim of CTQS (i.e., lower cost, faster time to market, higher quality and better service), a serviced oriented model of product related problem-solving as a service (P-SaaS) is presented with three-layer structure: task bade workflow layer, knowledge based solution layer and cognition based decision-making layer in this paper. For knowledge oriented PPs solving, each task/subtask is taken as a piece of knowledge service, and the function based task decision for problem solving is organized and implemented by independent Web applications on the microservice architecture platform. Each application is considered as an agent based on its independent function. To support this model, operational models and key enable technologies are detailed. Firstly, a task based workflow is constructed to orchestrate the meta-events of specific task through coordination agent. Secondly, focused on a specific task of PPs query, a similarity based knowledge flow is proposed to retrieve the finite set of alternative solutions with a set threshold value through core agent. Thirdly, a cognition based decision synthesis flow is used to improve the quality of alternative solutions by knowledge collaboration and fusion through the individual agent. Finally, a case on high voltage apparatus in XD company is introduced to validate the proposed models.",IEEE conference,no,"['knowledge', 'oriented', 'manufacturing', 'workflow', 'model', 'knowledge', 'collaboration', 'reuse', 'order', 'come', 'go', 'manufacturing', 'enterprise', 'production', 'problem', 'pps', 'realize', 'aim', 'lower', 'cost', 'faster', 'time', 'market', 'higher', 'quality', 'better', 'oriented', 'model', 'product', 'related', 'presented', 'structure', 'task', 'workflow', 'layer', 'knowledge', 'based', 'solution', 'layer', 'cognition', 'based', 'layer', 'paper', 'knowledge', 'oriented', 'pps', 'solving', 'taken', 'piece', 'knowledge', 'function', 'based', 'task', 'decision', 'problem', 'solving', 'organized', 'implemented', 'independent', 'web', 'platform', 'considered', 'agent', 'based', 'independent', 'function', 'support', 'model', 'operational', 'model', 'key', 'enable', 'technology', 'detailed', 'firstly', 'task', 'based', 'workflow', 'constructed', 'orchestrate', 'specific', 'task', 'coordination', 'agent', 'secondly', 'focused', 'specific', 'task', 'pps', 'query', 'similarity', 'based', 'knowledge', 'flow', 'proposed', 'set', 'alternative', 'solution', 'set', 'threshold', 'value', 'core', 'agent', 'cognition', 'based', 'decision', 'synthesis', 'flow', 'used', 'improve', 'quality', 'alternative', 'solution', 'knowledge', 'collaboration', 'fusion', 'individual', 'agent', 'finally', 'case', 'high', 'voltage', 'company', 'introduced', 'validate', 'proposed', 'model']"
"Flux: Groupon's automated, scalable, extensible machine learning platform As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company.","Flux: Groupon's automated, scalable, extensible machine learning platform","As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company.",IEEE conference,no,"['flux', 'groupon', 'automated', 'scalable', 'extensible', 'machine', 'learning', 'platform', 'machine', 'learning', 'becomes', 'driving', 'daily', 'operation', 'company', 'within', 'information', 'technology', 'sector', 'infrastructure', 'enables', 'automated', 'scalable', 'machine', 'learning', 'core', 'many', 'large', 'company', 'various', 'product', 'built', 'offered', 'open', 'company', 'numerous', 'aspect', 'groupon', 'business', 'driven', 'machine', 'learning', 'solve', 'scalability', 'issue', 'provide', 'seamless', 'collaboration', 'scientist', 'engineer', 'built', 'flux', 'deployment', 'execution', 'monitoring', 'machine', 'learning', 'model', 'flux', 'focus', 'enabling', 'scientist', 'build', 'model', 'prototype', 'language', 'tool', 'integrating', 'model', 'enterprise', 'production', 'manages', 'life', 'cycle', 'deployed', 'model', 'distributed', 'batch', 'mode', 'expose', 'use', 'case', 'design', 'focus', 'automation', 'easy', 'management', 'scalability', 'extensibility', 'flux', 'central', 'supervised', 'machine', 'learning', 'task', 'groupon', 'supporting', 'multiple', 'team', 'across', 'company']"
"A Distributed Peer to Peer Identity and Access Management for the Osmotic Computing Nowadays Osmotic Computing is emerging as one of the paradigms used to guarantee the Cloud Continuum, and this popularity is strictly related to the capacity to embrace inside it some hot topics like containers, microservices, orchestration and Function as a Service (FaaS). The Osmotic principle is quite simple, it aims to create a federated heterogeneous infrastructure, where an application's components can smoothly move following a concentration rule. In this work, we aim to solve two big constraints of Osmotic Computing related to the incapacity to manage dynamic access rules for accessing the applications inside the Osmotic Infrastructure and the incapacity to keep alive and secure the access to these applications even in presence of network disconnections. For overcoming these limits we designed and implemented a new Osmotic component, that acts as an eventually consistent distributed peer to peer access management system. This new component is used to keep a local Identity and Access Manager (IAM) that permits at any time to access the resource available in an Osmotic node and to update the access rules that allow or deny access to hosted applications. This component has been already integrated inside a Kubernetes based Osmotic Infrastructure and we presented two typical use cases where it can be exploited.",A Distributed Peer to Peer Identity and Access Management for the Osmotic Computing,"Nowadays Osmotic Computing is emerging as one of the paradigms used to guarantee the Cloud Continuum, and this popularity is strictly related to the capacity to embrace inside it some hot topics like containers, microservices, orchestration and Function as a Service (FaaS). The Osmotic principle is quite simple, it aims to create a federated heterogeneous infrastructure, where an application's components can smoothly move following a concentration rule. In this work, we aim to solve two big constraints of Osmotic Computing related to the incapacity to manage dynamic access rules for accessing the applications inside the Osmotic Infrastructure and the incapacity to keep alive and secure the access to these applications even in presence of network disconnections. For overcoming these limits we designed and implemented a new Osmotic component, that acts as an eventually consistent distributed peer to peer access management system. This new component is used to keep a local Identity and Access Manager (IAM) that permits at any time to access the resource available in an Osmotic node and to update the access rules that allow or deny access to hosted applications. This component has been already integrated inside a Kubernetes based Osmotic Infrastructure and we presented two typical use cases where it can be exploited.",IEEE conference,no,"['distributed', 'peer', 'peer', 'identity', 'access', 'management', 'osmotic', 'computing', 'nowadays', 'osmotic', 'computing', 'emerging', 'one', 'paradigm', 'used', 'guarantee', 'continuum', 'popularity', 'strictly', 'related', 'capacity', 'inside', 'topic', 'like', 'container', 'orchestration', 'function', 'faa', 'osmotic', 'principle', 'quite', 'simple', 'aim', 'create', 'federated', 'heterogeneous', 'infrastructure', 'move', 'following', 'rule', 'work', 'aim', 'solve', 'two', 'big', 'constraint', 'osmotic', 'computing', 'related', 'manage', 'dynamic', 'access', 'rule', 'inside', 'osmotic', 'infrastructure', 'keep', 'secure', 'access', 'even', 'presence', 'network', 'overcoming', 'limit', 'designed', 'implemented', 'new', 'osmotic', 'act', 'eventually', 'consistent', 'distributed', 'peer', 'peer', 'access', 'management', 'new', 'used', 'keep', 'local', 'identity', 'access', 'manager', 'time', 'access', 'resource', 'available', 'osmotic', 'node', 'update', 'access', 'rule', 'allow', 'access', 'hosted', 'already', 'integrated', 'inside', 'kubernetes', 'based', 'osmotic', 'infrastructure', 'presented', 'two', 'typical', 'use', 'case']"
"A Case Study in Computational Caching Microservices for HPC A case study is presented that provides computation caching (memoization) through a microservice architecture to high-performance computing (HPC) applications, particularly the ExMatEx proxy application CoEVP (Co-designed Embedded ViscoPlasticity Scale-bridging). CoEVP represents a class of multiscale physics methods in which inexpensive coarse-scale models are combined with expensive fine-scale models to simulate physical phenomena scalably across multiple time and length scales. Recently, CoEVP has employed interpolation based on previously executed fine-scale models in order to reduce the number of fine-scale evaluations needed to advance the simulation. Building on this work, we envision that distributed microservices composed to provide new capabilities to large-scale parallel applications can be an important component in simulating ever-larger systems at ever-greater fidelities. We explore three aspects of a microservice composition for interpolation-based memoization in our study. First, we present a cost assessment of CoEVP's current fine-scale modeling and interpolation approach. Second, we present an alternative interpolation strategy in which interpolation models are directly constructed on demand from previous fine-scale evaluations: a ""database of points"" rather than a ""database of models."" Third, we evaluate the characteristics of the two approaches with and without cross-process sharing of database entries. Lessons learned from the study are used to inform designs for future work in developing distributed, large-scale memoization services for HPC.",A Case Study in Computational Caching Microservices for HPC,"A case study is presented that provides computation caching (memoization) through a microservice architecture to high-performance computing (HPC) applications, particularly the ExMatEx proxy application CoEVP (Co-designed Embedded ViscoPlasticity Scale-bridging). CoEVP represents a class of multiscale physics methods in which inexpensive coarse-scale models are combined with expensive fine-scale models to simulate physical phenomena scalably across multiple time and length scales. Recently, CoEVP has employed interpolation based on previously executed fine-scale models in order to reduce the number of fine-scale evaluations needed to advance the simulation. Building on this work, we envision that distributed microservices composed to provide new capabilities to large-scale parallel applications can be an important component in simulating ever-larger systems at ever-greater fidelities. We explore three aspects of a microservice composition for interpolation-based memoization in our study. First, we present a cost assessment of CoEVP's current fine-scale modeling and interpolation approach. Second, we present an alternative interpolation strategy in which interpolation models are directly constructed on demand from previous fine-scale evaluations: a ""database of points"" rather than a ""database of models."" Third, we evaluate the characteristics of the two approaches with and without cross-process sharing of database entries. Lessons learned from the study are used to inform designs for future work in developing distributed, large-scale memoization services for HPC.",IEEE conference,no,"['case', 'study', 'computational', 'caching', 'hpc', 'case', 'study', 'presented', 'provides', 'computation', 'caching', 'memoization', 'computing', 'hpc', 'particularly', 'proxy', 'coevp', 'embedded', 'coevp', 'represents', 'class', 'method', 'model', 'combined', 'expensive', 'model', 'simulate', 'physical', 'phenomenon', 'across', 'multiple', 'time', 'scale', 'recently', 'coevp', 'employed', 'interpolation', 'based', 'previously', 'executed', 'model', 'order', 'reduce', 'number', 'evaluation', 'needed', 'advance', 'simulation', 'building', 'work', 'distributed', 'composed', 'provide', 'new', 'capability', 'parallel', 'important', 'explore', 'three', 'aspect', 'composition', 'memoization', 'study', 'first', 'present', 'cost', 'assessment', 'coevp', 'current', 'modeling', 'interpolation', 'second', 'present', 'alternative', 'interpolation', 'strategy', 'interpolation', 'model', 'directly', 'constructed', 'demand', 'previous', 'evaluation', 'database', 'point', 'rather', 'database', 'model', 'third', 'evaluate', 'characteristic', 'two', 'without', 'sharing', 'database', 'entry', 'lesson', 'learned', 'study', 'used', 'inform', 'design', 'future', 'work', 'developing', 'distributed', 'memoization', 'hpc']"
"AI-GeneSI: Exploiting generative AI for autonomous generation of the southbound interface in the IoT Virtual objects, which are representations in the digital world of physical entities, uses the data collected by one or several sensor nodes to operate. To overcome the diversity and heterogeneity of protocols implemented by different sensor nodes and the way in which sensor data is represented, it is convenient to exploit appropriate components referred to as “southbound interfaces” in this paper. The objective of the southbound interface is to convert the communication protocols implemented by sensor nodes and virtual objects and to harmonize data representations. The implementation of the southbound interfaces is not a complex task, however it is extremely specific of the current setting, which turns in low reusability of the code, and is time-consuming. In this paper, a methodology named AI-GeneSI is proposed to exploit Large Language Models (LLM)s to generate the code to communicate with the southbound interface. Such code is utilized to create and deploy a microservice which implements the southbound interface functions. A prototype of the proposed methodology has been implemented to demonstrate the feasibility of the proposed approach.",AI-GeneSI: Exploiting generative AI for autonomous generation of the southbound interface in the IoT,"Virtual objects, which are representations in the digital world of physical entities, uses the data collected by one or several sensor nodes to operate. To overcome the diversity and heterogeneity of protocols implemented by different sensor nodes and the way in which sensor data is represented, it is convenient to exploit appropriate components referred to as “southbound interfaces” in this paper. The objective of the southbound interface is to convert the communication protocols implemented by sensor nodes and virtual objects and to harmonize data representations. The implementation of the southbound interfaces is not a complex task, however it is extremely specific of the current setting, which turns in low reusability of the code, and is time-consuming. In this paper, a methodology named AI-GeneSI is proposed to exploit Large Language Models (LLM)s to generate the code to communicate with the southbound interface. Such code is utilized to create and deploy a microservice which implements the southbound interface functions. A prototype of the proposed methodology has been implemented to demonstrate the feasibility of the proposed approach.",IEEE conference,no,"['exploiting', 'ai', 'autonomous', 'generation', 'southbound', 'interface', 'iot', 'virtual', 'object', 'representation', 'digital', 'world', 'physical', 'entity', 'us', 'collected', 'one', 'several', 'sensor', 'node', 'operate', 'overcome', 'diversity', 'heterogeneity', 'protocol', 'implemented', 'different', 'sensor', 'node', 'way', 'sensor', 'convenient', 'exploit', 'appropriate', 'referred', 'southbound', 'interface', 'paper', 'objective', 'southbound', 'interface', 'convert', 'communication', 'protocol', 'implemented', 'sensor', 'node', 'virtual', 'object', 'representation', 'implementation', 'southbound', 'interface', 'complex', 'task', 'however', 'extremely', 'specific', 'current', 'setting', 'turn', 'low', 'reusability', 'code', 'paper', 'methodology', 'named', 'proposed', 'exploit', 'large', 'language', 'model', 'generate', 'code', 'communicate', 'southbound', 'interface', 'code', 'utilized', 'create', 'deploy', 'implement', 'southbound', 'interface', 'function', 'prototype', 'proposed', 'methodology', 'implemented', 'demonstrate', 'feasibility', 'proposed']"
"ECdo: An Edge Computing Distributed Data-Driven Evolutionary Optimization Platform Surrogate-assisted evolutionary algorithms (SAEAs) have become a popular method to solve data-driven optimization problems (DOPs), which are common in industry. However, with the development of the Internet of Things, data are collected, processed, and stored in a distributed manner, leading a new optimization paradigm for SAEAs. To make SAEAs adapt to these distributed DOPs, this paper employs the edge computing paradigm to develop a platform that provides technical support for SAEAs with distributed structures, named ECdo. Specifically, the platform utilizes KubeEdge, an open-source edge computing framework, to mount the cluster and combines microservice interface design with the containerization strategy to offer a flexible deployment approach for distributed SAEAs. In addition, an efficient and stable internal communication mechanism is designed for the interaction between distributed components within the platform. To demonstrate the application of ECdo, we take the examples of a class of distributed DOPs, in which the objective and constraints are expensive and need to be approximated by accumulated data. These problems are known as distributed and expensive constrained optimization problems (DECOPs). We implement a distributed SAEA on ECdo to address DECOPs in real-world scenarios. Experiments show that the ECdo can provide the expected implementation for distributed SAEAs with good network tolerance under tough network conditions.",ECdo: An Edge Computing Distributed Data-Driven Evolutionary Optimization Platform,"Surrogate-assisted evolutionary algorithms (SAEAs) have become a popular method to solve data-driven optimization problems (DOPs), which are common in industry. However, with the development of the Internet of Things, data are collected, processed, and stored in a distributed manner, leading a new optimization paradigm for SAEAs. To make SAEAs adapt to these distributed DOPs, this paper employs the edge computing paradigm to develop a platform that provides technical support for SAEAs with distributed structures, named ECdo. Specifically, the platform utilizes KubeEdge, an open-source edge computing framework, to mount the cluster and combines microservice interface design with the containerization strategy to offer a flexible deployment approach for distributed SAEAs. In addition, an efficient and stable internal communication mechanism is designed for the interaction between distributed components within the platform. To demonstrate the application of ECdo, we take the examples of a class of distributed DOPs, in which the objective and constraints are expensive and need to be approximated by accumulated data. These problems are known as distributed and expensive constrained optimization problems (DECOPs). We implement a distributed SAEA on ECdo to address DECOPs in real-world scenarios. Experiments show that the ECdo can provide the expected implementation for distributed SAEAs with good network tolerance under tough network conditions.",IEEE conference,no,"['ecdo', 'edge', 'computing', 'distributed', 'evolutionary', 'optimization', 'platform', 'evolutionary', 'algorithm', 'saeas', 'become', 'popular', 'method', 'solve', 'optimization', 'problem', 'dops', 'common', 'industry', 'however', 'development', 'internet', 'thing', 'collected', 'processed', 'stored', 'distributed', 'manner', 'leading', 'new', 'optimization', 'paradigm', 'saeas', 'make', 'saeas', 'adapt', 'distributed', 'dops', 'paper', 'employ', 'edge', 'computing', 'paradigm', 'develop', 'platform', 'provides', 'technical', 'support', 'saeas', 'distributed', 'structure', 'named', 'ecdo', 'specifically', 'platform', 'utilizes', 'kubeedge', 'edge', 'computing', 'framework', 'cluster', 'combine', 'interface', 'design', 'containerization', 'strategy', 'offer', 'flexible', 'deployment', 'distributed', 'saeas', 'addition', 'efficient', 'stable', 'internal', 'communication', 'mechanism', 'designed', 'interaction', 'distributed', 'within', 'platform', 'demonstrate', 'ecdo', 'take', 'example', 'class', 'distributed', 'dops', 'objective', 'constraint', 'expensive', 'need', 'problem', 'known', 'distributed', 'expensive', 'optimization', 'problem', 'implement', 'distributed', 'ecdo', 'address', 'scenario', 'experiment', 'show', 'ecdo', 'provide', 'expected', 'implementation', 'distributed', 'saeas', 'good', 'network', 'tolerance', 'network', 'condition']"
"EmoStream: Real-time Emotion Prediction through Speech with Apache Flink and Kafka Integration Advancements in real-time emotion recognition systems have witnessed a surge in interest, fuelled by their potential applications in diverse fields such as human computer interaction, mental health monitoring, and user experience enhancement. Existing approaches often focus on singular modalities, such as speech analysis or facial expressions. However, a comprehensive understanding of human emotion necessitates the integration of multiple modalities. This research introduces a novel approach, a Multimodal Real-time Emotion Prediction System (MREPS), which combines speech analysis with streaming data processing. The system begins by capturing a continuous stream of audio data, which undergoes Speech-to-Text (STT) conversion using cloud-based services like Google Cloud Speech-to-Text. Simultaneously, facial expression recognition and physiological signal analysis can be integrated for a more comprehensive emotional profile. The real-time nature of the data necessitates the use of a robust streaming architecture, leveraging technologies such as Apache Kafka for data ingestion and Apache Flink for stream processing. A crucial component of the system is the Emotion Prediction API, a microservice housing a machine learning model for emotion classification. This model is trained on diverse datasets to ensure accuracy across various languages, cultures, and individual differences. The real-time API endpoint continuously processes the multimodal input, providing dynamic emotion predictions.",EmoStream: Real-time Emotion Prediction through Speech with Apache Flink and Kafka Integration,"Advancements in real-time emotion recognition systems have witnessed a surge in interest, fuelled by their potential applications in diverse fields such as human computer interaction, mental health monitoring, and user experience enhancement. Existing approaches often focus on singular modalities, such as speech analysis or facial expressions. However, a comprehensive understanding of human emotion necessitates the integration of multiple modalities. This research introduces a novel approach, a Multimodal Real-time Emotion Prediction System (MREPS), which combines speech analysis with streaming data processing. The system begins by capturing a continuous stream of audio data, which undergoes Speech-to-Text (STT) conversion using cloud-based services like Google Cloud Speech-to-Text. Simultaneously, facial expression recognition and physiological signal analysis can be integrated for a more comprehensive emotional profile. The real-time nature of the data necessitates the use of a robust streaming architecture, leveraging technologies such as Apache Kafka for data ingestion and Apache Flink for stream processing. A crucial component of the system is the Emotion Prediction API, a microservice housing a machine learning model for emotion classification. This model is trained on diverse datasets to ensure accuracy across various languages, cultures, and individual differences. The real-time API endpoint continuously processes the multimodal input, providing dynamic emotion predictions.",IEEE conference,no,"['emotion', 'prediction', 'speech', 'apache', 'kafka', 'integration', 'advancement', 'emotion', 'recognition', 'surge', 'interest', 'potential', 'diverse', 'field', 'human', 'computer', 'interaction', 'health', 'monitoring', 'user', 'experience', 'enhancement', 'existing', 'often', 'focus', 'speech', 'analysis', 'expression', 'however', 'comprehensive', 'understanding', 'human', 'emotion', 'necessitates', 'integration', 'multiple', 'research', 'introduces', 'novel', 'multimodal', 'emotion', 'prediction', 'combine', 'speech', 'analysis', 'streaming', 'processing', 'begin', 'capturing', 'continuous', 'stream', 'conversion', 'using', 'like', 'google', 'simultaneously', 'expression', 'recognition', 'signal', 'analysis', 'integrated', 'comprehensive', 'profile', 'nature', 'necessitates', 'use', 'robust', 'streaming', 'leveraging', 'technology', 'apache', 'kafka', 'apache', 'stream', 'processing', 'crucial', 'emotion', 'prediction', 'api', 'machine', 'learning', 'model', 'emotion', 'classification', 'model', 'trained', 'diverse', 'datasets', 'ensure', 'accuracy', 'across', 'various', 'language', 'individual', 'difference', 'api', 'endpoint', 'continuously', 'process', 'multimodal', 'input', 'providing', 'dynamic', 'emotion', 'prediction']"
"A Performance Modelling Approach for SLA-Aware Resource Recommendation in Cloud Native Network Functions Network Function Virtualization (NFV) becomes the primary driver for the evolution of 5G networks, and in recent years, Network Function Cloudification (NFC) proved to be an inevitable part of this evolution. Microservice architecture also becomes the de facto choice for designing a modern Cloud Native Network Function (CNF) due to its ability to decouple components of each CNF into multiple independently manageable microservices. Even though taking advantage of microservice architecture in designing CNFs solves specific problems, this additional granularity makes estimating resource requirements for a Production Environment (PE) a complex task and sometimes leads to an over-provisioned PE. Traditionally, performance engineers dimension each CNF within a Service Function Chain (SFC) in a smaller Performance Testing Environment (PTE) through a series of performance benchmarks. Then, considering the Quality of Service (QoS) constraints of a Service Provider (SP) that are guaranteed in the Service Level Agreement (SLA), they estimate the required resources to set up the PE. In this paper, we used a machine learning approach to model the impact of each microservice's resource configuration (i.e., CPU and memory) on the QoS metrics (i.e. serving throughput and latency) of each SFC in a PTE. Then, considering an SP's Service Level Objectives (SLO), we proposed an algorithm to predict each microservice's resource capacities in a PE. We evaluated the accuracy of our prediction on a prototype of a cloud native 5G Home Subscriber Server (HSS). Our model showed 95%-78% accuracy in a PE that has 2-5 times more computing resources than the PTE.",A Performance Modelling Approach for SLA-Aware Resource Recommendation in Cloud Native Network Functions,"Network Function Virtualization (NFV) becomes the primary driver for the evolution of 5G networks, and in recent years, Network Function Cloudification (NFC) proved to be an inevitable part of this evolution. Microservice architecture also becomes the de facto choice for designing a modern Cloud Native Network Function (CNF) due to its ability to decouple components of each CNF into multiple independently manageable microservices. Even though taking advantage of microservice architecture in designing CNFs solves specific problems, this additional granularity makes estimating resource requirements for a Production Environment (PE) a complex task and sometimes leads to an over-provisioned PE. Traditionally, performance engineers dimension each CNF within a Service Function Chain (SFC) in a smaller Performance Testing Environment (PTE) through a series of performance benchmarks. Then, considering the Quality of Service (QoS) constraints of a Service Provider (SP) that are guaranteed in the Service Level Agreement (SLA), they estimate the required resources to set up the PE. In this paper, we used a machine learning approach to model the impact of each microservice's resource configuration (i.e., CPU and memory) on the QoS metrics (i.e. serving throughput and latency) of each SFC in a PTE. Then, considering an SP's Service Level Objectives (SLO), we proposed an algorithm to predict each microservice's resource capacities in a PE. We evaluated the accuracy of our prediction on a prototype of a cloud native 5G Home Subscriber Server (HSS). Our model showed 95%-78% accuracy in a PE that has 2-5 times more computing resources than the PTE.",IEEE conference,no,"['performance', 'modelling', 'resource', 'recommendation', 'native', 'network', 'function', 'network', 'function', 'virtualization', 'nfv', 'becomes', 'primary', 'driver', 'evolution', 'network', 'recent', 'year', 'network', 'function', 'proved', 'inevitable', 'part', 'evolution', 'also', 'becomes', 'de', 'facto', 'choice', 'designing', 'modern', 'native', 'network', 'function', 'cnf', 'due', 'ability', 'decouple', 'cnf', 'multiple', 'independently', 'manageable', 'even', 'though', 'taking', 'advantage', 'designing', 'solves', 'specific', 'problem', 'additional', 'granularity', 'make', 'resource', 'requirement', 'production', 'environment', 'pe', 'complex', 'task', 'lead', 'pe', 'traditionally', 'performance', 'engineer', 'dimension', 'cnf', 'within', 'function', 'chain', 'sfc', 'smaller', 'performance', 'testing', 'environment', 'pte', 'series', 'performance', 'benchmark', 'considering', 'quality', 'qos', 'constraint', 'provider', 'sp', 'guaranteed', 'level', 'agreement', 'sla', 'estimate', 'required', 'resource', 'set', 'pe', 'paper', 'used', 'machine', 'learning', 'model', 'impact', 'resource', 'configuration', 'cpu', 'memory', 'qos', 'metric', 'serving', 'throughput', 'latency', 'sfc', 'pte', 'considering', 'sp', 'level', 'objective', 'slo', 'proposed', 'algorithm', 'predict', 'resource', 'capacity', 'pe', 'evaluated', 'accuracy', 'prediction', 'prototype', 'native', 'home', 'subscriber', 'server', 'model', 'showed', 'accuracy', 'pe', 'time', 'computing', 'resource', 'pte']"
"Container management as emerging workload for operating systems Operating-system-level virtualization is becoming increasingly important for server applications since it provides containers as a foundation of the emerging microservice architecture, which enables agile application development, deployment, and operation - the essential characteristics in modern cloud-based services. Agility in the microservice architecture heavily depends on fast management operations for containers, such as create, start, and stop. Since containers rely on administrative kernel services provided by the host operating system, the microservice architecture can be considered as a new workload for an operating system as it stresses those services differently from traditional workloads. We studied the scalability of container management operations for Docker, one of the most popular container management systems, from two aspects: core and container scalability, which indicate how much the number of processor cores and number of containers affect container management performance, respectively. We propose a hierarchical analysis approach to identify scalability bottlenecks where we analyze multiple layers of a software stack from the top to bottom layer. Our analysis reveals that core scalability has bottlenecks at a virtualization layer for storage and network devices, and that container scalability has bottlenecks at various components that inquire mount points. While those bottlenecks exist in a daemon process of Docker, the root causes are a couple of interfaces of the underlying kernel. This implies the operating system has room for improvement to more efficiently host emerging microservice applications.",Container management as emerging workload for operating systems,"Operating-system-level virtualization is becoming increasingly important for server applications since it provides containers as a foundation of the emerging microservice architecture, which enables agile application development, deployment, and operation - the essential characteristics in modern cloud-based services. Agility in the microservice architecture heavily depends on fast management operations for containers, such as create, start, and stop. Since containers rely on administrative kernel services provided by the host operating system, the microservice architecture can be considered as a new workload for an operating system as it stresses those services differently from traditional workloads. We studied the scalability of container management operations for Docker, one of the most popular container management systems, from two aspects: core and container scalability, which indicate how much the number of processor cores and number of containers affect container management performance, respectively. We propose a hierarchical analysis approach to identify scalability bottlenecks where we analyze multiple layers of a software stack from the top to bottom layer. Our analysis reveals that core scalability has bottlenecks at a virtualization layer for storage and network devices, and that container scalability has bottlenecks at various components that inquire mount points. While those bottlenecks exist in a daemon process of Docker, the root causes are a couple of interfaces of the underlying kernel. This implies the operating system has room for improvement to more efficiently host emerging microservice applications.",IEEE conference,no,"['container', 'management', 'emerging', 'workload', 'operating', 'virtualization', 'becoming', 'increasingly', 'important', 'server', 'since', 'provides', 'container', 'foundation', 'emerging', 'enables', 'agile', 'development', 'deployment', 'operation', 'essential', 'characteristic', 'modern', 'agility', 'heavily', 'depends', 'fast', 'management', 'operation', 'container', 'create', 'start', 'stop', 'since', 'container', 'rely', 'kernel', 'provided', 'host', 'operating', 'considered', 'new', 'workload', 'operating', 'traditional', 'workload', 'studied', 'scalability', 'container', 'management', 'operation', 'docker', 'one', 'popular', 'container', 'management', 'two', 'aspect', 'core', 'container', 'scalability', 'indicate', 'much', 'number', 'core', 'number', 'container', 'affect', 'container', 'management', 'performance', 'respectively', 'propose', 'hierarchical', 'analysis', 'identify', 'scalability', 'bottleneck', 'analyze', 'multiple', 'layer', 'stack', 'top', 'layer', 'analysis', 'reveals', 'core', 'scalability', 'bottleneck', 'virtualization', 'layer', 'storage', 'network', 'device', 'container', 'scalability', 'bottleneck', 'various', 'point', 'bottleneck', 'exist', 'process', 'docker', 'root', 'cause', 'interface', 'underlying', 'kernel', 'operating', 'room', 'improvement', 'efficiently', 'host', 'emerging']"
"Choreographies in Microservice-Based Automation Architectures : Next Level of Flexibility for Industrial Cyber-Physical Systems Modularization is seen as one core building block for highly flexible production systems to ensure profitability in process as well as manufacturing industries in their increasing volatile markets. Production modules encapsulate local control algorithms and thus, form Industrial Cyber-Physical Systems (ICPS). They, as pre-automated modular units fulfill the characteristics of micro-service architectures. These architectures use orchestration and choreographies as complementary association methods. In this contribution, the applicability and the advantages of service choreographies are analyzed from a more practical perspective. It is shown that choreographies have their strengths in the decentral association of (sub-)elementary services to new and independent automation functions. Thus, the services of production modules can be combined more flexibly through decentralized control. Two simplified examples and a first experiment closes the practical insight to choreographies in industrial automation systems.",Choreographies in Microservice-Based Automation Architectures : Next Level of Flexibility for Industrial Cyber-Physical Systems,"Modularization is seen as one core building block for highly flexible production systems to ensure profitability in process as well as manufacturing industries in their increasing volatile markets. Production modules encapsulate local control algorithms and thus, form Industrial Cyber-Physical Systems (ICPS). They, as pre-automated modular units fulfill the characteristics of micro-service architectures. These architectures use orchestration and choreographies as complementary association methods. In this contribution, the applicability and the advantages of service choreographies are analyzed from a more practical perspective. It is shown that choreographies have their strengths in the decentral association of (sub-)elementary services to new and independent automation functions. Thus, the services of production modules can be combined more flexibly through decentralized control. Two simplified examples and a first experiment closes the practical insight to choreographies in industrial automation systems.",IEEE conference,no,"['choreography', 'automation', 'next', 'level', 'flexibility', 'industrial', 'modularization', 'seen', 'one', 'core', 'building', 'block', 'highly', 'flexible', 'production', 'ensure', 'process', 'well', 'manufacturing', 'industry', 'increasing', 'volatile', 'market', 'production', 'module', 'local', 'control', 'algorithm', 'thus', 'form', 'industrial', 'modular', 'unit', 'fulfill', 'characteristic', 'use', 'orchestration', 'choreography', 'method', 'contribution', 'applicability', 'advantage', 'choreography', 'analyzed', 'practical', 'perspective', 'shown', 'choreography', 'strength', 'new', 'independent', 'automation', 'function', 'thus', 'production', 'module', 'combined', 'flexibly', 'decentralized', 'control', 'two', 'simplified', 'example', 'first', 'experiment', 'close', 'practical', 'insight', 'choreography', 'industrial', 'automation']"
"Towards Latency Sensitive Cloud Native Applications: A Performance Study on AWS Microservices, serverless architectures, cloud native programming are novel paradigms and techniques which could significantly reduce the burden on both developers and operators of future services. Several types of applications fit in well with the new concepts easing the life of different stakeholders while enabling cloud-grade service deployments. However, latency sensitive applications with strict delay constraints between different components pose additional challenges on the platforms. In order to gain benefit from recent cloud technologies for latency sensitive applications as well, a comprehensive performance analysis of available platforms and relevant components is a crucial first step. In this paper, we address one of the most widely used and versatile cloud platforms, namely Amazon Web Services (AWS), and reveal the delay characteristics of key components and services which impact the overall performance of latency sensitive applications. Our contribution is threefold. First, we define a detailed measurement methodology for CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS. Second, we provide a comprehensive analysis of AWS components focusing on delay characteristics. Third, we attempt to adjust a drone control application to the platform and investigate the performance on today's system.",Towards Latency Sensitive Cloud Native Applications: A Performance Study on AWS,"Microservices, serverless architectures, cloud native programming are novel paradigms and techniques which could significantly reduce the burden on both developers and operators of future services. Several types of applications fit in well with the new concepts easing the life of different stakeholders while enabling cloud-grade service deployments. However, latency sensitive applications with strict delay constraints between different components pose additional challenges on the platforms. In order to gain benefit from recent cloud technologies for latency sensitive applications as well, a comprehensive performance analysis of available platforms and relevant components is a crucial first step. In this paper, we address one of the most widely used and versatile cloud platforms, namely Amazon Web Services (AWS), and reveal the delay characteristics of key components and services which impact the overall performance of latency sensitive applications. Our contribution is threefold. First, we define a detailed measurement methodology for CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS. Second, we provide a comprehensive analysis of AWS components focusing on delay characteristics. Third, we attempt to adjust a drone control application to the platform and investigate the performance on today's system.",IEEE conference,no,"['towards', 'latency', 'sensitive', 'native', 'performance', 'study', 'aws', 'serverless', 'native', 'programming', 'novel', 'paradigm', 'technique', 'could', 'significantly', 'reduce', 'burden', 'developer', 'operator', 'future', 'several', 'type', 'fit', 'well', 'new', 'concept', 'life', 'different', 'stakeholder', 'enabling', 'deployment', 'however', 'latency', 'sensitive', 'strict', 'delay', 'constraint', 'different', 'pose', 'additional', 'challenge', 'platform', 'order', 'gain', 'benefit', 'recent', 'technology', 'latency', 'sensitive', 'well', 'comprehensive', 'performance', 'analysis', 'available', 'platform', 'relevant', 'crucial', 'first', 'step', 'paper', 'address', 'one', 'widely', 'used', 'versatile', 'platform', 'namely', 'amazon', 'web', 'aws', 'reveal', 'delay', 'characteristic', 'key', 'impact', 'overall', 'performance', 'latency', 'sensitive', 'contribution', 'first', 'define', 'detailed', 'measurement', 'methodology', 'platform', 'specifically', 'aws', 'second', 'provide', 'comprehensive', 'analysis', 'aws', 'focusing', 'delay', 'characteristic', 'third', 'attempt', 'control', 'platform', 'investigate', 'performance', 'today']"
"DSLs for Model Driven Development of Secure Interoperable Automation Systems with EdgeX Foundry Automation systems involve a range of cyber-physical system components such as sensors, actuators, control equipment, machines, robots, AGVs, etc. Seamless interoperability among these entities is a significant challenge. A well-designed Industrial Internet of Things (IIoT) platform at the network edge can offer several services by acting as a transformation engine between these field devices and various enterprise applications. We consider the EdgeX Foundry platform as such an IIoT middleware, discuss how EdgeX can provide ready-to-use integration of IoT devices, and show how we connect it with a low-code XMDD coordination layer that interfaces with EdgeX microservices through a Native DSL mechanism. We consider this technology landscape from the point of view of a building automation system example that supports high reconfigurability and security. We show how to produce all the essential elements of a complex Web based application to control the considered building systems. We demonstrate various features of the application's data and process models, how DSLs play a role at various levels, and how to add security capabilities that go beyond the cross-layer concerns and mechanisms offered by EdgeX. To this end, we introduce a declarative policy layer to be implemented using the open source ADD-Lib in form of an additional DSL for Attribute Based Encryption, with the aim of further enriching the capabilities around the EdgeX platform.",DSLs for Model Driven Development of Secure Interoperable Automation Systems with EdgeX Foundry,"Automation systems involve a range of cyber-physical system components such as sensors, actuators, control equipment, machines, robots, AGVs, etc. Seamless interoperability among these entities is a significant challenge. A well-designed Industrial Internet of Things (IIoT) platform at the network edge can offer several services by acting as a transformation engine between these field devices and various enterprise applications. We consider the EdgeX Foundry platform as such an IIoT middleware, discuss how EdgeX can provide ready-to-use integration of IoT devices, and show how we connect it with a low-code XMDD coordination layer that interfaces with EdgeX microservices through a Native DSL mechanism. We consider this technology landscape from the point of view of a building automation system example that supports high reconfigurability and security. We show how to produce all the essential elements of a complex Web based application to control the considered building systems. We demonstrate various features of the application's data and process models, how DSLs play a role at various levels, and how to add security capabilities that go beyond the cross-layer concerns and mechanisms offered by EdgeX. To this end, we introduce a declarative policy layer to be implemented using the open source ADD-Lib in form of an additional DSL for Attribute Based Encryption, with the aim of further enriching the capabilities around the EdgeX platform.",IEEE conference,no,"['dsl', 'model', 'driven', 'development', 'secure', 'interoperable', 'automation', 'edgex', 'foundry', 'automation', 'involve', 'range', 'sensor', 'actuator', 'control', 'equipment', 'machine', 'robot', 'etc', 'seamless', 'interoperability', 'among', 'entity', 'significant', 'challenge', 'industrial', 'internet', 'thing', 'iiot', 'platform', 'network', 'edge', 'offer', 'several', 'transformation', 'engine', 'field', 'device', 'various', 'enterprise', 'consider', 'edgex', 'foundry', 'platform', 'iiot', 'middleware', 'discus', 'edgex', 'provide', 'integration', 'iot', 'device', 'show', 'connect', 'coordination', 'layer', 'interface', 'edgex', 'native', 'dsl', 'mechanism', 'consider', 'technology', 'landscape', 'point', 'view', 'building', 'automation', 'example', 'support', 'high', 'security', 'show', 'produce', 'essential', 'element', 'complex', 'web', 'based', 'control', 'considered', 'building', 'demonstrate', 'various', 'feature', 'process', 'model', 'dsl', 'play', 'role', 'various', 'level', 'add', 'security', 'capability', 'go', 'beyond', 'concern', 'mechanism', 'offered', 'edgex', 'end', 'introduce', 'declarative', 'policy', 'layer', 'implemented', 'using', 'open', 'source', 'form', 'additional', 'dsl', 'attribute', 'based', 'encryption', 'aim', 'capability', 'around', 'edgex', 'platform']"
"SAVI-IoT: A Self-Managing Containerized IoT Platform Internet of Things (IoT) as a service is the ultimate goal of employing cloud computing paradigm for initiating IoT application scenarios. Due to the nature of IoT ecosystems, an IoT application should be distributed, programmable and autonomic; also, it requires to support heterogeneity, security and privacy by following design patterns involved in creating IoT systems. A multi-layer cloud architecture comprising of a high-capacity core center that is connected, through high speed links, to geographically distributed smart edges seem appropriate for highly distributed and heterogeneous IoT applications. Building upon our previous initiatives and inspired by the Infrastructure as Code (IoC) paradigm, in this paper, we propose and evaluate a hierarchical, programmable and autonomic IoT platform based on the microservice models. Our platform supports big data, local/edge data processing, high level of programmability and runtime autonomic management. The autonomic management system ensures the service availability, quality of service and optimized resource utilization in the whole IoT application components autonomously. The primary results affirm a promising future of our platform toward realization of IoT as a service.",SAVI-IoT: A Self-Managing Containerized IoT Platform,"Internet of Things (IoT) as a service is the ultimate goal of employing cloud computing paradigm for initiating IoT application scenarios. Due to the nature of IoT ecosystems, an IoT application should be distributed, programmable and autonomic; also, it requires to support heterogeneity, security and privacy by following design patterns involved in creating IoT systems. A multi-layer cloud architecture comprising of a high-capacity core center that is connected, through high speed links, to geographically distributed smart edges seem appropriate for highly distributed and heterogeneous IoT applications. Building upon our previous initiatives and inspired by the Infrastructure as Code (IoC) paradigm, in this paper, we propose and evaluate a hierarchical, programmable and autonomic IoT platform based on the microservice models. Our platform supports big data, local/edge data processing, high level of programmability and runtime autonomic management. The autonomic management system ensures the service availability, quality of service and optimized resource utilization in the whole IoT application components autonomously. The primary results affirm a promising future of our platform toward realization of IoT as a service.",IEEE conference,no,"['containerized', 'iot', 'platform', 'internet', 'thing', 'iot', 'goal', 'employing', 'computing', 'paradigm', 'iot', 'scenario', 'due', 'nature', 'iot', 'ecosystem', 'iot', 'distributed', 'programmable', 'autonomic', 'also', 'requires', 'support', 'heterogeneity', 'security', 'privacy', 'following', 'design', 'pattern', 'involved', 'creating', 'iot', 'core', 'center', 'connected', 'high', 'speed', 'link', 'geographically', 'distributed', 'smart', 'edge', 'appropriate', 'highly', 'distributed', 'heterogeneous', 'iot', 'building', 'upon', 'previous', 'initiative', 'inspired', 'infrastructure', 'code', 'paradigm', 'paper', 'propose', 'evaluate', 'hierarchical', 'programmable', 'autonomic', 'iot', 'platform', 'based', 'model', 'platform', 'support', 'big', 'processing', 'high', 'level', 'runtime', 'autonomic', 'management', 'autonomic', 'management', 'ensures', 'availability', 'quality', 'optimized', 'resource', 'utilization', 'whole', 'iot', 'autonomously', 'primary', 'result', 'promising', 'future', 'platform', 'toward', 'iot']"
"Predicting Performance and Cost of Serverless Computing Functions with SAAF Next generation software built for the cloud recently has embraced serverless computing platforms that use temporary infrastructure to host microservices offering building blocks for resilient, loosely coupled systems that are scalable, easy to manage, and extend. Serverless architectures enable decomposing software into independent components packaged and run using isolated containers or microVMs. This decomposition approach enables application hosting using very fine-grained cloud infrastructure enabling cost savings as deployments are billed granularly for resource use. Adoption of serverless platforms promise reduced hosting costs while achieving high availability, fault tolerance, and dynamic elasticity. These benefits are offset by pricing obfuscation, as performance variance from CPU heterogeneity, multitenancy, and provisioning variation obscure the true cost of hosting applications with serverless platforms. Where determining hosting costs for traditional VM-based application deployments simply involves accounting for the number of VMs and their uptime, predicting hosting costs for serverless applications can be far more complex. To address these challenges, we introduce the Serverless Application Analytics Framework (SAAF), a tool that allows profiling FaaS workload performance, resource utilization, and infrastructure to enable accurate performance predictions. We apply Linux CPU time accounting principles and multiple regression to estimate FaaS function runtime. We predict runtime using a series of increasingly variant compute bound workloads that execute across heterogeneous CPUs, different memory settings, and to alternate FaaS platforms evaluating our approach for 77 different scenarios. We found that the mean absolute percentage error of our runtime predictions for these scenarios was just ~3.49% resulting in an average cost error of $6.46 for 1-million FaaS function workloads averaging $150.45 in price.",Predicting Performance and Cost of Serverless Computing Functions with SAAF,"Next generation software built for the cloud recently has embraced serverless computing platforms that use temporary infrastructure to host microservices offering building blocks for resilient, loosely coupled systems that are scalable, easy to manage, and extend. Serverless architectures enable decomposing software into independent components packaged and run using isolated containers or microVMs. This decomposition approach enables application hosting using very fine-grained cloud infrastructure enabling cost savings as deployments are billed granularly for resource use. Adoption of serverless platforms promise reduced hosting costs while achieving high availability, fault tolerance, and dynamic elasticity. These benefits are offset by pricing obfuscation, as performance variance from CPU heterogeneity, multitenancy, and provisioning variation obscure the true cost of hosting applications with serverless platforms. Where determining hosting costs for traditional VM-based application deployments simply involves accounting for the number of VMs and their uptime, predicting hosting costs for serverless applications can be far more complex. To address these challenges, we introduce the Serverless Application Analytics Framework (SAAF), a tool that allows profiling FaaS workload performance, resource utilization, and infrastructure to enable accurate performance predictions. We apply Linux CPU time accounting principles and multiple regression to estimate FaaS function runtime. We predict runtime using a series of increasingly variant compute bound workloads that execute across heterogeneous CPUs, different memory settings, and to alternate FaaS platforms evaluating our approach for 77 different scenarios. We found that the mean absolute percentage error of our runtime predictions for these scenarios was just ~3.49% resulting in an average cost error of $6.46 for 1-million FaaS function workloads averaging $150.45 in price.",IEEE conference,no,"['predicting', 'performance', 'cost', 'serverless', 'computing', 'function', 'next', 'generation', 'built', 'recently', 'serverless', 'computing', 'platform', 'use', 'infrastructure', 'host', 'offering', 'building', 'block', 'resilient', 'loosely', 'coupled', 'scalable', 'easy', 'manage', 'extend', 'serverless', 'enable', 'decomposing', 'independent', 'packaged', 'run', 'using', 'isolated', 'container', 'decomposition', 'enables', 'hosting', 'using', 'infrastructure', 'enabling', 'cost', 'saving', 'deployment', 'resource', 'use', 'adoption', 'serverless', 'platform', 'promise', 'reduced', 'hosting', 'cost', 'achieving', 'high', 'availability', 'fault', 'tolerance', 'dynamic', 'elasticity', 'benefit', 'performance', 'cpu', 'heterogeneity', 'provisioning', 'true', 'cost', 'hosting', 'serverless', 'platform', 'hosting', 'cost', 'traditional', 'deployment', 'simply', 'involves', 'accounting', 'number', 'vms', 'predicting', 'hosting', 'cost', 'serverless', 'far', 'complex', 'address', 'challenge', 'introduce', 'serverless', 'analytics', 'framework', 'tool', 'allows', 'profiling', 'faa', 'workload', 'performance', 'resource', 'utilization', 'infrastructure', 'enable', 'accurate', 'performance', 'prediction', 'apply', 'cpu', 'time', 'accounting', 'principle', 'multiple', 'regression', 'estimate', 'faa', 'function', 'runtime', 'predict', 'runtime', 'using', 'series', 'increasingly', 'compute', 'workload', 'execute', 'across', 'heterogeneous', 'cpu', 'different', 'memory', 'setting', 'faa', 'platform', 'evaluating', 'different', 'scenario', 'found', 'mean', 'error', 'runtime', 'prediction', 'scenario', 'resulting', 'average', 'cost', 'error', 'faa', 'function', 'workload', 'price']"
"Techniques for Realizing Secure, Resilient and Differentiated 5G Operations The 5G ecosystem is designed as a highly sophisticated and modularized architecture that decouples the radio access network (RAN), the multi-access edge computing (MEC) and the mobile core to enable different and scalable deployments. It leverages modern principles of virtualized network functions, microservices-based service chaining, and cloud-native software stacks. Moreover, it provides built-in security and mechanisms for slicing. Despite all these capabilities, there remain many gaps and opportunities for additional capabilities to support end-to-end secure operations for applications across many domains. Although 5G supports mechanisms for network slicing and tunneling, new algorithms and mechanisms that can adapt network slice configurations dynamically to accommodate urgent and mission-critical traffic are needed. Such slices must be secure, interference-aware, and free of side channel attacks. Resilience of the 5G ecosystem itself requires an effective means for observability and (semi-)autonomous self-healing capabilities. To address this plethora of challenges, this paper presents the SECurity and REsiliency TEchniques for Differentiated 5G OPerationS (SECRETED 5G OPS) project, which is investigating fundamental new solutions that center on the zero trust, network slicing, and network augmentation dimensions, which together will achieve secure and differentiated operations in 5G networks. SECRETED 5G OPS solutions are designed to be easily deployable, minimally invasive to the existing infrastructure, not require modifications to user equipment other than possibly firmware upgrades, economically viable, standards compliant, and compliant to regulations.","Techniques for Realizing Secure, Resilient and Differentiated 5G Operations","The 5G ecosystem is designed as a highly sophisticated and modularized architecture that decouples the radio access network (RAN), the multi-access edge computing (MEC) and the mobile core to enable different and scalable deployments. It leverages modern principles of virtualized network functions, microservices-based service chaining, and cloud-native software stacks. Moreover, it provides built-in security and mechanisms for slicing. Despite all these capabilities, there remain many gaps and opportunities for additional capabilities to support end-to-end secure operations for applications across many domains. Although 5G supports mechanisms for network slicing and tunneling, new algorithms and mechanisms that can adapt network slice configurations dynamically to accommodate urgent and mission-critical traffic are needed. Such slices must be secure, interference-aware, and free of side channel attacks. Resilience of the 5G ecosystem itself requires an effective means for observability and (semi-)autonomous self-healing capabilities. To address this plethora of challenges, this paper presents the SECurity and REsiliency TEchniques for Differentiated 5G OPerationS (SECRETED 5G OPS) project, which is investigating fundamental new solutions that center on the zero trust, network slicing, and network augmentation dimensions, which together will achieve secure and differentiated operations in 5G networks. SECRETED 5G OPS solutions are designed to be easily deployable, minimally invasive to the existing infrastructure, not require modifications to user equipment other than possibly firmware upgrades, economically viable, standards compliant, and compliant to regulations.",IEEE conference,no,"['technique', 'realizing', 'secure', 'resilient', 'differentiated', 'operation', 'ecosystem', 'designed', 'highly', 'sophisticated', 'radio', 'access', 'network', 'ran', 'edge', 'computing', 'mec', 'mobile', 'core', 'enable', 'different', 'scalable', 'deployment', 'leverage', 'modern', 'principle', 'virtualized', 'network', 'function', 'stack', 'moreover', 'provides', 'security', 'mechanism', 'slicing', 'despite', 'capability', 'remain', 'many', 'gap', 'opportunity', 'additional', 'capability', 'support', 'secure', 'operation', 'across', 'many', 'domain', 'although', 'support', 'mechanism', 'network', 'slicing', 'new', 'algorithm', 'mechanism', 'adapt', 'network', 'slice', 'configuration', 'dynamically', 'accommodate', 'urgent', 'traffic', 'needed', 'slice', 'must', 'secure', 'free', 'side', 'channel', 'attack', 'resilience', 'ecosystem', 'requires', 'effective', 'mean', 'observability', 'autonomous', 'capability', 'address', 'plethora', 'challenge', 'paper', 'present', 'security', 'resiliency', 'technique', 'differentiated', 'operation', 'project', 'investigating', 'fundamental', 'new', 'solution', 'center', 'zero', 'trust', 'network', 'slicing', 'network', 'dimension', 'together', 'achieve', 'secure', 'differentiated', 'operation', 'network', 'solution', 'designed', 'easily', 'deployable', 'existing', 'infrastructure', 'require', 'modification', 'user', 'equipment', 'possibly', 'upgrade', 'viable', 'standard', 'compliant', 'compliant']"
"Enabling IoT connectivity for Modbus networks by using IoT edge gateways This paper aims at presenting the implementation of a functional model of IoT gateway. The Gateway aims at extending the connectivity of Modbus devices networks to IoT by performing local data processing. Within the implementation process, we've used modern development technologies, such as .NET Core, and a microservice-based application architecture. The implementation is based on a bus data scanning component, using pre-processed commands for read and write actions. Scanned data is stored locally, and when there is a change in the values, it is transmitted to all interested clients via an MQTT broker. A case study was conducted to select the best broker. This study aims at highlighting the performance of three brokers (ActiveMQ, Mosquitto and HiveMQ) by transmitting a continuous data flow. The IoT gateway was implemented using Raspberry PI devices.",Enabling IoT connectivity for Modbus networks by using IoT edge gateways,"This paper aims at presenting the implementation of a functional model of IoT gateway. The Gateway aims at extending the connectivity of Modbus devices networks to IoT by performing local data processing. Within the implementation process, we've used modern development technologies, such as .NET Core, and a microservice-based application architecture. The implementation is based on a bus data scanning component, using pre-processed commands for read and write actions. Scanned data is stored locally, and when there is a change in the values, it is transmitted to all interested clients via an MQTT broker. A case study was conducted to select the best broker. This study aims at highlighting the performance of three brokers (ActiveMQ, Mosquitto and HiveMQ) by transmitting a continuous data flow. The IoT gateway was implemented using Raspberry PI devices.",IEEE conference,no,"['enabling', 'iot', 'connectivity', 'network', 'using', 'iot', 'edge', 'gateway', 'paper', 'aim', 'presenting', 'implementation', 'functional', 'model', 'iot', 'gateway', 'gateway', 'aim', 'extending', 'connectivity', 'device', 'network', 'iot', 'performing', 'local', 'processing', 'within', 'implementation', 'process', 'used', 'modern', 'development', 'technology', 'core', 'implementation', 'based', 'bus', 'using', 'command', 'write', 'action', 'stored', 'locally', 'change', 'value', 'interested', 'client', 'via', 'mqtt', 'broker', 'case', 'study', 'conducted', 'select', 'best', 'broker', 'study', 'aim', 'highlighting', 'performance', 'three', 'broker', 'continuous', 'flow', 'iot', 'gateway', 'implemented', 'using', 'device']"
"Microservice-Based Architecture for an Energy Management System This article proposes a microservice-based architecture for an energy management system (MS-EMS) to address the fragility, poor flexibility, and hardware dependence of EMSs. Compared with the service-oriented architecture (SOA), the proposed architecture can significantly improve the load performance and scalability of an EMS through fine-grained decomposition of the system and decentralized data management. Container and cluster technologies are used to manage the microservices. A k-fault (k ≥ 2)-tolerant model is proposed to improve the reliability of the MS-EMS. The model employs containerized microservices as essential components to achieve a parallel connection of the essential components using the horizontal-scale technology of the containers. On the other hand, a MILP model-based algorithm for managing computing resources is also suggested. By minimizing the number of worker nodes of the cluster, where the MS-EMS is deployed, we can improve the utilization of the computing resources and avoid unnecessary costs. The result of the performance analysis showed that the reliability of the MS-EMS is 99.99965625%, which is two orders and one order of magnitude higher than those of the existing EMSs and the SOA-based EMS (S-EMS), respectively. Moreover, the cost of the MS-EMS is also lower than those of the existing EMSs and S-EMSs. The proposed architecture is implemented in a realpower system and has shown favorable operation performances, indicating a promising prospect for future applications.",Microservice-Based Architecture for an Energy Management System,"This article proposes a microservice-based architecture for an energy management system (MS-EMS) to address the fragility, poor flexibility, and hardware dependence of EMSs. Compared with the service-oriented architecture (SOA), the proposed architecture can significantly improve the load performance and scalability of an EMS through fine-grained decomposition of the system and decentralized data management. Container and cluster technologies are used to manage the microservices. A k-fault (k ≥ 2)-tolerant model is proposed to improve the reliability of the MS-EMS. The model employs containerized microservices as essential components to achieve a parallel connection of the essential components using the horizontal-scale technology of the containers. On the other hand, a MILP model-based algorithm for managing computing resources is also suggested. By minimizing the number of worker nodes of the cluster, where the MS-EMS is deployed, we can improve the utilization of the computing resources and avoid unnecessary costs. The result of the performance analysis showed that the reliability of the MS-EMS is 99.99965625%, which is two orders and one order of magnitude higher than those of the existing EMSs and the SOA-based EMS (S-EMS), respectively. Moreover, the cost of the MS-EMS is also lower than those of the existing EMSs and S-EMSs. The proposed architecture is implemented in a realpower system and has shown favorable operation performances, indicating a promising prospect for future applications.",IEEE journal,no,"['energy', 'management', 'article', 'proposes', 'energy', 'management', 'address', 'poor', 'flexibility', 'hardware', 'dependence', 'emss', 'compared', 'soa', 'proposed', 'significantly', 'improve', 'load', 'performance', 'scalability', 'em', 'decomposition', 'decentralized', 'management', 'container', 'cluster', 'technology', 'used', 'manage', 'model', 'proposed', 'improve', 'reliability', 'model', 'employ', 'containerized', 'essential', 'achieve', 'parallel', 'connection', 'essential', 'using', 'technology', 'container', 'hand', 'algorithm', 'managing', 'computing', 'resource', 'also', 'minimizing', 'number', 'worker', 'node', 'cluster', 'deployed', 'improve', 'utilization', 'computing', 'resource', 'avoid', 'unnecessary', 'cost', 'result', 'performance', 'analysis', 'showed', 'reliability', 'two', 'order', 'one', 'order', 'magnitude', 'higher', 'existing', 'emss', 'em', 'respectively', 'moreover', 'cost', 'also', 'lower', 'existing', 'emss', 'proposed', 'implemented', 'shown', 'operation', 'performance', 'indicating', 'promising', 'future']"
"Involving Stakeholders in the Implementation of Microservice-Based Systems: A Case Study in an Ambient-Assisted Living System Microservice-based systems promote agility and rapid business development. Some features, such as fast time-to-market, scalability and optimal response times, have encouraged stakeholders to get more involved in the development and implementation of microservices architectures in order to translate their business vision into the implementation of the architecture. Although some techniques allow the inclusion of the stakeholders' perspective in the design of microservice architectures, few proposals consider such perspectives in the selection and evaluation of technologies that implement microservice architectures. Indeed, the qualities that characterize microservice-based systems strongly depend on the suitable selection of technologies, such as application frameworks and platforms. This article proposes a collaborative technique that includes stakeholders and software architects in the selection and evaluation of application frameworks and platforms to implement microservice-based systems. We evaluated the technique in an industrial case of design and implementation of an Ambient-Assisted Living (AAL) system, which combines microservice architecture and Internet-of-Medical-Things (IoMT) sensors. The case results indicate that the proposed technique supported stakeholders in the pragmatic evaluation of alternative technological solutions. Additionally, it allowed the implementation of an AAL system that satisfies the quality specifications of stakeholders and end-users. This initial study suggests that actively including stakeholders in the implementation of microservice-based systems allows architects to make design decisions that better consider stakeholders viewpoints as well as managing their expectations.",Involving Stakeholders in the Implementation of Microservice-Based Systems: A Case Study in an Ambient-Assisted Living System,"Microservice-based systems promote agility and rapid business development. Some features, such as fast time-to-market, scalability and optimal response times, have encouraged stakeholders to get more involved in the development and implementation of microservices architectures in order to translate their business vision into the implementation of the architecture. Although some techniques allow the inclusion of the stakeholders' perspective in the design of microservice architectures, few proposals consider such perspectives in the selection and evaluation of technologies that implement microservice architectures. Indeed, the qualities that characterize microservice-based systems strongly depend on the suitable selection of technologies, such as application frameworks and platforms. This article proposes a collaborative technique that includes stakeholders and software architects in the selection and evaluation of application frameworks and platforms to implement microservice-based systems. We evaluated the technique in an industrial case of design and implementation of an Ambient-Assisted Living (AAL) system, which combines microservice architecture and Internet-of-Medical-Things (IoMT) sensors. The case results indicate that the proposed technique supported stakeholders in the pragmatic evaluation of alternative technological solutions. Additionally, it allowed the implementation of an AAL system that satisfies the quality specifications of stakeholders and end-users. This initial study suggests that actively including stakeholders in the implementation of microservice-based systems allows architects to make design decisions that better consider stakeholders viewpoints as well as managing their expectations.",IEEE journal,no,"['involving', 'stakeholder', 'implementation', 'case', 'study', 'living', 'promote', 'agility', 'rapid', 'business', 'development', 'feature', 'fast', 'scalability', 'optimal', 'response', 'time', 'stakeholder', 'get', 'involved', 'development', 'implementation', 'order', 'business', 'vision', 'implementation', 'although', 'technique', 'allow', 'inclusion', 'stakeholder', 'perspective', 'design', 'proposal', 'consider', 'perspective', 'selection', 'evaluation', 'technology', 'implement', 'indeed', 'quality', 'characterize', 'strongly', 'suitable', 'selection', 'technology', 'framework', 'platform', 'article', 'proposes', 'collaborative', 'technique', 'includes', 'stakeholder', 'architect', 'selection', 'evaluation', 'framework', 'platform', 'implement', 'evaluated', 'technique', 'industrial', 'case', 'design', 'implementation', 'living', 'combine', 'sensor', 'case', 'result', 'indicate', 'proposed', 'technique', 'supported', 'stakeholder', 'pragmatic', 'evaluation', 'alternative', 'technological', 'solution', 'additionally', 'implementation', 'satisfies', 'quality', 'specification', 'stakeholder', 'initial', 'study', 'suggests', 'including', 'stakeholder', 'implementation', 'allows', 'architect', 'make', 'design', 'decision', 'better', 'consider', 'stakeholder', 'well', 'managing', 'expectation']"
"Semantic Integration of Plug-and-Play Software Components for Industrial Edges Based on Microservices The industrial cyber-physical system enables collaboration between distributed nodes across industrial clouds and edge devices. Flexibility and interoperability could be enhanced significantly by introducing the service-oriented architecture to industrial edge devices. From the industrial edge computing perspective, software components shall be dynamically composed across heterogeneous edge devices to perform various functionalities. In this paper, a knowledge-driven Microservice-based architecture to enable plug-and-play software components is proposed for industrial edges. These software components can be dynamically configured based on the orchestration of microservices with the support of the knowledge base and the reasoning process. These semantically enhanced plug-and-play microservices could provide rapid online reconfiguration without any programming efforts. The use of the plug-and-play software components is demonstrated by an assembly line example.",Semantic Integration of Plug-and-Play Software Components for Industrial Edges Based on Microservices,"The industrial cyber-physical system enables collaboration between distributed nodes across industrial clouds and edge devices. Flexibility and interoperability could be enhanced significantly by introducing the service-oriented architecture to industrial edge devices. From the industrial edge computing perspective, software components shall be dynamically composed across heterogeneous edge devices to perform various functionalities. In this paper, a knowledge-driven Microservice-based architecture to enable plug-and-play software components is proposed for industrial edges. These software components can be dynamically configured based on the orchestration of microservices with the support of the knowledge base and the reasoning process. These semantically enhanced plug-and-play microservices could provide rapid online reconfiguration without any programming efforts. The use of the plug-and-play software components is demonstrated by an assembly line example.",IEEE journal,no,"['semantic', 'integration', 'industrial', 'edge', 'based', 'industrial', 'enables', 'collaboration', 'distributed', 'node', 'across', 'industrial', 'edge', 'device', 'flexibility', 'interoperability', 'could', 'enhanced', 'significantly', 'introducing', 'industrial', 'edge', 'device', 'industrial', 'edge', 'computing', 'perspective', 'shall', 'dynamically', 'composed', 'across', 'heterogeneous', 'edge', 'device', 'perform', 'various', 'functionality', 'paper', 'enable', 'proposed', 'industrial', 'edge', 'dynamically', 'based', 'orchestration', 'support', 'knowledge', 'base', 'reasoning', 'process', 'enhanced', 'could', 'provide', 'rapid', 'online', 'reconfiguration', 'without', 'programming', 'effort', 'use', 'demonstrated', 'assembly', 'line', 'example']"
"An End-to-End Implementation of a Service-Oriented Architecture for Data-Driven Smart Buildings Buildings connect with multiple information systems like Building Management Systems (BMS), Energy Management Systems (EMS), IoT devices, Building Information Models (BIM), the electricity grid, weather services, etc. Data-driven smart building software demands seamless integration of the above systems and their data. The lack of a system architecture with well-defined Application Programming Interfaces (APIs) poses a significant challenge for developing reusable, modular and scalable applications. This article presents a service-oriented system architecture designed with data-driven smart buildings in mind. The architecture relies on the Zachman framework and consists of seven service categories: 1) existing business applications, 2) new microservice-based applications, 3) databases, 4) integration software, 5) infrastructure services, 6) shared services, and 7) user interfaces. It closely resembles the MACH architectural principles: Microservices, API-first, Cloud-based components, and Headless principles. This architecture is implemented as a proof-of-concept, including three smart building applications. These include a Digital Twin application integrating sensor data with a BIM model, a web application merging real-time sensor data with semantic building graphs, and a data exploration tool using sensor data, the Brick ontology, and Grafana dashboards. Future implementations include real-time control applications such as Model Predictive Control (MPC). The proposed architecture and its implementations provide a blueprint for a reusable, modular, and scalable architecture in the smart building domain.",An End-to-End Implementation of a Service-Oriented Architecture for Data-Driven Smart Buildings,"Buildings connect with multiple information systems like Building Management Systems (BMS), Energy Management Systems (EMS), IoT devices, Building Information Models (BIM), the electricity grid, weather services, etc. Data-driven smart building software demands seamless integration of the above systems and their data. The lack of a system architecture with well-defined Application Programming Interfaces (APIs) poses a significant challenge for developing reusable, modular and scalable applications. This article presents a service-oriented system architecture designed with data-driven smart buildings in mind. The architecture relies on the Zachman framework and consists of seven service categories: 1) existing business applications, 2) new microservice-based applications, 3) databases, 4) integration software, 5) infrastructure services, 6) shared services, and 7) user interfaces. It closely resembles the MACH architectural principles: Microservices, API-first, Cloud-based components, and Headless principles. This architecture is implemented as a proof-of-concept, including three smart building applications. These include a Digital Twin application integrating sensor data with a BIM model, a web application merging real-time sensor data with semantic building graphs, and a data exploration tool using sensor data, the Brick ontology, and Grafana dashboards. Future implementations include real-time control applications such as Model Predictive Control (MPC). The proposed architecture and its implementations provide a blueprint for a reusable, modular, and scalable architecture in the smart building domain.",IEEE journal,no,"['implementation', 'smart', 'building', 'building', 'connect', 'multiple', 'information', 'like', 'building', 'management', 'energy', 'management', 'em', 'iot', 'device', 'building', 'information', 'model', 'electricity', 'grid', 'etc', 'smart', 'building', 'demand', 'seamless', 'integration', 'lack', 'programming', 'interface', 'apis', 'pose', 'significant', 'challenge', 'developing', 'reusable', 'modular', 'scalable', 'article', 'present', 'designed', 'smart', 'building', 'mind', 'relies', 'framework', 'consists', 'category', 'existing', 'business', 'new', 'database', 'integration', 'infrastructure', 'shared', 'user', 'interface', 'closely', 'architectural', 'principle', 'principle', 'implemented', 'including', 'three', 'smart', 'building', 'include', 'digital', 'twin', 'integrating', 'sensor', 'model', 'web', 'sensor', 'semantic', 'building', 'graph', 'exploration', 'tool', 'using', 'sensor', 'ontology', 'grafana', 'dashboard', 'future', 'implementation', 'include', 'control', 'model', 'predictive', 'control', 'proposed', 'implementation', 'provide', 'blueprint', 'reusable', 'modular', 'scalable', 'smart', 'building', 'domain']"
"Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications Microservices architecture advocates decentralized data ownership for building software systems. Particularly, in the Database per Service pattern, each microservice is supposed to maintain its own database and to handle the data related to its functionality. When implementing microservices in practice, however, there seems to be a paradox: The de facto technology (i.e., containerization) for microservice implementation is claimed to be unsuitable for the microservice component (i.e., database) in production environments, mainly due to the data persistence issues (e.g., dangling volumes) and security concerns. As a result, the existing discussions generally suggest replacing database containers with cloud database services, while leaving the on-premises microservice implementation out of consideration. After identifying three statelessness-dominant application scenarios, we proposed container-native data persistence as a conditional solution to enable resilient database containers in production. In essence, this data persistence solution distinguishes stateless data access (i.e., reading) from stateful data processing (i.e., creating, updating, and deleting), and thus it aims at the development of stateless microservices for suitable applications. In addition to developing our proposal, this research is particularly focused on its validation, via prototyping the solution and evaluating its performance, and via applying this solution to two real-world microservice applications. From the industrial perspective, the validation results have proved the feasibility, usability, and efficiency of fully containerized microservices for production in applicable situations. From the academic perspective, this research has shed light on the operation-side micro-optimization of individual microservices, which fundamentally expands the scope of “software micro-optimization” and reveals new research opportunities.",Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications,"Microservices architecture advocates decentralized data ownership for building software systems. Particularly, in the Database per Service pattern, each microservice is supposed to maintain its own database and to handle the data related to its functionality. When implementing microservices in practice, however, there seems to be a paradox: The de facto technology (i.e., containerization) for microservice implementation is claimed to be unsuitable for the microservice component (i.e., database) in production environments, mainly due to the data persistence issues (e.g., dangling volumes) and security concerns. As a result, the existing discussions generally suggest replacing database containers with cloud database services, while leaving the on-premises microservice implementation out of consideration. After identifying three statelessness-dominant application scenarios, we proposed container-native data persistence as a conditional solution to enable resilient database containers in production. In essence, this data persistence solution distinguishes stateless data access (i.e., reading) from stateful data processing (i.e., creating, updating, and deleting), and thus it aims at the development of stateless microservices for suitable applications. In addition to developing our proposal, this research is particularly focused on its validation, via prototyping the solution and evaluating its performance, and via applying this solution to two real-world microservice applications. From the industrial perspective, the validation results have proved the feasibility, usability, and efficiency of fully containerized microservices for production in applicable situations. From the academic perspective, this research has shed light on the operation-side micro-optimization of individual microservices, which fundamentally expands the scope of “software micro-optimization” and reveals new research opportunities.",IEEE journal,no,"['long', 'live', 'image', 'enabling', 'resilient', 'production', 'database', 'container', 'decentralized', 'ownership', 'building', 'particularly', 'database', 'per', 'pattern', 'maintain', 'database', 'handle', 'related', 'functionality', 'implementing', 'practice', 'however', 'seems', 'de', 'facto', 'technology', 'containerization', 'implementation', 'database', 'production', 'environment', 'mainly', 'due', 'persistence', 'issue', 'volume', 'security', 'concern', 'result', 'existing', 'discussion', 'generally', 'suggest', 'replacing', 'database', 'container', 'database', 'leaving', 'implementation', 'consideration', 'identifying', 'three', 'scenario', 'proposed', 'persistence', 'solution', 'enable', 'resilient', 'database', 'container', 'production', 'persistence', 'solution', 'stateless', 'access', 'reading', 'stateful', 'processing', 'creating', 'updating', 'thus', 'aim', 'development', 'stateless', 'suitable', 'addition', 'developing', 'proposal', 'research', 'particularly', 'focused', 'validation', 'via', 'solution', 'evaluating', 'performance', 'via', 'applying', 'solution', 'two', 'industrial', 'perspective', 'validation', 'result', 'proved', 'feasibility', 'usability', 'efficiency', 'fully', 'containerized', 'production', 'applicable', 'situation', 'academic', 'perspective', 'research', 'light', 'individual', 'scope', 'reveals', 'new', 'research', 'opportunity']"
"Telemonitoring System for Infectious Disease Prediction in Elderly People Based on a Novel Microservice Architecture This article describes the design, development and implementation of a set of microservices based on an architecture that enables detection and assisted clinical diagnosis within the field of infectious diseases of elderly patients, via a telemonitoring system. The proposed system is designed to continuously update a medical database fed with vital signs from biosensor kits applied by nurses to elderly people on a daily basis. The database is hosted in the cloud and is managed by a flexible microservices software architecture. The computational paradigms of the edge and the cloud were used in the implementation of a hybrid cloud architecture in order to support versatile high-performance applications under the microservices pattern for the pre-diagnosis of infectious diseases in elderly patients. The results of an analysis of the usability of the equipment, the performance of the architecture and the service concept show that the proposed e-health system is feasible and innovative. The system components are also selected to give a cost-effective implementation for people living in disadvantaged areas. The proposed e-health system is also suitable for distributed computing, big data and NoSQL structures, thus allowing the immediate application of machine learning and AI algorithms to discover knowledge patterns from the overall population.",Telemonitoring System for Infectious Disease Prediction in Elderly People Based on a Novel Microservice Architecture,"This article describes the design, development and implementation of a set of microservices based on an architecture that enables detection and assisted clinical diagnosis within the field of infectious diseases of elderly patients, via a telemonitoring system. The proposed system is designed to continuously update a medical database fed with vital signs from biosensor kits applied by nurses to elderly people on a daily basis. The database is hosted in the cloud and is managed by a flexible microservices software architecture. The computational paradigms of the edge and the cloud were used in the implementation of a hybrid cloud architecture in order to support versatile high-performance applications under the microservices pattern for the pre-diagnosis of infectious diseases in elderly patients. The results of an analysis of the usability of the equipment, the performance of the architecture and the service concept show that the proposed e-health system is feasible and innovative. The system components are also selected to give a cost-effective implementation for people living in disadvantaged areas. The proposed e-health system is also suitable for distributed computing, big data and NoSQL structures, thus allowing the immediate application of machine learning and AI algorithms to discover knowledge patterns from the overall population.",IEEE journal,no,"['infectious', 'disease', 'prediction', 'elderly', 'people', 'based', 'novel', 'article', 'describes', 'design', 'development', 'implementation', 'set', 'based', 'enables', 'detection', 'clinical', 'diagnosis', 'within', 'field', 'infectious', 'disease', 'elderly', 'patient', 'via', 'proposed', 'designed', 'continuously', 'update', 'medical', 'database', 'vital', 'kit', 'applied', 'elderly', 'people', 'daily', 'basis', 'database', 'hosted', 'managed', 'flexible', 'computational', 'paradigm', 'edge', 'used', 'implementation', 'hybrid', 'order', 'support', 'versatile', 'pattern', 'infectious', 'disease', 'elderly', 'patient', 'result', 'analysis', 'usability', 'equipment', 'performance', 'concept', 'show', 'proposed', 'feasible', 'innovative', 'also', 'selected', 'give', 'implementation', 'people', 'living', 'area', 'proposed', 'also', 'suitable', 'distributed', 'computing', 'big', 'nosql', 'structure', 'thus', 'allowing', 'machine', 'learning', 'ai', 'algorithm', 'discover', 'knowledge', 'pattern', 'overall']"
"A Service Oriented Architecture for the Digitalization and Automation of Distribution Grids Modern distribution grids are complex systems that need advanced management for their secure and reliable operation. The Information and Communication Technology domain today offers unprecedented opportunities for the smart design of tools in support of grid operators. This paper presents a new philosophy for the digitalization and automation of distribution grids, based on a modular architecture of microservices implemented via container technology. This architecture enables a service-oriented deployment of the intelligence needed in the Distribution Management Systems, moving beyond the traditional view of monolithic software installations in the control rooms. The proposed architecture unlocks a broad set of possibilities, including cloud-based implementations, extension of legacy systems and fast integration of machine learning-based analytic tools. Moreover, it potentially opens a completely new market of turnkey services for distribution grid management, thus avoiding large upfront investments for grid operators. This paper presents the main concepts and benefits of the proposed philosophy, together with an example of field implementation based on open source components carried out in the context of the European project SOGNO.",A Service Oriented Architecture for the Digitalization and Automation of Distribution Grids,"Modern distribution grids are complex systems that need advanced management for their secure and reliable operation. The Information and Communication Technology domain today offers unprecedented opportunities for the smart design of tools in support of grid operators. This paper presents a new philosophy for the digitalization and automation of distribution grids, based on a modular architecture of microservices implemented via container technology. This architecture enables a service-oriented deployment of the intelligence needed in the Distribution Management Systems, moving beyond the traditional view of monolithic software installations in the control rooms. The proposed architecture unlocks a broad set of possibilities, including cloud-based implementations, extension of legacy systems and fast integration of machine learning-based analytic tools. Moreover, it potentially opens a completely new market of turnkey services for distribution grid management, thus avoiding large upfront investments for grid operators. This paper presents the main concepts and benefits of the proposed philosophy, together with an example of field implementation based on open source components carried out in the context of the European project SOGNO.",IEEE journal,no,"['oriented', 'digitalization', 'automation', 'distribution', 'grid', 'modern', 'distribution', 'grid', 'complex', 'need', 'advanced', 'management', 'secure', 'reliable', 'operation', 'information', 'communication', 'technology', 'domain', 'today', 'offer', 'unprecedented', 'opportunity', 'smart', 'design', 'tool', 'support', 'grid', 'operator', 'paper', 'present', 'new', 'philosophy', 'digitalization', 'automation', 'distribution', 'grid', 'based', 'modular', 'implemented', 'via', 'container', 'technology', 'enables', 'deployment', 'intelligence', 'needed', 'distribution', 'management', 'moving', 'beyond', 'traditional', 'view', 'monolithic', 'installation', 'control', 'room', 'proposed', 'broad', 'set', 'possibility', 'including', 'implementation', 'extension', 'legacy', 'fast', 'integration', 'machine', 'analytic', 'tool', 'moreover', 'potentially', 'open', 'completely', 'new', 'market', 'distribution', 'grid', 'management', 'thus', 'avoiding', 'large', 'investment', 'grid', 'operator', 'paper', 'present', 'main', 'concept', 'benefit', 'proposed', 'philosophy', 'together', 'example', 'field', 'implementation', 'based', 'open', 'source', 'carried', 'context', 'european', 'project']"
"On Optimization of Next-Generation Microservice-Based Core Networks Next-generation mobile core networks are required to be scalable and capable of efficiently utilizing heterogeneous bare metal resources that may include edge servers. To this end, microservice-based solutions where control plane procedures are deconstructed in their fundamental building blocks are gaining momentum. This letter proposes an optimization framework delivering the partitioning and mapping of large-scale microservice graphs onto heterogeneous bare metal deployments while minimizing the total network traffic among servers. An efficient heuristic strategy for solving the optimization problem is also provided. Simulation results show that, with the proposed framework, a microservice-based core can consistently support the requested load in heterogeneous bare metal deployments even when alternative architecture fails. Besides, our framework ensures an overall reduction in the control plane-related network traffic if compared to current core architectures.",On Optimization of Next-Generation Microservice-Based Core Networks,"Next-generation mobile core networks are required to be scalable and capable of efficiently utilizing heterogeneous bare metal resources that may include edge servers. To this end, microservice-based solutions where control plane procedures are deconstructed in their fundamental building blocks are gaining momentum. This letter proposes an optimization framework delivering the partitioning and mapping of large-scale microservice graphs onto heterogeneous bare metal deployments while minimizing the total network traffic among servers. An efficient heuristic strategy for solving the optimization problem is also provided. Simulation results show that, with the proposed framework, a microservice-based core can consistently support the requested load in heterogeneous bare metal deployments even when alternative architecture fails. Besides, our framework ensures an overall reduction in the control plane-related network traffic if compared to current core architectures.",IEEE journal,no,"['optimization', 'core', 'network', 'mobile', 'core', 'network', 'required', 'scalable', 'capable', 'efficiently', 'utilizing', 'heterogeneous', 'bare', 'metal', 'resource', 'may', 'include', 'edge', 'server', 'end', 'solution', 'control', 'plane', 'procedure', 'fundamental', 'building', 'block', 'gaining', 'momentum', 'proposes', 'optimization', 'framework', 'delivering', 'partitioning', 'mapping', 'graph', 'onto', 'heterogeneous', 'bare', 'metal', 'deployment', 'minimizing', 'total', 'network', 'traffic', 'among', 'server', 'efficient', 'heuristic', 'strategy', 'solving', 'optimization', 'problem', 'also', 'provided', 'simulation', 'result', 'show', 'proposed', 'framework', 'core', 'consistently', 'support', 'requested', 'load', 'heterogeneous', 'bare', 'metal', 'deployment', 'even', 'alternative', 'besides', 'framework', 'ensures', 'overall', 'reduction', 'control', 'network', 'traffic', 'compared', 'current', 'core']"
"Containerized Microservices: A Survey of Resource Management Frameworks The growing adoption of microservice architectures (MSAs) has led to major research and development efforts to address their challenges and improve their performance, reliability, and robustness. Important aspects of MSA that are not sufficiently covered in the open literature include efficient cloud resource allocation and optimal power management. Other aspects of MSA remain widely scattered in the literature, including cost analysis, service level agreements (SLAs), and demand-driven scaling. In this article, we examine recent cloud frameworks for containerized microservices with a focus on efficient resource utilization using auto-scaling. We classify these frameworks on the basis of their resource allocation models and underlying hardware resources. We highlight current MSA trends and identify workload-driven resource sharing within microservice meshes and SLA streamlining as two key areas for future microservice research.",Containerized Microservices: A Survey of Resource Management Frameworks,"The growing adoption of microservice architectures (MSAs) has led to major research and development efforts to address their challenges and improve their performance, reliability, and robustness. Important aspects of MSA that are not sufficiently covered in the open literature include efficient cloud resource allocation and optimal power management. Other aspects of MSA remain widely scattered in the literature, including cost analysis, service level agreements (SLAs), and demand-driven scaling. In this article, we examine recent cloud frameworks for containerized microservices with a focus on efficient resource utilization using auto-scaling. We classify these frameworks on the basis of their resource allocation models and underlying hardware resources. We highlight current MSA trends and identify workload-driven resource sharing within microservice meshes and SLA streamlining as two key areas for future microservice research.",IEEE journal,no,"['containerized', 'survey', 'resource', 'management', 'framework', 'growing', 'adoption', 'msas', 'led', 'major', 'research', 'development', 'effort', 'address', 'challenge', 'improve', 'performance', 'reliability', 'robustness', 'important', 'aspect', 'msa', 'covered', 'open', 'literature', 'include', 'efficient', 'resource', 'allocation', 'optimal', 'power', 'management', 'aspect', 'msa', 'remain', 'widely', 'literature', 'including', 'cost', 'analysis', 'level', 'agreement', 'slas', 'scaling', 'article', 'examine', 'recent', 'framework', 'containerized', 'focus', 'efficient', 'resource', 'utilization', 'using', 'classify', 'framework', 'basis', 'resource', 'allocation', 'model', 'underlying', 'hardware', 'resource', 'highlight', 'current', 'msa', 'trend', 'identify', 'resource', 'sharing', 'within', 'mesh', 'sla', 'streamlining', 'two', 'key', 'area', 'future', 'research']"
"Design of Industrial Edge Applications Based on IEC 61499 Microservices and Containers Industrial automation is entering a new era of the Industrial Internet with enhanced computing, communication, and storage capabilities provided by cloud computing and field devices. The paradigm of automation systems is shifting from the ISA-95 pyramid to the two-layers architecture: industrial cloud and edge computing. Industrial software is also evolving under the new architecture in ways for which dedicated software applications are no longer suited. Service-based industrial cloud and edge applications provide maximum flexibility, interoperability, and efficiency by combining the IEC 61499 standard, microservice architecture, and container technology. This article provides orchestration methods and deployment procedures for the OT-IT hybrid industrial edge applications. The feasibility of the proposed approach is demonstrated by an industrial case study with accompanying performance analysis.",Design of Industrial Edge Applications Based on IEC 61499 Microservices and Containers,"Industrial automation is entering a new era of the Industrial Internet with enhanced computing, communication, and storage capabilities provided by cloud computing and field devices. The paradigm of automation systems is shifting from the ISA-95 pyramid to the two-layers architecture: industrial cloud and edge computing. Industrial software is also evolving under the new architecture in ways for which dedicated software applications are no longer suited. Service-based industrial cloud and edge applications provide maximum flexibility, interoperability, and efficiency by combining the IEC 61499 standard, microservice architecture, and container technology. This article provides orchestration methods and deployment procedures for the OT-IT hybrid industrial edge applications. The feasibility of the proposed approach is demonstrated by an industrial case study with accompanying performance analysis.",IEEE journal,no,"['design', 'industrial', 'edge', 'based', 'iec', 'container', 'industrial', 'automation', 'new', 'era', 'industrial', 'internet', 'enhanced', 'computing', 'communication', 'storage', 'capability', 'provided', 'computing', 'field', 'device', 'paradigm', 'automation', 'shifting', 'industrial', 'edge', 'computing', 'industrial', 'also', 'evolving', 'new', 'way', 'dedicated', 'longer', 'suited', 'industrial', 'edge', 'provide', 'maximum', 'flexibility', 'interoperability', 'efficiency', 'combining', 'iec', 'standard', 'container', 'technology', 'article', 'provides', 'orchestration', 'method', 'deployment', 'procedure', 'hybrid', 'industrial', 'edge', 'feasibility', 'proposed', 'demonstrated', 'industrial', 'case', 'study', 'performance', 'analysis']"
"A New Data Processing Architecture for Multi-Scenario Applications in Aviation Manufacturing The development of industry 4.0 has spurred the transformation of traditional manufacturing into modern industrial Internet-of-Things. The most notable feature during this transition is the improvement of digitization and intelligence based on the massive data drives. In such a data-driven environment, the processing, storage, and utilization of the industry data get more and more important. Usually, the traditional data processing architecture runs as a one-way streamline, which cannot adapt to the different requirements of the multi-scenario application. This paper proposed a new industrial big data processing architecture called Phi architecture, which can realize many functions such as batch data processing and stream data processing, distributed storage and access, and real-time control. Compared with other data processing architecture, the Phi architecture combined with edge computing and feedback control has the ability to deal with the different demands in aviation manufacturing. Next, the new architecture is designed for microservices pattern, which improves the flexibility and stability of the architecture, and makes it independent operated in multi-scenarios, such as state monitoring of workshop, adaptive data acquisition, feedback control, and user-oriented information classification. As a proof of concept, the architecture has been tested in a simulation digital manufacturing workshop. The results verify the improved effectiveness of the Phi architecture on the data feedback control and real-time processing. And, the development of microservices architecture greatly improves the efficiency, adaptability, and extensibility of the manufacturing process.",A New Data Processing Architecture for Multi-Scenario Applications in Aviation Manufacturing,"The development of industry 4.0 has spurred the transformation of traditional manufacturing into modern industrial Internet-of-Things. The most notable feature during this transition is the improvement of digitization and intelligence based on the massive data drives. In such a data-driven environment, the processing, storage, and utilization of the industry data get more and more important. Usually, the traditional data processing architecture runs as a one-way streamline, which cannot adapt to the different requirements of the multi-scenario application. This paper proposed a new industrial big data processing architecture called Phi architecture, which can realize many functions such as batch data processing and stream data processing, distributed storage and access, and real-time control. Compared with other data processing architecture, the Phi architecture combined with edge computing and feedback control has the ability to deal with the different demands in aviation manufacturing. Next, the new architecture is designed for microservices pattern, which improves the flexibility and stability of the architecture, and makes it independent operated in multi-scenarios, such as state monitoring of workshop, adaptive data acquisition, feedback control, and user-oriented information classification. As a proof of concept, the architecture has been tested in a simulation digital manufacturing workshop. The results verify the improved effectiveness of the Phi architecture on the data feedback control and real-time processing. And, the development of microservices architecture greatly improves the efficiency, adaptability, and extensibility of the manufacturing process.",IEEE journal,no,"['new', 'processing', 'manufacturing', 'development', 'industry', 'transformation', 'traditional', 'manufacturing', 'modern', 'industrial', 'notable', 'feature', 'transition', 'improvement', 'intelligence', 'based', 'massive', 'drive', 'environment', 'processing', 'storage', 'utilization', 'industry', 'get', 'important', 'usually', 'traditional', 'processing', 'run', 'adapt', 'different', 'requirement', 'paper', 'proposed', 'new', 'industrial', 'big', 'processing', 'called', 'phi', 'realize', 'many', 'function', 'batch', 'processing', 'stream', 'processing', 'distributed', 'storage', 'access', 'control', 'compared', 'processing', 'phi', 'combined', 'edge', 'computing', 'feedback', 'control', 'ability', 'deal', 'different', 'demand', 'manufacturing', 'next', 'new', 'designed', 'pattern', 'improves', 'flexibility', 'stability', 'make', 'independent', 'operated', 'state', 'monitoring', 'adaptive', 'acquisition', 'feedback', 'control', 'information', 'classification', 'proof', 'concept', 'tested', 'simulation', 'digital', 'manufacturing', 'result', 'verify', 'improved', 'effectiveness', 'phi', 'feedback', 'control', 'processing', 'development', 'greatly', 'improves', 'efficiency', 'adaptability', 'extensibility', 'manufacturing', 'process']"
"Distributed Digital Twins as Proxies-Unlocking Composability and Flexibility for Purpose-Oriented Digital Twins In the realm of the Industrial Internet of Things (IoT) and Industrial Cyber-Physical Systems (ICPS), Digital Twins (DTs) have revolutionized the management of physical entities. However, existing implementations often face constraints due to hardware-centric approaches and limited flexibility. This article introduces a transformative paradigm that harnesses the potential of distributed digital twins as proxies, enabling software-centricity and unlocking composability and flexibility for purpose-oriented digital twin development and deployment. The proposed microservices-based architecture, rooted in service-oriented architecture (SOA) and microservices principles, emphasizes reusability, modularity, and scalability. Leveraging the Lean Digital Twin Methodology and packaged business capabilities expedites digital twin creation and deployment, facilitating dynamic responses to evolving industrial demands. This architecture segments the industrial realm into physical and virtual spaces, where core components are responsible for digital twin management, deployment, and secure interactions. By abstracting and virtualizing physical entities into individual digital twins, this approach lays the groundwork for purpose-oriented composite digital twin creation. Our key contributions involve a comprehensive exposition of the architecture, a practical proof-of-concept (PoC) implementation, and the application of the architecture in a use-case scenario. Additionally, we provide an analysis, including a quantitative evaluation of the proxy aspect and a qualitative comparison with traditional approaches. This assessment emphasizes key properties such as reusability, modularity, abstraction, discoverability, and security, transcending the limitations of contemporary industrial systems and enabling agile, adaptable digital proxies to meet modern industrial demands.",Distributed Digital Twins as Proxies-Unlocking Composability and Flexibility for Purpose-Oriented Digital Twins,"In the realm of the Industrial Internet of Things (IoT) and Industrial Cyber-Physical Systems (ICPS), Digital Twins (DTs) have revolutionized the management of physical entities. However, existing implementations often face constraints due to hardware-centric approaches and limited flexibility. This article introduces a transformative paradigm that harnesses the potential of distributed digital twins as proxies, enabling software-centricity and unlocking composability and flexibility for purpose-oriented digital twin development and deployment. The proposed microservices-based architecture, rooted in service-oriented architecture (SOA) and microservices principles, emphasizes reusability, modularity, and scalability. Leveraging the Lean Digital Twin Methodology and packaged business capabilities expedites digital twin creation and deployment, facilitating dynamic responses to evolving industrial demands. This architecture segments the industrial realm into physical and virtual spaces, where core components are responsible for digital twin management, deployment, and secure interactions. By abstracting and virtualizing physical entities into individual digital twins, this approach lays the groundwork for purpose-oriented composite digital twin creation. Our key contributions involve a comprehensive exposition of the architecture, a practical proof-of-concept (PoC) implementation, and the application of the architecture in a use-case scenario. Additionally, we provide an analysis, including a quantitative evaluation of the proxy aspect and a qualitative comparison with traditional approaches. This assessment emphasizes key properties such as reusability, modularity, abstraction, discoverability, and security, transcending the limitations of contemporary industrial systems and enabling agile, adaptable digital proxies to meet modern industrial demands.",IEEE journal,no,"['distributed', 'digital', 'twin', 'composability', 'flexibility', 'digital', 'twin', 'realm', 'industrial', 'internet', 'thing', 'iot', 'industrial', 'digital', 'twin', 'revolutionized', 'management', 'physical', 'entity', 'however', 'existing', 'implementation', 'often', 'face', 'constraint', 'due', 'limited', 'flexibility', 'article', 'introduces', 'paradigm', 'harness', 'potential', 'distributed', 'digital', 'twin', 'proxy', 'enabling', 'composability', 'flexibility', 'digital', 'twin', 'development', 'deployment', 'proposed', 'soa', 'principle', 'emphasizes', 'reusability', 'modularity', 'scalability', 'leveraging', 'digital', 'twin', 'methodology', 'packaged', 'business', 'capability', 'digital', 'twin', 'creation', 'deployment', 'facilitating', 'dynamic', 'response', 'evolving', 'industrial', 'demand', 'segment', 'industrial', 'realm', 'physical', 'virtual', 'space', 'core', 'responsible', 'digital', 'twin', 'management', 'deployment', 'secure', 'interaction', 'abstracting', 'physical', 'entity', 'individual', 'digital', 'twin', 'composite', 'digital', 'twin', 'creation', 'key', 'contribution', 'involve', 'comprehensive', 'practical', 'implementation', 'scenario', 'additionally', 'provide', 'analysis', 'including', 'quantitative', 'evaluation', 'proxy', 'aspect', 'qualitative', 'comparison', 'traditional', 'assessment', 'emphasizes', 'key', 'property', 'reusability', 'modularity', 'abstraction', 'security', 'limitation', 'industrial', 'enabling', 'agile', 'adaptable', 'digital', 'proxy', 'meet', 'modern', 'industrial', 'demand']"
"Component-Based Microservices for Flexible and Scalable Automation of Industrial Bioprocesses Industry 4.0 involves the digital transformation of the industry with the integration and digitization of all industrial processes that make up the value chain, which is characterized by adaptability, flexibility, and efficiency to meet the needs of customers in today's market. Therefore, the adaptations of the new bioprocess industry require a lot of flexibility to react quickly and constantly to market changes and to be able to offer more specialized, customized products with high operational efficiency. This paper presents a flexible, scalable, and robust framework based on software components, container technology, microservice concepts, and the publish/subscribe paradigm. This framework allows new components to be added or removed online, without the need for system reconfiguration, while maintaining temporal and functional constraints in industrial automation systems. The main objective of the framework proposed is the use of components based on microservices, allowing easy implementation, scalability, and fast maintenance, without losing or degrading the robustness from previous developments. Finally, the effectiveness of the proposed framework was verified in two case studies (1) a soursop soda making process is presented, with a fuzzy controller implemented to keep the pasteurizer output flow constant (UHT) and (2) an automatic storage tank selection and filling process with actuated valves to direct the fluid to the corresponding tank at the time to start the process. The results showed that the platform provided a high-fidelity design, analysis, and testing environment for the flow of cyber information and its effect on the physical operation in a beverage processing plant with high demand for flexibility, scalability, and robustness of its processes, as they were experimentally verified in a real production process.",Component-Based Microservices for Flexible and Scalable Automation of Industrial Bioprocesses,"Industry 4.0 involves the digital transformation of the industry with the integration and digitization of all industrial processes that make up the value chain, which is characterized by adaptability, flexibility, and efficiency to meet the needs of customers in today's market. Therefore, the adaptations of the new bioprocess industry require a lot of flexibility to react quickly and constantly to market changes and to be able to offer more specialized, customized products with high operational efficiency. This paper presents a flexible, scalable, and robust framework based on software components, container technology, microservice concepts, and the publish/subscribe paradigm. This framework allows new components to be added or removed online, without the need for system reconfiguration, while maintaining temporal and functional constraints in industrial automation systems. The main objective of the framework proposed is the use of components based on microservices, allowing easy implementation, scalability, and fast maintenance, without losing or degrading the robustness from previous developments. Finally, the effectiveness of the proposed framework was verified in two case studies (1) a soursop soda making process is presented, with a fuzzy controller implemented to keep the pasteurizer output flow constant (UHT) and (2) an automatic storage tank selection and filling process with actuated valves to direct the fluid to the corresponding tank at the time to start the process. The results showed that the platform provided a high-fidelity design, analysis, and testing environment for the flow of cyber information and its effect on the physical operation in a beverage processing plant with high demand for flexibility, scalability, and robustness of its processes, as they were experimentally verified in a real production process.",IEEE journal,no,"['flexible', 'scalable', 'automation', 'industrial', 'industry', 'involves', 'digital', 'transformation', 'industry', 'integration', 'industrial', 'process', 'make', 'value', 'chain', 'characterized', 'adaptability', 'flexibility', 'efficiency', 'meet', 'need', 'customer', 'today', 'market', 'therefore', 'adaptation', 'new', 'industry', 'require', 'lot', 'flexibility', 'react', 'quickly', 'constantly', 'market', 'change', 'able', 'offer', 'specialized', 'customized', 'product', 'high', 'operational', 'efficiency', 'paper', 'present', 'flexible', 'scalable', 'robust', 'framework', 'based', 'container', 'technology', 'concept', 'paradigm', 'framework', 'allows', 'new', 'added', 'online', 'without', 'need', 'reconfiguration', 'maintaining', 'temporal', 'functional', 'constraint', 'industrial', 'automation', 'main', 'objective', 'framework', 'proposed', 'use', 'based', 'allowing', 'easy', 'implementation', 'scalability', 'fast', 'maintenance', 'without', 'robustness', 'previous', 'development', 'finally', 'effectiveness', 'proposed', 'framework', 'verified', 'two', 'case', 'study', 'making', 'process', 'presented', 'fuzzy', 'controller', 'implemented', 'keep', 'output', 'flow', 'constant', 'automatic', 'storage', 'tank', 'selection', 'process', 'direct', 'corresponding', 'tank', 'time', 'start', 'process', 'result', 'showed', 'platform', 'provided', 'design', 'analysis', 'testing', 'environment', 'flow', 'cyber', 'information', 'effect', 'physical', 'operation', 'processing', 'high', 'demand', 'flexibility', 'scalability', 'robustness', 'process', 'verified', 'real', 'production', 'process']"
"Scheduling Multi-Component Applications Across Federated Edge Clusters With Phare The shift towards agile microservice architecture has enabled significant benefits for IT companies but has also resulted in increased complexity for Cloud orchestration tools. Traditional tools were designed for centralized data centers and are ineffective for locating microservices in geographically-distributed edge-like infrastructures. This paper presents Phare, a decentralized scheduling algorithm designed to optimize the placement of microservices by satisfying their computing and communication demands while minimizing deployment costs. Phare employs a heuristic-based approach to solve the NP-Hard scheduling problem, prioritizing the microservices with the more stringent requirements and placing them on the most convenient computing facilities, based on the concept of affinity, contributing to the field by providing a more holistic approach to resource scheduling in edge computing. We validate our approach against Firmament, the state-of-the-art workload scheduling algorithm for component-based applications, on simulated edge infrastructures with hundreds of clusters. Phare achieves up to a  $10\times $  reduction in terms of deployment costs compared to Firmament while providing a much lower scheduling latency.",Scheduling Multi-Component Applications Across Federated Edge Clusters With Phare,"The shift towards agile microservice architecture has enabled significant benefits for IT companies but has also resulted in increased complexity for Cloud orchestration tools. Traditional tools were designed for centralized data centers and are ineffective for locating microservices in geographically-distributed edge-like infrastructures. This paper presents Phare, a decentralized scheduling algorithm designed to optimize the placement of microservices by satisfying their computing and communication demands while minimizing deployment costs. Phare employs a heuristic-based approach to solve the NP-Hard scheduling problem, prioritizing the microservices with the more stringent requirements and placing them on the most convenient computing facilities, based on the concept of affinity, contributing to the field by providing a more holistic approach to resource scheduling in edge computing. We validate our approach against Firmament, the state-of-the-art workload scheduling algorithm for component-based applications, on simulated edge infrastructures with hundreds of clusters. Phare achieves up to a  $10\times $  reduction in terms of deployment costs compared to Firmament while providing a much lower scheduling latency.",IEEE journal,no,"['scheduling', 'across', 'federated', 'edge', 'cluster', 'phare', 'shift', 'towards', 'agile', 'enabled', 'significant', 'benefit', 'company', 'also', 'increased', 'complexity', 'orchestration', 'tool', 'traditional', 'tool', 'designed', 'centralized', 'center', 'ineffective', 'infrastructure', 'paper', 'present', 'phare', 'decentralized', 'scheduling', 'algorithm', 'designed', 'optimize', 'placement', 'computing', 'communication', 'demand', 'minimizing', 'deployment', 'cost', 'phare', 'employ', 'solve', 'scheduling', 'problem', 'requirement', 'placing', 'convenient', 'computing', 'facility', 'based', 'concept', 'affinity', 'contributing', 'field', 'providing', 'holistic', 'resource', 'scheduling', 'edge', 'computing', 'validate', 'workload', 'scheduling', 'algorithm', 'simulated', 'edge', 'infrastructure', 'hundred', 'cluster', 'phare', 'achieves', 'reduction', 'term', 'deployment', 'cost', 'compared', 'providing', 'much', 'lower', 'scheduling', 'latency']"
"The Shape of Your Cloud: How to Design and Run Polylithic Cloud Applications Nowadays the major trend in IT dictates deploying applications in the cloud, cutting the monolithic software into small, easily manageable and developable components, and running them in a microservice scheme. With these choices come the questions: which cloud service types to choose from the several available options, and how to distribute the monolith in order to best resonate with the selected cloud features. We propose a model that presents monolithic applications in a novel way and focuses on key properties that are crucial in the development of cloud-native applications. The model focuses on the organization of scaling units, and it accounts for the cost of provisioned resources in scale-out periods and invocation delays among the application components. We analyze dis-aggregated monolithic applications that are deployed in the cloud, offering both Container-as-a-Service (CaaS) and Function-as-a-Service (FaaS) platforms. We showcase the efficiency of our proposed optimization solution by presenting the reduction in operation costs as an illustrative example. We propose to group similarly low scale components together in CaaS, while running dynamically scaled components in FaaS. By doing so, the price is decreased as unnecessary memory provisioning is eliminated, while application response time does not show any degradation.",The Shape of Your Cloud: How to Design and Run Polylithic Cloud Applications,"Nowadays the major trend in IT dictates deploying applications in the cloud, cutting the monolithic software into small, easily manageable and developable components, and running them in a microservice scheme. With these choices come the questions: which cloud service types to choose from the several available options, and how to distribute the monolith in order to best resonate with the selected cloud features. We propose a model that presents monolithic applications in a novel way and focuses on key properties that are crucial in the development of cloud-native applications. The model focuses on the organization of scaling units, and it accounts for the cost of provisioned resources in scale-out periods and invocation delays among the application components. We analyze dis-aggregated monolithic applications that are deployed in the cloud, offering both Container-as-a-Service (CaaS) and Function-as-a-Service (FaaS) platforms. We showcase the efficiency of our proposed optimization solution by presenting the reduction in operation costs as an illustrative example. We propose to group similarly low scale components together in CaaS, while running dynamically scaled components in FaaS. By doing so, the price is decreased as unnecessary memory provisioning is eliminated, while application response time does not show any degradation.",IEEE journal,no,"['shape', 'design', 'run', 'nowadays', 'major', 'trend', 'deploying', 'monolithic', 'small', 'easily', 'manageable', 'running', 'scheme', 'choice', 'come', 'question', 'type', 'choose', 'several', 'available', 'option', 'distribute', 'monolith', 'order', 'best', 'selected', 'feature', 'propose', 'model', 'present', 'monolithic', 'novel', 'way', 'focus', 'key', 'property', 'crucial', 'development', 'model', 'focus', 'organization', 'scaling', 'unit', 'account', 'cost', 'provisioned', 'resource', 'period', 'invocation', 'delay', 'among', 'analyze', 'monolithic', 'deployed', 'offering', 'caas', 'faa', 'platform', 'showcase', 'efficiency', 'proposed', 'optimization', 'solution', 'presenting', 'reduction', 'operation', 'cost', 'illustrative', 'example', 'propose', 'group', 'low', 'scale', 'together', 'caas', 'running', 'dynamically', 'scaled', 'faa', 'price', 'unnecessary', 'memory', 'provisioning', 'eliminated', 'response', 'time', 'show', 'degradation']"
"Reducing Microservices Interference and Deployment Time in Resource-Constrained Cloud Systems In resource-constrained cloud systems, e.g., at the network edge or in private clouds, it is essential to deploy microservices (MSs) efficiently. Unlike most of the existing approaches, we tackle this issue by accounting for two important facts: (i) the interference that arises when MSs compete for the same resources and degrades their performance, and (ii) the MSs’ deployment time. In particular, we first present some experiments highlighting the impact of interference on the throughput of MSs co-located in the same server, as well as the benefits of MSs’ parallel deployment. Then, we formulate an optimization problem that minimizes the number of used servers while meeting the MSs’ performance requirements. In light of the problem complexity, we design a low-complexity heuristic, called iPlace, that clusters together MSs competing for resources as diverse as possible and, hence, interfering as little as possible. Importantly, clustering MSs also allows us to exploit the benefit of parallel deployment, which greatly reduces the deployment time as compared to the sequential approach applied in prior art and by default in state-of-the-art orchestrators. Our numerical results show that iPlace closely matches the optimum and uses 21-92% fewer servers compared to alternative schemes while proving to be highly scalable. Further, by deploying MSs in parallel using Kubernetes, iPlace reduces the deployment time by 69% compared to state-of-the-art solutions.",Reducing Microservices Interference and Deployment Time in Resource-Constrained Cloud Systems,"In resource-constrained cloud systems, e.g., at the network edge or in private clouds, it is essential to deploy microservices (MSs) efficiently. Unlike most of the existing approaches, we tackle this issue by accounting for two important facts: (i) the interference that arises when MSs compete for the same resources and degrades their performance, and (ii) the MSs’ deployment time. In particular, we first present some experiments highlighting the impact of interference on the throughput of MSs co-located in the same server, as well as the benefits of MSs’ parallel deployment. Then, we formulate an optimization problem that minimizes the number of used servers while meeting the MSs’ performance requirements. In light of the problem complexity, we design a low-complexity heuristic, called iPlace, that clusters together MSs competing for resources as diverse as possible and, hence, interfering as little as possible. Importantly, clustering MSs also allows us to exploit the benefit of parallel deployment, which greatly reduces the deployment time as compared to the sequential approach applied in prior art and by default in state-of-the-art orchestrators. Our numerical results show that iPlace closely matches the optimum and uses 21-92% fewer servers compared to alternative schemes while proving to be highly scalable. Further, by deploying MSs in parallel using Kubernetes, iPlace reduces the deployment time by 69% compared to state-of-the-art solutions.",IEEE journal,no,"['reducing', 'interference', 'deployment', 'time', 'network', 'edge', 'private', 'essential', 'deploy', 'ms', 'efficiently', 'unlike', 'existing', 'tackle', 'issue', 'accounting', 'two', 'important', 'fact', 'interference', 'arises', 'ms', 'resource', 'performance', 'ii', 'ms', 'deployment', 'time', 'particular', 'first', 'present', 'experiment', 'highlighting', 'impact', 'interference', 'throughput', 'ms', 'server', 'well', 'benefit', 'ms', 'parallel', 'deployment', 'formulate', 'optimization', 'problem', 'minimizes', 'number', 'used', 'server', 'meeting', 'ms', 'performance', 'requirement', 'light', 'problem', 'complexity', 'design', 'heuristic', 'called', 'iplace', 'cluster', 'together', 'ms', 'resource', 'diverse', 'possible', 'hence', 'little', 'possible', 'clustering', 'ms', 'also', 'allows', 'u', 'exploit', 'benefit', 'parallel', 'deployment', 'greatly', 'reduces', 'deployment', 'time', 'compared', 'sequential', 'applied', 'prior', 'art', 'default', 'orchestrator', 'numerical', 'result', 'show', 'iplace', 'closely', 'match', 'us', 'fewer', 'server', 'compared', 'alternative', 'scheme', 'highly', 'scalable', 'deploying', 'ms', 'parallel', 'using', 'kubernetes', 'iplace', 'reduces', 'deployment', 'time', 'compared', 'solution']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",ACM,yes,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",DeepScaling: Autoscaling Microservices With Stable CPU Utilization for Large Scale Production Cloud Systems,"Cloud service providers often provision excessive resources to meet the desired Service Level Objectives (SLOs), by setting lower CPU utilization targets. This can result in a waste of resources and a noticeable increase in power consumption in large-scale cloud deployments. To address this issue, this paper presents DeepScaling, an innovative solution for minimizing resource cost while ensuring SLO requirements are met in a dynamic, large-scale production microservice-based system. We propose DeepScaling, which introduces three innovative components to adaptively refine the target CPU utilization of servers in the data center, and we maintain it at a stable value to meet SLO constraints while using minimum amount of system resources. First, DeepScaling forecasts workloads for each service using a Spatio-temporal Graph Neural Network. Secondly, it estimates CPU utilization with a Deep Neural Network, considering factors such as periodic tasks and traffic. Finally, it uses a modified Deep Q-Network (DQN) to generate an autoscaling policy that controls service resources to maximize service stability while meeting SLOs. Evaluation of DeepScaling in Ant Group’s large-scale cloud environment shows that it outperforms state-of-the-art autoscaling approaches in terms of maintaining stable performance and resource savings. The deployment of DeepScaling in the real-world environment of 1900+ microservices saves the provisioning of over 100,000 CPU cores per day, on average.",IEEE journal,no,"['deepscaling', 'autoscaling', 'stable', 'cpu', 'utilization', 'large', 'scale', 'production', 'provider', 'often', 'provision', 'excessive', 'resource', 'meet', 'desired', 'level', 'objective', 'slos', 'setting', 'lower', 'cpu', 'utilization', 'target', 'result', 'waste', 'resource', 'noticeable', 'increase', 'power', 'consumption', 'deployment', 'address', 'issue', 'paper', 'present', 'deepscaling', 'innovative', 'solution', 'minimizing', 'resource', 'cost', 'ensuring', 'slo', 'requirement', 'met', 'dynamic', 'production', 'propose', 'deepscaling', 'introduces', 'three', 'innovative', 'adaptively', 'refine', 'target', 'cpu', 'utilization', 'server', 'center', 'maintain', 'stable', 'value', 'meet', 'slo', 'constraint', 'using', 'minimum', 'amount', 'resource', 'first', 'deepscaling', 'forecast', 'workload', 'using', 'graph', 'neural', 'network', 'secondly', 'estimate', 'cpu', 'utilization', 'deep', 'neural', 'network', 'considering', 'factor', 'periodic', 'task', 'traffic', 'finally', 'us', 'modified', 'deep', 'dqn', 'generate', 'autoscaling', 'policy', 'control', 'resource', 'maximize', 'stability', 'meeting', 'slos', 'evaluation', 'deepscaling', 'ant', 'group', 'environment', 'show', 'outperforms', 'autoscaling', 'term', 'maintaining', 'stable', 'performance', 'resource', 'saving', 'deployment', 'deepscaling', 'environment', 'save', 'provisioning', 'cpu', 'core', 'per', 'day', 'average']"
"Service Based Virtual RAN Architecture for Next Generation Cellular Systems Service based architecture (SBA) is a paradigm shift from Service-Oriented Architecture (SOA) to microservices, combining their principles. Network virtualization enables the application of SBA in cellular systems. To better guide the software design of this virtualized cellular system with SBA, this paper presents a software perspective and a positional approach to using fundamental development principles for adapting SBA in virtualized Radio Access Networks (vRANs). First, we present the motivation for using an SBA in cellular radio systems. Then, we explore the critical requirements, key principles, and components for the software to provide radio services in SBA. We also explore the potential of applying SBA-based Radio Access Network (RAN) by comparing the functional split requirements of 5G RAN with existing open-source software and accelerated hardware implementations of service bus, and discuss the limitations of SBA. Finally, we present some discussions, future directions, and a roadmap of applying such a high-level design perspective of SBA to next-generation RAN infrastructure.",Service Based Virtual RAN Architecture for Next Generation Cellular Systems,"Service based architecture (SBA) is a paradigm shift from Service-Oriented Architecture (SOA) to microservices, combining their principles. Network virtualization enables the application of SBA in cellular systems. To better guide the software design of this virtualized cellular system with SBA, this paper presents a software perspective and a positional approach to using fundamental development principles for adapting SBA in virtualized Radio Access Networks (vRANs). First, we present the motivation for using an SBA in cellular radio systems. Then, we explore the critical requirements, key principles, and components for the software to provide radio services in SBA. We also explore the potential of applying SBA-based Radio Access Network (RAN) by comparing the functional split requirements of 5G RAN with existing open-source software and accelerated hardware implementations of service bus, and discuss the limitations of SBA. Finally, we present some discussions, future directions, and a roadmap of applying such a high-level design perspective of SBA to next-generation RAN infrastructure.",IEEE journal,no,"['based', 'virtual', 'ran', 'next', 'generation', 'cellular', 'based', 'sba', 'paradigm', 'shift', 'soa', 'combining', 'principle', 'network', 'virtualization', 'enables', 'sba', 'cellular', 'better', 'guide', 'design', 'virtualized', 'cellular', 'sba', 'paper', 'present', 'perspective', 'using', 'fundamental', 'development', 'principle', 'adapting', 'sba', 'virtualized', 'radio', 'access', 'network', 'first', 'present', 'motivation', 'using', 'sba', 'cellular', 'radio', 'explore', 'critical', 'requirement', 'key', 'principle', 'provide', 'radio', 'sba', 'also', 'explore', 'potential', 'applying', 'radio', 'access', 'network', 'ran', 'comparing', 'functional', 'split', 'requirement', 'ran', 'existing', 'hardware', 'implementation', 'bus', 'discus', 'limitation', 'sba', 'finally', 'present', 'discussion', 'future', 'direction', 'applying', 'design', 'perspective', 'sba', 'ran', 'infrastructure']"
"CoScal: Multifaceted Scaling of Microservices With Reinforcement Learning The emerging trend towards moving from monolithic applications to microservices has raised new performance challenges in cloud computing environments. Compared with traditional monolithic applications, the microservices are lightweight, fine-grained, and must be executed in a shorter time. Efficient scaling approaches are required to ensure microservices’ system performance under diverse workloads with strict Quality of Service (QoS) requirements and optimize resource provisioning. To solve this problem, we investigate the trade-offs between the dominant scaling techniques, including horizontal scaling, vertical scaling, and brownout in terms of execution cost and response time. We first present a prediction algorithm based on gradient recurrent units to accurately predict workloads assisting in scaling to achieve efficient scaling. Further, we propose a multi-faceted scaling approach using reinforcement learning called CoScal to learn the scaling techniques efficiently. The proposed CoScal approach takes full advantage of data-driven decisions and improves the system performance in terms of high communication cost and delay. We validate our proposed solution by implementing a containerized microservice prototype system and evaluated with two microservice applications. The extensive experiments demonstrate that CoScal reduces response time by 19%-29% and decreases the connection time of services by 16% when compared with the state-of-the-art scaling techniques for Sock Shop application. CoScal can also improve the number of successful transactions with 6%-10% for Stan’s Robot Shop application.",CoScal: Multifaceted Scaling of Microservices With Reinforcement Learning,"The emerging trend towards moving from monolithic applications to microservices has raised new performance challenges in cloud computing environments. Compared with traditional monolithic applications, the microservices are lightweight, fine-grained, and must be executed in a shorter time. Efficient scaling approaches are required to ensure microservices’ system performance under diverse workloads with strict Quality of Service (QoS) requirements and optimize resource provisioning. To solve this problem, we investigate the trade-offs between the dominant scaling techniques, including horizontal scaling, vertical scaling, and brownout in terms of execution cost and response time. We first present a prediction algorithm based on gradient recurrent units to accurately predict workloads assisting in scaling to achieve efficient scaling. Further, we propose a multi-faceted scaling approach using reinforcement learning called CoScal to learn the scaling techniques efficiently. The proposed CoScal approach takes full advantage of data-driven decisions and improves the system performance in terms of high communication cost and delay. We validate our proposed solution by implementing a containerized microservice prototype system and evaluated with two microservice applications. The extensive experiments demonstrate that CoScal reduces response time by 19%-29% and decreases the connection time of services by 16% when compared with the state-of-the-art scaling techniques for Sock Shop application. CoScal can also improve the number of successful transactions with 6%-10% for Stan’s Robot Shop application.",IEEE journal,no,"['coscal', 'scaling', 'reinforcement', 'learning', 'emerging', 'trend', 'towards', 'moving', 'monolithic', 'new', 'performance', 'challenge', 'computing', 'environment', 'compared', 'traditional', 'monolithic', 'lightweight', 'must', 'executed', 'time', 'efficient', 'scaling', 'required', 'ensure', 'performance', 'diverse', 'workload', 'strict', 'quality', 'qos', 'requirement', 'optimize', 'resource', 'provisioning', 'solve', 'problem', 'investigate', 'dominant', 'scaling', 'technique', 'including', 'horizontal', 'scaling', 'vertical', 'scaling', 'term', 'execution', 'cost', 'response', 'time', 'first', 'present', 'prediction', 'algorithm', 'based', 'gradient', 'recurrent', 'unit', 'accurately', 'predict', 'workload', 'assisting', 'scaling', 'achieve', 'efficient', 'scaling', 'propose', 'scaling', 'using', 'reinforcement', 'learning', 'called', 'coscal', 'learn', 'scaling', 'technique', 'efficiently', 'proposed', 'coscal', 'take', 'full', 'advantage', 'decision', 'improves', 'performance', 'term', 'high', 'communication', 'cost', 'delay', 'validate', 'proposed', 'solution', 'implementing', 'containerized', 'prototype', 'evaluated', 'two', 'extensive', 'experiment', 'demonstrate', 'coscal', 'reduces', 'response', 'time', 'decrease', 'connection', 'time', 'compared', 'scaling', 'technique', 'sock', 'shop', 'coscal', 'also', 'improve', 'number', 'successful', 'transaction', 'robot', 'shop']"
"Edge Computing and Microservices Middleware for Home Energy Management Systems A middleware software can be seem as an abstraction layer between hardware and user applications, that facilitates the development and deployment of services in various scenarios, such as those found in Home Energy Management Systems (HEMS). There are several middleware proposals for HEMS, with most of them taking the cloud computing approach. This approach is unconcerned about computing resources but raises a dependency on external connections. This paper presents a middleware for energy management systems, based on the concept of edge computing for smart homes. The paper presents a reference model for the proposed architecture, considering specific requirements for this type of application. The proposed architecture employs the concept of microservices for data access and system configuration. The proposed middleware is designed to work with embedded systems under computational constraints, such as processing capability and storage, to reduce costs and allow its application closer to the user. The middleware is open and customizable to meet the developer’s needs. The proposed solution was implemented and tested in a university laboratory, as well as at the Eldorado Research Institute to confirm the effectiveness of the middleware. The proposal stands out from others found in the literature as it can be implemented using low cost hardware. In addition to using microservices concepts, the proposed middleware is a valuable option for applications that need an edge computing approach. A performance analysis was carried out, using low cost hardware with limited resources. The results show that the proposal can handle a significant number of devices, offering low latency and low error rate, and consuming few processing resources and memory.",Edge Computing and Microservices Middleware for Home Energy Management Systems,"A middleware software can be seem as an abstraction layer between hardware and user applications, that facilitates the development and deployment of services in various scenarios, such as those found in Home Energy Management Systems (HEMS). There are several middleware proposals for HEMS, with most of them taking the cloud computing approach. This approach is unconcerned about computing resources but raises a dependency on external connections. This paper presents a middleware for energy management systems, based on the concept of edge computing for smart homes. The paper presents a reference model for the proposed architecture, considering specific requirements for this type of application. The proposed architecture employs the concept of microservices for data access and system configuration. The proposed middleware is designed to work with embedded systems under computational constraints, such as processing capability and storage, to reduce costs and allow its application closer to the user. The middleware is open and customizable to meet the developer’s needs. The proposed solution was implemented and tested in a university laboratory, as well as at the Eldorado Research Institute to confirm the effectiveness of the middleware. The proposal stands out from others found in the literature as it can be implemented using low cost hardware. In addition to using microservices concepts, the proposed middleware is a valuable option for applications that need an edge computing approach. A performance analysis was carried out, using low cost hardware with limited resources. The results show that the proposal can handle a significant number of devices, offering low latency and low error rate, and consuming few processing resources and memory.",IEEE journal,no,"['edge', 'computing', 'middleware', 'home', 'energy', 'management', 'middleware', 'abstraction', 'layer', 'hardware', 'user', 'facilitates', 'development', 'deployment', 'various', 'scenario', 'found', 'home', 'energy', 'management', 'several', 'middleware', 'proposal', 'taking', 'computing', 'computing', 'resource', 'raise', 'dependency', 'external', 'connection', 'paper', 'present', 'middleware', 'energy', 'management', 'based', 'concept', 'edge', 'computing', 'smart', 'home', 'paper', 'present', 'reference', 'model', 'proposed', 'considering', 'specific', 'requirement', 'type', 'proposed', 'employ', 'concept', 'access', 'configuration', 'proposed', 'middleware', 'designed', 'work', 'embedded', 'computational', 'constraint', 'processing', 'capability', 'storage', 'reduce', 'cost', 'allow', 'user', 'middleware', 'open', 'meet', 'developer', 'need', 'proposed', 'solution', 'implemented', 'tested', 'university', 'well', 'research', 'effectiveness', 'middleware', 'proposal', 'stand', 'others', 'found', 'literature', 'implemented', 'using', 'low', 'cost', 'hardware', 'addition', 'using', 'concept', 'proposed', 'middleware', 'valuable', 'option', 'need', 'edge', 'computing', 'performance', 'analysis', 'carried', 'using', 'low', 'cost', 'hardware', 'limited', 'resource', 'result', 'show', 'proposal', 'handle', 'significant', 'number', 'device', 'offering', 'low', 'latency', 'low', 'error', 'rate', 'consuming', 'processing', 'resource', 'memory']"
"OPACA: Toward an Open, Language- and Platform-Independent API for Containerized Agents While multi-agent frameworks can provide many advanced features, they often suffer from not being able to seamlessly interact with the outside world, e.g., with web-services or other multi-agent frameworks. This may be one factor that hinders a broader application of multi-agent systems in production systems. A possible solution to this problem is the combination of multi-agent systems with the concepts of micro-services and containerization, providing language-agnostic open interfaces, as well as encapsulation and modularity. In this paper, we propose an API and reference implementation that can be employed by multi-agent systems based on different languages and frameworks. Each agent component is encapsulated in a container and is accessed through its parent runtime platform, which takes care of aspects such as authentication, input validation, monitoring and other infrastructure tasks. Multiple runtime platforms can then be connected to form systems of distributed, heterogeneous multi-agent societies.","OPACA: Toward an Open, Language- and Platform-Independent API for Containerized Agents","While multi-agent frameworks can provide many advanced features, they often suffer from not being able to seamlessly interact with the outside world, e.g., with web-services or other multi-agent frameworks. This may be one factor that hinders a broader application of multi-agent systems in production systems. A possible solution to this problem is the combination of multi-agent systems with the concepts of micro-services and containerization, providing language-agnostic open interfaces, as well as encapsulation and modularity. In this paper, we propose an API and reference implementation that can be employed by multi-agent systems based on different languages and frameworks. Each agent component is encapsulated in a container and is accessed through its parent runtime platform, which takes care of aspects such as authentication, input validation, monitoring and other infrastructure tasks. Multiple runtime platforms can then be connected to form systems of distributed, heterogeneous multi-agent societies.",IEEE journal,no,"['toward', 'open', 'api', 'containerized', 'agent', 'framework', 'provide', 'many', 'advanced', 'feature', 'often', 'suffer', 'able', 'seamlessly', 'interact', 'outside', 'world', 'framework', 'may', 'one', 'factor', 'hinders', 'broader', 'production', 'possible', 'solution', 'problem', 'combination', 'concept', 'containerization', 'providing', 'open', 'interface', 'well', 'modularity', 'paper', 'propose', 'api', 'reference', 'implementation', 'employed', 'based', 'different', 'language', 'framework', 'agent', 'container', 'accessed', 'runtime', 'platform', 'take', 'care', 'aspect', 'authentication', 'input', 'validation', 'monitoring', 'infrastructure', 'task', 'multiple', 'runtime', 'platform', 'connected', 'form', 'distributed', 'heterogeneous']"
"Joint Deployment and Request Routing for Microservice Call Graphs in Data Centers Microservices are an architectural and organizational paradigm for Internet application development. In cloud data centers, delay-sensitive applications receive massive user requests, which are fed into multiple queues and subsequently served by multiple microservice instances. Accordingly, effective deployment of multiple queues and containers can significantly reduce queuing delay, processing delay, and communication delay. Due to the increased complexity of call dependencies and probabilistic routing paths, the deployment of service instances fully interacts with request routing, bringing great difficulties to service orchestration. In this case, it is valuable to simultaneously consider service deployment and request routing in a fine-grained manner. However, most existing studies considered them as two independent components with local optimization, while data dependencies and the instance-level deployment are ignored. Therefore, this paper proposes to jointly optimize the deployment and request routing of microservice call graphs based on fine-grained queuing network analysis and container orchestration. We first formulate the problem as a mixed-integer nonlinear program and exploit open Jackson queuing networks to model intrinsic data dependencies and analyze response latency. To optimize the overall cost and latency, this paper presents an efficient two-stage heuristic algorithm, which consists of a resource-splitting-based deployment approach and a partition-mapping-based routing method. Further, this paper also provides mathematical analysis on the performance and complexity of the proposed algorithm. Finally, comprehensive trace-driven experiments demonstrate that the overall performance of our approach is better than existing microservice benchmarks. The average deployment cost is reduced by 27.4% and end-to-end response latency is reduced by 15.1% on average.",Joint Deployment and Request Routing for Microservice Call Graphs in Data Centers,"Microservices are an architectural and organizational paradigm for Internet application development. In cloud data centers, delay-sensitive applications receive massive user requests, which are fed into multiple queues and subsequently served by multiple microservice instances. Accordingly, effective deployment of multiple queues and containers can significantly reduce queuing delay, processing delay, and communication delay. Due to the increased complexity of call dependencies and probabilistic routing paths, the deployment of service instances fully interacts with request routing, bringing great difficulties to service orchestration. In this case, it is valuable to simultaneously consider service deployment and request routing in a fine-grained manner. However, most existing studies considered them as two independent components with local optimization, while data dependencies and the instance-level deployment are ignored. Therefore, this paper proposes to jointly optimize the deployment and request routing of microservice call graphs based on fine-grained queuing network analysis and container orchestration. We first formulate the problem as a mixed-integer nonlinear program and exploit open Jackson queuing networks to model intrinsic data dependencies and analyze response latency. To optimize the overall cost and latency, this paper presents an efficient two-stage heuristic algorithm, which consists of a resource-splitting-based deployment approach and a partition-mapping-based routing method. Further, this paper also provides mathematical analysis on the performance and complexity of the proposed algorithm. Finally, comprehensive trace-driven experiments demonstrate that the overall performance of our approach is better than existing microservice benchmarks. The average deployment cost is reduced by 27.4% and end-to-end response latency is reduced by 15.1% on average.",IEEE journal,no,"['deployment', 'request', 'routing', 'call', 'graph', 'center', 'architectural', 'organizational', 'paradigm', 'internet', 'development', 'center', 'massive', 'user', 'request', 'multiple', 'queue', 'subsequently', 'multiple', 'instance', 'accordingly', 'effective', 'deployment', 'multiple', 'queue', 'container', 'significantly', 'reduce', 'queuing', 'delay', 'processing', 'delay', 'communication', 'delay', 'due', 'increased', 'complexity', 'call', 'dependency', 'probabilistic', 'routing', 'path', 'deployment', 'instance', 'fully', 'request', 'routing', 'bringing', 'great', 'difficulty', 'orchestration', 'case', 'valuable', 'simultaneously', 'consider', 'deployment', 'request', 'routing', 'manner', 'however', 'existing', 'study', 'considered', 'two', 'independent', 'local', 'optimization', 'dependency', 'deployment', 'therefore', 'paper', 'proposes', 'optimize', 'deployment', 'request', 'routing', 'call', 'graph', 'based', 'queuing', 'network', 'analysis', 'container', 'orchestration', 'first', 'formulate', 'problem', 'program', 'exploit', 'open', 'queuing', 'network', 'model', 'dependency', 'analyze', 'response', 'latency', 'optimize', 'overall', 'cost', 'latency', 'paper', 'present', 'efficient', 'heuristic', 'algorithm', 'consists', 'deployment', 'routing', 'method', 'paper', 'also', 'provides', 'mathematical', 'analysis', 'performance', 'complexity', 'proposed', 'algorithm', 'finally', 'comprehensive', 'experiment', 'demonstrate', 'overall', 'performance', 'better', 'existing', 'benchmark', 'average', 'deployment', 'cost', 'reduced', 'response', 'latency', 'reduced', 'average']"
"A Progressive Web Application Based on Microservices Combining Geospatial Data and the Internet of Things Modern Web applications combine information from different sources, such as Web services, static resources, or real-time sensors data. The Internet of Things (IoT) is increasingly being used in these applications to show useful, updated information. However, the information related to the IoT devices is commonly displayed on dashboards for monitoring and control purposes and is not often combined with other types of data. In addition, it is important to base information on the location displayed in the user context. In this paper, we propose the use of a software architecture based on microservices and micro frontends for assisting the user in the friendly, seamless acquisition of geospatial data and information concerning the IoT. Our solution orchestrates those microservices and a component-based progressive Web application (PWA). The main microservice handles the creation of component configurations using a selection graph consisting of component tags and other descriptive properties and also contextual information about the application user. To demonstrate how the proposed architecture works, we present a scenario in which the Web application is dynamically built up by combining the geospatial information, the data acquired from the IoT sensors, and other complementary data.",A Progressive Web Application Based on Microservices Combining Geospatial Data and the Internet of Things,"Modern Web applications combine information from different sources, such as Web services, static resources, or real-time sensors data. The Internet of Things (IoT) is increasingly being used in these applications to show useful, updated information. However, the information related to the IoT devices is commonly displayed on dashboards for monitoring and control purposes and is not often combined with other types of data. In addition, it is important to base information on the location displayed in the user context. In this paper, we propose the use of a software architecture based on microservices and micro frontends for assisting the user in the friendly, seamless acquisition of geospatial data and information concerning the IoT. Our solution orchestrates those microservices and a component-based progressive Web application (PWA). The main microservice handles the creation of component configurations using a selection graph consisting of component tags and other descriptive properties and also contextual information about the application user. To demonstrate how the proposed architecture works, we present a scenario in which the Web application is dynamically built up by combining the geospatial information, the data acquired from the IoT sensors, and other complementary data.",IEEE journal,no,"['web', 'based', 'combining', 'geospatial', 'internet', 'thing', 'modern', 'web', 'combine', 'information', 'different', 'source', 'web', 'static', 'resource', 'sensor', 'internet', 'thing', 'iot', 'increasingly', 'used', 'show', 'useful', 'updated', 'information', 'however', 'information', 'related', 'iot', 'device', 'commonly', 'displayed', 'dashboard', 'monitoring', 'control', 'purpose', 'often', 'combined', 'type', 'addition', 'important', 'base', 'information', 'location', 'displayed', 'user', 'context', 'paper', 'propose', 'use', 'based', 'micro', 'assisting', 'user', 'seamless', 'acquisition', 'geospatial', 'information', 'concerning', 'iot', 'solution', 'web', 'main', 'handle', 'creation', 'configuration', 'using', 'selection', 'graph', 'consisting', 'tag', 'property', 'also', 'contextual', 'information', 'user', 'demonstrate', 'proposed', 'work', 'present', 'scenario', 'web', 'dynamically', 'built', 'combining', 'geospatial', 'information', 'acquired', 'iot', 'sensor']"
"A Unified Control Platform and Architecture for the Integration of Wind-Hydrogen Systems Into the Grid Hydrogen is a promising energy vector for achieving renewable integration into the grid, thus fostering the decarbonization of the energy sector. This paper presents the control platform architecture of a real hydrogen-based energy production, storage, and re-electrification system (HESS) paired to a wind farm located in north Norway and connected to the main grid. The HESS consists of an electrolyser, a hydrogen tank, and a fuel cell. The control platform includes the management software, the control algorithms, and the automation technologies operating the HESS in order to address the three use cases (electricity storage, mini-grid, and fuel production) identified in the IEA-HIA Task24 final report, that promote the integration of wind energy into the main grid. The control algorithms have been already developed by the same authors in other papers using mixed-logical dynamical modeling, and implemented via a two-layer model predictive control scheme for each use case, and are quickly introduced in order to make evident their integration into the presented architecture. Simulation test runs with real equipment data, wind generation, load profiles, and market prices are also reported so as to highlight the control platform performances.Note to Practitioners—The paper develops the integration between the management platform of a HESS, paired to a real wind farm in northern Norway, and the control algorithms aimed at scheduling hydrogen production and re-electrification on the basis of several forecast streams about exogenous conditions and different possible operating modes of the wind-hydrogen system. The control algorithms address the three use cases identified by the IEA-HIA in the final report of Task 24 about the integration of wind energy into the grid, namely i) electricity storage, where the HESS is operated in order to enable the wind farm to power smoothing; ii) mini-grid, where the wind farm and the HESS form a mini-grid with a local load (small town) and the HESS is therefore operated in order to fulfill it without and with grid support (in this case buying and selling electricity to the market is also handled); and iii) fuel production, where the HESS is operated in order to fulfill a hydrogen demand (e.g., due to fuel cell vehicles). In addition to the specific objectives of each use case, the developed control algorithms also optimize the HESS operating costs and typically address two time-scale behaviors to appropriately handle corresponding long and short terms dynamics. The management platform of the HESS is arranged in three layers (physical, control, and supervision layers), and located in the cloud. The physical layer targets the physical components, sensors, and actuators. The automation layer includes all local controllers and modules used for measurement, and several servers for interactions between the higher and lower layers of the control architecture and databases. In the supervision layer, the execution of control algorithms and clients for remote diagnoses, monitoring, and top-management activities are located. Since each layer performs specific functionalities, a multi-tier architecture is implemented and the communications among the layers occur through services and microservices.",A Unified Control Platform and Architecture for the Integration of Wind-Hydrogen Systems Into the Grid,"Hydrogen is a promising energy vector for achieving renewable integration into the grid, thus fostering the decarbonization of the energy sector. This paper presents the control platform architecture of a real hydrogen-based energy production, storage, and re-electrification system (HESS) paired to a wind farm located in north Norway and connected to the main grid. The HESS consists of an electrolyser, a hydrogen tank, and a fuel cell. The control platform includes the management software, the control algorithms, and the automation technologies operating the HESS in order to address the three use cases (electricity storage, mini-grid, and fuel production) identified in the IEA-HIA Task24 final report, that promote the integration of wind energy into the main grid. The control algorithms have been already developed by the same authors in other papers using mixed-logical dynamical modeling, and implemented via a two-layer model predictive control scheme for each use case, and are quickly introduced in order to make evident their integration into the presented architecture. Simulation test runs with real equipment data, wind generation, load profiles, and market prices are also reported so as to highlight the control platform performances.Note to Practitioners—The paper develops the integration between the management platform of a HESS, paired to a real wind farm in northern Norway, and the control algorithms aimed at scheduling hydrogen production and re-electrification on the basis of several forecast streams about exogenous conditions and different possible operating modes of the wind-hydrogen system. The control algorithms address the three use cases identified by the IEA-HIA in the final report of Task 24 about the integration of wind energy into the grid, namely i) electricity storage, where the HESS is operated in order to enable the wind farm to power smoothing; ii) mini-grid, where the wind farm and the HESS form a mini-grid with a local load (small town) and the HESS is therefore operated in order to fulfill it without and with grid support (in this case buying and selling electricity to the market is also handled); and iii) fuel production, where the HESS is operated in order to fulfill a hydrogen demand (e.g., due to fuel cell vehicles). In addition to the specific objectives of each use case, the developed control algorithms also optimize the HESS operating costs and typically address two time-scale behaviors to appropriately handle corresponding long and short terms dynamics. The management platform of the HESS is arranged in three layers (physical, control, and supervision layers), and located in the cloud. The physical layer targets the physical components, sensors, and actuators. The automation layer includes all local controllers and modules used for measurement, and several servers for interactions between the higher and lower layers of the control architecture and databases. In the supervision layer, the execution of control algorithms and clients for remote diagnoses, monitoring, and top-management activities are located. Since each layer performs specific functionalities, a multi-tier architecture is implemented and the communications among the layers occur through services and microservices.",IEEE journal,no,"['unified', 'control', 'platform', 'integration', 'grid', 'hydrogen', 'promising', 'energy', 'vector', 'achieving', 'integration', 'grid', 'thus', 'fostering', 'energy', 'sector', 'paper', 'present', 'control', 'platform', 'real', 'energy', 'production', 'storage', 'hess', 'paired', 'wind', 'farm', 'located', 'connected', 'main', 'grid', 'hess', 'consists', 'hydrogen', 'tank', 'fuel', 'cell', 'control', 'platform', 'includes', 'management', 'control', 'algorithm', 'automation', 'technology', 'operating', 'hess', 'order', 'address', 'three', 'use', 'case', 'electricity', 'storage', 'fuel', 'production', 'identified', 'final', 'report', 'promote', 'integration', 'wind', 'energy', 'main', 'grid', 'control', 'algorithm', 'already', 'developed', 'author', 'paper', 'using', 'modeling', 'implemented', 'via', 'model', 'predictive', 'control', 'scheme', 'use', 'case', 'quickly', 'introduced', 'order', 'make', 'integration', 'presented', 'simulation', 'test', 'run', 'real', 'equipment', 'wind', 'generation', 'load', 'profile', 'market', 'price', 'also', 'reported', 'highlight', 'control', 'platform', 'paper', 'develops', 'integration', 'management', 'platform', 'hess', 'paired', 'real', 'wind', 'farm', 'control', 'algorithm', 'aimed', 'scheduling', 'hydrogen', 'production', 'basis', 'several', 'forecast', 'stream', 'condition', 'different', 'possible', 'operating', 'mode', 'control', 'algorithm', 'address', 'three', 'use', 'case', 'identified', 'final', 'report', 'task', 'integration', 'wind', 'energy', 'grid', 'namely', 'electricity', 'storage', 'hess', 'operated', 'order', 'enable', 'wind', 'farm', 'power', 'ii', 'wind', 'farm', 'hess', 'form', 'local', 'load', 'small', 'hess', 'therefore', 'operated', 'order', 'fulfill', 'without', 'grid', 'support', 'case', 'electricity', 'market', 'also', 'handled', 'iii', 'fuel', 'production', 'hess', 'operated', 'order', 'fulfill', 'hydrogen', 'demand', 'due', 'fuel', 'cell', 'vehicle', 'addition', 'specific', 'objective', 'use', 'case', 'developed', 'control', 'algorithm', 'also', 'optimize', 'hess', 'operating', 'cost', 'typically', 'address', 'two', 'behavior', 'handle', 'corresponding', 'long', 'short', 'term', 'dynamic', 'management', 'platform', 'hess', 'three', 'layer', 'physical', 'control', 'supervision', 'layer', 'located', 'physical', 'layer', 'target', 'physical', 'sensor', 'actuator', 'automation', 'layer', 'includes', 'local', 'controller', 'module', 'used', 'measurement', 'several', 'server', 'interaction', 'higher', 'lower', 'layer', 'control', 'database', 'supervision', 'layer', 'execution', 'control', 'algorithm', 'client', 'remote', 'diagnosis', 'monitoring', 'activity', 'located', 'since', 'layer', 'performs', 'specific', 'functionality', 'implemented', 'communication', 'among', 'layer', 'occur']"
"Resource-Aware Dynamic Service Deployment for Local IoT Edge Computing: Healthcare Use Case Edge Computing is a novel computing paradigm moving server resources closer to end-devices. In the context of IoT, Edge Computing is a centric technology for enabling reliable, context-aware and low-latency services for several application areas such as smart healthcare, smart industry and smart cities. In our previous work, we have proposed a three-tier IoT Edge architecture and a virtual decentralized service platform based on lightweight microservices, called nanoservices, running on it. Together, these proposals form a basis for virtualizing the available local computational capacity and utilizing it to provide localized resource-efficient IoT services based on the applications’ need. Furthermore, locally-deployed functions are resilient to access network problems and can limit the propagation of sensitive user data for improved privacy. In this paper, we propose an automatic service and resource discovery mechanism for efficient on-the-fly deployment of nanoservices on local IoT nodes. As use case, we have selected a healthcare remote monitoring scenario, which requires high service reliability and availability in a highly dynamic environment. Based on the selected use case, we propose a real-world prototype implementation of the proposed mechanism on Raspberry Pi platform. We evaluate the performance and resource-efficiency of the proposed resource matching function with two alternative deployment approaches: containerized and non-containerized deployment. The results show that the containerized deployment is more resource-efficient, while the resource discovery and matching process takes approximately 6–17 seconds, where containerization adds only 1–1.5 seconds. This can be considered a feasible price for streamlined service management, scalability, resource-efficiency and fault-tolerance.",Resource-Aware Dynamic Service Deployment for Local IoT Edge Computing: Healthcare Use Case,"Edge Computing is a novel computing paradigm moving server resources closer to end-devices. In the context of IoT, Edge Computing is a centric technology for enabling reliable, context-aware and low-latency services for several application areas such as smart healthcare, smart industry and smart cities. In our previous work, we have proposed a three-tier IoT Edge architecture and a virtual decentralized service platform based on lightweight microservices, called nanoservices, running on it. Together, these proposals form a basis for virtualizing the available local computational capacity and utilizing it to provide localized resource-efficient IoT services based on the applications’ need. Furthermore, locally-deployed functions are resilient to access network problems and can limit the propagation of sensitive user data for improved privacy. In this paper, we propose an automatic service and resource discovery mechanism for efficient on-the-fly deployment of nanoservices on local IoT nodes. As use case, we have selected a healthcare remote monitoring scenario, which requires high service reliability and availability in a highly dynamic environment. Based on the selected use case, we propose a real-world prototype implementation of the proposed mechanism on Raspberry Pi platform. We evaluate the performance and resource-efficiency of the proposed resource matching function with two alternative deployment approaches: containerized and non-containerized deployment. The results show that the containerized deployment is more resource-efficient, while the resource discovery and matching process takes approximately 6–17 seconds, where containerization adds only 1–1.5 seconds. This can be considered a feasible price for streamlined service management, scalability, resource-efficiency and fault-tolerance.",IEEE journal,no,"['dynamic', 'deployment', 'local', 'iot', 'edge', 'computing', 'healthcare', 'use', 'case', 'edge', 'computing', 'novel', 'computing', 'paradigm', 'moving', 'server', 'resource', 'context', 'iot', 'edge', 'computing', 'centric', 'technology', 'enabling', 'reliable', 'several', 'area', 'smart', 'healthcare', 'smart', 'industry', 'smart', 'city', 'previous', 'work', 'proposed', 'iot', 'edge', 'virtual', 'decentralized', 'platform', 'based', 'lightweight', 'called', 'running', 'together', 'proposal', 'form', 'basis', 'available', 'local', 'computational', 'capacity', 'utilizing', 'provide', 'iot', 'based', 'need', 'furthermore', 'function', 'resilient', 'access', 'network', 'problem', 'limit', 'propagation', 'sensitive', 'user', 'improved', 'privacy', 'paper', 'propose', 'automatic', 'resource', 'discovery', 'mechanism', 'efficient', 'deployment', 'local', 'iot', 'node', 'use', 'case', 'selected', 'healthcare', 'remote', 'monitoring', 'scenario', 'requires', 'high', 'reliability', 'availability', 'highly', 'dynamic', 'environment', 'based', 'selected', 'use', 'case', 'propose', 'prototype', 'implementation', 'proposed', 'mechanism', 'platform', 'evaluate', 'performance', 'proposed', 'resource', 'matching', 'function', 'two', 'alternative', 'deployment', 'containerized', 'deployment', 'result', 'show', 'containerized', 'deployment', 'resource', 'discovery', 'matching', 'process', 'take', 'approximately', 'second', 'containerization', 'add', 'second', 'considered', 'feasible', 'price', 'streamlined', 'management', 'scalability']"
"Fog Computing: Survey of Trends, Architectures, Requirements, and Research Directions Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.","Fog Computing: Survey of Trends, Architectures, Requirements, and Research Directions","Emerging technologies such as the Internet of Things (IoT) require latency-aware computation for real-time application processing. In IoT environments, connected things generate a huge amount of data, which are generally referred to as big data. Data generated from IoT devices are generally processed in a cloud infrastructure because of the on-demand services and scalability features of the cloud computing paradigm. However, processing IoT application requests on the cloud exclusively is not an efficient solution for some IoT applications, especially time-sensitive ones. To address this issue, Fog computing, which resides in between cloud and IoT devices, was proposed. In general, in the Fog computing environment, IoT devices are connected to Fog devices. These Fog devices are located in close proximity to users and are responsible for intermediate computation and storage. One of the key challenges in running IoT applications in a Fog computing environment are resource allocation and task scheduling. Fog computing research is still in its infancy, and taxonomy-based investigation into the requirements of Fog infrastructure, platform, and applications mapped to current research is still required. This survey will help the industry and research community synthesize and identify the requirements for Fog computing. This paper starts with an overview of Fog computing in which the definition of Fog computing, research trends, and the technical differences between Fog and cloud are reviewed. Then, we investigate numerous proposed Fog computing architectures and describe the components of these architectures in detail. From this, the role of each component will be defined, which will help in the deployment of Fog computing. Next, a taxonomy of Fog computing is proposed by considering the requirements of the Fog computing paradigm. We also discuss existing research works and gaps in resource allocation and scheduling, fault tolerance, simulation tools, and Fog-based microservices. Finally, by addressing the limitations of current research works, we present some open issues, which will determine the future research direction for the Fog computing paradigm.",IEEE journal,no,"['fog', 'computing', 'survey', 'trend', 'requirement', 'research', 'direction', 'emerging', 'technology', 'internet', 'thing', 'iot', 'require', 'computation', 'processing', 'iot', 'environment', 'connected', 'thing', 'generate', 'huge', 'amount', 'generally', 'referred', 'big', 'generated', 'iot', 'device', 'generally', 'processed', 'infrastructure', 'scalability', 'feature', 'computing', 'paradigm', 'however', 'processing', 'iot', 'request', 'efficient', 'solution', 'iot', 'especially', 'one', 'address', 'issue', 'fog', 'computing', 'iot', 'device', 'proposed', 'general', 'fog', 'computing', 'environment', 'iot', 'device', 'connected', 'fog', 'device', 'fog', 'device', 'located', 'close', 'proximity', 'user', 'responsible', 'intermediate', 'computation', 'storage', 'one', 'key', 'challenge', 'running', 'iot', 'fog', 'computing', 'environment', 'resource', 'allocation', 'task', 'scheduling', 'fog', 'computing', 'research', 'still', 'investigation', 'requirement', 'fog', 'infrastructure', 'platform', 'mapped', 'current', 'research', 'still', 'required', 'survey', 'help', 'industry', 'research', 'community', 'identify', 'requirement', 'fog', 'computing', 'paper', 'start', 'overview', 'fog', 'computing', 'definition', 'fog', 'computing', 'research', 'trend', 'technical', 'difference', 'fog', 'investigate', 'numerous', 'proposed', 'fog', 'computing', 'describe', 'detail', 'role', 'defined', 'help', 'deployment', 'fog', 'computing', 'next', 'taxonomy', 'fog', 'computing', 'proposed', 'considering', 'requirement', 'fog', 'computing', 'paradigm', 'also', 'discus', 'existing', 'research', 'work', 'gap', 'resource', 'allocation', 'scheduling', 'fault', 'tolerance', 'simulation', 'tool', 'finally', 'addressing', 'limitation', 'current', 'research', 'work', 'present', 'open', 'issue', 'determine', 'future', 'research', 'direction', 'fog', 'computing', 'paradigm']"
"Refining Microservices Placement Employing Workload Profiling Over Multiple Kubernetes Clusters As cloud-native computing is becoming the de-facto paradigm in the cloud field, Microservices Architecture has attracted attention from industries and researchers for agility and efficiency. Moreover, with the popularity of the IoT in the context of edge computing, cloud-native applications that utilize geographically-distributed multiple resources are emerging. In line with this trend, there is an increasing demand for microservices placement that selectively use optimal resources. However, optimal microservices placement is a significant challenge because microservices are dynamic and complex, depending on diversified workloads. Besides, generalizing workloads' characteristics consisting of complex microservices is realistically challenging. Thus, microservices deployment with mathematically structured algorithms based on simulation is less practical. As an alternative, a microservices placement framework is required that can reflect the characteristics of workloads derived from empirical profiling. Therefore, in this research work, we propose a refinement framework for profiling-based microservices placement to identify and respond to workload characteristics in a practical way. To achieve this goal, we perform profiling experiments with selected workloads to derive delicate resource requirements. Then, we perform microservices placement with a greedy-based heuristic algorithm that considers application performance by using resource requirements derived from the profiled results. Finally, we verify the proposed concept by comparing the experimental results that use our work and those that don't.",Refining Microservices Placement Employing Workload Profiling Over Multiple Kubernetes Clusters,"As cloud-native computing is becoming the de-facto paradigm in the cloud field, Microservices Architecture has attracted attention from industries and researchers for agility and efficiency. Moreover, with the popularity of the IoT in the context of edge computing, cloud-native applications that utilize geographically-distributed multiple resources are emerging. In line with this trend, there is an increasing demand for microservices placement that selectively use optimal resources. However, optimal microservices placement is a significant challenge because microservices are dynamic and complex, depending on diversified workloads. Besides, generalizing workloads' characteristics consisting of complex microservices is realistically challenging. Thus, microservices deployment with mathematically structured algorithms based on simulation is less practical. As an alternative, a microservices placement framework is required that can reflect the characteristics of workloads derived from empirical profiling. Therefore, in this research work, we propose a refinement framework for profiling-based microservices placement to identify and respond to workload characteristics in a practical way. To achieve this goal, we perform profiling experiments with selected workloads to derive delicate resource requirements. Then, we perform microservices placement with a greedy-based heuristic algorithm that considers application performance by using resource requirements derived from the profiled results. Finally, we verify the proposed concept by comparing the experimental results that use our work and those that don't.",IEEE journal,no,"['placement', 'employing', 'workload', 'profiling', 'multiple', 'kubernetes', 'cluster', 'computing', 'becoming', 'paradigm', 'field', 'attracted', 'attention', 'industry', 'researcher', 'agility', 'efficiency', 'moreover', 'popularity', 'iot', 'context', 'edge', 'computing', 'utilize', 'multiple', 'resource', 'emerging', 'line', 'trend', 'increasing', 'demand', 'placement', 'selectively', 'use', 'optimal', 'resource', 'however', 'optimal', 'placement', 'significant', 'challenge', 'dynamic', 'complex', 'depending', 'workload', 'besides', 'workload', 'characteristic', 'consisting', 'complex', 'challenging', 'thus', 'deployment', 'structured', 'algorithm', 'based', 'simulation', 'less', 'practical', 'alternative', 'placement', 'framework', 'required', 'characteristic', 'workload', 'derived', 'empirical', 'profiling', 'therefore', 'research', 'work', 'propose', 'refinement', 'framework', 'placement', 'identify', 'respond', 'workload', 'characteristic', 'practical', 'way', 'achieve', 'goal', 'perform', 'profiling', 'experiment', 'selected', 'workload', 'derive', 'resource', 'requirement', 'perform', 'placement', 'heuristic', 'algorithm', 'considers', 'performance', 'using', 'resource', 'requirement', 'derived', 'result', 'finally', 'verify', 'proposed', 'concept', 'comparing', 'experimental', 'result', 'use', 'work']"
"Privacy-Preserving Microservices in Industrial Internet-of-Things-Driven Smart Applications Machine learning (ML) algorithms can effectively perform analytics and inferences for building smart applications, such as early detection of diseases in the Industrial Internet of Things (IIoT) and smart healthcare systems. The main components of ML, including training and testing phases, can be decomposed into microservices to improve service quality, along with fast implementation and integration with the edge and cloud services. However, the execution of ML in an edge-cloud environment introduces privacy risks to data owners (e.g., patients). In this article, we present a privacy-preserving ML framework by leveraging microservice technology for safeguarding healthcare IIoT systems. More specifically, we develop a microservice-based distributed privacy-preserving technique using differential privacy (DP) and a radial basis function network (RBFN) to balance between privacy protection and model performance in edge networks. We conduct extensive experiments to evaluate the performance of the proposed technique. The results revealed that DP has a significant influence on the model’s performance and achieves more than 90% accuracy with an epsilon value over 0.4, enhancing data protection and analytics through the implementation of microservices.",Privacy-Preserving Microservices in Industrial Internet-of-Things-Driven Smart Applications,"Machine learning (ML) algorithms can effectively perform analytics and inferences for building smart applications, such as early detection of diseases in the Industrial Internet of Things (IIoT) and smart healthcare systems. The main components of ML, including training and testing phases, can be decomposed into microservices to improve service quality, along with fast implementation and integration with the edge and cloud services. However, the execution of ML in an edge-cloud environment introduces privacy risks to data owners (e.g., patients). In this article, we present a privacy-preserving ML framework by leveraging microservice technology for safeguarding healthcare IIoT systems. More specifically, we develop a microservice-based distributed privacy-preserving technique using differential privacy (DP) and a radial basis function network (RBFN) to balance between privacy protection and model performance in edge networks. We conduct extensive experiments to evaluate the performance of the proposed technique. The results revealed that DP has a significant influence on the model’s performance and achieves more than 90% accuracy with an epsilon value over 0.4, enhancing data protection and analytics through the implementation of microservices.",IEEE journal,no,"['industrial', 'smart', 'machine', 'learning', 'ml', 'algorithm', 'effectively', 'perform', 'analytics', 'inference', 'building', 'smart', 'early', 'detection', 'disease', 'industrial', 'internet', 'thing', 'iiot', 'smart', 'healthcare', 'main', 'ml', 'including', 'training', 'testing', 'phase', 'decomposed', 'improve', 'quality', 'along', 'fast', 'implementation', 'integration', 'edge', 'however', 'execution', 'ml', 'environment', 'introduces', 'privacy', 'risk', 'patient', 'article', 'present', 'ml', 'framework', 'leveraging', 'technology', 'healthcare', 'iiot', 'specifically', 'develop', 'distributed', 'technique', 'using', 'privacy', 'basis', 'function', 'network', 'balance', 'privacy', 'protection', 'model', 'performance', 'edge', 'network', 'conduct', 'extensive', 'experiment', 'evaluate', 'performance', 'proposed', 'technique', 'result', 'revealed', 'significant', 'influence', 'model', 'performance', 'achieves', 'accuracy', 'value', 'enhancing', 'protection', 'analytics', 'implementation']"
"DRPC: Distributed Reinforcement Learning Approach for Scalable Resource Provisioning in Container-Based Clusters Microservices have transformed monolithic applications into lightweight, self-contained, and isolated application components, establishing themselves as a dominant paradigm for application development and deployment in public clouds such as Google and Alibaba. Autoscaling emerges as an efficient strategy for managing resources allocated to microservices’ replicas. However, the dynamic and intricate dependencies within microservice chains present challenges to the effective management of scaled microservices. Additionally, the centralized autoscaling approach can encounter scalability issues, especially in the management of large-scale microservice-based clusters. To address these challenges and enhance scalability, we propose an innovative distributed resource provisioning approach for microservices based on the Twin Delayed Deep Deterministic Policy Gradient algorithm. This approach enables effective autoscaling decisions and decentralizes responsibilities from a central node to distributed nodes. Comparative results with state-of-the-art approaches, obtained from a realistic testbed and traces, indicate that our approach reduces the average response time by 15% and the number of failed requests by 24%, validating improved scalability as the number of requests increases.",DRPC: Distributed Reinforcement Learning Approach for Scalable Resource Provisioning in Container-Based Clusters,"Microservices have transformed monolithic applications into lightweight, self-contained, and isolated application components, establishing themselves as a dominant paradigm for application development and deployment in public clouds such as Google and Alibaba. Autoscaling emerges as an efficient strategy for managing resources allocated to microservices’ replicas. However, the dynamic and intricate dependencies within microservice chains present challenges to the effective management of scaled microservices. Additionally, the centralized autoscaling approach can encounter scalability issues, especially in the management of large-scale microservice-based clusters. To address these challenges and enhance scalability, we propose an innovative distributed resource provisioning approach for microservices based on the Twin Delayed Deep Deterministic Policy Gradient algorithm. This approach enables effective autoscaling decisions and decentralizes responsibilities from a central node to distributed nodes. Comparative results with state-of-the-art approaches, obtained from a realistic testbed and traces, indicate that our approach reduces the average response time by 15% and the number of failed requests by 24%, validating improved scalability as the number of requests increases.",IEEE journal,no,"['distributed', 'reinforcement', 'learning', 'scalable', 'resource', 'provisioning', 'cluster', 'transformed', 'monolithic', 'lightweight', 'isolated', 'establishing', 'dominant', 'paradigm', 'development', 'deployment', 'public', 'google', 'alibaba', 'autoscaling', 'emerges', 'efficient', 'strategy', 'managing', 'resource', 'allocated', 'however', 'dynamic', 'intricate', 'dependency', 'within', 'chain', 'present', 'challenge', 'effective', 'management', 'scaled', 'additionally', 'centralized', 'autoscaling', 'scalability', 'issue', 'especially', 'management', 'cluster', 'address', 'challenge', 'enhance', 'scalability', 'propose', 'innovative', 'distributed', 'resource', 'provisioning', 'based', 'twin', 'deep', 'policy', 'gradient', 'algorithm', 'enables', 'effective', 'autoscaling', 'decision', 'responsibility', 'central', 'node', 'distributed', 'node', 'comparative', 'result', 'obtained', 'realistic', 'testbed', 'trace', 'indicate', 'reduces', 'average', 'response', 'time', 'number', 'request', 'improved', 'scalability', 'number', 'request', 'increase']"
"Multi-Site Resource Allocation in a QoS-Aware 5G Infrastructure Network softwarization has paved the way for 5G technologies, and a wide-range of (radically new) verticals. As the telecommunications infrastructure evolves into a sort of distributed datacenter, multiple tenants such as vertical industries and network service providers share its aggregate pool of resources (e.g., networking, computing, etc.) in a layered “as-a-Service” approach exposed as slice abstractions. The challenge remains in the coordination of various stakeholders’ assets in realizing end-to-end network slices and supporting the multi-site deployment and chaining of the micro-service components needed to implement cloud-native vertical applications (vApps). In this context, particular care must be taken to ensure that the required resources are identified, made available and managed in a way that satisfies the vApp requirements, allows for a fair share of resources and has a reasonable impact on the overall vApp deployment time. With these challenges in mind, this paper presents the Resource Selection Optimizer (RSO)– a software-service in the MATILDA Operations Support System (OSS), whose main goal is to select the most appropriate network and computing resources (according to some criterion) among a list of options provided by the Wide-area Infrastructure Manager (WIM). It consists of three submodules that respectively handle: (i) the aggregation of vApp components based on affinities, (ii) the forecasting of (micro-) datacenter resources utilization, (iii) and the multi-site placement of the (aggregated) vApp micro-service components. The RSO’s performance is mainly evaluated in terms of the execution times of its submodules while varying their respective input parameters, and additionally, three selection policies are also compared. Experimental results aim to highlight the RSO behavior in both execution times and deployment costs, as well as the RSO interactions with other OSS submodules and network platform components, not only for multi-site vApp deployment but also for other network/services management operations.",Multi-Site Resource Allocation in a QoS-Aware 5G Infrastructure,"Network softwarization has paved the way for 5G technologies, and a wide-range of (radically new) verticals. As the telecommunications infrastructure evolves into a sort of distributed datacenter, multiple tenants such as vertical industries and network service providers share its aggregate pool of resources (e.g., networking, computing, etc.) in a layered “as-a-Service” approach exposed as slice abstractions. The challenge remains in the coordination of various stakeholders’ assets in realizing end-to-end network slices and supporting the multi-site deployment and chaining of the micro-service components needed to implement cloud-native vertical applications (vApps). In this context, particular care must be taken to ensure that the required resources are identified, made available and managed in a way that satisfies the vApp requirements, allows for a fair share of resources and has a reasonable impact on the overall vApp deployment time. With these challenges in mind, this paper presents the Resource Selection Optimizer (RSO)– a software-service in the MATILDA Operations Support System (OSS), whose main goal is to select the most appropriate network and computing resources (according to some criterion) among a list of options provided by the Wide-area Infrastructure Manager (WIM). It consists of three submodules that respectively handle: (i) the aggregation of vApp components based on affinities, (ii) the forecasting of (micro-) datacenter resources utilization, (iii) and the multi-site placement of the (aggregated) vApp micro-service components. The RSO’s performance is mainly evaluated in terms of the execution times of its submodules while varying their respective input parameters, and additionally, three selection policies are also compared. Experimental results aim to highlight the RSO behavior in both execution times and deployment costs, as well as the RSO interactions with other OSS submodules and network platform components, not only for multi-site vApp deployment but also for other network/services management operations.",IEEE journal,no,"['resource', 'allocation', 'infrastructure', 'network', 'way', 'technology', 'new', 'vertical', 'telecommunication', 'infrastructure', 'evolves', 'sort', 'distributed', 'datacenter', 'multiple', 'tenant', 'vertical', 'industry', 'network', 'provider', 'share', 'aggregate', 'pool', 'resource', 'networking', 'computing', 'etc', 'layered', 'exposed', 'slice', 'abstraction', 'challenge', 'remains', 'coordination', 'various', 'stakeholder', 'asset', 'realizing', 'network', 'slice', 'supporting', 'deployment', 'needed', 'implement', 'vertical', 'context', 'particular', 'care', 'must', 'taken', 'ensure', 'required', 'resource', 'identified', 'made', 'available', 'managed', 'way', 'satisfies', 'vapp', 'requirement', 'allows', 'fair', 'share', 'resource', 'reasonable', 'impact', 'overall', 'vapp', 'deployment', 'time', 'challenge', 'mind', 'paper', 'present', 'resource', 'selection', 'rso', 'operation', 'support', 'os', 'whose', 'main', 'goal', 'select', 'appropriate', 'network', 'computing', 'resource', 'according', 'criterion', 'among', 'list', 'option', 'provided', 'infrastructure', 'manager', 'consists', 'three', 'submodules', 'respectively', 'handle', 'aggregation', 'vapp', 'based', 'affinity', 'ii', 'forecasting', 'datacenter', 'resource', 'utilization', 'iii', 'placement', 'aggregated', 'vapp', 'rso', 'performance', 'mainly', 'evaluated', 'term', 'execution', 'time', 'submodules', 'varying', 'respective', 'input', 'parameter', 'additionally', 'three', 'selection', 'policy', 'also', 'compared', 'experimental', 'result', 'aim', 'highlight', 'rso', 'behavior', 'execution', 'time', 'deployment', 'cost', 'well', 'rso', 'interaction', 'os', 'submodules', 'network', 'platform', 'vapp', 'deployment', 'also', 'management', 'operation']"
"Delay-Aware Container Scheduling in Kubernetes Kubernetes is a powerful tool to manage containerized applications, which is also regarded as one promising platform to support microservices in edge computing. The scheduler is a key component of Kubernetes. It allocates each pod (i.e., a set of running containers) to one worker node (i.e., a machine). The default scheduler in Kubernetes is designed for the cloud environment containing homogeneous nodes. However, IoT edge nodes usually have various computing power and network bandwidth. This article proposes a delay-aware container scheduling (DACS) algorithm to address the issue of node heterogeneity in edge computing. To efficiently assign pods to worker nodes, DACS takes account of not only residual resources of worker nodes but also potential delays caused by the pod assignment. We build a Kubernetes cluster by VMware to evaluate system performance. Experimental results reveal that DACS can significantly reduce both processing and network delays, thereby helping Kubernetes perform more efficiently in an edge environment.",Delay-Aware Container Scheduling in Kubernetes,"Kubernetes is a powerful tool to manage containerized applications, which is also regarded as one promising platform to support microservices in edge computing. The scheduler is a key component of Kubernetes. It allocates each pod (i.e., a set of running containers) to one worker node (i.e., a machine). The default scheduler in Kubernetes is designed for the cloud environment containing homogeneous nodes. However, IoT edge nodes usually have various computing power and network bandwidth. This article proposes a delay-aware container scheduling (DACS) algorithm to address the issue of node heterogeneity in edge computing. To efficiently assign pods to worker nodes, DACS takes account of not only residual resources of worker nodes but also potential delays caused by the pod assignment. We build a Kubernetes cluster by VMware to evaluate system performance. Experimental results reveal that DACS can significantly reduce both processing and network delays, thereby helping Kubernetes perform more efficiently in an edge environment.",IEEE journal,no,"['container', 'scheduling', 'kubernetes', 'kubernetes', 'powerful', 'tool', 'manage', 'containerized', 'also', 'one', 'promising', 'platform', 'support', 'edge', 'computing', 'scheduler', 'key', 'kubernetes', 'pod', 'set', 'running', 'container', 'one', 'worker', 'node', 'machine', 'default', 'scheduler', 'kubernetes', 'designed', 'environment', 'containing', 'homogeneous', 'node', 'however', 'iot', 'edge', 'node', 'usually', 'various', 'computing', 'power', 'network', 'bandwidth', 'article', 'proposes', 'container', 'scheduling', 'dacs', 'algorithm', 'address', 'issue', 'node', 'heterogeneity', 'edge', 'computing', 'efficiently', 'pod', 'worker', 'node', 'dacs', 'take', 'account', 'resource', 'worker', 'node', 'also', 'potential', 'delay', 'caused', 'pod', 'assignment', 'build', 'kubernetes', 'cluster', 'evaluate', 'performance', 'experimental', 'result', 'reveal', 'dacs', 'significantly', 'reduce', 'processing', 'network', 'delay', 'thereby', 'helping', 'kubernetes', 'perform', 'efficiently', 'edge', 'environment']"
"Operating Latency Sensitive Applications on Public Serverless Edge Cloud Platforms Cloud native programming and serverless architectures provide a novel way of software development and operation. A new generation of applications can be realized with features never seen before while the burden on developers and operators will be reduced significantly. However, latency sensitive applications, such as various distributed IoT services, generally do not fit in well with the new concepts and today's platforms. In this article, we adapt the cloud native approach and related operating techniques for latency sensitive IoT applications operated on public serverless platforms. We argue that solely adding cloud resources to the edge is not enough and other mechanisms and operation layers are required to achieve the desired level of quality. Our contribution is threefold. First, we propose a novel system on top of a public serverless edge cloud platform, which can dynamically optimize and deploy the microservice-based software layout based on live performance measurements. We add two control loops and the corresponding mechanisms which are responsible for the online reoptimization at different timescales. The first one addresses the steady-state operation, while the second one provides fast latency control by directly reconfiguring the serverless runtime environments. Second, we apply our general concepts to one of today's most widely used and versatile public cloud platforms, namely, Amazon's AWS, and its edge extension for IoT applications, called Greengrass. Third, we characterize the main operation phases and evaluate the overall performance of the system. We analyze the performance characteristics of the two control loops and investigate different implementation options.",Operating Latency Sensitive Applications on Public Serverless Edge Cloud Platforms,"Cloud native programming and serverless architectures provide a novel way of software development and operation. A new generation of applications can be realized with features never seen before while the burden on developers and operators will be reduced significantly. However, latency sensitive applications, such as various distributed IoT services, generally do not fit in well with the new concepts and today's platforms. In this article, we adapt the cloud native approach and related operating techniques for latency sensitive IoT applications operated on public serverless platforms. We argue that solely adding cloud resources to the edge is not enough and other mechanisms and operation layers are required to achieve the desired level of quality. Our contribution is threefold. First, we propose a novel system on top of a public serverless edge cloud platform, which can dynamically optimize and deploy the microservice-based software layout based on live performance measurements. We add two control loops and the corresponding mechanisms which are responsible for the online reoptimization at different timescales. The first one addresses the steady-state operation, while the second one provides fast latency control by directly reconfiguring the serverless runtime environments. Second, we apply our general concepts to one of today's most widely used and versatile public cloud platforms, namely, Amazon's AWS, and its edge extension for IoT applications, called Greengrass. Third, we characterize the main operation phases and evaluate the overall performance of the system. We analyze the performance characteristics of the two control loops and investigate different implementation options.",IEEE journal,no,"['operating', 'latency', 'sensitive', 'public', 'serverless', 'edge', 'platform', 'native', 'programming', 'serverless', 'provide', 'novel', 'way', 'development', 'operation', 'new', 'generation', 'realized', 'feature', 'seen', 'burden', 'developer', 'operator', 'reduced', 'significantly', 'however', 'latency', 'sensitive', 'various', 'distributed', 'iot', 'generally', 'fit', 'well', 'new', 'concept', 'today', 'platform', 'article', 'adapt', 'native', 'related', 'operating', 'technique', 'latency', 'sensitive', 'iot', 'operated', 'public', 'serverless', 'platform', 'argue', 'solely', 'adding', 'resource', 'edge', 'enough', 'mechanism', 'operation', 'layer', 'required', 'achieve', 'desired', 'level', 'quality', 'contribution', 'first', 'propose', 'novel', 'top', 'public', 'serverless', 'edge', 'platform', 'dynamically', 'optimize', 'deploy', 'based', 'live', 'performance', 'measurement', 'add', 'two', 'control', 'loop', 'corresponding', 'mechanism', 'responsible', 'online', 'different', 'first', 'one', 'address', 'operation', 'second', 'one', 'provides', 'fast', 'latency', 'control', 'directly', 'serverless', 'runtime', 'environment', 'second', 'apply', 'general', 'concept', 'one', 'today', 'widely', 'used', 'versatile', 'public', 'platform', 'namely', 'amazon', 'aws', 'edge', 'extension', 'iot', 'called', 'third', 'characterize', 'main', 'operation', 'phase', 'evaluate', 'overall', 'performance', 'analyze', 'performance', 'characteristic', 'two', 'control', 'loop', 'investigate', 'different', 'implementation', 'option']"
"A Micro-Service Approach to Cloud Native RAN for 5G and Beyond 5G aims to support diverse applications with programmable infrastructure. Traditional RAN based on purpose-built-in hardware and monolithic software lack resiliency, programmability, and business agility. Cloud native virtualized RAN (vRAN) solves the issues by designing telecom applications into micro-service in cloud environment. This enables flexible virtualized network function deployment, efficient service provisioning, and on-demand resource usage. However, designing telecom RAN applications as micro-services has no guidelines but faces challenges such as high-precision synchronization and real-time processing requirements. This paper first introduces cloud native tenets and the current state of RAN cloudification and then evaluates the micro-service design of RAN software components. A micro-service approach for vDU user plane was proposed in this study, and a vDU prototype was developed based on the Intel x86 computing platform. System validation results proved the feasibility of inter-Pod communication processing latency, and capacity analytics predicted substantial capacity improvement for the proposed vDU scheme. The paper concludes with a summary and open points on the way forward for cloud native vRAN transformation.",A Micro-Service Approach to Cloud Native RAN for 5G and Beyond,"5G aims to support diverse applications with programmable infrastructure. Traditional RAN based on purpose-built-in hardware and monolithic software lack resiliency, programmability, and business agility. Cloud native virtualized RAN (vRAN) solves the issues by designing telecom applications into micro-service in cloud environment. This enables flexible virtualized network function deployment, efficient service provisioning, and on-demand resource usage. However, designing telecom RAN applications as micro-services has no guidelines but faces challenges such as high-precision synchronization and real-time processing requirements. This paper first introduces cloud native tenets and the current state of RAN cloudification and then evaluates the micro-service design of RAN software components. A micro-service approach for vDU user plane was proposed in this study, and a vDU prototype was developed based on the Intel x86 computing platform. System validation results proved the feasibility of inter-Pod communication processing latency, and capacity analytics predicted substantial capacity improvement for the proposed vDU scheme. The paper concludes with a summary and open points on the way forward for cloud native vRAN transformation.",IEEE journal,no,"['native', 'ran', 'beyond', 'aim', 'support', 'diverse', 'programmable', 'infrastructure', 'traditional', 'ran', 'based', 'hardware', 'monolithic', 'lack', 'resiliency', 'business', 'agility', 'native', 'virtualized', 'ran', 'solves', 'issue', 'designing', 'telecom', 'environment', 'enables', 'flexible', 'virtualized', 'network', 'function', 'deployment', 'efficient', 'provisioning', 'resource', 'usage', 'however', 'designing', 'telecom', 'ran', 'guideline', 'face', 'challenge', 'synchronization', 'processing', 'requirement', 'paper', 'first', 'introduces', 'native', 'tenet', 'current', 'state', 'ran', 'design', 'ran', 'vdu', 'user', 'plane', 'proposed', 'study', 'vdu', 'prototype', 'developed', 'based', 'intel', 'computing', 'platform', 'validation', 'result', 'proved', 'feasibility', 'communication', 'processing', 'latency', 'capacity', 'analytics', 'predicted', 'substantial', 'capacity', 'improvement', 'proposed', 'vdu', 'scheme', 'paper', 'open', 'point', 'way', 'forward', 'native', 'transformation']"
"Architectural Design of Cloud Applications: A Performance-Aware Cost Minimization Approach Cloud Computing has assumed a relevant role in the ICT, profoundly influencing the life-cycle of modern applications in the manner they are designed, developed, and deployed and operated. In this article, we tackle the problem of supporting the design-time analysis of Cloud applications to identify a cost-optimized strategy for allocating components onto Cloud Virtual Machine infrastructural services, taking performance requirements into account. We present an approach and a tool, SPACE4Cloud, that supports users in modeling the architecture of an application, in defining performance requirements as well as deployment constraints, and then in mapping each architecture component into a corresponding VM service, minimizing total costs. An optimization algorithm supports the mapping and determines the Cloud configuration that minimizes the execution costs of the application over a daily time horizon. The benefits of this approach are demonstrated in the context of an industrial case study. Furthermore, we show that SPACE4Cloud leads to a cost reduction up to 60 percent, when compared to a first-principle technique based on utilization thresholds, like the ones typically used in practice, and that our solution is able to solve large problem instances within a time frame compatible with a fast-paced design process (less than half an hour in the worst case). Finally, we show that SPACE4Cloud is suitable to model even microservice-based applications and to compute the corresponding optimized deployment configuration which is compared with a state-of-the art meta-heuristic alternative method, achieving savings between 21 and 85 percent.",Architectural Design of Cloud Applications: A Performance-Aware Cost Minimization Approach,"Cloud Computing has assumed a relevant role in the ICT, profoundly influencing the life-cycle of modern applications in the manner they are designed, developed, and deployed and operated. In this article, we tackle the problem of supporting the design-time analysis of Cloud applications to identify a cost-optimized strategy for allocating components onto Cloud Virtual Machine infrastructural services, taking performance requirements into account. We present an approach and a tool, SPACE4Cloud, that supports users in modeling the architecture of an application, in defining performance requirements as well as deployment constraints, and then in mapping each architecture component into a corresponding VM service, minimizing total costs. An optimization algorithm supports the mapping and determines the Cloud configuration that minimizes the execution costs of the application over a daily time horizon. The benefits of this approach are demonstrated in the context of an industrial case study. Furthermore, we show that SPACE4Cloud leads to a cost reduction up to 60 percent, when compared to a first-principle technique based on utilization thresholds, like the ones typically used in practice, and that our solution is able to solve large problem instances within a time frame compatible with a fast-paced design process (less than half an hour in the worst case). Finally, we show that SPACE4Cloud is suitable to model even microservice-based applications and to compute the corresponding optimized deployment configuration which is compared with a state-of-the art meta-heuristic alternative method, achieving savings between 21 and 85 percent.",IEEE journal,no,"['architectural', 'design', 'cost', 'minimization', 'computing', 'relevant', 'role', 'ict', 'modern', 'manner', 'designed', 'developed', 'deployed', 'operated', 'article', 'tackle', 'problem', 'supporting', 'analysis', 'identify', 'strategy', 'onto', 'virtual', 'machine', 'taking', 'performance', 'requirement', 'account', 'present', 'tool', 'support', 'user', 'modeling', 'defining', 'performance', 'requirement', 'well', 'deployment', 'constraint', 'mapping', 'corresponding', 'vm', 'minimizing', 'total', 'cost', 'optimization', 'algorithm', 'support', 'mapping', 'configuration', 'minimizes', 'execution', 'cost', 'daily', 'time', 'benefit', 'demonstrated', 'context', 'industrial', 'case', 'study', 'furthermore', 'show', 'lead', 'cost', 'reduction', 'compared', 'technique', 'based', 'utilization', 'threshold', 'like', 'one', 'typically', 'used', 'practice', 'solution', 'able', 'solve', 'large', 'problem', 'instance', 'within', 'time', 'frame', 'compatible', 'design', 'process', 'less', 'hour', 'case', 'finally', 'show', 'suitable', 'model', 'even', 'compute', 'corresponding', 'optimized', 'deployment', 'configuration', 'compared', 'art', 'alternative', 'method', 'achieving', 'saving']"
"An NFV-Based Service Framework for IoT Applications in Edge Computing Environments Emerging Internet of Things (IoT) applications share the same characteristics of involving multiple processing components (i.e., function modules) and requiring a massive amount of data to be processed with low latency. To meet these needs, edge/fog computing has been proposed for next-generation mobile networks to migrate the computing from the cloud to the edge of the network. Thanks to the development of Network Functions Virtualization (NFV), with which edge computing platform can virtualize function modules and deploy them on any edge devices to provide flexible services on the edge networks. However, such platform would need to deal with complicated function module calling relationship (i.e., call graph) of applications and user mobility, and both are not thoroughly considered by existing works of NFV and edge computing. In this paper, based on our previous idea of virtual local-hub (VLH), we propose a complete design of edge computing framework, which applies NFV technology on edge computing environment for IoT applications. To handle the complicated call graphs of IoT applications with better resource utilization, the VLH framework adapts the technologies of container-based virtualization and microservice architecture, which enables remote function module sharing on the edge computing environment. The framework includes the heuristic algorithm for function module allocation with the objective of minimizing total bandwidth consumption. We also present a design of protocols for system operations and mobility handling in the framework. Then we implement the framework on commodity hardware as a testbed. Via simulations under a large-scale environment with practical settings and experiments on the testbed under real-world scenarios, we demonstrate and verify the effectiveness and practicability of the proposed VLH framework for IoT application service provision.",An NFV-Based Service Framework for IoT Applications in Edge Computing Environments,"Emerging Internet of Things (IoT) applications share the same characteristics of involving multiple processing components (i.e., function modules) and requiring a massive amount of data to be processed with low latency. To meet these needs, edge/fog computing has been proposed for next-generation mobile networks to migrate the computing from the cloud to the edge of the network. Thanks to the development of Network Functions Virtualization (NFV), with which edge computing platform can virtualize function modules and deploy them on any edge devices to provide flexible services on the edge networks. However, such platform would need to deal with complicated function module calling relationship (i.e., call graph) of applications and user mobility, and both are not thoroughly considered by existing works of NFV and edge computing. In this paper, based on our previous idea of virtual local-hub (VLH), we propose a complete design of edge computing framework, which applies NFV technology on edge computing environment for IoT applications. To handle the complicated call graphs of IoT applications with better resource utilization, the VLH framework adapts the technologies of container-based virtualization and microservice architecture, which enables remote function module sharing on the edge computing environment. The framework includes the heuristic algorithm for function module allocation with the objective of minimizing total bandwidth consumption. We also present a design of protocols for system operations and mobility handling in the framework. Then we implement the framework on commodity hardware as a testbed. Via simulations under a large-scale environment with practical settings and experiments on the testbed under real-world scenarios, we demonstrate and verify the effectiveness and practicability of the proposed VLH framework for IoT application service provision.",IEEE journal,no,"['framework', 'iot', 'edge', 'computing', 'environment', 'emerging', 'internet', 'thing', 'iot', 'share', 'characteristic', 'involving', 'multiple', 'processing', 'function', 'module', 'requiring', 'massive', 'amount', 'processed', 'low', 'latency', 'meet', 'need', 'computing', 'proposed', 'mobile', 'network', 'migrate', 'computing', 'edge', 'network', 'thanks', 'development', 'network', 'function', 'virtualization', 'nfv', 'edge', 'computing', 'platform', 'function', 'module', 'deploy', 'edge', 'device', 'provide', 'flexible', 'edge', 'network', 'however', 'platform', 'would', 'need', 'deal', 'complicated', 'function', 'module', 'calling', 'relationship', 'call', 'graph', 'user', 'mobility', 'considered', 'existing', 'work', 'nfv', 'edge', 'computing', 'paper', 'based', 'previous', 'idea', 'virtual', 'vlh', 'propose', 'complete', 'design', 'edge', 'computing', 'framework', 'applies', 'nfv', 'technology', 'edge', 'computing', 'environment', 'iot', 'handle', 'complicated', 'call', 'graph', 'iot', 'better', 'resource', 'utilization', 'vlh', 'framework', 'technology', 'virtualization', 'enables', 'remote', 'function', 'module', 'sharing', 'edge', 'computing', 'environment', 'framework', 'includes', 'heuristic', 'algorithm', 'function', 'module', 'allocation', 'objective', 'minimizing', 'total', 'bandwidth', 'consumption', 'also', 'present', 'design', 'protocol', 'operation', 'mobility', 'handling', 'framework', 'implement', 'framework', 'commodity', 'hardware', 'testbed', 'via', 'simulation', 'environment', 'practical', 'setting', 'experiment', 'testbed', 'scenario', 'demonstrate', 'verify', 'effectiveness', 'proposed', 'vlh', 'framework', 'iot', 'provision']"
"Toward Bio-Inspired Auto-Scaling Algorithms: An Elasticity Approach for Container Orchestration Platforms The wide adoption of microservices architectures has introduced an unprecedented granularisation of computing that requires the coordinated execution of multiple containers with diverse lifetimes and with potentially different auto-scaling requirements. These applications are managed by means of container orchestration platforms and existing centralised approaches for auto-scaling face challenges when used for the timely adaptation of the elasticity required for the different application components. This paper studies the impact of integrating bio-inspired approaches for dynamic distributed auto-scaling on container orchestration platforms. With a focus on running self-managed containers, we compare alternative configuration options for the container life cycle. The performance of the proposed models is validated through simulations subjected to both synthetic and real-world workloads. Also, multiple scaling options are assessed with the purpose of identifying exceptional cases and improvement areas. Furthermore, a nontraditional metric for scaling measurement is introduced to substitute classic analytical approaches. We found out connections for two related worlds (biological systems and software container elasticity procedures) and we open a new research area in software containers that features potential self-guided container elasticity activities.",Toward Bio-Inspired Auto-Scaling Algorithms: An Elasticity Approach for Container Orchestration Platforms,"The wide adoption of microservices architectures has introduced an unprecedented granularisation of computing that requires the coordinated execution of multiple containers with diverse lifetimes and with potentially different auto-scaling requirements. These applications are managed by means of container orchestration platforms and existing centralised approaches for auto-scaling face challenges when used for the timely adaptation of the elasticity required for the different application components. This paper studies the impact of integrating bio-inspired approaches for dynamic distributed auto-scaling on container orchestration platforms. With a focus on running self-managed containers, we compare alternative configuration options for the container life cycle. The performance of the proposed models is validated through simulations subjected to both synthetic and real-world workloads. Also, multiple scaling options are assessed with the purpose of identifying exceptional cases and improvement areas. Furthermore, a nontraditional metric for scaling measurement is introduced to substitute classic analytical approaches. We found out connections for two related worlds (biological systems and software container elasticity procedures) and we open a new research area in software containers that features potential self-guided container elasticity activities.",IEEE journal,no,"['toward', 'algorithm', 'elasticity', 'container', 'orchestration', 'platform', 'wide', 'adoption', 'introduced', 'unprecedented', 'computing', 'requires', 'coordinated', 'execution', 'multiple', 'container', 'diverse', 'lifetime', 'potentially', 'different', 'requirement', 'managed', 'mean', 'container', 'orchestration', 'platform', 'existing', 'face', 'challenge', 'used', 'timely', 'adaptation', 'elasticity', 'required', 'different', 'paper', 'study', 'impact', 'integrating', 'dynamic', 'distributed', 'container', 'orchestration', 'platform', 'focus', 'running', 'container', 'compare', 'alternative', 'configuration', 'option', 'container', 'life', 'cycle', 'performance', 'proposed', 'model', 'validated', 'simulation', 'synthetic', 'workload', 'also', 'multiple', 'scaling', 'option', 'assessed', 'purpose', 'identifying', 'case', 'improvement', 'area', 'furthermore', 'metric', 'scaling', 'measurement', 'introduced', 'classic', 'analytical', 'found', 'connection', 'two', 'related', 'world', 'biological', 'container', 'elasticity', 'procedure', 'open', 'new', 'research', 'area', 'container', 'feature', 'potential', 'container', 'elasticity', 'activity']"
"Enabling Emulation and Evaluation of IEC 61850 Networks With TITAN Sensing and monitoring electrical signals and power device parameters within the smart grid network infrastructure plays a fundamental role in assessing the smart devices' proper functioning. Nevertheless, a key challenge for academic teaching and researching purposes is the elevated cost of real electronic devices, such as current and potential transformers, or even intelligent electronic devices. Therefore, traffic emulators are a valuable solution for the evaluation of new smart grid communication proposals. In this work, we propose TITAN, a tool to support the evaluation of automation systems' communication networks. TITAN enables the emulation of IEC 61850 communication, ranging from data sensing to data acquisition, thus supporting extensive research and development on this fundamental smart grid domain. This tool can interact and communicate with real elements, such as intelligent electronic devices. It enables the emulation of voltage and current data sensing communication, as well as the implementation of different data acquisition schemes by an emulated supervisory system. Our main contributions are: (i) TITAN, a tool with a distributed microservices architecture to execute communication traffic generation tasks; (ii) a user-friendly interface to integrate and manage all components; and (iii) a proof of concept testbed using TITAN and real teleprotection devices. Real case studies using TITAN reveal its feasibility in supporting low-cost testbeds for research, teaching, and testing purposes in sensing and acquisition for automation systems.",Enabling Emulation and Evaluation of IEC 61850 Networks With TITAN,"Sensing and monitoring electrical signals and power device parameters within the smart grid network infrastructure plays a fundamental role in assessing the smart devices' proper functioning. Nevertheless, a key challenge for academic teaching and researching purposes is the elevated cost of real electronic devices, such as current and potential transformers, or even intelligent electronic devices. Therefore, traffic emulators are a valuable solution for the evaluation of new smart grid communication proposals. In this work, we propose TITAN, a tool to support the evaluation of automation systems' communication networks. TITAN enables the emulation of IEC 61850 communication, ranging from data sensing to data acquisition, thus supporting extensive research and development on this fundamental smart grid domain. This tool can interact and communicate with real elements, such as intelligent electronic devices. It enables the emulation of voltage and current data sensing communication, as well as the implementation of different data acquisition schemes by an emulated supervisory system. Our main contributions are: (i) TITAN, a tool with a distributed microservices architecture to execute communication traffic generation tasks; (ii) a user-friendly interface to integrate and manage all components; and (iii) a proof of concept testbed using TITAN and real teleprotection devices. Real case studies using TITAN reveal its feasibility in supporting low-cost testbeds for research, teaching, and testing purposes in sensing and acquisition for automation systems.",IEEE journal,no,"['enabling', 'emulation', 'evaluation', 'iec', 'network', 'titan', 'sensing', 'monitoring', 'signal', 'power', 'device', 'parameter', 'within', 'smart', 'grid', 'network', 'infrastructure', 'play', 'fundamental', 'role', 'assessing', 'smart', 'device', 'proper', 'functioning', 'nevertheless', 'key', 'challenge', 'academic', 'teaching', 'purpose', 'cost', 'real', 'electronic', 'device', 'current', 'potential', 'even', 'intelligent', 'electronic', 'device', 'therefore', 'traffic', 'valuable', 'solution', 'evaluation', 'new', 'smart', 'grid', 'communication', 'proposal', 'work', 'propose', 'titan', 'tool', 'support', 'evaluation', 'automation', 'communication', 'network', 'titan', 'enables', 'emulation', 'iec', 'communication', 'sensing', 'acquisition', 'thus', 'supporting', 'extensive', 'research', 'development', 'fundamental', 'smart', 'grid', 'domain', 'tool', 'interact', 'communicate', 'real', 'element', 'intelligent', 'electronic', 'device', 'enables', 'emulation', 'voltage', 'current', 'sensing', 'communication', 'well', 'implementation', 'different', 'acquisition', 'scheme', 'main', 'contribution', 'titan', 'tool', 'distributed', 'execute', 'communication', 'traffic', 'generation', 'task', 'ii', 'interface', 'integrate', 'manage', 'iii', 'proof', 'concept', 'testbed', 'using', 'titan', 'real', 'device', 'real', 'case', 'study', 'using', 'titan', 'reveal', 'feasibility', 'supporting', 'research', 'teaching', 'testing', 'purpose', 'sensing', 'acquisition', 'automation']"
"Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model’s effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability.",Model-Driven Dependability and Power Consumption Quantification of Kubernetes-Based Cloud-Fog Continuum,"System dependability is pivotal for the reliable execution of designated computing functions. With the emergence of cloud-fog computing and microservices architectures, new challenges and opportunities arise in evaluating system dependability. Enhancing dependability in microservices often involves component replication, potentially increasing energy costs. Thus, discerning optimal redundancy strategies and understanding their energy implications is crucial for both cost efficiency and ecological sustainability. This paper presents a model-driven approach to evaluate the dependability and energy consumption of cloud-fog systems, utilizing Kubernetes, a container application orchestration platform. The developed model considers various determinants affecting system dependability, including hardware and software reliability, resource accessibility, and support personnel availability. Empirical studies validate the model’s effectiveness, demonstrating a 22.33% increase in system availability with only a 1.33% rise in energy consumption. Moreover, this methodology provides a structured framework for understanding cloud-fog system dependability, serves as a reference for comparing dependability across different systems, and aids in resource allocation optimization. This research significantly contributes to the efforts to enhance cloud-fog system dependability.",IEEE journal,no,"['dependability', 'power', 'consumption', 'continuum', 'dependability', 'pivotal', 'reliable', 'execution', 'computing', 'function', 'emergence', 'computing', 'new', 'challenge', 'opportunity', 'arise', 'evaluating', 'dependability', 'enhancing', 'dependability', 'often', 'involves', 'replication', 'potentially', 'increasing', 'energy', 'cost', 'thus', 'optimal', 'redundancy', 'strategy', 'understanding', 'energy', 'implication', 'crucial', 'cost', 'efficiency', 'sustainability', 'paper', 'present', 'evaluate', 'dependability', 'energy', 'consumption', 'utilizing', 'kubernetes', 'container', 'orchestration', 'platform', 'developed', 'model', 'considers', 'various', 'affecting', 'dependability', 'including', 'hardware', 'reliability', 'resource', 'accessibility', 'support', 'personnel', 'availability', 'empirical', 'study', 'validate', 'model', 'effectiveness', 'demonstrating', 'increase', 'availability', 'rise', 'energy', 'consumption', 'moreover', 'methodology', 'provides', 'structured', 'framework', 'understanding', 'dependability', 'serf', 'reference', 'comparing', 'dependability', 'across', 'different', 'aid', 'resource', 'allocation', 'optimization', 'research', 'significantly', 'contributes', 'effort', 'enhance', 'dependability']"
"Bootstrapping Service Mesh Implementations with Istio: Build reliable, scalable, and secure microservices on Kubernetes with Service Mesh A step-by-step guide to Istio Service Mesh implementation, with examples of complex and distributed workloads built using microservices architecture and deployed in Kubernetes Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn the design, implementation, and troubleshooting of Istio in a clear and concise formatGrasp concepts, ideas, and solutions that can be readily applied in real work environmentsSee Istio in action through examples that cover Terraform, GitOps, AWS, Kubernetes, and GoBook DescriptionIstio is a game-changer in managing connectivity and operational efficiency of microservices, but implementing and using it in applications can be challenging. This book will help you overcome these challenges and gain insights into Istio's features and functionality layer by layer with the help of easy-to-follow examples. It will let you focus on implementing and deploying Istio on the cloud and in production environments instead of dealing with the complexity of demo apps.  You'll learn the installation, architecture, and components of Istio Service Mesh, perform multi-cluster installation, and integrate legacy workloads deployed on virtual machines. As you advance, you'll understand how to secure microservices from threats, perform multi-cluster deployments on Kubernetes, use load balancing, monitor application traffic, implement service discovery and management, and much more. You’ll also explore other Service Mesh technologies such as Linkerd, Consul, Kuma, and Gloo Mesh. In addition to observing and operating Istio using Kiali, Prometheus, Grafana and Jaeger, you'll perform zero-trust security and reliable communication between distributed applications. After reading this book, you'll be equipped with the practical knowledge and skills needed to use and operate Istio effectively.What you will learnGet an overview of Service Mesh and the problems it solvesBecome well-versed with the fundamentals of Istio, its architecture, installation, and deploymentExtend the Istio data plane using WebAssembly (Wasm) and learn why Envoy is used as a data planeUnderstand how to use OPA Gatekeeper to automate Istio’s best practicesManage communication between microservices using IstioExplore different ways to secure the communication between microservicesGet insights into traffic flow in the Service MeshLearn best practices to deploy and operate Istio in production environmentsWho this book is forThe book is for DevOps engineers, SREs, cloud and software developers, sysadmins, and architects who have been using microservices in Kubernetes-based environments. It addresses challenges in application networking during microservice communications. Working experience on Kubernetes, along with knowledge of DevOps, application networking, security, and programming languages like Golang, will assist with understanding the concepts covered.","Bootstrapping Service Mesh Implementations with Istio: Build reliable, scalable, and secure microservices on Kubernetes with Service Mesh","A step-by-step guide to Istio Service Mesh implementation, with examples of complex and distributed workloads built using microservices architecture and deployed in Kubernetes Purchase of the print or Kindle book includes a free PDF eBookKey FeaturesLearn the design, implementation, and troubleshooting of Istio in a clear and concise formatGrasp concepts, ideas, and solutions that can be readily applied in real work environmentsSee Istio in action through examples that cover Terraform, GitOps, AWS, Kubernetes, and GoBook DescriptionIstio is a game-changer in managing connectivity and operational efficiency of microservices, but implementing and using it in applications can be challenging. This book will help you overcome these challenges and gain insights into Istio's features and functionality layer by layer with the help of easy-to-follow examples. It will let you focus on implementing and deploying Istio on the cloud and in production environments instead of dealing with the complexity of demo apps.  You'll learn the installation, architecture, and components of Istio Service Mesh, perform multi-cluster installation, and integrate legacy workloads deployed on virtual machines. As you advance, you'll understand how to secure microservices from threats, perform multi-cluster deployments on Kubernetes, use load balancing, monitor application traffic, implement service discovery and management, and much more. You’ll also explore other Service Mesh technologies such as Linkerd, Consul, Kuma, and Gloo Mesh. In addition to observing and operating Istio using Kiali, Prometheus, Grafana and Jaeger, you'll perform zero-trust security and reliable communication between distributed applications. After reading this book, you'll be equipped with the practical knowledge and skills needed to use and operate Istio effectively.What you will learnGet an overview of Service Mesh and the problems it solvesBecome well-versed with the fundamentals of Istio, its architecture, installation, and deploymentExtend the Istio data plane using WebAssembly (Wasm) and learn why Envoy is used as a data planeUnderstand how to use OPA Gatekeeper to automate Istio’s best practicesManage communication between microservices using IstioExplore different ways to secure the communication between microservicesGet insights into traffic flow in the Service MeshLearn best practices to deploy and operate Istio in production environmentsWho this book is forThe book is for DevOps engineers, SREs, cloud and software developers, sysadmins, and architects who have been using microservices in Kubernetes-based environments. It addresses challenges in application networking during microservice communications. Working experience on Kubernetes, along with knowledge of DevOps, application networking, security, and programming languages like Golang, will assist with understanding the concepts covered.",IEEE book,no,"['mesh', 'implementation', 'istio', 'build', 'reliable', 'scalable', 'secure', 'kubernetes', 'mesh', 'guide', 'istio', 'mesh', 'implementation', 'example', 'complex', 'distributed', 'workload', 'built', 'using', 'deployed', 'kubernetes', 'book', 'includes', 'free', 'featureslearn', 'design', 'implementation', 'istio', 'clear', 'concept', 'idea', 'solution', 'applied', 'real', 'work', 'istio', 'action', 'example', 'cover', 'terraform', 'gitops', 'aws', 'kubernetes', 'managing', 'connectivity', 'operational', 'efficiency', 'implementing', 'using', 'challenging', 'book', 'help', 'overcome', 'challenge', 'gain', 'insight', 'istio', 'feature', 'functionality', 'layer', 'layer', 'help', 'example', 'let', 'focus', 'implementing', 'deploying', 'istio', 'production', 'environment', 'instead', 'dealing', 'complexity', 'demo', 'apps', 'learn', 'installation', 'istio', 'mesh', 'perform', 'installation', 'integrate', 'legacy', 'workload', 'deployed', 'virtual', 'machine', 'advance', 'understand', 'secure', 'threat', 'perform', 'deployment', 'kubernetes', 'use', 'load', 'balancing', 'monitor', 'traffic', 'implement', 'discovery', 'management', 'much', 'also', 'explore', 'mesh', 'technology', 'mesh', 'addition', 'operating', 'istio', 'using', 'prometheus', 'grafana', 'jaeger', 'perform', 'security', 'reliable', 'communication', 'distributed', 'reading', 'book', 'equipped', 'practical', 'knowledge', 'skill', 'needed', 'use', 'operate', 'istio', 'overview', 'mesh', 'problem', 'fundamental', 'istio', 'installation', 'istio', 'plane', 'using', 'learn', 'used', 'use', 'automate', 'istio', 'best', 'communication', 'using', 'different', 'way', 'secure', 'communication', 'insight', 'traffic', 'flow', 'best', 'practice', 'deploy', 'operate', 'istio', 'production', 'book', 'book', 'devops', 'engineer', 'developer', 'architect', 'using', 'environment', 'address', 'challenge', 'networking', 'communication', 'working', 'experience', 'kubernetes', 'along', 'knowledge', 'devops', 'networking', 'security', 'programming', 'language', 'like', 'golang', 'assist', 'understanding', 'concept', 'covered']"
"Architectural Considerations for the Cloud Environment This chapter discusses the open issues and considerations in planning an environment that serve the best interest of one's internal and external customers. Cloud providers may provide the underlying physical and virtualized resources needed to run various cloud services. They also may create the actual applications and business services that operate in these environments. Clearly, all the components in the cloud provider model must be managed. There have to be services to support the business, manage configurations, and provision the right resources on demand. Management services must also support interoperability and service portability. Placing microservices into containers supported by orchestration services and well‐defined application programming interface is transforming the way businesses gain control over performance in complex computing environments. Making policies and rules operational in a hybrid cloud environment means that these dictates must be integrated from an architectural perspective.",Architectural Considerations for the Cloud Environment,"This chapter discusses the open issues and considerations in planning an environment that serve the best interest of one's internal and external customers. Cloud providers may provide the underlying physical and virtualized resources needed to run various cloud services. They also may create the actual applications and business services that operate in these environments. Clearly, all the components in the cloud provider model must be managed. There have to be services to support the business, manage configurations, and provision the right resources on demand. Management services must also support interoperability and service portability. Placing microservices into containers supported by orchestration services and well‐defined application programming interface is transforming the way businesses gain control over performance in complex computing environments. Making policies and rules operational in a hybrid cloud environment means that these dictates must be integrated from an architectural perspective.",IEEE book,no,"['architectural', 'consideration', 'environment', 'chapter', 'discusses', 'open', 'issue', 'consideration', 'planning', 'environment', 'serve', 'best', 'interest', 'one', 'internal', 'external', 'customer', 'provider', 'may', 'provide', 'underlying', 'physical', 'virtualized', 'resource', 'needed', 'run', 'various', 'also', 'may', 'create', 'actual', 'business', 'operate', 'environment', 'clearly', 'provider', 'model', 'must', 'managed', 'support', 'business', 'manage', 'configuration', 'provision', 'right', 'resource', 'demand', 'management', 'must', 'also', 'support', 'interoperability', 'portability', 'placing', 'container', 'supported', 'orchestration', 'programming', 'interface', 'transforming', 'way', 'business', 'gain', 'control', 'performance', 'complex', 'computing', 'environment', 'making', 'policy', 'rule', 'operational', 'hybrid', 'environment', 'mean', 'must', 'integrated', 'architectural', 'perspective']"
"Designing Production-Grade and Large-Scale IoT Solutions: A comprehensive and practical guide to implementing end-to-end IoT solutions Get to grips with key IoT aspects along with modern trends, architectures, and technologies that support IoT solutions, such as cloud computing, modern app architecture paradigms, and data analyticsKey FeaturesUnderstand the big picture of designing production-grade IoT solutions from an industry expertGet up and running with the development and designing aspects of the Internet of ThingsSolve business problems specific to your domain using different IoT platforms and technologiesBook DescriptionWith the rising demand for and recent enhancements in IoT, a developer with sound knowledge of IoT is the need of the hour. This book will help you design, build, and operate large-scale E2E IoT solutions to transform your business and products, increase revenue, and reduce operational costs. Starting with an overview of how IoT technologies can help you solve your business problems, this book will be a useful guide to helping you implement end-to-end IoT solution architecture. You'll learn to select IoT devices; real-time operating systems; IoT Edge covering Edge location, software, and hardware; and the best IoT connectivity for your IoT solution. As you progress, you'll work with IoT device management, IoT data analytics, IoT platforms, and put these components to work as part of your IoT solution. You'll also be able to build IoT backend cloud from scratch by leveraging the modern app architecture paradigms and cloud-native technologies such as containers and microservices. Finally, you'll discover best practices for different operational excellence pillars, including high availability, resiliency, reliability, security, cost optimization, and high performance, which should be applied for large-scale production-grade IoT solutions. By the end of this IoT book, you'll be confident in designing, building, and operating IoT solutions.What you will learnUnderstand the detailed anatomy of IoT solutions and explore their building blocksExplore IoT connectivity options and protocols used in designing IoT solutionsUnderstand the value of IoT platforms in building IoT solutionsExplore real-time operating systems used in microcontrollersAutomate device administration tasks with IoT device managementMaster different architecture paradigms and decisions in IoT solutionsBuild and gain insights from IoT analytics solutionsGet an overview of IoT solution operational excellence pillarsWho this book is forThis book is for E2E solution architects, systems and technical architects, and IoT developers looking to design, build, and operate E2E IoT applications and solutions. Basic knowledge of cloud computing, software engineering, and distributed system design will help you get the most out of this book.",Designing Production-Grade and Large-Scale IoT Solutions: A comprehensive and practical guide to implementing end-to-end IoT solutions,"Get to grips with key IoT aspects along with modern trends, architectures, and technologies that support IoT solutions, such as cloud computing, modern app architecture paradigms, and data analyticsKey FeaturesUnderstand the big picture of designing production-grade IoT solutions from an industry expertGet up and running with the development and designing aspects of the Internet of ThingsSolve business problems specific to your domain using different IoT platforms and technologiesBook DescriptionWith the rising demand for and recent enhancements in IoT, a developer with sound knowledge of IoT is the need of the hour. This book will help you design, build, and operate large-scale E2E IoT solutions to transform your business and products, increase revenue, and reduce operational costs. Starting with an overview of how IoT technologies can help you solve your business problems, this book will be a useful guide to helping you implement end-to-end IoT solution architecture. You'll learn to select IoT devices; real-time operating systems; IoT Edge covering Edge location, software, and hardware; and the best IoT connectivity for your IoT solution. As you progress, you'll work with IoT device management, IoT data analytics, IoT platforms, and put these components to work as part of your IoT solution. You'll also be able to build IoT backend cloud from scratch by leveraging the modern app architecture paradigms and cloud-native technologies such as containers and microservices. Finally, you'll discover best practices for different operational excellence pillars, including high availability, resiliency, reliability, security, cost optimization, and high performance, which should be applied for large-scale production-grade IoT solutions. By the end of this IoT book, you'll be confident in designing, building, and operating IoT solutions.What you will learnUnderstand the detailed anatomy of IoT solutions and explore their building blocksExplore IoT connectivity options and protocols used in designing IoT solutionsUnderstand the value of IoT platforms in building IoT solutionsExplore real-time operating systems used in microcontrollersAutomate device administration tasks with IoT device managementMaster different architecture paradigms and decisions in IoT solutionsBuild and gain insights from IoT analytics solutionsGet an overview of IoT solution operational excellence pillarsWho this book is forThis book is for E2E solution architects, systems and technical architects, and IoT developers looking to design, build, and operate E2E IoT applications and solutions. Basic knowledge of cloud computing, software engineering, and distributed system design will help you get the most out of this book.",IEEE book,no,"['designing', 'iot', 'solution', 'comprehensive', 'practical', 'guide', 'implementing', 'iot', 'solution', 'get', 'grip', 'key', 'iot', 'aspect', 'along', 'modern', 'trend', 'technology', 'support', 'iot', 'solution', 'computing', 'modern', 'app', 'paradigm', 'big', 'designing', 'iot', 'solution', 'industry', 'running', 'development', 'designing', 'aspect', 'internet', 'business', 'problem', 'specific', 'domain', 'using', 'different', 'iot', 'platform', 'demand', 'recent', 'enhancement', 'iot', 'developer', 'knowledge', 'iot', 'need', 'hour', 'book', 'help', 'design', 'build', 'operate', 'iot', 'solution', 'transform', 'business', 'product', 'increase', 'reduce', 'operational', 'cost', 'starting', 'overview', 'iot', 'technology', 'help', 'solve', 'business', 'problem', 'book', 'useful', 'guide', 'helping', 'implement', 'iot', 'solution', 'learn', 'select', 'iot', 'device', 'operating', 'iot', 'edge', 'covering', 'edge', 'location', 'hardware', 'best', 'iot', 'connectivity', 'iot', 'solution', 'progress', 'work', 'iot', 'device', 'management', 'iot', 'analytics', 'iot', 'platform', 'put', 'work', 'part', 'iot', 'solution', 'also', 'able', 'build', 'iot', 'backend', 'scratch', 'leveraging', 'modern', 'app', 'paradigm', 'technology', 'container', 'finally', 'discover', 'best', 'practice', 'different', 'operational', 'including', 'high', 'availability', 'resiliency', 'reliability', 'security', 'cost', 'optimization', 'high', 'performance', 'applied', 'iot', 'solution', 'end', 'iot', 'book', 'designing', 'building', 'operating', 'iot', 'learnunderstand', 'detailed', 'iot', 'solution', 'explore', 'building', 'iot', 'connectivity', 'option', 'protocol', 'used', 'designing', 'iot', 'value', 'iot', 'platform', 'building', 'iot', 'operating', 'used', 'device', 'administration', 'task', 'iot', 'device', 'different', 'paradigm', 'decision', 'iot', 'gain', 'insight', 'iot', 'analytics', 'overview', 'iot', 'solution', 'operational', 'book', 'forthis', 'book', 'solution', 'architect', 'technical', 'architect', 'iot', 'developer', 'looking', 'design', 'build', 'operate', 'iot', 'solution', 'basic', 'knowledge', 'computing', 'engineering', 'distributed', 'design', 'help', 'get', 'book']"
"Software‐Defined Fog Orchestration for IoT Services This chapter presents a scalable software‐defined orchestration architecture to intelligently compose and orchestrate thousands of heterogeneous Fog appliances (devices, servers). Specifically, it provides a resource filtering‐based resource assignment mechanism to optimize the resource utilization and fair resource sharing among multitenant Internet of things (IoT) applications. The chapter also presents a component selection and placement mechanism for containerized IoT microservices to minimize the latency by harnessing the network uncertainty and security while considering different applications’ requirement and capabilities. It describes a fog simulation scheme to simulate the aforementioned procedure by modeling the entities, their attributes, and actions. The chapter also provides the results of practical experiences on the orchestration and simulation. It outlines numerous difficulties and challenges to develop an orchestration framework across all layers within the Fog resource stack and describes a prototype orchestration system that makes use of some of the most promising mechanisms to tackle these challenges.",Software‐Defined Fog Orchestration for IoT Services,"This chapter presents a scalable software‐defined orchestration architecture to intelligently compose and orchestrate thousands of heterogeneous Fog appliances (devices, servers). Specifically, it provides a resource filtering‐based resource assignment mechanism to optimize the resource utilization and fair resource sharing among multitenant Internet of things (IoT) applications. The chapter also presents a component selection and placement mechanism for containerized IoT microservices to minimize the latency by harnessing the network uncertainty and security while considering different applications’ requirement and capabilities. It describes a fog simulation scheme to simulate the aforementioned procedure by modeling the entities, their attributes, and actions. The chapter also provides the results of practical experiences on the orchestration and simulation. It outlines numerous difficulties and challenges to develop an orchestration framework across all layers within the Fog resource stack and describes a prototype orchestration system that makes use of some of the most promising mechanisms to tackle these challenges.",IEEE book,no,"['fog', 'orchestration', 'iot', 'chapter', 'present', 'scalable', 'orchestration', 'intelligently', 'compose', 'orchestrate', 'thousand', 'heterogeneous', 'fog', 'device', 'server', 'specifically', 'provides', 'resource', 'resource', 'assignment', 'mechanism', 'optimize', 'resource', 'utilization', 'fair', 'resource', 'sharing', 'among', 'internet', 'thing', 'iot', 'chapter', 'also', 'present', 'selection', 'placement', 'mechanism', 'containerized', 'iot', 'minimize', 'latency', 'harnessing', 'network', 'uncertainty', 'security', 'considering', 'different', 'requirement', 'capability', 'describes', 'fog', 'simulation', 'scheme', 'simulate', 'aforementioned', 'procedure', 'modeling', 'entity', 'attribute', 'action', 'chapter', 'also', 'provides', 'result', 'practical', 'experience', 'orchestration', 'simulation', 'outline', 'numerous', 'difficulty', 'challenge', 'develop', 'orchestration', 'framework', 'across', 'layer', 'within', 'fog', 'resource', 'stack', 'describes', 'prototype', 'orchestration', 'make', 'use', 'promising', 'mechanism', 'tackle', 'challenge']"
"Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes,"The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",IEEE conference,no,"['istio', 'api', 'gateway', 'impact', 'reduce', 'latency', 'resource', 'usage', 'kubernetes', 'rapid', 'development', 'technology', 'caused', 'massive', 'demand', 'technology', 'infrastructure', 'play', 'vital', 'role', 'performance', 'unreliable', 'infrastructure', 'performance', 'resulting', 'poor', 'customer', 'experience', 'one', 'core', 'enables', 'accessibility', 'proxy', 'server', 'route', 'incoming', 'traffic', 'executing', 'business', 'logic', 'proxy', 'server', 'run', 'kubernetes', 'environment', 'abstraction', 'operating', 'virtualization', 'gateway', 'running', 'kubernetes', 'environment', 'gateway', 'server', 'excellent', 'performance', 'reliability', 'efficient', 'resource', 'utilization', 'required', 'selection', 'proxy', 'server', 'gateway', 'backend', 'significantly', 'impact', 'performance', 'user', 'perspective', 'experiment', 'conducted', 'research', 'test', 'backend', 'accessed', 'two', 'different', 'proxy', 'server', 'kubernetes', 'environment', 'research', 'compare', 'response', 'time', 'hardware', 'resource', 'utilization', 'proxy', 'server', 'common', 'mesh', 'paper', 'proposed', 'modern', 'evaluate', 'performance', 'common', 'paradigm', 'enterprise', 'high', 'request', 'rate', 'serve', 'client', 'lowest', 'latency', 'efficient', 'resource', 'result', 'show', 'istio', 'ingres', 'proxy', 'suitable', 'high', 'request', 'rate', 'nginx', 'ingres', 'proxy', 'performs', 'better', 'serving', 'lower', 'request']"
"Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",Istio API Gateway Impact to Reduce Microservice Latency and Resource Usage on Kubernetes,"The rapid development of technology has caused a massive demand for technology applications. Infrastructure plays a vital role in application performance, and unreliable infrastructure can hinder application performance, resulting in a poor customer experience. One of the core components that enables application accessibility is the proxy server, which routes incoming traffic to the application that executing business logic. The proxy server runs in the Kubernetes environment in the abstraction of operating system virtualization. As the gateway to applications running in the Kubernetes environment, a gateway server with excellent performance, reliability, and efficient resource utilization is required. The selection of the proxy server as the gateway to the backend application significantly impacts application performance from the user’s perspective. The experiment conducted in this research is to test the backend application accessed through two different proxy servers in the Kubernetes environment. This research compares the response time and hardware resource utilization of the proxy server application, representing common microservice architecture and mesh microservice. This paper proposed the modern microservice architecture and evaluate the performance of common microservice paradigm so enterprise application with high request rate can serve client with lowest latency and efficient resource. The results shows that the Istio ingress proxy is suitable for microservice applications with high request rates while nginx ingress proxy performs better when serving lower request.",IEEE conference,yes,"['istio', 'api', 'gateway', 'impact', 'reduce', 'latency', 'resource', 'usage', 'kubernetes', 'rapid', 'development', 'technology', 'caused', 'massive', 'demand', 'technology', 'infrastructure', 'play', 'vital', 'role', 'performance', 'unreliable', 'infrastructure', 'performance', 'resulting', 'poor', 'customer', 'experience', 'one', 'core', 'enables', 'accessibility', 'proxy', 'server', 'route', 'incoming', 'traffic', 'executing', 'business', 'logic', 'proxy', 'server', 'run', 'kubernetes', 'environment', 'abstraction', 'operating', 'virtualization', 'gateway', 'running', 'kubernetes', 'environment', 'gateway', 'server', 'excellent', 'performance', 'reliability', 'efficient', 'resource', 'utilization', 'required', 'selection', 'proxy', 'server', 'gateway', 'backend', 'significantly', 'impact', 'performance', 'user', 'perspective', 'experiment', 'conducted', 'research', 'test', 'backend', 'accessed', 'two', 'different', 'proxy', 'server', 'kubernetes', 'environment', 'research', 'compare', 'response', 'time', 'hardware', 'resource', 'utilization', 'proxy', 'server', 'common', 'mesh', 'paper', 'proposed', 'modern', 'evaluate', 'performance', 'common', 'paradigm', 'enterprise', 'high', 'request', 'rate', 'serve', 'client', 'lowest', 'latency', 'efficient', 'resource', 'result', 'show', 'istio', 'ingres', 'proxy', 'suitable', 'high', 'request', 'rate', 'nginx', 'ingres', 'proxy', 'performs', 'better', 'serving', 'lower', 'request']"
