text,Document Title,Abstract,source,bool,processed_text
"HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",ACM,no,"['hemirca', 'root', 'cause', 'analysis', 'heterogeneous', 'source', 'improves', 'scalability', 'resilience', 'agility', 'also', 'pose', 'significant', 'challenge', 'reliability', 'due', 'complexity', 'dynamic', 'nature', 'identifying', 'anomaly', 'promptly', 'crucial', 'quickly', 'propagate', 'cause', 'severe', 'damage', 'existing', 'metric', 'localization', 'rely', 'metric', 'correlation', 'overlook', 'monitoring', 'source', 'trace', 'first', 'identify', 'leverage', 'monotonic', 'correlation', 'heterogeneous', 'monitoring', 'motivated', 'propose', 'novel', 'framework', 'heterogeneous', 'source', 'root', 'cause', 'analysis', 'hemirca', 'hierarchical', 'root', 'cause', 'analysis', 'using', 'correlation', 'hemirca', 'based', 'key', 'observation', 'responsible', 'particular', 'type', 'fault', 'exhibit', 'monotonic', 'correlation', 'trend', 'associated', 'metric', 'anomaly', 'score', 'hemirca', 'first', 'calculates', 'anomaly', 'score', 'using', 'trace', 'exploit', 'correlation', 'metric', 'score', 'rank', 'suspicious', 'metric', 'hemirca', 'evaluated', 'two', 'datasets', 'collected', 'widely', 'used', 'result', 'show', 'hemirca', 'outperforms', 'large', 'margin', 'identifying', 'root', 'cause', 'level', 'metric', 'level', 'achieving', 'hit', 'ratio', 'average', 'respectively']"
"HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",ACM,yes,"['hemirca', 'root', 'cause', 'analysis', 'heterogeneous', 'source', 'improves', 'scalability', 'resilience', 'agility', 'also', 'pose', 'significant', 'challenge', 'reliability', 'due', 'complexity', 'dynamic', 'nature', 'identifying', 'anomaly', 'promptly', 'crucial', 'quickly', 'propagate', 'cause', 'severe', 'damage', 'existing', 'metric', 'localization', 'rely', 'metric', 'correlation', 'overlook', 'monitoring', 'source', 'trace', 'first', 'identify', 'leverage', 'monotonic', 'correlation', 'heterogeneous', 'monitoring', 'motivated', 'propose', 'novel', 'framework', 'heterogeneous', 'source', 'root', 'cause', 'analysis', 'hemirca', 'hierarchical', 'root', 'cause', 'analysis', 'using', 'correlation', 'hemirca', 'based', 'key', 'observation', 'responsible', 'particular', 'type', 'fault', 'exhibit', 'monotonic', 'correlation', 'trend', 'associated', 'metric', 'anomaly', 'score', 'hemirca', 'first', 'calculates', 'anomaly', 'score', 'using', 'trace', 'exploit', 'correlation', 'metric', 'score', 'rank', 'suspicious', 'metric', 'hemirca', 'evaluated', 'two', 'datasets', 'collected', 'widely', 'used', 'result', 'show', 'hemirca', 'outperforms', 'large', 'margin', 'identifying', 'root', 'cause', 'level', 'metric', 'level', 'achieving', 'hit', 'ratio', 'average', 'respectively']"
"Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features Cloud systems, typically comprised of various components (e.g., microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e., key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.",Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features,"Cloud systems, typically comprised of various components (e.g., microservices), are susceptible to performance issues, which may cause service-level agreement violations and financial losses. Identifying performance issues is thus of paramount importance for cloud vendors. In current practice, crucial metrics, i.e., key performance indicators (KPIs), are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3. The ablation study also proves the effectiveness of the relational-temporal features and the PU-learning strategy. Furthermore, we share the success stories of leveraging ISOLATE to identify performance issues in Huawei Cloud, which demonstrates its superiority in practice.",ACM,no,"['identifying', 'performance', 'issue', 'based', 'feature', 'typically', 'comprised', 'various', 'performance', 'issue', 'may', 'cause', 'agreement', 'violation', 'financial', 'loss', 'identifying', 'performance', 'issue', 'thus', 'paramount', 'importance', 'vendor', 'current', 'practice', 'crucial', 'metric', 'key', 'performance', 'indicator', 'monitored', 'provide', 'insight', 'operational', 'status', 'identifying', 'performance', 'issue', 'often', 'formulated', 'anomaly', 'detection', 'problem', 'analyzing', 'metric', 'independently', 'however', 'overlook', 'complex', 'dependency', 'existing', 'among', 'graph', 'neural', 'method', 'take', 'temporal', 'relational', 'information', 'account', 'however', 'correlation', 'violation', 'metric', 'serve', 'indicator', 'underlying', 'performance', 'issue', 'difficult', 'identify', 'furthermore', 'large', 'volume', 'result', 'vast', 'noisy', 'metric', 'complexity', 'render', 'engineer', 'fully', 'comprehend', 'correlation', 'making', 'challenging', 'identify', 'performance', 'issue', 'accurately', 'address', 'limitation', 'propose', 'identifying', 'performance', 'issue', 'based', 'feature', 'isolate', 'leverage', 'relational', 'temporal', 'feature', 'metric', 'identify', 'performance', 'issue', 'particular', 'adopts', 'graph', 'neural', 'network', 'attention', 'characterizing', 'relation', 'among', 'metric', 'extract', 'temporal', 'pattern', 'using', 'network', 'respectively', 'learned', 'graph', 'attention', 'weight', 'used', 'localize', 'metric', 'moreover', 'impact', 'noisy', 'isolate', 'utilizes', 'positive', 'learning', 'strategy', 'tag', 'based', 'small', 'portion', 'example', 'extensive', 'evaluation', 'public', 'industrial', 'datasets', 'show', 'isolate', 'outperforms', 'baseline', 'model', 'hit', 'rate', 'study', 'also', 'effectiveness', 'feature', 'strategy', 'furthermore', 'share', 'success', 'leveraging', 'isolate', 'identify', 'performance', 'issue', 'demonstrates', 'practice']"
"HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",ACM,no,"['hemirca', 'root', 'cause', 'analysis', 'heterogeneous', 'source', 'improves', 'scalability', 'resilience', 'agility', 'also', 'pose', 'significant', 'challenge', 'reliability', 'due', 'complexity', 'dynamic', 'nature', 'identifying', 'anomaly', 'promptly', 'crucial', 'quickly', 'propagate', 'cause', 'severe', 'damage', 'existing', 'metric', 'localization', 'rely', 'metric', 'correlation', 'overlook', 'monitoring', 'source', 'trace', 'first', 'identify', 'leverage', 'monotonic', 'correlation', 'heterogeneous', 'monitoring', 'motivated', 'propose', 'novel', 'framework', 'heterogeneous', 'source', 'root', 'cause', 'analysis', 'hemirca', 'hierarchical', 'root', 'cause', 'analysis', 'using', 'correlation', 'hemirca', 'based', 'key', 'observation', 'responsible', 'particular', 'type', 'fault', 'exhibit', 'monotonic', 'correlation', 'trend', 'associated', 'metric', 'anomaly', 'score', 'hemirca', 'first', 'calculates', 'anomaly', 'score', 'using', 'trace', 'exploit', 'correlation', 'metric', 'score', 'rank', 'suspicious', 'metric', 'hemirca', 'evaluated', 'two', 'datasets', 'collected', 'widely', 'used', 'result', 'show', 'hemirca', 'outperforms', 'large', 'margin', 'identifying', 'root', 'cause', 'level', 'metric', 'level', 'achieving', 'hit', 'ratio', 'average', 'respectively']"
"HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",HeMiRCA: Fine-Grained Root Cause Analysis for Microservices with Heterogeneous Data Sources,"Microservices architecture improves software scalability, resilience, and agility but also poses significant challenges to system reliability due to their complexity and dynamic nature. Identifying and resolving anomalies promptly is crucial because they can quickly propagate to other microservices and cause severe damage to the system. Existing root-cause metric localization approaches rely on metrics or metrics-anomalies correlations but overlook other monitoring data sources (e.g., traces). We are the first to identify and leverage the anomaly-aware monotonic correlation between heterogeneous monitoring data, motivated by which we propose a novel framework, Heterogeneous data sources in Microservice systems for Root Cause Analysis (HeMiRCA), for hierarchical root cause analysis using Spearman correlation. HeMiRCA is based on the key observation that the microservice responsible for a particular type of fault exhibits a monotonic correlation between the trends of its associated metrics and the trace-based anomaly score of the system. HeMiRCA first calculates time-series anomaly scores using traces and then exploits the correlations between multivariate metrics and the scores to rank the suspicious metrics and microservices. HeMiRCA has been evaluated on two datasets collected from widely used microservice systems. The results show that HeMiRCA outperforms the state-of-the-art approaches by a large margin in identifying root causes at both service level and metric level, achieving a top-1 hit ratio of 82.7\% and 74\% on average, respectively.",ACM,yes,"['hemirca', 'root', 'cause', 'analysis', 'heterogeneous', 'source', 'improves', 'scalability', 'resilience', 'agility', 'also', 'pose', 'significant', 'challenge', 'reliability', 'due', 'complexity', 'dynamic', 'nature', 'identifying', 'anomaly', 'promptly', 'crucial', 'quickly', 'propagate', 'cause', 'severe', 'damage', 'existing', 'metric', 'localization', 'rely', 'metric', 'correlation', 'overlook', 'monitoring', 'source', 'trace', 'first', 'identify', 'leverage', 'monotonic', 'correlation', 'heterogeneous', 'monitoring', 'motivated', 'propose', 'novel', 'framework', 'heterogeneous', 'source', 'root', 'cause', 'analysis', 'hemirca', 'hierarchical', 'root', 'cause', 'analysis', 'using', 'correlation', 'hemirca', 'based', 'key', 'observation', 'responsible', 'particular', 'type', 'fault', 'exhibit', 'monotonic', 'correlation', 'trend', 'associated', 'metric', 'anomaly', 'score', 'hemirca', 'first', 'calculates', 'anomaly', 'score', 'using', 'trace', 'exploit', 'correlation', 'metric', 'score', 'rank', 'suspicious', 'metric', 'hemirca', 'evaluated', 'two', 'datasets', 'collected', 'widely', 'used', 'result', 'show', 'hemirca', 'outperforms', 'large', 'margin', 'identifying', 'root', 'cause', 'level', 'metric', 'level', 'achieving', 'hit', 'ratio', 'average', 'respectively']"
"Intelligent Monitoring of Non-Invasive Network Blocking Faults Based on Cloud-Native Microservices Architecture Conventional non-intrusive network blocking fault intelligent monitoring methods mainly focus on network attack monitoring, and the monitoring content is relatively single, which affects the final monitoring results. Therefore, a non-intrusive network blocking fault intelligent monitoring method based on cloud-native microservice architecture is designed. We generate non-intrusive network blocking fault intelligent monitoring components, and design the tasks of monitoring nodes into components to monitor the blocking condition of non-intrusive network to meet the monitoring accuracy requirements. Based on the cloud-native microservice architecture, we construct a network blocking fault intelligent monitoring model, which consists of three levels: network manager, subnet group leader and agent, and monitors different network blocking fault areas, so as to realize the demand of fault intelligent monitoring. Dynamically configure the intelligent monitoring parameters of network blocking faults, configure the files and database of network blocking faults in the environment of microservice architecture, and form a practical configuration center to further optimize the accuracy of fault monitoring. Comparison experiments are used to verify that the monitoring results of this method are more accurate and can be applied in real life.",Intelligent Monitoring of Non-Invasive Network Blocking Faults Based on Cloud-Native Microservices Architecture,"Conventional non-intrusive network blocking fault intelligent monitoring methods mainly focus on network attack monitoring, and the monitoring content is relatively single, which affects the final monitoring results. Therefore, a non-intrusive network blocking fault intelligent monitoring method based on cloud-native microservice architecture is designed. We generate non-intrusive network blocking fault intelligent monitoring components, and design the tasks of monitoring nodes into components to monitor the blocking condition of non-intrusive network to meet the monitoring accuracy requirements. Based on the cloud-native microservice architecture, we construct a network blocking fault intelligent monitoring model, which consists of three levels: network manager, subnet group leader and agent, and monitors different network blocking fault areas, so as to realize the demand of fault intelligent monitoring. Dynamically configure the intelligent monitoring parameters of network blocking faults, configure the files and database of network blocking faults in the environment of microservice architecture, and form a practical configuration center to further optimize the accuracy of fault monitoring. Comparison experiments are used to verify that the monitoring results of this method are more accurate and can be applied in real life.",IEEE conference,no,"['intelligent', 'monitoring', 'network', 'blocking', 'fault', 'based', 'conventional', 'network', 'blocking', 'fault', 'intelligent', 'monitoring', 'method', 'mainly', 'focus', 'network', 'attack', 'monitoring', 'monitoring', 'content', 'relatively', 'single', 'affect', 'final', 'monitoring', 'result', 'therefore', 'network', 'blocking', 'fault', 'intelligent', 'monitoring', 'method', 'based', 'designed', 'generate', 'network', 'blocking', 'fault', 'intelligent', 'monitoring', 'design', 'task', 'monitoring', 'node', 'monitor', 'blocking', 'condition', 'network', 'meet', 'monitoring', 'accuracy', 'requirement', 'based', 'construct', 'network', 'blocking', 'fault', 'intelligent', 'monitoring', 'model', 'consists', 'three', 'level', 'network', 'manager', 'group', 'agent', 'monitor', 'different', 'network', 'blocking', 'fault', 'area', 'realize', 'demand', 'fault', 'intelligent', 'monitoring', 'dynamically', 'configure', 'intelligent', 'monitoring', 'parameter', 'network', 'blocking', 'fault', 'configure', 'file', 'database', 'network', 'blocking', 'fault', 'environment', 'form', 'practical', 'configuration', 'center', 'optimize', 'accuracy', 'fault', 'monitoring', 'comparison', 'experiment', 'used', 'verify', 'monitoring', 'result', 'method', 'accurate', 'applied', 'real', 'life']"
"A Case For Cross-Domain Observability to Debug Performance Issues in Microservices Many applications deployed in the cloud are usually refactored into small components called microservices that are deployed as containers in a Kubernetes environment. Such applications are deployed on a cluster of physical servers which are connected via the datacenter network.In such deployments, resources such as compute, memory, and network, are shared and hence some microservices (culprits) can misbehave and consume more resources. This interference among applications hosted on the same node leads to performance issues (e.g., high latency, packet loss) in the microservices (victims) followed by a delayed or low-quality response. Given the highly distributed and transient nature of the workloads, it’s extremely challenging to debug performance issues. Especially, given the nature of existing monitoring tools, which collect traces and analyze them at individual points (network, host, etc) in a disaggregated manner.In this paper, we argue toward a case for a cross-domain (network & host) monitoring and debugging framework which could provide the end-to-end observability to debug performance issues of applications and pin-point the root-cause whether it is on the sender-host, receiver-host or the network. We present the design and provide preliminary implementation details using eBPF (extended Berkeley Packet Filter) to elucidate the feasibility of the system.",A Case For Cross-Domain Observability to Debug Performance Issues in Microservices,"Many applications deployed in the cloud are usually refactored into small components called microservices that are deployed as containers in a Kubernetes environment. Such applications are deployed on a cluster of physical servers which are connected via the datacenter network.In such deployments, resources such as compute, memory, and network, are shared and hence some microservices (culprits) can misbehave and consume more resources. This interference among applications hosted on the same node leads to performance issues (e.g., high latency, packet loss) in the microservices (victims) followed by a delayed or low-quality response. Given the highly distributed and transient nature of the workloads, it’s extremely challenging to debug performance issues. Especially, given the nature of existing monitoring tools, which collect traces and analyze them at individual points (network, host, etc) in a disaggregated manner.In this paper, we argue toward a case for a cross-domain (network & host) monitoring and debugging framework which could provide the end-to-end observability to debug performance issues of applications and pin-point the root-cause whether it is on the sender-host, receiver-host or the network. We present the design and provide preliminary implementation details using eBPF (extended Berkeley Packet Filter) to elucidate the feasibility of the system.",IEEE conference,no,"['case', 'observability', 'debug', 'performance', 'issue', 'many', 'deployed', 'usually', 'refactored', 'small', 'called', 'deployed', 'container', 'kubernetes', 'environment', 'deployed', 'cluster', 'physical', 'server', 'connected', 'via', 'datacenter', 'deployment', 'resource', 'compute', 'memory', 'network', 'shared', 'hence', 'resource', 'interference', 'among', 'hosted', 'node', 'lead', 'performance', 'issue', 'high', 'latency', 'packet', 'loss', 'followed', 'response', 'given', 'highly', 'distributed', 'nature', 'workload', 'extremely', 'challenging', 'debug', 'performance', 'issue', 'especially', 'given', 'nature', 'existing', 'monitoring', 'tool', 'collect', 'trace', 'analyze', 'individual', 'point', 'network', 'host', 'etc', 'disaggregated', 'paper', 'argue', 'toward', 'case', 'network', 'host', 'monitoring', 'debugging', 'framework', 'could', 'provide', 'observability', 'debug', 'performance', 'issue', 'whether', 'network', 'present', 'design', 'provide', 'preliminary', 'implementation', 'detail', 'using', 'ebpf', 'extended', 'berkeley', 'packet', 'filter', 'feasibility']"
"A Conceptual Antifragile Microservice Framework for Reshaping Critical Infrastructures Recently, microservices have been examined as a solution for reshaping and improving the flexibility, scalability, and maintainability of critical infrastructure systems. However, microservice systems are also suffering from the presence of a substantial number of potentially vulnerable components that may threaten the protection of critical infrastructures. To address the problem, this paper proposes to leverage the concept of antifragility built in a framework for building self-learning microservice systems that could be strengthened by faults and threats instead of being deteriorated by them. To illustrate the approach, we instantiate the proposed approach of autonomous machine learning through an experimental evaluation on a benchmarking dataset of microservice faults.",A Conceptual Antifragile Microservice Framework for Reshaping Critical Infrastructures,"Recently, microservices have been examined as a solution for reshaping and improving the flexibility, scalability, and maintainability of critical infrastructure systems. However, microservice systems are also suffering from the presence of a substantial number of potentially vulnerable components that may threaten the protection of critical infrastructures. To address the problem, this paper proposes to leverage the concept of antifragility built in a framework for building self-learning microservice systems that could be strengthened by faults and threats instead of being deteriorated by them. To illustrate the approach, we instantiate the proposed approach of autonomous machine learning through an experimental evaluation on a benchmarking dataset of microservice faults.",IEEE conference,no,"['conceptual', 'framework', 'reshaping', 'critical', 'infrastructure', 'recently', 'examined', 'solution', 'reshaping', 'improving', 'flexibility', 'scalability', 'maintainability', 'critical', 'infrastructure', 'however', 'also', 'presence', 'substantial', 'number', 'potentially', 'vulnerable', 'may', 'protection', 'critical', 'infrastructure', 'address', 'problem', 'paper', 'proposes', 'leverage', 'concept', 'built', 'framework', 'building', 'could', 'fault', 'threat', 'instead', 'illustrate', 'instantiate', 'proposed', 'autonomous', 'machine', 'learning', 'experimental', 'evaluation', 'benchmarking', 'dataset', 'fault']"
"MAAD: A Distributed Anomaly Detection Architecture for Microservices Systems Anomaly detection plays a critical role in microservices systems by enabling system administrators to promptly detect and respond to anomalies. However, existing anomaly detection systems often necessitate the centralization of log and trace data from diverse system components and rely on resource-intensive statistical methods or deep learning models for analysis. This approach impedes real-time anomaly detection and requires a significant demand on computing resources. In this paper, we design a multi-agent-based, distributed anomaly detection architecture called MAAD to address the limitations. MAAD consists of a collection of agents that cooperate together to identify abnormal behaviors in a distributed manner. Each agent is deployed along with a single service and applies lightweight machine learning techniques to perform local anomaly detection based on its own logs, local context, and information extracted from its parent span service.To preserve the graph information in a microservices request, an agent can communicate essential features with each other, taking into account the collective patterns learned from the prior services. We evaluate the effectiveness of MAAD on two microservices datasets, TrainTicket and MicroSS, and show that MAAD achieved high precision (up to 95.8%) and recall (up to 99.6%), outperforming state-of-the-art centralized anomaly detection approaches. Compared to centralized approaches, MAAD reduces the amount of transferred data before anomaly detection by approximately 88%, facilitating real-time anomaly detection. Furthermore, the lightweight nature of MAAD allows for rapid anomaly detection with minimal impact on microservices systems. Compared to DeepLog, MAAD saves approximately 92% detection time without using GPU accelerators.",MAAD: A Distributed Anomaly Detection Architecture for Microservices Systems,"Anomaly detection plays a critical role in microservices systems by enabling system administrators to promptly detect and respond to anomalies. However, existing anomaly detection systems often necessitate the centralization of log and trace data from diverse system components and rely on resource-intensive statistical methods or deep learning models for analysis. This approach impedes real-time anomaly detection and requires a significant demand on computing resources. In this paper, we design a multi-agent-based, distributed anomaly detection architecture called MAAD to address the limitations. MAAD consists of a collection of agents that cooperate together to identify abnormal behaviors in a distributed manner. Each agent is deployed along with a single service and applies lightweight machine learning techniques to perform local anomaly detection based on its own logs, local context, and information extracted from its parent span service.To preserve the graph information in a microservices request, an agent can communicate essential features with each other, taking into account the collective patterns learned from the prior services. We evaluate the effectiveness of MAAD on two microservices datasets, TrainTicket and MicroSS, and show that MAAD achieved high precision (up to 95.8%) and recall (up to 99.6%), outperforming state-of-the-art centralized anomaly detection approaches. Compared to centralized approaches, MAAD reduces the amount of transferred data before anomaly detection by approximately 88%, facilitating real-time anomaly detection. Furthermore, the lightweight nature of MAAD allows for rapid anomaly detection with minimal impact on microservices systems. Compared to DeepLog, MAAD saves approximately 92% detection time without using GPU accelerators.",IEEE conference,no,"['maad', 'distributed', 'anomaly', 'detection', 'anomaly', 'detection', 'play', 'critical', 'role', 'enabling', 'administrator', 'promptly', 'detect', 'respond', 'anomaly', 'however', 'existing', 'anomaly', 'detection', 'often', 'necessitate', 'log', 'trace', 'diverse', 'rely', 'statistical', 'method', 'deep', 'learning', 'model', 'analysis', 'anomaly', 'detection', 'requires', 'significant', 'demand', 'computing', 'resource', 'paper', 'design', 'distributed', 'anomaly', 'detection', 'called', 'maad', 'address', 'limitation', 'maad', 'consists', 'collection', 'agent', 'together', 'identify', 'abnormal', 'behavior', 'distributed', 'manner', 'agent', 'deployed', 'along', 'single', 'applies', 'lightweight', 'machine', 'learning', 'technique', 'perform', 'local', 'anomaly', 'detection', 'based', 'log', 'local', 'context', 'information', 'extracted', 'span', 'preserve', 'graph', 'information', 'request', 'agent', 'communicate', 'essential', 'feature', 'taking', 'account', 'pattern', 'learned', 'prior', 'evaluate', 'effectiveness', 'maad', 'two', 'datasets', 'show', 'maad', 'achieved', 'high', 'precision', 'recall', 'outperforming', 'centralized', 'anomaly', 'detection', 'compared', 'centralized', 'maad', 'reduces', 'amount', 'anomaly', 'detection', 'approximately', 'facilitating', 'anomaly', 'detection', 'furthermore', 'lightweight', 'nature', 'maad', 'allows', 'rapid', 'anomaly', 'detection', 'minimal', 'impact', 'compared', 'maad', 'save', 'approximately', 'detection', 'time', 'without', 'using', 'accelerator']"
"TraceNet: Operation Aware Root Cause Localization of Microservice System Anomalies Microservice architecture has been widely-adopted to enable scalable, flexible, and resilient applications. However, numerous components and complex dependencies pose daunting challenges for anomaly root cause localization. Existing localization methods rely on supervision or coarse-grained analysis. In this paper, we propose TraceNet, an algorithm to locate root causes, based on a fine-grained construction of microservice dependency at the operation level. Firstly, to mitigate the interference from anomaly propagation, TraceNet builds the microservice dependency graph by distinguishing different operations, which depicts the complex microservice dependency at the operation dimension. Then, TraceNet classifies abnormality into inner-abnormality and outer-abnormality, and quantifies the abnormality of microservices by an edge-operation-microservice aggregation, Finally, TraceNet locates root causes based on the largest anomaly subgraph and candidates' different impacts, without human interference. Our experimental evaluation on an open dataset shows that TraceNet can effectively locate root causes, with 90% mean average precision, outperforming state-of-the-art methods.",TraceNet: Operation Aware Root Cause Localization of Microservice System Anomalies,"Microservice architecture has been widely-adopted to enable scalable, flexible, and resilient applications. However, numerous components and complex dependencies pose daunting challenges for anomaly root cause localization. Existing localization methods rely on supervision or coarse-grained analysis. In this paper, we propose TraceNet, an algorithm to locate root causes, based on a fine-grained construction of microservice dependency at the operation level. Firstly, to mitigate the interference from anomaly propagation, TraceNet builds the microservice dependency graph by distinguishing different operations, which depicts the complex microservice dependency at the operation dimension. Then, TraceNet classifies abnormality into inner-abnormality and outer-abnormality, and quantifies the abnormality of microservices by an edge-operation-microservice aggregation, Finally, TraceNet locates root causes based on the largest anomaly subgraph and candidates' different impacts, without human interference. Our experimental evaluation on an open dataset shows that TraceNet can effectively locate root causes, with 90% mean average precision, outperforming state-of-the-art methods.",IEEE conference,no,"['tracenet', 'operation', 'aware', 'root', 'cause', 'localization', 'anomaly', 'enable', 'scalable', 'flexible', 'resilient', 'however', 'numerous', 'complex', 'dependency', 'pose', 'daunting', 'challenge', 'anomaly', 'root', 'cause', 'localization', 'existing', 'localization', 'method', 'rely', 'supervision', 'analysis', 'paper', 'propose', 'tracenet', 'algorithm', 'locate', 'root', 'cause', 'based', 'construction', 'dependency', 'operation', 'level', 'firstly', 'mitigate', 'interference', 'anomaly', 'propagation', 'tracenet', 'build', 'dependency', 'graph', 'different', 'operation', 'complex', 'dependency', 'operation', 'dimension', 'tracenet', 'abnormality', 'abnormality', 'aggregation', 'finally', 'tracenet', 'root', 'cause', 'based', 'largest', 'anomaly', 'subgraph', 'candidate', 'different', 'impact', 'without', 'human', 'interference', 'experimental', 'evaluation', 'open', 'dataset', 'show', 'tracenet', 'effectively', 'locate', 'root', 'cause', 'mean', 'average', 'precision', 'outperforming', 'method']"
"MicroMCM: Fine-grained Root Cause Localization for Microservice Systems Based on Multiple Causal Inference Methods Microservice architecture has become a prevalent approach for developing large-scale applications due to its scalability, flexibility, and agility. However, the large-scale deployment and frequent updates of microservices pose challenges for operational personnel in diagnosing performance issues. To address this, we propose MicroMCM, a framework that enables fine-grained, automated, and real-time root cause localization. MicroMCM dynamically selects different causal inference (CI) methods based on diverse anomaly patterns to construct causal graphs and utilizes root cause inference techniques to identity the root cause metrics. We conduct experiments for both coarse-grained and fine-grained root cause localization to evaluate the performance of MicroMCM. The results demonstrate that MicroMCM outperforms baseline methods, exhibiting superior localization capabilities.",MicroMCM: Fine-grained Root Cause Localization for Microservice Systems Based on Multiple Causal Inference Methods,"Microservice architecture has become a prevalent approach for developing large-scale applications due to its scalability, flexibility, and agility. However, the large-scale deployment and frequent updates of microservices pose challenges for operational personnel in diagnosing performance issues. To address this, we propose MicroMCM, a framework that enables fine-grained, automated, and real-time root cause localization. MicroMCM dynamically selects different causal inference (CI) methods based on diverse anomaly patterns to construct causal graphs and utilizes root cause inference techniques to identity the root cause metrics. We conduct experiments for both coarse-grained and fine-grained root cause localization to evaluate the performance of MicroMCM. The results demonstrate that MicroMCM outperforms baseline methods, exhibiting superior localization capabilities.",IEEE conference,no,"['micromcm', 'root', 'cause', 'localization', 'based', 'multiple', 'causal', 'inference', 'method', 'become', 'prevalent', 'developing', 'due', 'scalability', 'flexibility', 'agility', 'however', 'deployment', 'frequent', 'update', 'pose', 'challenge', 'operational', 'personnel', 'diagnosing', 'performance', 'issue', 'address', 'propose', 'micromcm', 'framework', 'enables', 'automated', 'root', 'cause', 'localization', 'micromcm', 'dynamically', 'selects', 'different', 'causal', 'inference', 'method', 'based', 'diverse', 'anomaly', 'pattern', 'construct', 'causal', 'graph', 'utilizes', 'root', 'cause', 'inference', 'technique', 'identity', 'root', 'cause', 'metric', 'conduct', 'experiment', 'root', 'cause', 'localization', 'evaluate', 'performance', 'micromcm', 'result', 'demonstrate', 'micromcm', 'outperforms', 'baseline', 'method', 'superior', 'localization', 'capability']"
"Nonintrusive Measurement on Temporal and Spatial Features of Microservice Inferences The high flexibility of microservice architecture provides notable divergence among the internal software stack within the same application. Microservice-based applications are commonly deployed in data centers by users yet they have no clue of what is exactly provided by the service provider. In this case, there could exist those providers could be able to replace the internal software without noticing, making a contract-level fault and offloading risks to users. To better profile the microservice, we propose a framework that provides nonintrusive measurements on microservice inference, or MEME, to detect fraudulent behaviors of cloud service providers on microservice-based applications. We design MEME using performance portrait and fast Fourier transform (FFT). We model the microservice-based application with a set of cohorts and use FFT to obtain the signal formed by the main frequency components of average response time. Our model represents the performance portrait of the microservice-based application. The performance portrait is similar to a fingerprint that carries internal software information. In our experiments, we take a two-tier microservice-based application containing databases as an example. Empirical results show that MEME can provide a distinguishable profile of the performance portrait of data services in a temporal and spatial manner, which allows us to identify the software type.",Nonintrusive Measurement on Temporal and Spatial Features of Microservice Inferences,"The high flexibility of microservice architecture provides notable divergence among the internal software stack within the same application. Microservice-based applications are commonly deployed in data centers by users yet they have no clue of what is exactly provided by the service provider. In this case, there could exist those providers could be able to replace the internal software without noticing, making a contract-level fault and offloading risks to users. To better profile the microservice, we propose a framework that provides nonintrusive measurements on microservice inference, or MEME, to detect fraudulent behaviors of cloud service providers on microservice-based applications. We design MEME using performance portrait and fast Fourier transform (FFT). We model the microservice-based application with a set of cohorts and use FFT to obtain the signal formed by the main frequency components of average response time. Our model represents the performance portrait of the microservice-based application. The performance portrait is similar to a fingerprint that carries internal software information. In our experiments, we take a two-tier microservice-based application containing databases as an example. Empirical results show that MEME can provide a distinguishable profile of the performance portrait of data services in a temporal and spatial manner, which allows us to identify the software type.",IEEE conference,no,"['nonintrusive', 'measurement', 'temporal', 'spatial', 'feature', 'inference', 'high', 'flexibility', 'provides', 'notable', 'among', 'internal', 'stack', 'within', 'commonly', 'deployed', 'center', 'user', 'yet', 'provided', 'provider', 'case', 'could', 'exist', 'provider', 'could', 'able', 'internal', 'without', 'making', 'fault', 'offloading', 'risk', 'user', 'better', 'profile', 'propose', 'framework', 'provides', 'nonintrusive', 'measurement', 'inference', 'meme', 'detect', 'behavior', 'provider', 'design', 'meme', 'using', 'performance', 'portrait', 'fast', 'transform', 'model', 'set', 'use', 'obtain', 'signal', 'main', 'frequency', 'average', 'response', 'time', 'model', 'represents', 'performance', 'portrait', 'performance', 'portrait', 'similar', 'carry', 'internal', 'information', 'experiment', 'take', 'containing', 'database', 'example', 'empirical', 'result', 'show', 'meme', 'provide', 'profile', 'performance', 'portrait', 'temporal', 'spatial', 'manner', 'allows', 'u', 'identify', 'type']"
"Conceptual and Comparative Analysis of Application Metrics in Microservices Cloud computing represents an extensively implemented paradigm for provisioning distributed services, offering a significant degree of scalability for global applications. Nonetheless, when confronted with the necessity to scale, the system encounters monitoring challenges, as it must contend with an increased volume of requests while simultaneously accommodating fluctuating demands across various geographic regions. Aside from that, detecting errors in such a model becomes increasingly difficult, because of the many abstraction layers and interconnected microservices a cloud solution has. In that context, metrics can be used to identify errors and monitor the system's state. The substantial diversity in the types of services and the metrics themselves introduces a formidable complexity to the analysis of an entire cluster. Therefore, it is important to identify the essential metrics in microservices that can be used to recognize issues or bottlenecks. In pursuit of this objective, a cloud-based solution was implemented within an Amazon Web Services Kubernetes cluster to emulate the functionality of an online retail store and an automated testing framework was made to inject errors in different parts of this application while its metrics were obtained. In that way, it was possible to identify the effects that errors have on the metrics of components in the system, rendering the monitoring of the cluster a more direct process and reducing the amount of data to be analyzed in order to identify the presence of errors in a cloud environment.",Conceptual and Comparative Analysis of Application Metrics in Microservices,"Cloud computing represents an extensively implemented paradigm for provisioning distributed services, offering a significant degree of scalability for global applications. Nonetheless, when confronted with the necessity to scale, the system encounters monitoring challenges, as it must contend with an increased volume of requests while simultaneously accommodating fluctuating demands across various geographic regions. Aside from that, detecting errors in such a model becomes increasingly difficult, because of the many abstraction layers and interconnected microservices a cloud solution has. In that context, metrics can be used to identify errors and monitor the system's state. The substantial diversity in the types of services and the metrics themselves introduces a formidable complexity to the analysis of an entire cluster. Therefore, it is important to identify the essential metrics in microservices that can be used to recognize issues or bottlenecks. In pursuit of this objective, a cloud-based solution was implemented within an Amazon Web Services Kubernetes cluster to emulate the functionality of an online retail store and an automated testing framework was made to inject errors in different parts of this application while its metrics were obtained. In that way, it was possible to identify the effects that errors have on the metrics of components in the system, rendering the monitoring of the cluster a more direct process and reducing the amount of data to be analyzed in order to identify the presence of errors in a cloud environment.",IEEE conference,no,"['conceptual', 'comparative', 'analysis', 'metric', 'computing', 'represents', 'extensively', 'implemented', 'paradigm', 'provisioning', 'distributed', 'offering', 'significant', 'degree', 'scalability', 'global', 'nonetheless', 'necessity', 'scale', 'monitoring', 'challenge', 'must', 'increased', 'volume', 'request', 'simultaneously', 'fluctuating', 'demand', 'across', 'various', 'detecting', 'error', 'model', 'becomes', 'increasingly', 'difficult', 'many', 'abstraction', 'layer', 'interconnected', 'solution', 'context', 'metric', 'used', 'identify', 'error', 'monitor', 'state', 'substantial', 'diversity', 'type', 'metric', 'introduces', 'complexity', 'analysis', 'entire', 'cluster', 'therefore', 'important', 'identify', 'essential', 'metric', 'used', 'issue', 'bottleneck', 'objective', 'solution', 'implemented', 'within', 'amazon', 'web', 'kubernetes', 'cluster', 'functionality', 'online', 'retail', 'store', 'automated', 'testing', 'framework', 'made', 'error', 'different', 'part', 'metric', 'obtained', 'way', 'possible', 'identify', 'effect', 'error', 'metric', 'rendering', 'monitoring', 'cluster', 'direct', 'process', 'reducing', 'amount', 'analyzed', 'order', 'identify', 'presence', 'error', 'environment']"
"Anomaly Detection and Root Cause Analysis of Microservices Energy Consumption With the expansion of cloud computing and data centers, the need has arisen to tackle their environmental impact. The increasing adoption of microservice architectures, while offering scalability and flexibility, poses new challenges in the effective management of systems’ energy consumption.This study analyzes experimentally the effectiveness, with respect to energy consumption, of algorithms for Anomaly Detection (AD) and Root Cause Analysis (RCA) for (containerized) microservices systems. The study analyzes five AD and three RCA algorithms. Metrics to assess the effectiveness of AD algorithms are Precision, Recall, and F-Score. For RCA algorithms, the chose metric is Precision at level k. Two subjects of different complexity are used: Sock Shop and UNI-Cloud. Experiments use a cross-over paired comparison design, involving multiple randomized runs for robust measures.The experiments show that AD algorithms exhibit a relatively moderate performance. The mean adjusted Precision for Sock Shop is 61.5%, while it is 75% for the best-performing algorithms (BIRCH, KNN, and SVM) on UNI-Cloud. The Recall and F-Score for UNI-Cloud, for the same algorithms, are 75%, while for Sock Shop KNN yields the best outcome at roughly 45%. MicroRCA and RCD emerge as the top-performing algorithms for RCA.We found that the effectiveness of AD algorithms is strongly influenced by anomaly thresholds, emphasizing the importance of careful tuning such algorithms. RCA algorithms reveal promising results, particularly RCD and MicroRCA, which showed robust performance. However, challenges remain, as seen with the ϵ-diagnosis algorithm, suggesting the need for further refinement.For DevOps engineers, the findings highlight the need to carefully select and tune AD and RCA algorithms for energy, and to take into account system topology and monitoring configurations.",Anomaly Detection and Root Cause Analysis of Microservices Energy Consumption,"With the expansion of cloud computing and data centers, the need has arisen to tackle their environmental impact. The increasing adoption of microservice architectures, while offering scalability and flexibility, poses new challenges in the effective management of systems’ energy consumption.This study analyzes experimentally the effectiveness, with respect to energy consumption, of algorithms for Anomaly Detection (AD) and Root Cause Analysis (RCA) for (containerized) microservices systems. The study analyzes five AD and three RCA algorithms. Metrics to assess the effectiveness of AD algorithms are Precision, Recall, and F-Score. For RCA algorithms, the chose metric is Precision at level k. Two subjects of different complexity are used: Sock Shop and UNI-Cloud. Experiments use a cross-over paired comparison design, involving multiple randomized runs for robust measures.The experiments show that AD algorithms exhibit a relatively moderate performance. The mean adjusted Precision for Sock Shop is 61.5%, while it is 75% for the best-performing algorithms (BIRCH, KNN, and SVM) on UNI-Cloud. The Recall and F-Score for UNI-Cloud, for the same algorithms, are 75%, while for Sock Shop KNN yields the best outcome at roughly 45%. MicroRCA and RCD emerge as the top-performing algorithms for RCA.We found that the effectiveness of AD algorithms is strongly influenced by anomaly thresholds, emphasizing the importance of careful tuning such algorithms. RCA algorithms reveal promising results, particularly RCD and MicroRCA, which showed robust performance. However, challenges remain, as seen with the ϵ-diagnosis algorithm, suggesting the need for further refinement.For DevOps engineers, the findings highlight the need to carefully select and tune AD and RCA algorithms for energy, and to take into account system topology and monitoring configurations.",IEEE conference,no,"['anomaly', 'detection', 'root', 'cause', 'analysis', 'energy', 'consumption', 'expansion', 'computing', 'center', 'need', 'tackle', 'environmental', 'impact', 'increasing', 'adoption', 'offering', 'scalability', 'flexibility', 'pose', 'new', 'challenge', 'effective', 'management', 'energy', 'study', 'analyzes', 'effectiveness', 'respect', 'energy', 'consumption', 'algorithm', 'anomaly', 'detection', 'ad', 'root', 'cause', 'analysis', 'rca', 'containerized', 'study', 'analyzes', 'five', 'ad', 'three', 'rca', 'algorithm', 'metric', 'assess', 'effectiveness', 'ad', 'algorithm', 'precision', 'recall', 'rca', 'algorithm', 'metric', 'precision', 'level', 'two', 'subject', 'different', 'complexity', 'used', 'sock', 'shop', 'experiment', 'use', 'paired', 'comparison', 'design', 'involving', 'multiple', 'run', 'robust', 'experiment', 'show', 'ad', 'algorithm', 'exhibit', 'relatively', 'performance', 'mean', 'precision', 'sock', 'shop', 'algorithm', 'recall', 'algorithm', 'sock', 'shop', 'yield', 'best', 'outcome', 'emerge', 'algorithm', 'found', 'effectiveness', 'ad', 'algorithm', 'strongly', 'anomaly', 'threshold', 'emphasizing', 'importance', 'careful', 'algorithm', 'rca', 'algorithm', 'reveal', 'promising', 'result', 'particularly', 'showed', 'robust', 'performance', 'however', 'challenge', 'remain', 'seen', 'algorithm', 'need', 'devops', 'engineer', 'finding', 'highlight', 'need', 'carefully', 'select', 'ad', 'rca', 'algorithm', 'energy', 'take', 'account', 'topology', 'monitoring', 'configuration']"
"A layered framework for root cause diagnosis of microservices Microservice-based architectures feature function-ally independent, well-defined and fine-grained components suit-able for loosely coupled deployments and for building reli-able cloud-native applications. Despite the advantages of this approach, component interactions introduce complexity, thus turning boundary -spanning service operation into a daunting challenge. As systems grow in size, complexity can easily outgrow the cognitive capacity of human operators, who are unable to effectively diagnose faulty microservices. We address this problem by proposing a novel framework to diagnose faulty microservices. Through failure injection and an experimental assessment, our layered diagnosis framework using service response analysis, timing constraints, causality and a ranking algorithm from traces, is able to effectively diagnose faulty microservices. Empirical evaluation of the proposed approach, by examining 130 experi-ments in a representative microservice application in the presence of faults, shows that it can achieve approximately 89% specificity and 77% recall.",A layered framework for root cause diagnosis of microservices,"Microservice-based architectures feature function-ally independent, well-defined and fine-grained components suit-able for loosely coupled deployments and for building reli-able cloud-native applications. Despite the advantages of this approach, component interactions introduce complexity, thus turning boundary -spanning service operation into a daunting challenge. As systems grow in size, complexity can easily outgrow the cognitive capacity of human operators, who are unable to effectively diagnose faulty microservices. We address this problem by proposing a novel framework to diagnose faulty microservices. Through failure injection and an experimental assessment, our layered diagnosis framework using service response analysis, timing constraints, causality and a ranking algorithm from traces, is able to effectively diagnose faulty microservices. Empirical evaluation of the proposed approach, by examining 130 experi-ments in a representative microservice application in the presence of faults, shows that it can achieve approximately 89% specificity and 77% recall.",IEEE conference,no,"['layered', 'framework', 'root', 'cause', 'diagnosis', 'feature', 'independent', 'loosely', 'coupled', 'deployment', 'building', 'despite', 'advantage', 'interaction', 'introduce', 'complexity', 'thus', 'boundary', 'operation', 'daunting', 'challenge', 'grow', 'size', 'complexity', 'easily', 'capacity', 'human', 'operator', 'effectively', 'diagnose', 'faulty', 'address', 'problem', 'proposing', 'novel', 'framework', 'diagnose', 'faulty', 'failure', 'injection', 'experimental', 'assessment', 'layered', 'diagnosis', 'framework', 'using', 'response', 'analysis', 'constraint', 'causality', 'ranking', 'algorithm', 'trace', 'able', 'effectively', 'diagnose', 'faulty', 'empirical', 'evaluation', 'proposed', 'examining', 'representative', 'presence', 'fault', 'show', 'achieve', 'approximately', 'specificity', 'recall']"
"Active Prewarning Technology for Smart Grid Based on Unsupervised Learning This paper proposes an early warning technology for power grid cloud platform microservice architecture alarm information based on unsupervised learning, discovers potential abnormal patterns and rules, sends out early warning signals in a timely manner, and improves the ability to identify and process power grid cloud platform microservice architecture alarms. According to the above steps, based on the establishment of the power grid business microservice operation alarm log alarm information knowledge base, this article will study the cloud platform operation monitoring data scoring algorithm, classify the cloud platform monitoring information, and then filter out the suspected abnormal data. For different business types of complex scenarios, research on abnormal information classification mining technology based on statistical models, proximity, clustering and classification, apply improved K-means clustering[1] and LOF outlier detection technology[2] to establish a combined detection model, and detect anomalies The data is clustered and outliers detected to achieve active early warning technology of unsupervised learning.",Active Prewarning Technology for Smart Grid Based on Unsupervised Learning,"This paper proposes an early warning technology for power grid cloud platform microservice architecture alarm information based on unsupervised learning, discovers potential abnormal patterns and rules, sends out early warning signals in a timely manner, and improves the ability to identify and process power grid cloud platform microservice architecture alarms. According to the above steps, based on the establishment of the power grid business microservice operation alarm log alarm information knowledge base, this article will study the cloud platform operation monitoring data scoring algorithm, classify the cloud platform monitoring information, and then filter out the suspected abnormal data. For different business types of complex scenarios, research on abnormal information classification mining technology based on statistical models, proximity, clustering and classification, apply improved K-means clustering[1] and LOF outlier detection technology[2] to establish a combined detection model, and detect anomalies The data is clustered and outliers detected to achieve active early warning technology of unsupervised learning.",IEEE conference,no,"['active', 'technology', 'smart', 'grid', 'based', 'unsupervised', 'learning', 'paper', 'proposes', 'early', 'warning', 'technology', 'power', 'grid', 'platform', 'alarm', 'information', 'based', 'unsupervised', 'learning', 'potential', 'abnormal', 'pattern', 'rule', 'early', 'warning', 'signal', 'timely', 'manner', 'improves', 'ability', 'identify', 'process', 'power', 'grid', 'platform', 'alarm', 'according', 'step', 'based', 'power', 'grid', 'business', 'operation', 'alarm', 'log', 'alarm', 'information', 'knowledge', 'base', 'article', 'study', 'platform', 'operation', 'monitoring', 'scoring', 'algorithm', 'classify', 'platform', 'monitoring', 'information', 'filter', 'abnormal', 'different', 'business', 'type', 'complex', 'scenario', 'research', 'abnormal', 'information', 'classification', 'mining', 'technology', 'based', 'statistical', 'model', 'proximity', 'clustering', 'classification', 'apply', 'improved', 'clustering', 'outlier', 'detection', 'technology', 'establish', 'combined', 'detection', 'model', 'detect', 'anomaly', 'outlier', 'detected', 'achieve', 'active', 'early', 'warning', 'technology', 'unsupervised', 'learning']"
"SparseRCA: Unsupervised Root Cause Analysis in Sparse Microservice Testing Traces Microservice architecture has become a predominant paradigm in the software industry. This architecture necessitates robust end-to-end testing to ensure seamless integration of all components before deployment. Rapidly pinpointing issues when test cases fail is crucial for enhancing software development efficiency. However, in testing environments, the available trace is often sparse, and the system is continuously upgrading, which renders existing microservice-based root cause analysis (RCA) ineffective. To address these challenges, we propose SparseRCA. By assessing the abnormality of the exclusive latency, SparseRCA directly determines the probability of the root cause, solving the challenge of not being able to fully obtain the fault propagation information, such as call relationships in sparse trace scenarios. At the same time, by reconstructing the exclusive latency using the decoupled atomic span units, it solves the problem of latency prediction for new traces caused by frequent upgrades. We evaluate SparseRCA on real-world datasets from a large e-commerce system’s testing environment, where it demonstrates significant improvements over existing models. Our findings underscore the effectiveness of SparseRCA in addressing the challenges of RCA in microservice testing environments.",SparseRCA: Unsupervised Root Cause Analysis in Sparse Microservice Testing Traces,"Microservice architecture has become a predominant paradigm in the software industry. This architecture necessitates robust end-to-end testing to ensure seamless integration of all components before deployment. Rapidly pinpointing issues when test cases fail is crucial for enhancing software development efficiency. However, in testing environments, the available trace is often sparse, and the system is continuously upgrading, which renders existing microservice-based root cause analysis (RCA) ineffective. To address these challenges, we propose SparseRCA. By assessing the abnormality of the exclusive latency, SparseRCA directly determines the probability of the root cause, solving the challenge of not being able to fully obtain the fault propagation information, such as call relationships in sparse trace scenarios. At the same time, by reconstructing the exclusive latency using the decoupled atomic span units, it solves the problem of latency prediction for new traces caused by frequent upgrades. We evaluate SparseRCA on real-world datasets from a large e-commerce system’s testing environment, where it demonstrates significant improvements over existing models. Our findings underscore the effectiveness of SparseRCA in addressing the challenges of RCA in microservice testing environments.",IEEE conference,no,"['sparserca', 'unsupervised', 'root', 'cause', 'analysis', 'sparse', 'testing', 'trace', 'become', 'paradigm', 'industry', 'necessitates', 'robust', 'testing', 'ensure', 'seamless', 'integration', 'deployment', 'rapidly', 'pinpointing', 'issue', 'test', 'case', 'fail', 'crucial', 'enhancing', 'development', 'efficiency', 'however', 'testing', 'environment', 'available', 'trace', 'often', 'sparse', 'continuously', 'upgrading', 'render', 'existing', 'root', 'cause', 'analysis', 'rca', 'ineffective', 'address', 'challenge', 'propose', 'sparserca', 'assessing', 'abnormality', 'exclusive', 'latency', 'sparserca', 'directly', 'probability', 'root', 'cause', 'solving', 'challenge', 'able', 'fully', 'obtain', 'fault', 'propagation', 'information', 'call', 'relationship', 'sparse', 'trace', 'scenario', 'time', 'exclusive', 'latency', 'using', 'decoupled', 'span', 'unit', 'solves', 'problem', 'latency', 'prediction', 'new', 'trace', 'caused', 'frequent', 'upgrade', 'evaluate', 'sparserca', 'datasets', 'large', 'testing', 'environment', 'demonstrates', 'significant', 'improvement', 'existing', 'model', 'finding', 'effectiveness', 'sparserca', 'addressing', 'challenge', 'rca', 'testing', 'environment']"
"An Anomaly Monitoring and Early Warning Method for Power Grid Microservice Network Based on Log Visualisation and Analysis With the increasing complexity of power grid microservice networks, monitoring and detecting anomalies in real-time has become a significant challenge. Traditional methods struggle to capture the sequential and contextual relationships in log data. To address this, we propose a novel anomaly monitoring and early warning method based on the Bi-LSTM-ATT architecture, which integrates bidirectional long short-term memory (Bi-LSTM) networks and an attention mechanism. This model effectively captures both forward and backward dependencies in log sequences while focusing on critical features related to anomalies. The proposed method was tested using real-world log data from power grid microservices, and the experimental results show that it significantly outperforms traditional approaches such as PCA, Invariant Mining (IM), and N-gram in terms of precision, recall, and F1-score. The Bi-LSTM-ATT model provides a robust and accurate approach for real-time anomaly detection, contributing to enhanced operational stability and reliability in power grid systems.",An Anomaly Monitoring and Early Warning Method for Power Grid Microservice Network Based on Log Visualisation and Analysis,"With the increasing complexity of power grid microservice networks, monitoring and detecting anomalies in real-time has become a significant challenge. Traditional methods struggle to capture the sequential and contextual relationships in log data. To address this, we propose a novel anomaly monitoring and early warning method based on the Bi-LSTM-ATT architecture, which integrates bidirectional long short-term memory (Bi-LSTM) networks and an attention mechanism. This model effectively captures both forward and backward dependencies in log sequences while focusing on critical features related to anomalies. The proposed method was tested using real-world log data from power grid microservices, and the experimental results show that it significantly outperforms traditional approaches such as PCA, Invariant Mining (IM), and N-gram in terms of precision, recall, and F1-score. The Bi-LSTM-ATT model provides a robust and accurate approach for real-time anomaly detection, contributing to enhanced operational stability and reliability in power grid systems.",IEEE conference,no,"['anomaly', 'monitoring', 'early', 'warning', 'method', 'power', 'grid', 'network', 'based', 'log', 'analysis', 'increasing', 'complexity', 'power', 'grid', 'network', 'monitoring', 'detecting', 'anomaly', 'become', 'significant', 'challenge', 'traditional', 'method', 'capture', 'sequential', 'contextual', 'relationship', 'log', 'address', 'propose', 'novel', 'anomaly', 'monitoring', 'early', 'warning', 'method', 'based', 'integrates', 'long', 'memory', 'network', 'attention', 'mechanism', 'model', 'effectively', 'capture', 'forward', 'dependency', 'log', 'sequence', 'focusing', 'critical', 'feature', 'related', 'anomaly', 'proposed', 'method', 'tested', 'using', 'log', 'power', 'grid', 'experimental', 'result', 'show', 'significantly', 'outperforms', 'traditional', 'mining', 'term', 'precision', 'recall', 'model', 'provides', 'robust', 'accurate', 'anomaly', 'detection', 'contributing', 'enhanced', 'operational', 'stability', 'reliability', 'power', 'grid']"
"Unsupervised Anomaly Detection on Distributed Log Tracing through Deep Learning Large modern software systems have a distributed microservice architecture. Such systems are built from loosely connected and independently deployable components - microservices. Microservices interact with each other with the help of various protocols, forming a sequence of internal calls in order to process a request that comes to the system. This interaction is recorded in the form of logs, which are produced by services at various points of execution and contain useful debugging information. Distributed tracing links individual logs into traces. Thanks to this, it is possible to analyze all logs related to one specific request. Traces are extremely convenient for analyzing incidents that occur during system operation. This work is dedicated to the automatic analysis of log traces to identify abnormal behavior in the operation of distributed systems. The solution is based on the application of deep machine learning methods for analyzing log sequences. The logs are cleaned and vectorized, and then used to train a model based on the long short-term memory autoencoder. The solution is tested on the TraceBench open dataset. As a result, high values of precision and recall metrics were obtained.",Unsupervised Anomaly Detection on Distributed Log Tracing through Deep Learning,"Large modern software systems have a distributed microservice architecture. Such systems are built from loosely connected and independently deployable components - microservices. Microservices interact with each other with the help of various protocols, forming a sequence of internal calls in order to process a request that comes to the system. This interaction is recorded in the form of logs, which are produced by services at various points of execution and contain useful debugging information. Distributed tracing links individual logs into traces. Thanks to this, it is possible to analyze all logs related to one specific request. Traces are extremely convenient for analyzing incidents that occur during system operation. This work is dedicated to the automatic analysis of log traces to identify abnormal behavior in the operation of distributed systems. The solution is based on the application of deep machine learning methods for analyzing log sequences. The logs are cleaned and vectorized, and then used to train a model based on the long short-term memory autoencoder. The solution is tested on the TraceBench open dataset. As a result, high values of precision and recall metrics were obtained.",IEEE conference,no,"['unsupervised', 'anomaly', 'detection', 'distributed', 'log', 'tracing', 'deep', 'learning', 'large', 'modern', 'distributed', 'built', 'loosely', 'connected', 'independently', 'deployable', 'interact', 'help', 'various', 'protocol', 'sequence', 'internal', 'call', 'order', 'process', 'request', 'come', 'interaction', 'form', 'log', 'produced', 'various', 'point', 'execution', 'contain', 'useful', 'debugging', 'information', 'distributed', 'tracing', 'link', 'individual', 'log', 'trace', 'thanks', 'possible', 'analyze', 'log', 'related', 'one', 'specific', 'request', 'trace', 'extremely', 'convenient', 'analyzing', 'incident', 'occur', 'operation', 'work', 'dedicated', 'automatic', 'analysis', 'log', 'trace', 'identify', 'abnormal', 'behavior', 'operation', 'distributed', 'solution', 'based', 'deep', 'machine', 'learning', 'method', 'analyzing', 'log', 'sequence', 'log', 'used', 'model', 'based', 'long', 'memory', 'solution', 'tested', 'open', 'dataset', 'result', 'high', 'value', 'precision', 'recall', 'metric', 'obtained']"
"Enabling Auto-Heal Micro-Service Framework (AHMF) Based on APM Audit And Proactive Fault Discovery in Cloud Monitoring In Computer Science paradigm, at any domain, Data is a crucial component and it becomes highly sensitive when it is handled within financial domains. Technology domain companies heavily rolling out on Cloud-Native applications and legacy business domain companies also started adapting into Cloud based platforms. So, there is a lot of data movement happening in Cloud services. In terms of Data integrity and availability this possesses a greater challenge. This study will enable the auto-healing of Transactional data loss or incorrect data caused by many internal and external factors by developing the AHMF which is a framework that can be plug-in to any microservice design solutions available in Cloud. This study is mainly focuses on Insurance domain as use case which handles highly sensitive payment data. This framework also helps to monitor and trace all the transactions using the Audit functions which can be a critical factor in understanding the data movement within Microservices. Though the lost data or incorrect data is being fixed by any system, with a real-time system the transactional time also plays a vital role in critical applications like payments. Hence this study also emphasizes the importance of keeping System SLA with marginal SLA breaches.",Enabling Auto-Heal Micro-Service Framework (AHMF) Based on APM Audit And Proactive Fault Discovery in Cloud Monitoring,"In Computer Science paradigm, at any domain, Data is a crucial component and it becomes highly sensitive when it is handled within financial domains. Technology domain companies heavily rolling out on Cloud-Native applications and legacy business domain companies also started adapting into Cloud based platforms. So, there is a lot of data movement happening in Cloud services. In terms of Data integrity and availability this possesses a greater challenge. This study will enable the auto-healing of Transactional data loss or incorrect data caused by many internal and external factors by developing the AHMF which is a framework that can be plug-in to any microservice design solutions available in Cloud. This study is mainly focuses on Insurance domain as use case which handles highly sensitive payment data. This framework also helps to monitor and trace all the transactions using the Audit functions which can be a critical factor in understanding the data movement within Microservices. Though the lost data or incorrect data is being fixed by any system, with a real-time system the transactional time also plays a vital role in critical applications like payments. Hence this study also emphasizes the importance of keeping System SLA with marginal SLA breaches.",IEEE conference,no,"['enabling', 'framework', 'based', 'audit', 'proactive', 'fault', 'discovery', 'monitoring', 'computer', 'paradigm', 'domain', 'crucial', 'becomes', 'highly', 'sensitive', 'handled', 'within', 'financial', 'domain', 'technology', 'domain', 'company', 'heavily', 'legacy', 'business', 'domain', 'company', 'also', 'started', 'adapting', 'based', 'platform', 'lot', 'movement', 'happening', 'term', 'integrity', 'availability', 'greater', 'challenge', 'study', 'enable', 'loss', 'caused', 'many', 'internal', 'external', 'factor', 'developing', 'framework', 'design', 'solution', 'available', 'study', 'mainly', 'focus', 'domain', 'use', 'case', 'handle', 'highly', 'sensitive', 'payment', 'framework', 'also', 'help', 'monitor', 'trace', 'transaction', 'using', 'audit', 'function', 'critical', 'factor', 'understanding', 'movement', 'within', 'though', 'lost', 'fixed', 'time', 'also', 'play', 'vital', 'role', 'critical', 'like', 'payment', 'hence', 'study', 'also', 'emphasizes', 'importance', 'keeping', 'sla', 'sla', 'breach']"
"G-Cause: Parameter-free Global Diagnosis for Hyperscale Web Service Infrastructures Hyperscale web service infrastructures are becoming increasingly complex and facing a variety of threats, raising the demand for more sophisticated automated operations and diagnosis solutions. Existing anomaly root cause localization approaches often focus on Service-level components without drilling down to the lower-level resources where services are deployed, hindering the implementation of fine-grained failure fix measures. This paper introduces a challenging task called global diagnosis and addresses it by proposing a technique called G-Cause, which is applicable to both Service-level and host-level root cause analysis scenarios. G-Cause builds a highly adaptive diagnostic framework based on the frequency-domain and time-domain characteristics of monitoring metrics, allowing it to handle global diagnosis requirements from app to host with minimal parameter adjustments. We deploy and validate our approach in two typical scenarios: homogeneous metric diagnosis from app to microservice, and heterogeneous metric diagnosis for various host resources. The results demonstrate that G-Cause outperforms state-of-the-art diagnosis algorithms while providing strong interpretability. Our approach helps operators understand the core mechanism of anomaly propagation and adjust their management strategies more effectively. With these strengths, G-Cause successfully services our global product operations and also makes an impressive contribution in many other workflows.",G-Cause: Parameter-free Global Diagnosis for Hyperscale Web Service Infrastructures,"Hyperscale web service infrastructures are becoming increasingly complex and facing a variety of threats, raising the demand for more sophisticated automated operations and diagnosis solutions. Existing anomaly root cause localization approaches often focus on Service-level components without drilling down to the lower-level resources where services are deployed, hindering the implementation of fine-grained failure fix measures. This paper introduces a challenging task called global diagnosis and addresses it by proposing a technique called G-Cause, which is applicable to both Service-level and host-level root cause analysis scenarios. G-Cause builds a highly adaptive diagnostic framework based on the frequency-domain and time-domain characteristics of monitoring metrics, allowing it to handle global diagnosis requirements from app to host with minimal parameter adjustments. We deploy and validate our approach in two typical scenarios: homogeneous metric diagnosis from app to microservice, and heterogeneous metric diagnosis for various host resources. The results demonstrate that G-Cause outperforms state-of-the-art diagnosis algorithms while providing strong interpretability. Our approach helps operators understand the core mechanism of anomaly propagation and adjust their management strategies more effectively. With these strengths, G-Cause successfully services our global product operations and also makes an impressive contribution in many other workflows.",IEEE conference,no,"['global', 'diagnosis', 'web', 'infrastructure', 'web', 'infrastructure', 'becoming', 'increasingly', 'complex', 'facing', 'variety', 'threat', 'demand', 'sophisticated', 'automated', 'operation', 'diagnosis', 'solution', 'existing', 'anomaly', 'root', 'cause', 'localization', 'often', 'focus', 'without', 'resource', 'deployed', 'implementation', 'failure', 'measure', 'paper', 'introduces', 'challenging', 'task', 'called', 'global', 'diagnosis', 'address', 'proposing', 'technique', 'called', 'applicable', 'root', 'cause', 'analysis', 'scenario', 'build', 'highly', 'adaptive', 'diagnostic', 'framework', 'based', 'characteristic', 'monitoring', 'metric', 'allowing', 'handle', 'global', 'diagnosis', 'requirement', 'app', 'host', 'minimal', 'parameter', 'deploy', 'validate', 'two', 'typical', 'scenario', 'homogeneous', 'metric', 'diagnosis', 'app', 'heterogeneous', 'metric', 'diagnosis', 'various', 'host', 'resource', 'result', 'demonstrate', 'outperforms', 'diagnosis', 'algorithm', 'providing', 'strong', 'help', 'operator', 'understand', 'core', 'mechanism', 'anomaly', 'propagation', 'management', 'strategy', 'effectively', 'strength', 'successfully', 'global', 'product', 'operation', 'also', 'make', 'contribution', 'many', 'workflow']"
"Towards More Effective and Explainable Fault Management Using Cross-Layer Service Topology As microservice architecture becomes prominent, existing fault management techniques to deal with service disruption become limiting mainly due to the amount of data needed to be analyzed. This paper emphasizes the need to consider the cross-layer topology of the cloud service to intelligently identify and correlate the observability data and assist in implementing efficient and more accurate fault management techniques that can provide better explainability. Towards this goal, the paper presents a tool that discovers the cross-layer topology for a cloud microservice application and discusses the benefits of using cross-layer service topology to implement effective fault management.",Towards More Effective and Explainable Fault Management Using Cross-Layer Service Topology,"As microservice architecture becomes prominent, existing fault management techniques to deal with service disruption become limiting mainly due to the amount of data needed to be analyzed. This paper emphasizes the need to consider the cross-layer topology of the cloud service to intelligently identify and correlate the observability data and assist in implementing efficient and more accurate fault management techniques that can provide better explainability. Towards this goal, the paper presents a tool that discovers the cross-layer topology for a cloud microservice application and discusses the benefits of using cross-layer service topology to implement effective fault management.",IEEE conference,no,"['towards', 'effective', 'fault', 'management', 'using', 'topology', 'becomes', 'prominent', 'existing', 'fault', 'management', 'technique', 'deal', 'become', 'limiting', 'mainly', 'due', 'amount', 'needed', 'analyzed', 'paper', 'emphasizes', 'need', 'consider', 'topology', 'intelligently', 'identify', 'observability', 'assist', 'implementing', 'efficient', 'accurate', 'fault', 'management', 'technique', 'provide', 'better', 'towards', 'goal', 'paper', 'present', 'tool', 'topology', 'discusses', 'benefit', 'using', 'topology', 'implement', 'effective', 'fault', 'management']"
ML and Network Traces to M.A.R.S MARS is a Microservice Architecture Recovery Solution that uses Machine Learning and lightweight Network Traces to recover the architecture of applications in order to deploy network security policies and protect the organization against complex threats that may exploit several vulnerabilities to breach an application and either exfiltrate sensitive data or carry out denial of service attacks. The adoption of such security policies is often hindered by the lack of suitable documentation. This paper describes a novel methodology that uses machine learning on captured network traces to recover application architectures.,ML and Network Traces to M.A.R.S,MARS is a Microservice Architecture Recovery Solution that uses Machine Learning and lightweight Network Traces to recover the architecture of applications in order to deploy network security policies and protect the organization against complex threats that may exploit several vulnerabilities to breach an application and either exfiltrate sensitive data or carry out denial of service attacks. The adoption of such security policies is often hindered by the lack of suitable documentation. This paper describes a novel methodology that uses machine learning on captured network traces to recover application architectures.,IEEE conference,no,"['ml', 'network', 'trace', 'recovery', 'solution', 'us', 'machine', 'learning', 'lightweight', 'network', 'trace', 'recover', 'order', 'deploy', 'network', 'security', 'policy', 'protect', 'organization', 'complex', 'threat', 'may', 'exploit', 'several', 'vulnerability', 'breach', 'either', 'sensitive', 'carry', 'denial', 'attack', 'adoption', 'security', 'policy', 'often', 'lack', 'suitable', 'documentation', 'paper', 'describes', 'novel', 'methodology', 'us', 'machine', 'learning', 'captured', 'network', 'trace', 'recover']"
"MicroDiag: Fine-grained Performance Diagnosis for Microservice Systems Microservice architecture has emerged as a popular pattern for developing large-scale applications for its benefits of flexibility, scalability, and agility. However, the large number of services and complex dependencies make it difficult and time-consuming to diagnose performance issues. We propose Micro-Diag, an automated system to localize root causes of performance issues in microservice systems at a fine granularity, including not only locating the faulty component but also discovering detailed information for its abnormality. MicroDiag constructs a component dependency graph and performs causal inference on diverse anomaly symptoms to derive a metrics causality graph, which is used to infer root causes. Our experimental evaluation on a microservice benchmark running in a Kubernetes cluster shows that MicroDiag localizes root causes well, with 97% precision of the top 3 most likely root causes, outperforming state-of-the-art methods by at least 31.1%.",MicroDiag: Fine-grained Performance Diagnosis for Microservice Systems,"Microservice architecture has emerged as a popular pattern for developing large-scale applications for its benefits of flexibility, scalability, and agility. However, the large number of services and complex dependencies make it difficult and time-consuming to diagnose performance issues. We propose Micro-Diag, an automated system to localize root causes of performance issues in microservice systems at a fine granularity, including not only locating the faulty component but also discovering detailed information for its abnormality. MicroDiag constructs a component dependency graph and performs causal inference on diverse anomaly symptoms to derive a metrics causality graph, which is used to infer root causes. Our experimental evaluation on a microservice benchmark running in a Kubernetes cluster shows that MicroDiag localizes root causes well, with 97% precision of the top 3 most likely root causes, outperforming state-of-the-art methods by at least 31.1%.",IEEE conference,no,"['microdiag', 'performance', 'diagnosis', 'emerged', 'popular', 'pattern', 'developing', 'benefit', 'flexibility', 'scalability', 'agility', 'however', 'large', 'number', 'complex', 'dependency', 'make', 'difficult', 'diagnose', 'performance', 'issue', 'propose', 'automated', 'localize', 'root', 'cause', 'performance', 'issue', 'fine', 'granularity', 'including', 'faulty', 'also', 'detailed', 'information', 'abnormality', 'microdiag', 'construct', 'dependency', 'graph', 'performs', 'causal', 'inference', 'diverse', 'anomaly', 'symptom', 'derive', 'metric', 'causality', 'graph', 'used', 'root', 'cause', 'experimental', 'evaluation', 'benchmark', 'running', 'kubernetes', 'cluster', 'show', 'microdiag', 'root', 'cause', 'well', 'precision', 'top', 'likely', 'root', 'cause', 'outperforming', 'method', 'least']"
"Self Adjusting Log Observability for Cloud Native Applications With the increasing complexity of modern applications, particularly those relying on microservices architectures, the volume of observability data, encompassing logs, metrics, traces, etc., has surged significantly. This is further exacerbated by extensive cloud deployments, where observability is crucial for comprehending the health and performance of these systems, leading operations teams to collect as much data as possible for the “fear of missing out”. However, the collection, storage, and analysis of observability data entail significant costs, both in terms of resources and finances. Specifically, logs comprise the most substantial portion of observability data volume, thus exerting the greatest impact on observability cost. Moreover, logs also exhibit unstructured and noisy characteristics, where the efficacy of downstream AI for IT operations (AIOps tasks or Day-2 operations), such as fault classification, fault diagnosis, log anomaly detection etc., can be negatively impacted by log data volume. Hence, striking a balance between the verbosity of log observability and its impact on day-2 operations and debuggability is essential. In this paper, we introduce an autonomous system named SALO, which stands for Self-Adjusting Log Observability. SALO selectively collects logs based on real-time necessity, location, and granularity, as opposed to the conventional practice of collecting indiscriminately from all the components continuously. Our experiments show that SALO drastically decreases the log volume, by as much as 95 %, while still maintaining data quality for downstream AIOps usage, especially for post-hoc diagnosis tasks. Operating on a reduced volume of log data not only decreases storage, transfer, and retention costs but also streamlines observability pipelines, making them leaner, more efficient, and less resource-hungry.",Self Adjusting Log Observability for Cloud Native Applications,"With the increasing complexity of modern applications, particularly those relying on microservices architectures, the volume of observability data, encompassing logs, metrics, traces, etc., has surged significantly. This is further exacerbated by extensive cloud deployments, where observability is crucial for comprehending the health and performance of these systems, leading operations teams to collect as much data as possible for the “fear of missing out”. However, the collection, storage, and analysis of observability data entail significant costs, both in terms of resources and finances. Specifically, logs comprise the most substantial portion of observability data volume, thus exerting the greatest impact on observability cost. Moreover, logs also exhibit unstructured and noisy characteristics, where the efficacy of downstream AI for IT operations (AIOps tasks or Day-2 operations), such as fault classification, fault diagnosis, log anomaly detection etc., can be negatively impacted by log data volume. Hence, striking a balance between the verbosity of log observability and its impact on day-2 operations and debuggability is essential. In this paper, we introduce an autonomous system named SALO, which stands for Self-Adjusting Log Observability. SALO selectively collects logs based on real-time necessity, location, and granularity, as opposed to the conventional practice of collecting indiscriminately from all the components continuously. Our experiments show that SALO drastically decreases the log volume, by as much as 95 %, while still maintaining data quality for downstream AIOps usage, especially for post-hoc diagnosis tasks. Operating on a reduced volume of log data not only decreases storage, transfer, and retention costs but also streamlines observability pipelines, making them leaner, more efficient, and less resource-hungry.",IEEE conference,no,"['self', 'log', 'observability', 'native', 'increasing', 'complexity', 'modern', 'particularly', 'relying', 'volume', 'observability', 'encompassing', 'log', 'metric', 'trace', 'significantly', 'extensive', 'deployment', 'observability', 'crucial', 'health', 'performance', 'leading', 'operation', 'team', 'collect', 'much', 'possible', 'missing', 'however', 'collection', 'storage', 'analysis', 'observability', 'significant', 'cost', 'term', 'resource', 'finance', 'specifically', 'log', 'comprise', 'substantial', 'portion', 'observability', 'volume', 'thus', 'impact', 'observability', 'cost', 'moreover', 'log', 'also', 'exhibit', 'noisy', 'characteristic', 'efficacy', 'ai', 'operation', 'aiops', 'task', 'operation', 'fault', 'classification', 'fault', 'diagnosis', 'log', 'anomaly', 'detection', 'log', 'volume', 'hence', 'balance', 'log', 'observability', 'impact', 'operation', 'essential', 'paper', 'introduce', 'autonomous', 'named', 'salo', 'stand', 'log', 'observability', 'salo', 'selectively', 'collect', 'log', 'based', 'necessity', 'location', 'granularity', 'conventional', 'practice', 'collecting', 'continuously', 'experiment', 'show', 'salo', 'drastically', 'decrease', 'log', 'volume', 'much', 'still', 'maintaining', 'quality', 'aiops', 'usage', 'especially', 'diagnosis', 'task', 'operating', 'reduced', 'volume', 'log', 'decrease', 'storage', 'transfer', 'cost', 'also', 'observability', 'pipeline', 'making', 'efficient', 'less']"
"Sparrow Tracer: Scalable Real Time Metrics from Event Log Pipelines at Twitter Streaming event pipelines are one of the core components of Twitter's Data Infrastructure [1]. Twitter Sparrow is a project responsible for aggregating, processing and delivering user action generated events from microservices to data warehouses and data lakes in real time. User action generated events are converted into datasets used for data processing and data analytics use cases. This project is built using different on premise and cloud services. One of the important requirements of such a streaming event pipeline is the ability to measure important metrics such as latency, event count, event drop vs. success rate, and more. These metrics are responsible for defining the health of the streaming pipeline as well as providing valuable insights to users of the events. In this paper we introduce Sparrow Tracer which is a novel way to capture the event metrics using the concept of tracer events.",Sparrow Tracer: Scalable Real Time Metrics from Event Log Pipelines at Twitter,"Streaming event pipelines are one of the core components of Twitter's Data Infrastructure [1]. Twitter Sparrow is a project responsible for aggregating, processing and delivering user action generated events from microservices to data warehouses and data lakes in real time. User action generated events are converted into datasets used for data processing and data analytics use cases. This project is built using different on premise and cloud services. One of the important requirements of such a streaming event pipeline is the ability to measure important metrics such as latency, event count, event drop vs. success rate, and more. These metrics are responsible for defining the health of the streaming pipeline as well as providing valuable insights to users of the events. In this paper we introduce Sparrow Tracer which is a novel way to capture the event metrics using the concept of tracer events.",IEEE conference,no,"['sparrow', 'tracer', 'scalable', 'real', 'time', 'metric', 'event', 'log', 'pipeline', 'twitter', 'streaming', 'event', 'pipeline', 'one', 'core', 'twitter', 'infrastructure', 'twitter', 'sparrow', 'project', 'responsible', 'processing', 'delivering', 'user', 'action', 'generated', 'event', 'lake', 'real', 'time', 'user', 'action', 'generated', 'event', 'datasets', 'used', 'processing', 'analytics', 'use', 'case', 'project', 'built', 'using', 'different', 'premise', 'one', 'important', 'requirement', 'streaming', 'event', 'pipeline', 'ability', 'measure', 'important', 'metric', 'latency', 'event', 'count', 'event', 'success', 'rate', 'metric', 'responsible', 'defining', 'health', 'streaming', 'pipeline', 'well', 'providing', 'valuable', 'insight', 'user', 'event', 'paper', 'introduce', 'sparrow', 'tracer', 'novel', 'way', 'capture', 'event', 'metric', 'using', 'concept', 'tracer', 'event']"
"Using Distributed Tracing to Identify Inefficient Resources Composition in Cloud Applications Cloud-Applications are the new industry standard way of designing Web-Applications. With Cloud Computing, Applications are usually designed as microservices, and developers can take advantage of thousands of such existing microservices, involving several hundred of cross-component communications on different physical resources.Microservices orchestration (as Kubernetes) is an automatic process, which manages each component lifecycle, and notably their allocation on the different resources of the cloud infrastructure. Whereas such automatic cloud technologies ease development and deployment, they nevertheless obscure debugging and performance analysis. In order to gain insight on the composition of services, distributed tracing recently emerged as a way to get the decomposition of the activity of each component within a cloud infrastructure. This paper aims at providing methodologies and tools (leveraging state-of-the-art tracing) for getting a wider view of application behaviours, especially focusing on application performance assessment.In this paper, we focus on using distributed traces and allocation information from microservices to model their dependencies as a hierarchical property graph. By applying graph rewriting operations, we managed to project and filter communications observed between microservices at higher abstraction layers like the machine nodes, the zones or regions. Finally, in this paper we propose an implementation of the model running on a microservices shopping application deployed on a Zonal Kubernetes cluster monitored by OpenTelemetry traces. We propose using the flow hierarchy metric on the graph model to pinpoint cycles that reveal inefficient resource composition inducing possible performance issues and economic waste.",Using Distributed Tracing to Identify Inefficient Resources Composition in Cloud Applications,"Cloud-Applications are the new industry standard way of designing Web-Applications. With Cloud Computing, Applications are usually designed as microservices, and developers can take advantage of thousands of such existing microservices, involving several hundred of cross-component communications on different physical resources.Microservices orchestration (as Kubernetes) is an automatic process, which manages each component lifecycle, and notably their allocation on the different resources of the cloud infrastructure. Whereas such automatic cloud technologies ease development and deployment, they nevertheless obscure debugging and performance analysis. In order to gain insight on the composition of services, distributed tracing recently emerged as a way to get the decomposition of the activity of each component within a cloud infrastructure. This paper aims at providing methodologies and tools (leveraging state-of-the-art tracing) for getting a wider view of application behaviours, especially focusing on application performance assessment.In this paper, we focus on using distributed traces and allocation information from microservices to model their dependencies as a hierarchical property graph. By applying graph rewriting operations, we managed to project and filter communications observed between microservices at higher abstraction layers like the machine nodes, the zones or regions. Finally, in this paper we propose an implementation of the model running on a microservices shopping application deployed on a Zonal Kubernetes cluster monitored by OpenTelemetry traces. We propose using the flow hierarchy metric on the graph model to pinpoint cycles that reveal inefficient resource composition inducing possible performance issues and economic waste.",IEEE conference,no,"['using', 'distributed', 'tracing', 'identify', 'inefficient', 'resource', 'composition', 'new', 'industry', 'standard', 'way', 'designing', 'computing', 'usually', 'designed', 'developer', 'take', 'advantage', 'thousand', 'existing', 'involving', 'several', 'hundred', 'communication', 'different', 'physical', 'orchestration', 'kubernetes', 'automatic', 'process', 'manages', 'lifecycle', 'allocation', 'different', 'resource', 'infrastructure', 'whereas', 'automatic', 'technology', 'ease', 'development', 'deployment', 'nevertheless', 'debugging', 'performance', 'analysis', 'order', 'gain', 'insight', 'composition', 'distributed', 'tracing', 'recently', 'emerged', 'way', 'get', 'decomposition', 'activity', 'within', 'infrastructure', 'paper', 'aim', 'providing', 'methodology', 'tool', 'leveraging', 'tracing', 'getting', 'view', 'behaviour', 'especially', 'focusing', 'performance', 'paper', 'focus', 'using', 'distributed', 'trace', 'allocation', 'information', 'model', 'dependency', 'hierarchical', 'property', 'graph', 'applying', 'graph', 'operation', 'managed', 'project', 'filter', 'communication', 'observed', 'higher', 'abstraction', 'layer', 'like', 'machine', 'node', 'zone', 'finally', 'paper', 'propose', 'implementation', 'model', 'running', 'shopping', 'deployed', 'kubernetes', 'cluster', 'monitored', 'trace', 'propose', 'using', 'flow', 'hierarchy', 'metric', 'graph', 'model', 'pinpoint', 'cycle', 'reveal', 'inefficient', 'resource', 'composition', 'possible', 'performance', 'issue', 'economic', 'waste']"
"Root-Cause Metric Location for Microservice Systems via Log Anomaly Detection Microservice systems are typically fragile and failures are inevitable in them due to their complexity and large scale. However, it is challenging to localize the root-cause metric due to its complicated dependencies and the huge number of various metrics. Existing methods are based on either correlation between metrics or correlation between metrics and failures. All of them ignore the key data source in microservice, i.e., logs. In this paper, we propose a novel root-cause metric localization approach by incorporating log anomaly detection. Our approach is based on a key observation, the value of root-cause metric should be changed along with the change of the log anomaly score of the system caused by the failure. Specifically, our approach includes two components, collecting anomaly scores by log anomaly detection algorithm and identifying root-cause metric by robust correlation analysis with data augmentation. Experiments on an open-source benchmark microservice system have demonstrated our approach can identify root-cause metrics more accurately than existing methods and only require a short localization time. Therefore, our approach can assist engineers to save much effort in diagnosing and mitigating failures as soon as possible.",Root-Cause Metric Location for Microservice Systems via Log Anomaly Detection,"Microservice systems are typically fragile and failures are inevitable in them due to their complexity and large scale. However, it is challenging to localize the root-cause metric due to its complicated dependencies and the huge number of various metrics. Existing methods are based on either correlation between metrics or correlation between metrics and failures. All of them ignore the key data source in microservice, i.e., logs. In this paper, we propose a novel root-cause metric localization approach by incorporating log anomaly detection. Our approach is based on a key observation, the value of root-cause metric should be changed along with the change of the log anomaly score of the system caused by the failure. Specifically, our approach includes two components, collecting anomaly scores by log anomaly detection algorithm and identifying root-cause metric by robust correlation analysis with data augmentation. Experiments on an open-source benchmark microservice system have demonstrated our approach can identify root-cause metrics more accurately than existing methods and only require a short localization time. Therefore, our approach can assist engineers to save much effort in diagnosing and mitigating failures as soon as possible.",IEEE conference,no,"['metric', 'location', 'via', 'log', 'anomaly', 'detection', 'typically', 'failure', 'inevitable', 'due', 'complexity', 'large', 'scale', 'however', 'challenging', 'localize', 'metric', 'due', 'complicated', 'dependency', 'huge', 'number', 'various', 'metric', 'existing', 'method', 'based', 'either', 'correlation', 'metric', 'correlation', 'metric', 'failure', 'key', 'source', 'log', 'paper', 'propose', 'novel', 'metric', 'localization', 'incorporating', 'log', 'anomaly', 'detection', 'based', 'key', 'observation', 'value', 'metric', 'changed', 'along', 'change', 'log', 'anomaly', 'score', 'caused', 'failure', 'specifically', 'includes', 'two', 'collecting', 'anomaly', 'score', 'log', 'anomaly', 'detection', 'algorithm', 'identifying', 'metric', 'robust', 'correlation', 'analysis', 'experiment', 'benchmark', 'demonstrated', 'identify', 'metric', 'accurately', 'existing', 'method', 'require', 'short', 'localization', 'time', 'therefore', 'assist', 'engineer', 'save', 'much', 'effort', 'diagnosing', 'mitigating', 'failure', 'soon', 'possible']"
"Insights into Multi-Layered Fault Propagation and Analysis in a Cloud Stack Emerging application modernisation efforts are pushing new application services to be built and existing monoliths to be refactored as loosely coupled distributed components (e.g, microservices) for independent scaling and management in cloud. With dynamic operating conditions, component failures, complex component interconnections across the cloud stack, etc., it becomes a challenge to develop effective fault management techniques at the granularity of a multi-layered cloud application service. This paper emphasises on considering faults, errors and failure across the components in different layers of a cloud stack for effective fault management.",Insights into Multi-Layered Fault Propagation and Analysis in a Cloud Stack,"Emerging application modernisation efforts are pushing new application services to be built and existing monoliths to be refactored as loosely coupled distributed components (e.g, microservices) for independent scaling and management in cloud. With dynamic operating conditions, component failures, complex component interconnections across the cloud stack, etc., it becomes a challenge to develop effective fault management techniques at the granularity of a multi-layered cloud application service. This paper emphasises on considering faults, errors and failure across the components in different layers of a cloud stack for effective fault management.",IEEE conference,no,"['insight', 'fault', 'propagation', 'analysis', 'stack', 'emerging', 'effort', 'pushing', 'new', 'built', 'existing', 'monolith', 'refactored', 'loosely', 'coupled', 'distributed', 'independent', 'scaling', 'management', 'dynamic', 'operating', 'condition', 'failure', 'complex', 'across', 'stack', 'becomes', 'challenge', 'develop', 'effective', 'fault', 'management', 'technique', 'granularity', 'paper', 'emphasis', 'considering', 'fault', 'error', 'failure', 'across', 'different', 'layer', 'stack', 'effective', 'fault', 'management']"
"Combining Network Data Analytics Function and Machine Learning for Abnormal Traffic Detection in Beyond 5G The Network Data Analytics Function (NWDAF) is a key component of the 5G Core Network (CN) architecture whose role is to generate analytics and insights from the network data to accommodate end users and improve the network performance. NWDAF allows the collection, processing, and analysis of network data to enable a variety of applications, such as User Equipment (UE) mobility analytics and UE abnormal behaviour. Although defined by 3GPP, realizing these applications is still an open problem. To fill this gap: (i) we propose a microservices architecture of NWDAF to plug the 3GPP applications as mi-croservices enabling greater flexibility and scalability of NWDAF; (ii) devise a Machine Learning (ML) algorithm, specifically an LSTM Auto-encoder whose role is to detect abnormal traffic events using real network data extracted from the Milano dataset [1]; (iii) we integrate and test the abnormal traffic detection algorithm in the NWDAF based on OpenAirInterface (OAI) 5G CN and RAN [2]. The experimental results show the ability of NWDAF to collect data from a real 5G CN using 3GPP-compliant interfaces and detect abnormal traffic generated by a real UE using ML.",Combining Network Data Analytics Function and Machine Learning for Abnormal Traffic Detection in Beyond 5G,"The Network Data Analytics Function (NWDAF) is a key component of the 5G Core Network (CN) architecture whose role is to generate analytics and insights from the network data to accommodate end users and improve the network performance. NWDAF allows the collection, processing, and analysis of network data to enable a variety of applications, such as User Equipment (UE) mobility analytics and UE abnormal behaviour. Although defined by 3GPP, realizing these applications is still an open problem. To fill this gap: (i) we propose a microservices architecture of NWDAF to plug the 3GPP applications as mi-croservices enabling greater flexibility and scalability of NWDAF; (ii) devise a Machine Learning (ML) algorithm, specifically an LSTM Auto-encoder whose role is to detect abnormal traffic events using real network data extracted from the Milano dataset [1]; (iii) we integrate and test the abnormal traffic detection algorithm in the NWDAF based on OpenAirInterface (OAI) 5G CN and RAN [2]. The experimental results show the ability of NWDAF to collect data from a real 5G CN using 3GPP-compliant interfaces and detect abnormal traffic generated by a real UE using ML.",IEEE conference,no,"['combining', 'network', 'analytics', 'function', 'machine', 'learning', 'abnormal', 'traffic', 'detection', 'beyond', 'network', 'analytics', 'function', 'nwdaf', 'key', 'core', 'network', 'cn', 'whose', 'role', 'generate', 'analytics', 'insight', 'network', 'accommodate', 'end', 'user', 'improve', 'network', 'performance', 'nwdaf', 'allows', 'collection', 'processing', 'analysis', 'network', 'enable', 'variety', 'user', 'equipment', 'ue', 'mobility', 'analytics', 'ue', 'abnormal', 'behaviour', 'although', 'defined', 'realizing', 'still', 'open', 'problem', 'fill', 'gap', 'propose', 'nwdaf', 'enabling', 'greater', 'flexibility', 'scalability', 'nwdaf', 'ii', 'machine', 'learning', 'ml', 'algorithm', 'specifically', 'whose', 'role', 'detect', 'abnormal', 'traffic', 'event', 'using', 'real', 'network', 'extracted', 'dataset', 'iii', 'integrate', 'test', 'abnormal', 'traffic', 'detection', 'algorithm', 'nwdaf', 'based', 'cn', 'ran', 'experimental', 'result', 'show', 'ability', 'nwdaf', 'collect', 'real', 'cn', 'using', 'interface', 'detect', 'abnormal', 'traffic', 'generated', 'real', 'ue', 'using', 'ml']"
"Practical Root Cause Localization for Microservice Systems via Trace Analysis Microservice architecture is applied by an increasing number of systems because of its benefits on delivery, scalability, and autonomy. It is essential but challenging to localize root-cause microservices promptly when a fault occurs. Traces are helpful for root-cause microservice localization, and thus many recent approaches utilize them. However, these approaches are less practical due to relying on supervision or other unrealistic assumptions. To overcome their limitations, we propose a more practical root-cause microservice localization approach named TraceRCA. The key insight of TraceRCA is that a microservice with more abnormal and less normal traces passing through it is more likely to be the root cause. Based on it, TraceRCA is composed of trace anomaly detection, suspicious microservice set mining and microservice ranking. We conducted experiments on hundreds of injected faults in a widely-used open-source microservice benchmark and a production system. The results show that TraceRCA is effective in various situations. The top-1 accuracy of TraceRCA outperforms the state-of-the-art unsupervised approaches by 44.8%. Besides, TraceRCA is applied in a large commercial bank, and it helps operators localize root causes for real-world faults accurately and efficiently. We also share some lessons learned from our real-world deployment.",Practical Root Cause Localization for Microservice Systems via Trace Analysis,"Microservice architecture is applied by an increasing number of systems because of its benefits on delivery, scalability, and autonomy. It is essential but challenging to localize root-cause microservices promptly when a fault occurs. Traces are helpful for root-cause microservice localization, and thus many recent approaches utilize them. However, these approaches are less practical due to relying on supervision or other unrealistic assumptions. To overcome their limitations, we propose a more practical root-cause microservice localization approach named TraceRCA. The key insight of TraceRCA is that a microservice with more abnormal and less normal traces passing through it is more likely to be the root cause. Based on it, TraceRCA is composed of trace anomaly detection, suspicious microservice set mining and microservice ranking. We conducted experiments on hundreds of injected faults in a widely-used open-source microservice benchmark and a production system. The results show that TraceRCA is effective in various situations. The top-1 accuracy of TraceRCA outperforms the state-of-the-art unsupervised approaches by 44.8%. Besides, TraceRCA is applied in a large commercial bank, and it helps operators localize root causes for real-world faults accurately and efficiently. We also share some lessons learned from our real-world deployment.",IEEE conference,no,"['practical', 'root', 'cause', 'localization', 'via', 'trace', 'analysis', 'applied', 'increasing', 'number', 'benefit', 'delivery', 'scalability', 'autonomy', 'essential', 'challenging', 'localize', 'promptly', 'fault', 'occurs', 'trace', 'helpful', 'localization', 'thus', 'many', 'recent', 'utilize', 'however', 'less', 'practical', 'due', 'relying', 'supervision', 'overcome', 'limitation', 'propose', 'practical', 'localization', 'named', 'tracerca', 'key', 'insight', 'tracerca', 'abnormal', 'less', 'normal', 'trace', 'passing', 'likely', 'root', 'cause', 'based', 'tracerca', 'composed', 'trace', 'anomaly', 'detection', 'suspicious', 'set', 'mining', 'ranking', 'conducted', 'experiment', 'hundred', 'fault', 'benchmark', 'production', 'result', 'show', 'tracerca', 'effective', 'various', 'situation', 'accuracy', 'tracerca', 'outperforms', 'unsupervised', 'besides', 'tracerca', 'applied', 'large', 'commercial', 'bank', 'help', 'operator', 'localize', 'root', 'cause', 'fault', 'accurately', 'efficiently', 'also', 'share', 'lesson', 'learned', 'deployment']"
"A Toolbox for Realtime Timeseries Anomaly Detection Software architecture practice relies more and more on data-driven decision-making. Data-driven decisions are taken either by humans or by software agents via analyzing streams of timeseries data coming from different running systems. Since the quality of sensed data influences the analysis and subsequent decision-making, detecting data anomalies is an important and necessary part of any data analysis and data intelligence pipeline (such as those typically found in smart and self-adaptive systems). Although a number of data science libraries exist for timeseries anomaly detection, it is both time consuming and hard to plug realtime anomaly detection functionality in existing pipelines. The problem lies with the boilerplate code that needs to be provided for common tasks such as data ingestion, data transformation and preprocessing, invoking of model re-training when needed, and persisting of identified anomalies so that they can be acted upon or further analysed. In response, we created a toolbox for realtime anomaly detection that automates the above common tasks and modularizes the anomaly detection process in a number of clearly defined components. This serves as a plug-in solution for architecting and development of smart systems that have to adapt their behavior at runtime. In this paper, we describe the microservice architecture used by our toolbox and explain how to deploy it for obtaining an out-of-the-box solution for realtime anomaly detection out of ready-to-use components. We also provide an initial assessment of its performance.",A Toolbox for Realtime Timeseries Anomaly Detection,"Software architecture practice relies more and more on data-driven decision-making. Data-driven decisions are taken either by humans or by software agents via analyzing streams of timeseries data coming from different running systems. Since the quality of sensed data influences the analysis and subsequent decision-making, detecting data anomalies is an important and necessary part of any data analysis and data intelligence pipeline (such as those typically found in smart and self-adaptive systems). Although a number of data science libraries exist for timeseries anomaly detection, it is both time consuming and hard to plug realtime anomaly detection functionality in existing pipelines. The problem lies with the boilerplate code that needs to be provided for common tasks such as data ingestion, data transformation and preprocessing, invoking of model re-training when needed, and persisting of identified anomalies so that they can be acted upon or further analysed. In response, we created a toolbox for realtime anomaly detection that automates the above common tasks and modularizes the anomaly detection process in a number of clearly defined components. This serves as a plug-in solution for architecting and development of smart systems that have to adapt their behavior at runtime. In this paper, we describe the microservice architecture used by our toolbox and explain how to deploy it for obtaining an out-of-the-box solution for realtime anomaly detection out of ready-to-use components. We also provide an initial assessment of its performance.",IEEE conference,no,"['toolbox', 'realtime', 'timeseries', 'anomaly', 'detection', 'practice', 'relies', 'decision', 'taken', 'either', 'human', 'agent', 'via', 'analyzing', 'stream', 'timeseries', 'different', 'running', 'since', 'quality', 'influence', 'analysis', 'detecting', 'anomaly', 'important', 'necessary', 'part', 'analysis', 'intelligence', 'pipeline', 'typically', 'found', 'smart', 'although', 'number', 'library', 'exist', 'timeseries', 'anomaly', 'detection', 'time', 'consuming', 'hard', 'realtime', 'anomaly', 'detection', 'functionality', 'existing', 'pipeline', 'problem', 'lie', 'code', 'need', 'provided', 'common', 'task', 'transformation', 'invoking', 'model', 'needed', 'identified', 'anomaly', 'upon', 'analysed', 'response', 'created', 'toolbox', 'realtime', 'anomaly', 'detection', 'automates', 'common', 'task', 'anomaly', 'detection', 'process', 'number', 'clearly', 'defined', 'serf', 'solution', 'architecting', 'development', 'smart', 'adapt', 'behavior', 'runtime', 'paper', 'describe', 'used', 'toolbox', 'explain', 'deploy', 'solution', 'realtime', 'anomaly', 'detection', 'also', 'provide', 'initial', 'assessment', 'performance']"
"Localizing Failure Root Causes in a Microservice through Causality Inference An increasing number of Internet applications are applying microservice architecture due to its flexibility and clear logic. The stability of microservice is thus vitally important for these applications' quality of service. Accurate failure root cause localization can help operators quickly recover microservice failures and mitigate loss. Although cross-microservice failure root cause localization has been well studied, how to localize failure root causes in a microservice so as to quickly mitigate this microservice has not yet been studied. In this work, we propose a framework, MicroCause, to accurately localize the root cause monitoring indicators in a microservice. MicroCause combines a simple yet effective path condition time series (PCTS) algorithm which accurately captures the sequential relationship of time series data, and a novel temporal cause oriented random walk (TCORW) method integrating the causal relationship, temporal order, and priority information of monitoring data. We evaluate MicroCause based on 86 real-world failure tickets collected from a top tier global online shopping service. Our experiments show that the top 5 accuracy (AC@5) of MicroCause for intra-microservice failure root cause localization is 98.7%, which is greatly higher (by 33.4 %) than the best baseline method.",Localizing Failure Root Causes in a Microservice through Causality Inference,"An increasing number of Internet applications are applying microservice architecture due to its flexibility and clear logic. The stability of microservice is thus vitally important for these applications' quality of service. Accurate failure root cause localization can help operators quickly recover microservice failures and mitigate loss. Although cross-microservice failure root cause localization has been well studied, how to localize failure root causes in a microservice so as to quickly mitigate this microservice has not yet been studied. In this work, we propose a framework, MicroCause, to accurately localize the root cause monitoring indicators in a microservice. MicroCause combines a simple yet effective path condition time series (PCTS) algorithm which accurately captures the sequential relationship of time series data, and a novel temporal cause oriented random walk (TCORW) method integrating the causal relationship, temporal order, and priority information of monitoring data. We evaluate MicroCause based on 86 real-world failure tickets collected from a top tier global online shopping service. Our experiments show that the top 5 accuracy (AC@5) of MicroCause for intra-microservice failure root cause localization is 98.7%, which is greatly higher (by 33.4 %) than the best baseline method.",IEEE conference,no,"['failure', 'root', 'cause', 'causality', 'inference', 'increasing', 'number', 'internet', 'applying', 'due', 'flexibility', 'clear', 'logic', 'stability', 'thus', 'important', 'quality', 'accurate', 'failure', 'root', 'cause', 'localization', 'help', 'operator', 'quickly', 'recover', 'failure', 'mitigate', 'loss', 'although', 'failure', 'root', 'cause', 'localization', 'well', 'studied', 'localize', 'failure', 'root', 'cause', 'quickly', 'mitigate', 'yet', 'studied', 'work', 'propose', 'framework', 'microcause', 'accurately', 'localize', 'root', 'cause', 'monitoring', 'indicator', 'microcause', 'combine', 'simple', 'yet', 'effective', 'path', 'condition', 'time', 'series', 'algorithm', 'accurately', 'capture', 'sequential', 'relationship', 'time', 'series', 'novel', 'temporal', 'cause', 'oriented', 'random', 'method', 'integrating', 'causal', 'relationship', 'temporal', 'order', 'priority', 'information', 'monitoring', 'evaluate', 'microcause', 'based', 'failure', 'collected', 'top', 'global', 'online', 'shopping', 'experiment', 'show', 'top', 'accuracy', 'microcause', 'failure', 'root', 'cause', 'localization', 'greatly', 'higher', 'best', 'baseline', 'method']"
"Design and Implementation of an Intelligent Classification and Reliability Prediction Tool based on Cloud Platform Reliability prediction is of great importance for electronic products design, and an indispensable technique to ensure that electronic components, equipment and the overall system meet their required reliability targets as well. Present methods, handbooks, standards and software tools of reliability prediction were studied and compared. For industrial and commercial electronic equipment applications, the development and demand of reliability prediction tools were analyzed in this paper. Using new-generation information technologies such as cloud computing and machine learning, a lightweight component reliability prediction tool was designed based on the Spring Cloud microservice architecture. In addition, the naive Bayesian algorithm was adopted to achieve intelligent components classification, which enhances prediction efficiency significantly. An example was offered, which verified the effectiveness of the cloud-based reliability prediction tool. Its characteristics of low-cost, rapidly-deployed and easy-to-use were shown, which differ it from traditional prediction tools, and make it suitable for commercial companies and electronic products.",Design and Implementation of an Intelligent Classification and Reliability Prediction Tool based on Cloud Platform,"Reliability prediction is of great importance for electronic products design, and an indispensable technique to ensure that electronic components, equipment and the overall system meet their required reliability targets as well. Present methods, handbooks, standards and software tools of reliability prediction were studied and compared. For industrial and commercial electronic equipment applications, the development and demand of reliability prediction tools were analyzed in this paper. Using new-generation information technologies such as cloud computing and machine learning, a lightweight component reliability prediction tool was designed based on the Spring Cloud microservice architecture. In addition, the naive Bayesian algorithm was adopted to achieve intelligent components classification, which enhances prediction efficiency significantly. An example was offered, which verified the effectiveness of the cloud-based reliability prediction tool. Its characteristics of low-cost, rapidly-deployed and easy-to-use were shown, which differ it from traditional prediction tools, and make it suitable for commercial companies and electronic products.",IEEE conference,no,"['design', 'implementation', 'intelligent', 'classification', 'reliability', 'prediction', 'tool', 'based', 'platform', 'reliability', 'prediction', 'great', 'importance', 'electronic', 'product', 'design', 'technique', 'ensure', 'electronic', 'equipment', 'overall', 'meet', 'required', 'reliability', 'target', 'well', 'present', 'method', 'standard', 'tool', 'reliability', 'prediction', 'studied', 'compared', 'industrial', 'commercial', 'electronic', 'equipment', 'development', 'demand', 'reliability', 'prediction', 'tool', 'analyzed', 'paper', 'using', 'information', 'technology', 'computing', 'machine', 'learning', 'lightweight', 'reliability', 'prediction', 'tool', 'designed', 'based', 'spring', 'addition', 'naive', 'algorithm', 'adopted', 'achieve', 'intelligent', 'classification', 'enhances', 'prediction', 'efficiency', 'significantly', 'example', 'offered', 'verified', 'effectiveness', 'reliability', 'prediction', 'tool', 'characteristic', 'shown', 'traditional', 'prediction', 'tool', 'make', 'suitable', 'commercial', 'company', 'electronic', 'product']"
"Tracemesh: Scalable and Streaming Sampling for Distributed Traces Distributed tracing serves as a fundamental element in the monitoring of cloud-based and datacenter systems. It provides visibility into the full life cycle of a request or operation across multiple services, which is essential for understanding system dependencies and performance bottlenecks. To mitigate computational and storage overheads, most tracing frameworks adopt a uniform sampling strategy, which inevitably captures overlapping and redundant information. More advanced methods employ learning-based approaches to bias the sampling toward more informative traces. However, existing methods fall short of considering the high-dimensional and dynamic nature of trace data, which is essential for the production deployment of trace sampling. To address these practical challenges, in this paper we present Trace Mesh,a scalable and streaming sampler for distributed traces. Tracemesh employs Locality-Sensitivity Hashing (LSH) to improve sampling efficiency by projecting traces into a low-dimensional space while preserving their similarity. In this process, Tracemesh accommodates previously unseen trace features in a unified and streamlined way. Subsequently, Tracemesh samples traces through evolving clustering, which dynamically adjusts the sampling decision to avoid over-sampling of recurring traces. The proposed method is evaluated with trace data collected from both open-source microservice benchmarks and production service systems. Ex-perimental results demonstrate that Tracemesh outperforms state-of-the-art methods by a significant margin in both sampling accuracy and efficiency.",Tracemesh: Scalable and Streaming Sampling for Distributed Traces,"Distributed tracing serves as a fundamental element in the monitoring of cloud-based and datacenter systems. It provides visibility into the full life cycle of a request or operation across multiple services, which is essential for understanding system dependencies and performance bottlenecks. To mitigate computational and storage overheads, most tracing frameworks adopt a uniform sampling strategy, which inevitably captures overlapping and redundant information. More advanced methods employ learning-based approaches to bias the sampling toward more informative traces. However, existing methods fall short of considering the high-dimensional and dynamic nature of trace data, which is essential for the production deployment of trace sampling. To address these practical challenges, in this paper we present Trace Mesh,a scalable and streaming sampler for distributed traces. Tracemesh employs Locality-Sensitivity Hashing (LSH) to improve sampling efficiency by projecting traces into a low-dimensional space while preserving their similarity. In this process, Tracemesh accommodates previously unseen trace features in a unified and streamlined way. Subsequently, Tracemesh samples traces through evolving clustering, which dynamically adjusts the sampling decision to avoid over-sampling of recurring traces. The proposed method is evaluated with trace data collected from both open-source microservice benchmarks and production service systems. Ex-perimental results demonstrate that Tracemesh outperforms state-of-the-art methods by a significant margin in both sampling accuracy and efficiency.",IEEE conference,no,"['tracemesh', 'scalable', 'streaming', 'sampling', 'distributed', 'trace', 'distributed', 'tracing', 'serf', 'fundamental', 'element', 'monitoring', 'datacenter', 'provides', 'visibility', 'full', 'life', 'cycle', 'request', 'operation', 'across', 'multiple', 'essential', 'understanding', 'dependency', 'performance', 'bottleneck', 'mitigate', 'computational', 'storage', 'overhead', 'tracing', 'framework', 'adopt', 'uniform', 'sampling', 'strategy', 'capture', 'redundant', 'information', 'advanced', 'method', 'employ', 'sampling', 'toward', 'trace', 'however', 'existing', 'method', 'short', 'considering', 'dynamic', 'nature', 'trace', 'essential', 'production', 'deployment', 'trace', 'sampling', 'address', 'practical', 'challenge', 'paper', 'present', 'trace', 'mesh', 'scalable', 'streaming', 'distributed', 'trace', 'tracemesh', 'employ', 'improve', 'sampling', 'efficiency', 'trace', 'space', 'preserving', 'similarity', 'process', 'tracemesh', 'previously', 'trace', 'feature', 'unified', 'streamlined', 'way', 'subsequently', 'tracemesh', 'sample', 'trace', 'evolving', 'clustering', 'dynamically', 'sampling', 'decision', 'avoid', 'recurring', 'trace', 'proposed', 'method', 'evaluated', 'trace', 'collected', 'benchmark', 'production', 'result', 'demonstrate', 'tracemesh', 'outperforms', 'method', 'significant', 'margin', 'sampling', 'accuracy', 'efficiency']"
"CANON: Complex Analytics of Network of Networks for Modeling Adversarial Activities Networks are natural representations in modeling adversarial activities, such as smuggling, human trafficking, and illegal arms dealing. However, such activities are often covert and embedded across multiple domains and sources. They are generally not detectable and recognizable from the perspective of an isolated network, and only become apparent when multiple networks are analyzed in a unified m anner. T o t his e nd, we propose Complex Analytics of Network of Networks (CANON), a mathematical and computational framework for modeling adversarial activities from large-scale, multi-sourced data inputs. Central to our framework is a network-of-networks model, where nodes and edges can be defined across different domains and at multiple resolutions. Based on this model, we address the key challenges in modeling adversarial activities via four technical components, including optimization-based network alignment, network embedding and conditioning, approximate subgraph matching, and investigative subgraph discovery.In this paper, we describe the design and implementation of the individual components as well as integrating these components into a unified system using a modular microservice architecture. Extensive experiments have been conducted in both synthetics and real-world datasets to demonstrate the effectiveness of our proposed system under the DARPA Modeling Adversarial Activity (MAA) program.",CANON: Complex Analytics of Network of Networks for Modeling Adversarial Activities,"Networks are natural representations in modeling adversarial activities, such as smuggling, human trafficking, and illegal arms dealing. However, such activities are often covert and embedded across multiple domains and sources. They are generally not detectable and recognizable from the perspective of an isolated network, and only become apparent when multiple networks are analyzed in a unified m anner. T o t his e nd, we propose Complex Analytics of Network of Networks (CANON), a mathematical and computational framework for modeling adversarial activities from large-scale, multi-sourced data inputs. Central to our framework is a network-of-networks model, where nodes and edges can be defined across different domains and at multiple resolutions. Based on this model, we address the key challenges in modeling adversarial activities via four technical components, including optimization-based network alignment, network embedding and conditioning, approximate subgraph matching, and investigative subgraph discovery.In this paper, we describe the design and implementation of the individual components as well as integrating these components into a unified system using a modular microservice architecture. Extensive experiments have been conducted in both synthetics and real-world datasets to demonstrate the effectiveness of our proposed system under the DARPA Modeling Adversarial Activity (MAA) program.",IEEE conference,no,"['complex', 'analytics', 'network', 'network', 'modeling', 'adversarial', 'activity', 'network', 'natural', 'representation', 'modeling', 'adversarial', 'activity', 'human', 'arm', 'dealing', 'however', 'activity', 'often', 'embedded', 'across', 'multiple', 'domain', 'source', 'generally', 'perspective', 'isolated', 'network', 'become', 'apparent', 'multiple', 'network', 'analyzed', 'unified', 'propose', 'complex', 'analytics', 'network', 'network', 'mathematical', 'computational', 'framework', 'modeling', 'adversarial', 'activity', 'input', 'central', 'framework', 'model', 'node', 'edge', 'defined', 'across', 'different', 'domain', 'multiple', 'based', 'model', 'address', 'key', 'challenge', 'modeling', 'adversarial', 'activity', 'via', 'four', 'technical', 'including', 'network', 'alignment', 'network', 'subgraph', 'matching', 'subgraph', 'paper', 'describe', 'design', 'implementation', 'individual', 'well', 'integrating', 'unified', 'using', 'modular', 'extensive', 'experiment', 'conducted', 'synthetic', 'datasets', 'demonstrate', 'effectiveness', 'proposed', 'modeling', 'adversarial', 'activity', 'program']"
"Mercury: Anomaly Detection using Machine Learning Koçfinans is a financing company that provides their customers loans for both new and used vehicles via a web application called ROTA. Under Digital Transformation, Koçfinans rebuilt the ROTA application using microservice architecture and DevOps practices. As a result, the need for automated monitoring of the application and server ecosystem has emerged. In this study, we would like to present you the application that has been developed for automated monitoring of ROTA application, namely Mercury, and how the machine learning models have been used in our use case.",Mercury: Anomaly Detection using Machine Learning,"Koçfinans is a financing company that provides their customers loans for both new and used vehicles via a web application called ROTA. Under Digital Transformation, Koçfinans rebuilt the ROTA application using microservice architecture and DevOps practices. As a result, the need for automated monitoring of the application and server ecosystem has emerged. In this study, we would like to present you the application that has been developed for automated monitoring of ROTA application, namely Mercury, and how the machine learning models have been used in our use case.",IEEE conference,no,"['anomaly', 'detection', 'using', 'machine', 'learning', 'company', 'provides', 'customer', 'new', 'used', 'vehicle', 'via', 'web', 'called', 'rota', 'digital', 'transformation', 'rota', 'using', 'devops', 'practice', 'result', 'need', 'automated', 'monitoring', 'server', 'ecosystem', 'emerged', 'study', 'would', 'like', 'present', 'developed', 'automated', 'monitoring', 'rota', 'namely', 'machine', 'learning', 'model', 'used', 'use', 'case']"
"Learning to Simplify Distributed Systems Management Managing large-scale distributed systems is a difficult task. System administrators are responsible for the upkeep and maintenance of numerous components with complex dependencies. With the shift to microservices-based architectures, these systems can consist of 100s to 1000s of interconnected nodes. To combat this difficulty, administrators rely on analyzing logs and metrics collected from the different services. However, the number of available metrics for large systems presents complexity and scaling issues. To combat these issues, we present Minerva, an unsupervised Machine Learning (ML) framework for performing network diagnosis analysis. Minerva is composed of a multi-stage pipeline, where each component can act individually or cohesively to perform various management tasks. Our system offers a unified and extensible framework for managing the complexity of large networks, and presents administrators with a swiss-army knife for diagnosing the overall health of their systems. To demonstrate the feasibility of Minerva, we evaluate its performance on a production-scale system. We present use cases for the various management tools made available by Minerva, and show how these tools can be used to make strong inferences about the system using unsupervised techniques.",Learning to Simplify Distributed Systems Management,"Managing large-scale distributed systems is a difficult task. System administrators are responsible for the upkeep and maintenance of numerous components with complex dependencies. With the shift to microservices-based architectures, these systems can consist of 100s to 1000s of interconnected nodes. To combat this difficulty, administrators rely on analyzing logs and metrics collected from the different services. However, the number of available metrics for large systems presents complexity and scaling issues. To combat these issues, we present Minerva, an unsupervised Machine Learning (ML) framework for performing network diagnosis analysis. Minerva is composed of a multi-stage pipeline, where each component can act individually or cohesively to perform various management tasks. Our system offers a unified and extensible framework for managing the complexity of large networks, and presents administrators with a swiss-army knife for diagnosing the overall health of their systems. To demonstrate the feasibility of Minerva, we evaluate its performance on a production-scale system. We present use cases for the various management tools made available by Minerva, and show how these tools can be used to make strong inferences about the system using unsupervised techniques.",IEEE conference,no,"['learning', 'simplify', 'distributed', 'management', 'managing', 'distributed', 'difficult', 'task', 'administrator', 'responsible', 'maintenance', 'numerous', 'complex', 'dependency', 'shift', 'consist', 'interconnected', 'node', 'combat', 'difficulty', 'administrator', 'rely', 'analyzing', 'log', 'metric', 'collected', 'different', 'however', 'number', 'available', 'metric', 'large', 'present', 'complexity', 'scaling', 'issue', 'combat', 'issue', 'present', 'minerva', 'unsupervised', 'machine', 'learning', 'ml', 'framework', 'performing', 'network', 'diagnosis', 'analysis', 'minerva', 'composed', 'pipeline', 'act', 'individually', 'perform', 'various', 'management', 'task', 'offer', 'unified', 'extensible', 'framework', 'managing', 'complexity', 'large', 'network', 'present', 'administrator', 'diagnosing', 'overall', 'health', 'demonstrate', 'feasibility', 'minerva', 'evaluate', 'performance', 'present', 'use', 'case', 'various', 'management', 'tool', 'made', 'available', 'minerva', 'show', 'tool', 'used', 'make', 'strong', 'inference', 'using', 'unsupervised', 'technique']"
"μVerum: Intrusion Recovery for Microservice Applications Microservice architectures allow complex applications to be developed as a collection of loosely coupled components. The heterogeneous architecture of these applications makes the process of recovering from intrusions especially complex, error-prone, and time-consuming. Although there are several recovery mechanisms for monolithic applications, applying such mechanisms in microservices would not work due to the distribution of the components, the different technologies used by each service, and their scale. Moreover, it can be difficult to trace the services affected by an intrusion and which actions to revert. We propose  $\mu $ Verum, a framework for recovering microservices from intrusions that corrupt the application state. Our approach allows recovery of large-scale microservice applications by logging user requests and the operations that are propagated through several microservices. When a system administrator detects a faulty request,  $\mu $ Verum can execute compensating operations in each of the affected microservices. We implemented, evaluated, and made the code of  $\mu $ Verum available. Our experiments show that  $\mu $ Verum is able to revert the effects in an intrusion in one second while the application is running.",μVerum: Intrusion Recovery for Microservice Applications,"Microservice architectures allow complex applications to be developed as a collection of loosely coupled components. The heterogeneous architecture of these applications makes the process of recovering from intrusions especially complex, error-prone, and time-consuming. Although there are several recovery mechanisms for monolithic applications, applying such mechanisms in microservices would not work due to the distribution of the components, the different technologies used by each service, and their scale. Moreover, it can be difficult to trace the services affected by an intrusion and which actions to revert. We propose  $\mu $ Verum, a framework for recovering microservices from intrusions that corrupt the application state. Our approach allows recovery of large-scale microservice applications by logging user requests and the operations that are propagated through several microservices. When a system administrator detects a faulty request,  $\mu $ Verum can execute compensating operations in each of the affected microservices. We implemented, evaluated, and made the code of  $\mu $ Verum available. Our experiments show that  $\mu $ Verum is able to revert the effects in an intrusion in one second while the application is running.",IEEE journal,no,"['intrusion', 'recovery', 'allow', 'complex', 'developed', 'collection', 'loosely', 'coupled', 'heterogeneous', 'make', 'process', 'intrusion', 'especially', 'complex', 'although', 'several', 'recovery', 'mechanism', 'monolithic', 'applying', 'mechanism', 'would', 'work', 'due', 'distribution', 'different', 'technology', 'used', 'scale', 'moreover', 'difficult', 'trace', 'affected', 'intrusion', 'action', 'propose', 'verum', 'framework', 'intrusion', 'state', 'allows', 'recovery', 'logging', 'user', 'request', 'operation', 'several', 'administrator', 'faulty', 'request', 'verum', 'execute', 'operation', 'affected', 'implemented', 'evaluated', 'made', 'code', 'verum', 'available', 'experiment', 'show', 'verum', 'able', 'effect', 'intrusion', 'one', 'second', 'running']"
"MicroNet: Operation Aware Root Cause Identification of Microservice System Anomalies Microservice architecture has been widely adopted in large-scale applications. However, it also brings new challenges to ensuring reliable performance and maintenance due to the huge volume of data and complex dependencies of microservices. Existing approaches still suffer from the over-aggregation of data, interference from anomaly propagation, and ignoration of component differences. To solve these issues, this paper builds a root cause diagnosis framework at the operation granularity, named as MicroNet. Since operations are subfunctions of microservices, recorded as invocation purposes, we propose the operation-centric perspective, to realize fine-grained data aggregation and operation-level anomaly backtracking. We decompose the diagnosis task into four phases: dependency graph construction, anomaly detection, anomaly evaluation, and culprit location. To construct the invocation dependency accurately, we propose the concept of meta call, defined as the triple (caller, operation, callee), the smallest unit that can be aggregated. Based on the dependency graph, we quantify the operation’s abnormality by analyzing the operation execution process, to backtrack the propagated anomalies. Then, we customize a personalized PageRank algorithm to identify the root cause in which invocation latency and different invocation relationships are considered simultaneously. Our experimental evaluation on an open dataset shows that MicroNet can effectively locate root causes with 90% mean average precision, outperforming state-of-the-art methods.",MicroNet: Operation Aware Root Cause Identification of Microservice System Anomalies,"Microservice architecture has been widely adopted in large-scale applications. However, it also brings new challenges to ensuring reliable performance and maintenance due to the huge volume of data and complex dependencies of microservices. Existing approaches still suffer from the over-aggregation of data, interference from anomaly propagation, and ignoration of component differences. To solve these issues, this paper builds a root cause diagnosis framework at the operation granularity, named as MicroNet. Since operations are subfunctions of microservices, recorded as invocation purposes, we propose the operation-centric perspective, to realize fine-grained data aggregation and operation-level anomaly backtracking. We decompose the diagnosis task into four phases: dependency graph construction, anomaly detection, anomaly evaluation, and culprit location. To construct the invocation dependency accurately, we propose the concept of meta call, defined as the triple (caller, operation, callee), the smallest unit that can be aggregated. Based on the dependency graph, we quantify the operation’s abnormality by analyzing the operation execution process, to backtrack the propagated anomalies. Then, we customize a personalized PageRank algorithm to identify the root cause in which invocation latency and different invocation relationships are considered simultaneously. Our experimental evaluation on an open dataset shows that MicroNet can effectively locate root causes with 90% mean average precision, outperforming state-of-the-art methods.",IEEE journal,no,"['micronet', 'operation', 'aware', 'root', 'cause', 'identification', 'anomaly', 'widely', 'adopted', 'however', 'also', 'brings', 'new', 'challenge', 'ensuring', 'reliable', 'performance', 'maintenance', 'due', 'huge', 'volume', 'complex', 'dependency', 'existing', 'still', 'suffer', 'interference', 'anomaly', 'propagation', 'difference', 'solve', 'issue', 'paper', 'build', 'root', 'cause', 'diagnosis', 'framework', 'operation', 'granularity', 'named', 'micronet', 'since', 'operation', 'invocation', 'purpose', 'propose', 'perspective', 'realize', 'aggregation', 'anomaly', 'decompose', 'diagnosis', 'task', 'four', 'phase', 'dependency', 'graph', 'construction', 'anomaly', 'detection', 'anomaly', 'evaluation', 'location', 'construct', 'invocation', 'dependency', 'accurately', 'propose', 'concept', 'call', 'defined', 'operation', 'unit', 'aggregated', 'based', 'dependency', 'graph', 'quantify', 'operation', 'abnormality', 'analyzing', 'operation', 'execution', 'process', 'anomaly', 'customize', 'personalized', 'algorithm', 'identify', 'root', 'cause', 'invocation', 'latency', 'different', 'invocation', 'relationship', 'considered', 'simultaneously', 'experimental', 'evaluation', 'open', 'dataset', 'show', 'micronet', 'effectively', 'locate', 'root', 'cause', 'mean', 'average', 'precision', 'outperforming', 'method']"
"An Anomaly Detection Algorithm for Microservice Architecture Based on Robust Principal Component Analysis Microservice architecture (MSA) is a new software architecture, which divides a large single application and service into dozens of supporting microservices. With the increasingly popularity of MSA, the security issues of MSA get a lot of attention. In this paper, we propose an algorithm for mining causality and the root cause. Our algorithm consists of two parts: invocation chain anomaly analysis based on robust principal component analysis (RPCA) and a single indicator anomaly detection algorithm. The single indicator anomaly detection algorithm is composed of Isolation Forest (IF) algorithm, One-Class Support Vector Machine (SVM) algorithm, Local Outlier Factor (LOF) algorithm, and  $3\sigma $  principle. For general and network time-consuming anomaly in the process of the MSA, we formulate different anomaly time-consuming detection strategies. We select a batch of sample data and three batches of test data of the 2020 International AIOps Challenge to debug our algorithm. According to the scoring criteria of the competition organizers, our algorithm has an average score of 0.8304 (The full score is 1) in the four batches of data. Our proposed algorithm has higher accuracy than some traditional machine learning algorithms in anomaly detection.",An Anomaly Detection Algorithm for Microservice Architecture Based on Robust Principal Component Analysis,"Microservice architecture (MSA) is a new software architecture, which divides a large single application and service into dozens of supporting microservices. With the increasingly popularity of MSA, the security issues of MSA get a lot of attention. In this paper, we propose an algorithm for mining causality and the root cause. Our algorithm consists of two parts: invocation chain anomaly analysis based on robust principal component analysis (RPCA) and a single indicator anomaly detection algorithm. The single indicator anomaly detection algorithm is composed of Isolation Forest (IF) algorithm, One-Class Support Vector Machine (SVM) algorithm, Local Outlier Factor (LOF) algorithm, and  $3\sigma $  principle. For general and network time-consuming anomaly in the process of the MSA, we formulate different anomaly time-consuming detection strategies. We select a batch of sample data and three batches of test data of the 2020 International AIOps Challenge to debug our algorithm. According to the scoring criteria of the competition organizers, our algorithm has an average score of 0.8304 (The full score is 1) in the four batches of data. Our proposed algorithm has higher accuracy than some traditional machine learning algorithms in anomaly detection.",IEEE journal,no,"['anomaly', 'detection', 'algorithm', 'based', 'robust', 'principal', 'analysis', 'msa', 'new', 'divide', 'large', 'single', 'supporting', 'increasingly', 'popularity', 'msa', 'security', 'issue', 'msa', 'get', 'lot', 'attention', 'paper', 'propose', 'algorithm', 'mining', 'causality', 'root', 'cause', 'algorithm', 'consists', 'two', 'part', 'invocation', 'chain', 'anomaly', 'analysis', 'based', 'robust', 'principal', 'analysis', 'single', 'indicator', 'anomaly', 'detection', 'algorithm', 'single', 'indicator', 'anomaly', 'detection', 'algorithm', 'composed', 'isolation', 'forest', 'algorithm', 'support', 'vector', 'machine', 'algorithm', 'local', 'outlier', 'factor', 'algorithm', 'principle', 'general', 'network', 'anomaly', 'process', 'msa', 'formulate', 'different', 'anomaly', 'detection', 'strategy', 'select', 'batch', 'sample', 'three', 'batch', 'test', 'international', 'aiops', 'challenge', 'debug', 'algorithm', 'according', 'scoring', 'criterion', 'algorithm', 'average', 'score', 'full', 'score', 'four', 'batch', 'proposed', 'algorithm', 'higher', 'accuracy', 'traditional', 'machine', 'learning', 'algorithm', 'anomaly', 'detection']"
"Heterogeneous Data-Driven Failure Diagnosis for Microservice-Based Industrial Clouds Toward Consumer Digital Ecosystems Consumer digital ecosystems include a large volume of different types of applications, and those applications are usually deployed in industrial cloud computing systems. Currently, microservices are one of the most prevailing architectures for industrial clouds. Similar to other architectures, microservices may also produce failures, so failure diagnosis for microservices becomes an inevitable problem in industrial clouds. A majority of existing methods focus on statistical analysis for monitoring data or system topological structure. However, because these methods usually only harness service-level or machine-level metrics, they cannot complete fine-grained failure diagnosis, increasing the running risk of microservice-based industrial clouds. To tackle this issue, in this paper, we design a novel graph structure to represent failure dependencies, especially the heterogeneity, and name it as the heterogeneous failure dependence graph (HFDG). We propose a framework to inform engineers which type of and where failures occur in industrial clouds. The HFDG can be used to mine the propagation of failures between different types of components. We also propose a novel neural network model based on attention mechanism and heterogeneous graph neural network, to fully leverage the metric data and HFDG. We performed experiments on three large-scale public datasets from real-world microservices-based systems. The experimental results demonstrate the superior performance of our model compared to well-known baselines.",Heterogeneous Data-Driven Failure Diagnosis for Microservice-Based Industrial Clouds Toward Consumer Digital Ecosystems,"Consumer digital ecosystems include a large volume of different types of applications, and those applications are usually deployed in industrial cloud computing systems. Currently, microservices are one of the most prevailing architectures for industrial clouds. Similar to other architectures, microservices may also produce failures, so failure diagnosis for microservices becomes an inevitable problem in industrial clouds. A majority of existing methods focus on statistical analysis for monitoring data or system topological structure. However, because these methods usually only harness service-level or machine-level metrics, they cannot complete fine-grained failure diagnosis, increasing the running risk of microservice-based industrial clouds. To tackle this issue, in this paper, we design a novel graph structure to represent failure dependencies, especially the heterogeneity, and name it as the heterogeneous failure dependence graph (HFDG). We propose a framework to inform engineers which type of and where failures occur in industrial clouds. The HFDG can be used to mine the propagation of failures between different types of components. We also propose a novel neural network model based on attention mechanism and heterogeneous graph neural network, to fully leverage the metric data and HFDG. We performed experiments on three large-scale public datasets from real-world microservices-based systems. The experimental results demonstrate the superior performance of our model compared to well-known baselines.",IEEE journal,no,"['heterogeneous', 'failure', 'diagnosis', 'industrial', 'toward', 'consumer', 'digital', 'ecosystem', 'consumer', 'digital', 'ecosystem', 'include', 'large', 'volume', 'different', 'type', 'usually', 'deployed', 'industrial', 'computing', 'currently', 'one', 'prevailing', 'industrial', 'similar', 'may', 'also', 'produce', 'failure', 'failure', 'diagnosis', 'becomes', 'inevitable', 'problem', 'industrial', 'existing', 'method', 'focus', 'statistical', 'analysis', 'monitoring', 'structure', 'however', 'method', 'usually', 'harness', 'metric', 'complete', 'failure', 'diagnosis', 'increasing', 'running', 'risk', 'industrial', 'tackle', 'issue', 'paper', 'design', 'novel', 'graph', 'structure', 'represent', 'failure', 'dependency', 'especially', 'heterogeneity', 'heterogeneous', 'failure', 'dependence', 'graph', 'hfdg', 'propose', 'framework', 'inform', 'engineer', 'type', 'failure', 'occur', 'industrial', 'hfdg', 'used', 'propagation', 'failure', 'different', 'type', 'also', 'propose', 'novel', 'neural', 'network', 'model', 'based', 'attention', 'mechanism', 'heterogeneous', 'graph', 'neural', 'network', 'fully', 'leverage', 'metric', 'hfdg', 'performed', 'experiment', 'three', 'public', 'datasets', 'experimental', 'result', 'demonstrate', 'superior', 'performance', 'model', 'compared', 'baseline']"
"Workflow-Aware Automatic Fault Diagnosis for Microservice-Based Applications With Statistics Microservice architectures bring many benefits, e.g., faster delivery, improved scalability, and greater autonomy, so they are widely adopted to develop and operate Internet-based applications. How to effectively diagnose the faults of applications with lots of dynamic microservices has become a key to guarantee applications’ performance and reliability. As a microservice performs various behaviors in different workflows of processing requests, existing approaches often cannot accurately locate the root cause of an application with interactive microservices in a dynamic deployment environment. We propose a workflow-aware automatic fault diagnosis approach for microservice-based applications with statistics. We characterize traces across microservices with calling trees, and then learn trace patterns as baselines. For the faults affecting the workflows of processing requests, we estimate the workflows’ anomaly degrees, and then locate the microservices causing anomalies by comparing the difference between current traces and learned baselines with tree edit distance. For performance anomalies causing significantly increased response time, we employ principal component analysis to extract suspicious microservices with large fluctuation in response time. Finally, we evaluate our approach on three typical microservice-based applications with a series of experiments. The results show that our approach can accurately locate the microservices causing anomalies.",Workflow-Aware Automatic Fault Diagnosis for Microservice-Based Applications With Statistics,"Microservice architectures bring many benefits, e.g., faster delivery, improved scalability, and greater autonomy, so they are widely adopted to develop and operate Internet-based applications. How to effectively diagnose the faults of applications with lots of dynamic microservices has become a key to guarantee applications’ performance and reliability. As a microservice performs various behaviors in different workflows of processing requests, existing approaches often cannot accurately locate the root cause of an application with interactive microservices in a dynamic deployment environment. We propose a workflow-aware automatic fault diagnosis approach for microservice-based applications with statistics. We characterize traces across microservices with calling trees, and then learn trace patterns as baselines. For the faults affecting the workflows of processing requests, we estimate the workflows’ anomaly degrees, and then locate the microservices causing anomalies by comparing the difference between current traces and learned baselines with tree edit distance. For performance anomalies causing significantly increased response time, we employ principal component analysis to extract suspicious microservices with large fluctuation in response time. Finally, we evaluate our approach on three typical microservice-based applications with a series of experiments. The results show that our approach can accurately locate the microservices causing anomalies.",IEEE journal,no,"['automatic', 'fault', 'diagnosis', 'statistic', 'bring', 'many', 'benefit', 'faster', 'delivery', 'improved', 'scalability', 'greater', 'autonomy', 'widely', 'adopted', 'develop', 'operate', 'effectively', 'diagnose', 'fault', 'lot', 'dynamic', 'become', 'key', 'guarantee', 'performance', 'reliability', 'performs', 'various', 'behavior', 'different', 'workflow', 'processing', 'request', 'existing', 'often', 'accurately', 'locate', 'root', 'cause', 'interactive', 'dynamic', 'deployment', 'environment', 'propose', 'automatic', 'fault', 'diagnosis', 'statistic', 'characterize', 'trace', 'across', 'calling', 'tree', 'learn', 'trace', 'pattern', 'baseline', 'fault', 'affecting', 'workflow', 'processing', 'request', 'estimate', 'workflow', 'anomaly', 'degree', 'locate', 'causing', 'anomaly', 'comparing', 'difference', 'current', 'trace', 'learned', 'baseline', 'tree', 'distance', 'performance', 'anomaly', 'causing', 'significantly', 'increased', 'response', 'time', 'employ', 'principal', 'analysis', 'extract', 'suspicious', 'large', 'response', 'time', 'finally', 'evaluate', 'three', 'typical', 'series', 'experiment', 'result', 'show', 'accurately', 'locate', 'causing', 'anomaly']"
"Root Cause Analysis for Cloud-Native Applications Root cause analysis (RCA) is a critical component in maintaining the reliability and performance of modern cloud applications. However, due to the inherent complexity of cloud environments, traditional RCA techniques become insufficient in supporting system administrators in daily incident response routines. This article presents an RCA solution specifically designed for cloud applications, capable of pinpointing failure root causes and recreating complete fault trajectories from the root cause to the effect. The novelty of our approach lies in approximating causal symptom dependencies by synergizing several symptom correlation methods that assess symptoms in terms of structural, semantic, and temporal aspects. The solution integrates statistical methods with system structure and behavior mining, offering a more comprehensive analysis than existing techniques. Based on these concepts, in this work, we provide definitions and construction algorithms for RCA model structures used in the inference, propose a symptom correlation framework encompassing essential elements of symptom data analysis, and provide a detailed description of the elaborated root cause identification process. Functional evaluation on a live microservice-based system demonstrates the effectiveness of our approach in identifying root causes of complex failures across multiple cloud layers.",Root Cause Analysis for Cloud-Native Applications,"Root cause analysis (RCA) is a critical component in maintaining the reliability and performance of modern cloud applications. However, due to the inherent complexity of cloud environments, traditional RCA techniques become insufficient in supporting system administrators in daily incident response routines. This article presents an RCA solution specifically designed for cloud applications, capable of pinpointing failure root causes and recreating complete fault trajectories from the root cause to the effect. The novelty of our approach lies in approximating causal symptom dependencies by synergizing several symptom correlation methods that assess symptoms in terms of structural, semantic, and temporal aspects. The solution integrates statistical methods with system structure and behavior mining, offering a more comprehensive analysis than existing techniques. Based on these concepts, in this work, we provide definitions and construction algorithms for RCA model structures used in the inference, propose a symptom correlation framework encompassing essential elements of symptom data analysis, and provide a detailed description of the elaborated root cause identification process. Functional evaluation on a live microservice-based system demonstrates the effectiveness of our approach in identifying root causes of complex failures across multiple cloud layers.",IEEE journal,no,"['root', 'cause', 'analysis', 'root', 'cause', 'analysis', 'rca', 'critical', 'maintaining', 'reliability', 'performance', 'modern', 'however', 'due', 'inherent', 'complexity', 'environment', 'traditional', 'rca', 'technique', 'become', 'supporting', 'administrator', 'daily', 'incident', 'response', 'article', 'present', 'rca', 'solution', 'specifically', 'designed', 'capable', 'pinpointing', 'failure', 'root', 'cause', 'complete', 'fault', 'trajectory', 'root', 'cause', 'effect', 'lie', 'causal', 'symptom', 'dependency', 'several', 'symptom', 'correlation', 'method', 'assess', 'symptom', 'term', 'structural', 'semantic', 'temporal', 'aspect', 'solution', 'integrates', 'statistical', 'method', 'structure', 'behavior', 'mining', 'offering', 'comprehensive', 'analysis', 'existing', 'technique', 'based', 'concept', 'work', 'provide', 'definition', 'construction', 'algorithm', 'rca', 'model', 'structure', 'used', 'inference', 'propose', 'symptom', 'correlation', 'framework', 'encompassing', 'essential', 'element', 'symptom', 'analysis', 'provide', 'detailed', 'description', 'elaborated', 'root', 'cause', 'identification', 'process', 'functional', 'evaluation', 'live', 'demonstrates', 'effectiveness', 'identifying', 'root', 'cause', 'complex', 'failure', 'across', 'multiple', 'layer']"
